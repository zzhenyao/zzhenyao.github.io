<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AQA,AR论文汇总</title>
    <url>/2025/02/20/14-13-19/</url>
    <content><![CDATA[<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="2025"><a href="#2025" class="headerlink" title="2025"></a>2025</h2><hr>
<ul>
<li>[CSVT]Adaptive Spatiotemporal Graph Transformer Network for Action Quality Assessment |<a href="/2025/02/24/08-04-24/" title="Adaptive Spatiotemporal Graph Transformer Network for Action Quality Assessment">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Video</span>|</li>
</ul>
<hr>
<h2 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h2><hr>
<ul>
<li><p>[ICANN]A Graph Convolutional Siamese Network for the Assessment and Recognition of Physical Rehabilitation Exercises |<a href="/2025/02/20/10-18-48/" title="A Graph Convolutional Siamese Network for the Assessment and Recognition of Physical Rehabilitation Exercises">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</p>
</li>
<li><p>[ ]基于时空注意力解耦Transformer的人体行为评估方法 |<a href="/2025/02/22/15-01-28/" title="基于时空注意力解耦 Transformer 的人体行为评估方法">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</p>
</li>
</ul>
<hr>
<h2 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h2><hr>
<ul>
<li>[TNSRE]Graph convolutional networks for assessment of physical rehabilitation exercises |<a href="/2025/02/22/14-06-22/" title="Graph Convolutional Networks for Assessment of Physical Rehabilitation Exercises">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</li>
</ul>
<hr>
<h2 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h2><hr>
<ul>
<li><p>[PR]Skeleton-based human action evaluation using graph convolutional network for monitoring Alzheimer’s progression |<a href="/2025/02/21/15-02-29/" title="Skeleton-based human action evaluation using graph convolutional network for monitoring Alzheimer’s progression">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</p>
</li>
<li><p>[EMBC]Assessing Physical Rehabilitation Exercises using Graph Convolutional Network with Self-supervised regularization |<a href="/2025/02/22/13-05-46/" title="Assessing Physical Rehabilitation Exercises using Graph Convolutional Network with Self-supervised regularization">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</p>
</li>
</ul>
<hr>
<h2 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h2><hr>
<ul>
<li><p>[TNSRE]A Deep Learning Framework for Assessing Physical Rehabilitation Exercises |<a href="/2025/02/22/10-48-25/" title="A Deep Learning Framework for Assessing Physical Rehabilitation Exercises">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</p>
</li>
<li><p>[TIP]Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks |<a href="/2025/02/24/13-10-16/" title="Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</p>
</li>
</ul>
<hr>
<h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><hr>
<ul>
<li>[AAAI]Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition |<a href="/2024/09/13/10-54-10/" title="ST-GCN 时空图卷积神经网路">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</li>
</ul>
<hr>
<h2 id="2015"><a href="#2015" class="headerlink" title="2015"></a>2015</h2><hr>
<ul>
<li>[CVPR]Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition |<a href="/2025/02/20/10-44-55/" title="BRNNs-Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition">笔记</a>|<span style="color:rgb(255, 192, 203);">精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</li>
</ul>
<hr>
<h2 id="2014"><a href="#2014" class="headerlink" title="2014"></a>2014</h2><hr>
<ul>
<li>[TPAMI]Structured Time Series Analysis for Human Action Segmentation and Recognition |<a href="/2025/02/20/14-54-35/" title="Structured Time Series Analysis for Human Action Segmentation and Recognition">笔记</a>|<span style="color:rgb(255, 192, 203);">待精读</span>|<span style="color:rgb(144, 238, 144);">Skeleton</span>|</li>
</ul>
<hr>
<h2 id="2013"><a href="#2013" class="headerlink" title="2013"></a>2013</h2><hr>
<hr>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>论文阅读, AQA</tag>
      </tags>
  </entry>
  <entry>
    <title>ios开发相关</title>
    <url>/2025/05/31/08-57-25/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="175e2c06044bd628f47dd074154a79c6ba5f65bba3e4bba2ffee60095f5af2fe">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>ios开发</category>
      </categories>
      <tags>
        <tag>开发, ios</tag>
      </tags>
  </entry>
  <entry>
    <title>论文复现细节</title>
    <url>/2025/05/03/10-57-57/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="ecd9db626f1ab9975d2aaee2cdd5df3ea463c943471ffcfa2215713fab8121d7">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>AQA</category>
      </categories>
      <tags>
        <tag>AQA, 深度学习, 计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>AR异构网络</title>
    <url>/2025/02/27/09-33-05/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="dd0e1b29095f6871966161d57a8fe566e1b23a63bb725032ec1af67d2e004a4b">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AR, 论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks</title>
    <url>/2025/02/24/13-10-16/</url>
    <content><![CDATA[<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>然而，ST-GCN[20]的图构建过程存在三个缺点：(1)ST-GCN中使用的骨架图是基于人体的自然连通性启发式预定义的。因此，它不能保证是最优的动作识别任务。例如，两只手之间的关系对于识别“鼓掌”和“阅读”等类别很重要。然而，ST-GCN很难捕捉到两只手之间的依赖关系，因为它们在预定义的基于人体的图形中彼此相距很远。(2)神经网络是分层的，不同的层包含不同层次的语义。然而，在ST-GCN中应用的图的拓扑结构在所有层上都是固定的，这缺乏对不同层中包含的多层语义建模的灵活性和能力。(3)对于不同动作类的所有样本，一个固定的图结构可能不是最优的。对于“擦脸”、“摸头”等类，手和头的联系应该更强，但对于其他一些类，如“跳起来”、“坐下来”，则不是这样。这一事实表明，图结构应该是数据相关的，然而，ST-GCN不支持这一点。</p>
<p>从空间角度来看，某种动作通常与关节的关键子集相关并以其为特征。从时间的角度来看，一个动作流可能包含多个阶段，其中不同的子阶段或框架对最终识别具有不同程度的重要性。</p>
<p>提出了一个骨架引导裁剪策略。</p>
<blockquote>
<p>通过将其与骨架引导裁剪的RGB数据相结合，它获得了+2.8%和+6.1%的额外改进。</p>
</blockquote>
<h3 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h3><p>Zhang等人[12]介绍了基于lstm的模型中的<strong>视图转换机制，该机制可以自动将骨架数据转换成更有利的角度进行动作识别。</strong></p>
<p>关键在于如何从缺少顶点和边隐顺序的图中构造局部连通的邻域。</p>
<h3 id="GRAPH-CONVOLUTIONAL-NETWORKS"><a href="#GRAPH-CONVOLUTIONAL-NETWORKS" class="headerlink" title="GRAPH CONVOLUTIONAL NETWORKS"></a>GRAPH CONVOLUTIONAL NETWORKS</h3><h4 id="A-Graph-construction"><a href="#A-Graph-construction" class="headerlink" title="A. Graph construction"></a>A. Graph construction</h4><p><img src="/2025/02/24/13-10-16/image-20250224132838164.png" alt="image-20250224132838164"></p>
<h4 id="B-Graph-convolution"><a href="#B-Graph-convolution" class="headerlink" title="B. Graph convolution"></a>B. Graph convolution</h4><p>Graph convolutional neural networks</p>
<p>在GCN 相关发展上，有2 种主流：</p>
<ol>
<li>Spatial perspective：直接将Convolution filter 作用在Graph 的节点上，并透过特别设计的规则做特征抽取、正规化</li>
<li>Spectral perspective：利用Graph Laplace matrices 的Eigenvalues 和Eigenvectors 做Graph Fourier transform，并在频率域做Graph convolution</li>
</ol>
<p>而该论文在骨架上应用的是Spatial perspective 方法。</p>
<p>在Kernel size 的设定上其实是3，会将Bi 切分成3 个Subset：</p>
<ol>
<li>Si1：Convolution 关注的Vertex ( 红色 )</li>
<li>Si2：距离中心较近的<strong>向心</strong>Vertex ( 绿色)</li>
<li>Si3：距离中心较远的<strong>离心</strong>Vertex ( 蓝色)</li>
</ol>
<p>这边要留意的是，Weight vectors 是固定的，但邻近的Vertexes 数量( Bi ) 是浮动的，而为了要让每个Vertex 都能匹配到独特的Weight vector ，ST-GCN]有特别设计一个Mapping function ( li ) 。</p>
<blockquote>
<p> 按照重心划分子集，不会导致不能动作下子集不同吗？</p>
</blockquote>
<h4 id="C-Implementation"><a href="#C-Implementation" class="headerlink" title="C. Implementation"></a>C. Implementation</h4><p>$\begin{aligned}f_{out}(v_i)=\sum_{v_j\in\mathcal{B}_i}\frac{1}{Z_{ij}}f_{in}(v_j)\cdot w(l_i(v_j))\end{aligned}$</p>
<p>$l_i$:是一个映射函数，把邻接子集中的节点映射到相同的位置使用权重矩阵。</p>
<p>在实作上，整个网路的特征图是个C × T × N 的Tensor：</p>
<ul>
<li>C 表示Channel 数量</li>
<li>T 表示Temporal 上的长度</li>
<li>N 表示Vertexes 的数量</li>
</ul>
<p>而上个<a href="https://arxiv.org/pdf/1801.07455.pdf">ST-GCN</a>的公式可表述成：</p>
<p><img src="/2025/02/24/13-10-16/image-20250225111324744.png" alt="image-20250225111324744"></p>
<h3 id="MULTI-STREAM-ATTENTION-ENHANCED-ADAPTIVE-GRAPH-CONVOLUTIONAL-NETWORK"><a href="#MULTI-STREAM-ATTENTION-ENHANCED-ADAPTIVE-GRAPH-CONVOLUTIONAL-NETWORK" class="headerlink" title="MULTI-STREAM ATTENTION-ENHANCED ADAPTIVE GRAPH CONVOLUTIONAL NETWORK"></a>MULTI-STREAM ATTENTION-ENHANCED ADAPTIVE GRAPH CONVOLUTIONAL NETWORK</h3><p>注意力矩阵：</p>
<p><img src="/2025/02/24/13-10-16/image-20250225112037960.png" alt="image-20250225112037960"></p>
<ol>
<li>Bk：<br>尺寸上也是N × N，但差异在于：这边<strong>每个参数都是可被训练的</strong>，因此没有任何的约束，所以最终这部份的样貌会<strong>完全取决于训练资料</strong>，也就能<strong>依据不同行为类别而有所差异</strong>。<br>而且不同于前面的Ak 及Mk ，这边的连结性与强度是同时被学出来的，这边不直接在Mk 上做Attention 的原因在于：若原本Ak 上有些地方是0 ( 代表<strong>非</strong>骨架的连结) ，那从训练开始到结束都会是0 ，那其实会没办法产生出新的连结( 限制模型去看骨架以外的关联性) 。从这观点来看，Bk 的作法就比Mk 更具弹性。</li>
<li>Ck：<br>这部份是个Data-dependent graph ，会针对Graph 上的每个Sample 去学习。为了决定两个顶点之间的连结强度，这边使用Dot product 计算两顶点在Embedding space 的相似度，所以嵌入的是个Gaussian function ：<br><img src="/2025/02/24/13-10-16/image-20250225112313149.png" alt="image-20250225112313149"><br>而Ck 上的值由于经过Normalized ，对两顶点来说算是个Soft edge，若用softmax 的操作来看，那Ck 的计算可表述成：<br><img src="/2025/02/24/13-10-16/image-20250225112459952.png" alt="image-20250225112459952"></li>
</ol>
<h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><p><a href="https://medium.com/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7-%E5%80%92%E5%BA%95%E6%9C%89%E5%A4%9A%E6%99%BA%E6%85%A7/%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80-cvpr-2019-two-stream-adaptive-graph-convolutional-networks-for-skeleton-based-action-5e53cf21a496">2s-AGCN论文解读</a></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AR, 论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Adaptive Spatiotemporal Graph Transformer Network for Action Quality Assessment</title>
    <url>/2025/02/24/08-04-24/</url>
    <content><![CDATA[<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p><img src="/2025/02/24/08-04-24/image-20250224081931709.png" alt="image-20250224081931709"></p>
<h3 id="APPROACH"><a href="#APPROACH" class="headerlink" title="APPROACH"></a>APPROACH</h3><p>自适应图注意力块示意图：</p>
<p><img src="/2025/02/24/08-04-24/image-20250224082628382.png" alt="image-20250224082628382"></p>
<h4 id="A-Overall-framework："><a href="#A-Overall-framework：" class="headerlink" title="A. Overall framework："></a>A. Overall framework：</h4><p>划分N个不重叠的片段，送入预训练好的VST，得到片段特征。<br>（Adaptive Spatio-Temporal Graph，ASG）模块构建空间和时间特征。然后残差，聚合。</p>
<h4 id="B-Adaptive-Spatiotemporal-Graph-ASG-Module："><a href="#B-Adaptive-Spatiotemporal-Graph-ASG-Module：" class="headerlink" title="B. Adaptive Spatiotemporal Graph (ASG) Module："></a>B. Adaptive Spatiotemporal Graph (ASG) Module：</h4><p><strong>Adaptive graph attention block.</strong><br>自适应指的是带权值的邻接矩阵。权重是可学习的。<br>不同通道指的是什么？</p>
<p><strong>Spatial branch construction.</strong><br>1024维特征重组为32x32。每个节点有8个邻居。就是单纯的九宫格。</p>
<p><strong>Temporal branch construction.</strong>：<br>时间链接就是前后连接，单向图。</p>
<h4 id="C-Transformer-encoder"><a href="#C-Transformer-encoder" class="headerlink" title="C. Transformer encoder"></a>C. Transformer encoder</h4><h4 id="D-Level-focused-decoder"><a href="#D-Level-focused-decoder" class="headerlink" title="D. Level-focused decoder"></a>D. Level-focused decoder</h4><p>水平解耦分数。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AQA, 论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>基于时空注意力解耦 Transformer 的人体行为评估方法</title>
    <url>/2025/02/22/15-01-28/</url>
    <content><![CDATA[<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p><img src="/2025/02/22/15-01-28/image-20250223093846840.png" alt="image-20250223093846840"></p>
<p><img src="/2025/02/22/15-01-28/image-20250223093859405.png" alt="image-20250223093859405"></p>
<ul>
<li>空间解耦 Transformer 块（SDT）</li>
</ul>
<ul>
<li>全局时空注意力融合模块的方法结构图</li>
</ul>
<p><img src="/2025/02/22/15-01-28/image-20250223094823980.png" alt="image-20250223094823980"></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AQA, 论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Convolutional Networks for Assessment of Physical Rehabilitation Exercises</title>
    <url>/2025/02/22/14-06-22/</url>
    <content><![CDATA[<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2>]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AQA, 论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Assessing Physical Rehabilitation Exercises using Graph Convolutional Network with Self-supervised regularization</title>
    <url>/2025/02/22/13-05-46/</url>
    <content><![CDATA[<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><h3 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h3><p>使用GCN对比CNN。</p>
<h3 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h3><p>GCN+LSTM</p>
<h3 id="PROPOSED-METHOD"><a href="#PROPOSED-METHOD" class="headerlink" title="PROPOSED METHOD"></a>PROPOSED METHOD</h3><ul>
<li>Self-supervised regularization：自监督正则化<br>从1-T中，生成预测2-T+1。</li>
</ul>
<h3 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h3><p><img src="/2025/02/22/13-05-46/image-20250222135332790.png" alt="image-20250222135332790"></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AQA, 论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>A Deep Learning Framework for Assessing Physical Rehabilitation Exercises</title>
    <url>/2025/02/22/10-48-25/</url>
    <content><![CDATA[<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><h3 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h3><p>制定量化运动表现的指标，将性能指标映射为运动质量的数值分数的评分函数。模型用于编码运动数据和质量分数的关系。<br>性能指标是基于高斯混合模型对骨架关节点数据进行概率建模。<br>一种使用概率建模和自编码神经网络对康复数据进行降维的性能度量。</p>
<h3 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h3><p>传统的人体运动的数学建模和表示方法大致分为两类：自顶向下的方法，引入潜在状态来描述运动的时间动态；自底向上的方法，使用局部特征来表示运动。第一类中常用的方法包括卡尔曼滤波[ 12 ]，隐马尔可夫模型[ 13 ]和高斯混合模型[ 14 ]。这些方法的主要缺点源于对潜态(在Kalman滤波器中)之间的跃迁采用线性模型，或者对潜态(隐马尔可夫模型的典型代表)采用简单的内部结构。</p>
<h3 id="PROPOSED-METHOD"><a href="#PROPOSED-METHOD" class="headerlink" title="PROPOSED METHOD"></a>PROPOSED METHOD</h3><ul>
<li>Dimensionality Reduction：数据降维<br>自编码器神经网络进行降维。</li>
</ul>
<p>所提出的自编码器架构将输入的运动数据投影为代码表示，并将代码重新投影到运动数据中。</p>
<p><img src="/2025/02/22/10-48-25/image-20250222124147369.png" alt="image-20250222124147369"></p>
<blockquote>
<p>lstm用来做编码器解码器。</p>
</blockquote>
<ul>
<li>Performance Metric：性能指标<br>GMM是一种用混合高斯概率密度函数表示数据的参数概率模型。</li>
</ul>
<ul>
<li>Scoring Function：分数映射<br>分数映射到0-1。对比评分标准</li>
</ul>
<ul>
<li>Deep Learning Architecture for Rehabilitation Assessment：</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AQA, 论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Skeleton-based human action evaluation using graph convolutional network for monitoring Alzheimer’s progression</title>
    <url>/2025/02/21/15-02-29/</url>
    <content><![CDATA[<a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>介绍医疗识别领域和2T-GCN的分类和评估分数。</p>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><ul>
<li><p>Skeleton retrieval：骨架检索。A list of public benchmark datasets identified in a survey of human body skeleton representation [23]：<br>介绍骨架数据集，不一定都是AQA任务还有很多识别任务。</p>
</li>
<li><p>Skeleton representation：特征表示。<br>传统的几何手工特征HMM，SVM，K近邻。DL方法全面优于手工方法：CNN，LSTM。</p>
</li>
</ul>
<blockquote>
<p>不是传统的分数回归，而是二分类，然后在SoftMax得到一个分数？？？</p>
</blockquote>
<ul>
<li>Datasets and evaluation：数据集与评价<br>主要介绍了医疗领域的几个评估用的数据集</li>
</ul>
<h3 id="Our-2T-GCN-method"><a href="#Our-2T-GCN-method" class="headerlink" title="Our 2T-GCN method"></a>Our 2T-GCN method</h3><p>数据也是3D属性。</p>
<blockquote>
<p>关节点的卷积范围划分来源于st-gcn</p>
</blockquote>
<p><img src="/2025/02/21/15-02-29/image-20250222103106275.png" alt="image-20250222103106275"></p>
<p>将通过利用SoftMax层的概率输出来提供数值评估分数。</p>
<blockquote>
<p>通过计算神经元输出的值来的到运动质量分数，还不是得分函数模拟评分。</p>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AQA, 论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Structured Time Series Analysis for Human Action Segmentation and Recognition</title>
    <url>/2025/02/20/14-54-35/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AQA, 论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>BRNNs-Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition</title>
    <url>/2025/02/20/10-44-55/</url>
    <content><![CDATA[<a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>bidirectional recurrent neural networks (BRNNs).</p>
<p>The TPs methods are generally restricted by the width of the time windows and can only utilize limited contextual information. </p>
<p>As for HMMs, it is very difficult to obtain the temporal aligned sequences and the corresponding emission distributions.</p>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>three categories of approaches representing temporal dynamics by local features, sequential state transitions and RNN.</p>
<ul>
<li>Approaches with local features:<br>within a certain time window or differential quantities, it cannot globally capture the temporal evolution of actions.</li>
<li>Approaches with sequential state transitions:<br>very difficult task.</li>
<li>Approaches with RNN:</li>
</ul>
<h3 id="Our-Model"><a href="#Our-Model" class="headerlink" title="Our Model"></a>Our Model</h3><h4 id="Review-of-RNN-and-LSTM"><a href="#Review-of-RNN-and-LSTM" class="headerlink" title="Review of RNN and LSTM"></a>Review of RNN and LSTM</h4><p><a href="https://zhuanlan.zhihu.com/p/32085405">知乎-人人都能看懂的LSTM</a></p>
<p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">理解 LSTM 网络</a></p>
<h4 id="Hierarchical-RNN-for-Skeleton-Based-Action-Recognition"><a href="#Hierarchical-RNN-for-Skeleton-Based-Action-Recognition" class="headerlink" title="Hierarchical RNN for Skeleton Based Action Recognition"></a>Hierarchical RNN for Skeleton Based Action Recognition</h4><p><img src="/2025/02/20/10-44-55/image-20250220182141383.png" alt="image-20250220182141383"></p>
<p>Generally, the number of weights in a LSTM block is several times more than that in a tanh neuron. It is very easy to overfit the network with limited training sequences.</p>
<blockquote>
<p>只用一层lstm，减少过拟合。</p>
</blockquote>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>⊕ = concatenation，串联<br>用所有帧的输出特征加起来作为序列特征。</p>
<h4 id="Five-Comparative-Architectures"><a href="#Five-Comparative-Architectures" class="headerlink" title="Five Comparative Architectures"></a>Five Comparative Architectures</h4><p>消融实验</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>数据归一化：原点坐标为髋中心。</p>
<blockquote>
<p>数据预处理，对骨架数据进行增加信噪比？</p>
</blockquote>
<p>采用间隔采样，骨架数据较平滑。</p>
<h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><p>we adopt the strategies like adding the input noise, weight noise and early stopping [7, 8].<br>dropout strategy [39] does not work here.</p>
<blockquote>
<p>dropout在lstm长序列中不起作用。</p>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>论文阅读, AQA, RNN, skeleton</tag>
      </tags>
  </entry>
  <entry>
    <title>A Graph Convolutional Siamese Network for the Assessment and Recognition of Physical Rehabilitation Exercises</title>
    <url>/2025/02/20/10-18-48/</url>
    <content><![CDATA[<a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>outperforms on both action classification and assessment.</p>
<h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p><img src="/2025/02/20/10-18-48/image-20250221142539647.png" alt="image-20250221142539647"></p>
<p>Data Preprocessing：<br>According to the previous studies [4,5,8], data preprocessing is essential for a skeleton-based model.<br>Velocities：=bone angles。</p>
<p><img src="/2025/02/20/10-18-48/image-20250221143212784.png" alt="image-20250221143212784"></p>
<p>based on the ST-GCN block (Basic Block)。</p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3>]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>论文阅读, AQA, GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>第一个版本AQA框架下的实验日志</title>
    <url>/2025/01/03/08-04-36/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="d4a2bfafdf110d00c89d0428726570aa9331dba0bd3e840a9ce6ecc15f4c96fa">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>实验日志</category>
      </categories>
      <tags>
        <tag>实验日志, AQA</tag>
      </tags>
  </entry>
  <entry>
    <title>ResFNN残差网络分数分布回归模型</title>
    <url>/2024/12/11/08-56-37/</url>
    <content><![CDATA[<h1 id="阅读笔记：ResFNN论文"><a href="#阅读笔记：ResFNN论文" class="headerlink" title="阅读笔记：ResFNN论文"></a>阅读笔记：ResFNN论文</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><ul>
<li>介绍了基于AI的动作质量评估（AQA）在体育领域的重要性，尤其是在比赛评分、技能评估和康复医学中的应用。</li>
<li>提出了Residual Structure-Based Feedforward Neural Network (ResFNN) 模型，通过高效的动作特征学习改进评分性能。</li>
<li>方法：<ol>
<li>利用3D卷积网络提取视频特征。</li>
<li>使用ResFNN进行特征聚合和学习。</li>
<li>应用分布回归获取更精确的分数映射。</li>
</ol>
</li>
<li>在AQA-7、MTL-AQA和JIGSAWS数据集上的实验验证了模型的优越性。</li>
</ul>
<hr>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a><strong>引言</strong></h2><ul>
<li>强调AI和5G技术推动了智能运动设备的发展，例如智能相机、手环和鞋子。</li>
<li>动作质量评估（AQA）的核心挑战是评估连续动作中的细微差异，尤其是在相似场景下。</li>
<li>当前方法的问题：<ul>
<li>背景干扰。</li>
<li>相似性学习误差。</li>
<li>特征提取和聚合能力不足。</li>
</ul>
</li>
<li>本文的贡献：<ol>
<li>利用残差结构的前馈神经网络提升特征学习能力。</li>
<li>提出分数分布回归方法，解决评分主观性带来的不确定性。</li>
</ol>
</li>
</ul>
<hr>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a><strong>相关工作</strong></h2><h3 id="AQA"><a href="#AQA" class="headerlink" title="AQA"></a><strong>AQA</strong></h3><ul>
<li>早期基于传统机器学习的研究，使用支持向量回归和离散余弦变换等特征提取。</li>
<li>随着深度学习发展，方法逐渐转向C3D、LSTM、图神经网络等。</li>
<li>局限性：对初始特征的聚合与学习效果不足。</li>
</ul>
<h3 id="残差结构网络"><a href="#残差结构网络" class="headerlink" title="残差结构网络"></a><strong>残差结构网络</strong></h3><ul>
<li>He等人提出的残差网络（ResNet）解决了深层网络中的梯度爆炸和退化问题。</li>
<li>残差结构被广泛应用于不同领域，本研究引入残差前馈网络（ResFNN）用于特征聚合。</li>
</ul>
<h3 id="标签分布学习"><a href="#标签分布学习" class="headerlink" title="标签分布学习"></a><strong>标签分布学习</strong></h3><ul>
<li>标签分布学习（LDL）将标签建模为概率分布，用于处理多评分者的主观性问题。</li>
</ul>
<hr>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a><strong>方法</strong></h2><h3 id="框架概述"><a href="#框架概述" class="headerlink" title="框架概述"></a><strong>框架概述</strong></h3><ul>
<li>三个模块：<ol>
<li><strong>视频特征提取</strong>：通过I3D卷积网络提取特征。</li>
<li><strong>ResFNN</strong>：由6个残差块组成，每个块包含全连接层和快捷连接。</li>
<li><strong>分数分布回归</strong>：将动作质量编码为高斯分布，通过采样生成最终分数。</li>
</ol>
</li>
</ul>
<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a><strong>特征提取</strong></h3><ul>
<li>使用滑动窗口将视频分割为重叠的16帧片段。</li>
<li>提取片段特征并通过平均操作聚合为视频特征。</li>
</ul>
<h3 id="ResFNN"><a href="#ResFNN" class="headerlink" title="ResFNN"></a><strong>ResFNN</strong></h3><ul>
<li>每个残差块包含5个全连接层，采用ReLU激活和Dropout正则化。</li>
<li>快捷连接（Shortcut Connection）解决梯度爆炸与退化问题。</li>
</ul>
<h3 id="分数分布回归"><a href="#分数分布回归" class="headerlink" title="分数分布回归"></a><strong>分数分布回归</strong></h3><ul>
<li>将特征编码为高斯分布，采样生成预测分数。</li>
<li>通过分布偏差损失和重建损失优化模型。</li>
</ul>
<hr>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a><strong>实验</strong></h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a><strong>数据集</strong></h3><ol>
<li><strong>AQA-7</strong>：包括跳水、滑雪、体操等7种动作，共1189个视频样本。</li>
<li><strong>MTL-AQA</strong>：包含1412个跳水动作，提供多裁判评分。</li>
<li><strong>JIGSAWS</strong>：针对外科手术技能评估。</li>
</ol>
<h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a><strong>评价指标</strong></h3><ul>
<li>使用斯皮尔曼相关系数（ρ）评价预测分数与真实分数的相关性。</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h3><ul>
<li>在所有数据集上，ResFNN均优于现有方法，特别是在特征聚合能力和不确定性处理方面表现突出。</li>
</ul>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a><strong>消融实验</strong></h3><ul>
<li>验证ResFNN和分布回归模块的有效性。</li>
<li>使用6个残差块的模型效果最佳。</li>
</ul>
<hr>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h2><ul>
<li>提出了一种高效的残差前馈神经网络（ResFNN），解决了特征聚合与学习问题。</li>
<li>引入分布回归方法，处理动作评分中的不确定性。</li>
<li>未来计划：<ul>
<li>提升模型解释性。</li>
<li>轻量化模型设计以适应移动设备。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><strong>参考文献</strong></h2><ul>
<li>提供了与AQA相关的经典文献，包括C3D、LSTM以及分布回归等方法。</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>AQA,深度学习,神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer笔记</title>
    <url>/2024/12/05/14-10-53/</url>
    <content><![CDATA[<h1 id="Transformer-模型笔记"><a href="#Transformer-模型笔记" class="headerlink" title="Transformer 模型笔记"></a>Transformer 模型笔记</h1><p>Transformer 是一种基于自注意力机制的深度学习模型，广泛应用于自然语言处理任务，如机器翻译、文本生成等。</p>
<hr>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>Transformer 的整体架构由 <strong>Encoder</strong> 和 <strong>Decoder</strong> 两部分组成，每部分都包含若干个堆叠的子模块：</p>
<ol>
<li><p><strong>Encoder</strong></p>
<ul>
<li>输入：嵌入后的序列 $X \in \mathbb{R}^{n \times d}$</li>
<li>输出：编码后的序列表示 $Z \in \mathbb{R}^{n \times d}$</li>
<li>主要模块：<ul>
<li>多头自注意力（Multi-Head Self-Attention）</li>
<li>前馈网络（Feed-Forward Network, FFN）</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Decoder</strong></p>
<ul>
<li>输入：目标序列 $Y$ 和编码序列 $Z$</li>
<li>输出：解码序列 $Y’$</li>
<li>主要模块：<ul>
<li>多头自注意力（目标序列）</li>
<li>编码-解码注意力</li>
<li>前馈网络（FFN）</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h2><h3 id="1-自注意力机制（Self-Attention）"><a href="#1-自注意力机制（Self-Attention）" class="headerlink" title="1. 自注意力机制（Self-Attention）"></a>1. <strong>自注意力机制（Self-Attention）</strong></h3><p>自注意力机制通过计算序列中每个位置与其他位置的相关性，提取全局依赖关系。</p>
<h4 id="公式："><a href="#公式：" class="headerlink" title="公式："></a>公式：</h4><p>$<br>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V<br>$</p>
<ul>
<li>$Q, K, V$：分别是查询向量、键向量和值向量</li>
<li>$d_k$：键向量的维度，用于缩放避免梯度消失</li>
</ul>
<h4 id="多头注意力："><a href="#多头注意力：" class="headerlink" title="多头注意力："></a>多头注意力：</h4><p>$<br>\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O<br>$<br>每个头计算一个注意力机制：<br>$<br>\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)<br>$</p>
<hr>
<h3 id="2-前馈网络（FFN）"><a href="#2-前馈网络（FFN）" class="headerlink" title="2. 前馈网络（FFN）"></a>2. <strong>前馈网络（FFN）</strong></h3><p>FFN 是一个简单的两层全连接网络，作用于每个序列位置。</p>
<h4 id="公式：-1"><a href="#公式：-1" class="headerlink" title="公式："></a>公式：</h4><p>$<br>\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2<br>$</p>
<hr>
<h3 id="3-位置编码（Positional-Encoding）"><a href="#3-位置编码（Positional-Encoding）" class="headerlink" title="3. 位置编码（Positional Encoding）"></a>3. <strong>位置编码（Positional Encoding）</strong></h3><p>Transformer 通过位置编码加入序列的位置信息。</p>
<h4 id="公式：-2"><a href="#公式：-2" class="headerlink" title="公式："></a>公式：</h4><p>$<br>PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right), \quad<br>PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)<br>$</p>
<ul>
<li>$pos$：序列中的位置</li>
<li>$i$：嵌入向量的维度索引</li>
</ul>
<hr>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>Transformer 的损失函数通常是 <strong>交叉熵损失（Cross-Entropy Loss）</strong>，适用于序列到序列任务。</p>
<h4 id="公式：-3"><a href="#公式：-3" class="headerlink" title="公式："></a>公式：</h4><p>$\begin{aligned}<br>\mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T y_t \log \hat{y}_t<br>\end{aligned}$</p>
<ul>
<li>$y_t$：真实目标序列</li>
<li>$\hat{y}_t$：模型预测序列</li>
</ul>
<hr>
<h2 id="优化技巧"><a href="#优化技巧" class="headerlink" title="优化技巧"></a>优化技巧</h2><ol>
<li><p><strong>学习率调度</strong>：<br>使用 warmup 策略：<br>$<br>\text{lr} = d_\text{model}^{-0.5} \cdot \min(\text{step_num}^{-0.5}, \text{step_num} \cdot \text{warmup_steps}^{-1.5})<br>$</p>
</li>
<li><p><strong>正则化</strong>：</p>
<ul>
<li>Dropout：防止过拟合</li>
<li>Label Smoothing：缓解过拟合问题</li>
</ul>
</li>
</ol>
<hr>
<h2 id="优点和缺点"><a href="#优点和缺点" class="headerlink" title="优点和缺点"></a>优点和缺点</h2><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ol>
<li>并行计算能力强（无需递归）。</li>
<li>全局上下文建模能力强（通过注意力机制）。</li>
<li>对长序列具有良好的处理能力。</li>
</ol>
<h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><ol>
<li>对长序列的计算复杂度高，$O(n^2)$。</li>
<li>缺乏位置感知能力，需要通过位置编码补充。</li>
</ol>
<hr>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li>自然语言处理任务（机器翻译、文本摘要、语言模型等）</li>
<li>计算机视觉（Vision Transformer）</li>
<li>时间序列分析</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>论文阅读, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>图像美学评价的相关论文</title>
    <url>/2024/12/03/15-10-34/</url>
    <content><![CDATA[<h1 id="Theme-Aware："><a href="#Theme-Aware：" class="headerlink" title="Theme-Aware："></a>Theme-Aware：</h1><p><img src="/2024/12/03/15-10-34/QQ_1733209955060.png" alt=" "></p>
<p><img src="/2024/12/03/15-10-34/QQ_1733210026645.png" alt=" "></p>
<p>总体结构。1 )预训练视觉属性分析网络( VAAN )提取视觉属性特征。2 )对主题理解网络( TUN )进行预训练，提取图像主题特征。3 )利用美学网络提取通用的美学特征。4 )利用属性-主题图( ATG )挖掘图像主题与视觉属性之间的关系。5 )利用属性美学图( AAG )进一步挖掘主题感知视觉属性与通用美学特征之间的关系。</p>
<p>可解释性：</p>
<p><img src="/2024/12/03/15-10-34/QQ_1733210047888.png" alt=" "></p>
<p>Ic：有趣的内容；Oe：对象强调；Vc：鲜艳的颜色；Dof：景深；Ch：色彩和谐；G1：良好的照明；Gt：事实真相；pre：预测。</p>
<h1 id="explainable可解释："><a href="#explainable可解释：" class="headerlink" title="explainable可解释："></a>explainable可解释：</h1><p>图神经网络（GCN）建模语义属性与场景类别的关系，以提高模型的解释性和泛化能力。</p>
<p>可解释指的是为什么分数比较高，相当于把分数解析为多个属性，分数解耦？</p>
<h3 id="总体框架："><a href="#总体框架：" class="headerlink" title="总体框架："></a>总体框架：</h3><ol>
<li><strong>语义属性学习模块</strong>：利用多分支卷积网络预测五种语义属性（亮度、色彩、对比度、噪声、锐度）。</li>
<li><strong>场景类别预测模块</strong>：通过ResNet-50预测图像的场景类别。</li>
<li><strong>语义推理模块</strong>：基于GCN，挖掘语义属性与场景类别之间的内在关系，生成最终的图像质量分数。</li>
</ol>
<h3 id="2-2-模块细节"><a href="#2-2-模块细节" class="headerlink" title="2.2 模块细节"></a>2.2 模块细节</h3><h3 id="语义属性学习"><a href="#语义属性学习" class="headerlink" title="语义属性学习"></a>语义属性学习</h3><ul>
<li>通过ResNet-50提取图像特征并利用多分支网络分别预测五种语义属性。</li>
<li>损失函数：采用L1损失优化参数，保证预测语义属性与真实值的接近性。</li>
</ul>
<h3 id="场景类别预测"><a href="#场景类别预测" class="headerlink" title="场景类别预测"></a>场景类别预测</h3><ul>
<li>使用ResNet-50提取特征，并通过全连接层预测场景类别概率。</li>
<li>损失函数：采用L1损失优化预测类别与真实类别的差异。</li>
</ul>
<h3 id="语义推理"><a href="#语义推理" class="headerlink" title="语义推理"></a>语义推理</h3><ul>
<li>使用GCN建模语义属性与场景类别的关系，设计自定义的邻接矩阵表示节点间的关系。</li>
<li>最终通过多层感知机（MLP）生成质量分数。</li>
</ul>
<h2 id="可解释性："><a href="#可解释性：" class="headerlink" title="可解释性："></a>可解释性：</h2><p>可视化生成的激活图表明，SARQUE能有效捕捉影响图像质量的区域，且其预测的语义属性与人类感知一致。</p>
<p>生成分数分布，不是分数回归，可以看到不同子动作对分数分布的影响。</p>
<p><img src="/2024/12/03/15-10-34/QQ_1733210083467.png" alt=" "></p>
<p><img src="/2024/12/03/15-10-34/QQ_1733210105458.png" alt=" "></p>
<p><img src="/2024/12/03/15-10-34/QQ_1733210137041.png" alt=" "></p>
<h1 id="Coarse"><a href="#Coarse" class="headerlink" title="Coarse"></a>Coarse</h1><h3 id="关键创新"><a href="#关键创新" class="headerlink" title="关键创新"></a>关键创新</h3><ol>
<li><strong>粗到精的美学评估</strong>：首先进行粗粒度的二分类任务，然后进行更复杂的美学分数预测。</li>
<li><strong>动态属性选择</strong>：根据预测的美学二分类结果动态选择显著影响美学评分的属性。</li>
<li><strong>自注意力融合网络</strong>：使用自注意力机制探索美学属性与图像特征的交互，以提高最终的美学预测性能。</li>
</ol>
<p>CADAS模型主要由三个部分组成：</p>
<ol>
<li><strong>AttributeNet</strong>：一个层次化的网络，用于预测图像的候选美学属性。</li>
<li><strong>AestheticNet</strong>：用于进行粗粒度的美学二分类，判断图像是否美观。</li>
<li><strong>FusionNet</strong>：基于自注意力机制，融合从AestheticNet和AttributeNet中提取的特征，以进行精细的美学评分预测。</li>
</ol>
<p>根据心理学研究，图像的美学属性可以分为低级特征和高级特征。例如，<strong>光线</strong>和<strong>色彩</strong>属于低级特征，而<strong>构图</strong>和<strong>内容</strong>属于高级特征。AttributeNet通过层次化方式将这些属性分组并进行预测。</p>
<h3 id="可解释性分析"><a href="#可解释性分析" class="headerlink" title="可解释性分析"></a>可解释性分析</h3><p>CADAS不仅能够输出最终的美学评分，还能够预测图像中最具影响力的美学属性。通过对比高、美、中、低美学评分的图像，实验展示了CADAS如何通过选择和分析图像的显著美学属性提供直观的解释。</p>
<p><img src="/2024/12/03/15-10-34/QQ_1733210155931.png" alt=" "></p>
<p><img src="/2024/12/03/15-10-34/QQ_1733210177541.png" alt=" "></p>
<p><img src="/2024/12/03/15-10-34/QQ_1733210205074.png" alt=" "></p>
<h1 id="Multi-modality-IAC预处理："><a href="#Multi-modality-IAC预处理：" class="headerlink" title="Multi-modality-IAC预处理："></a>Multi-modality-IAC预处理：</h1><p>动机：图像分类的预处理模型，主要强调图像的分类特征，而不是美学评价需要的特征属性。</p>
<p>需要解决的问题： 1：属性数据- 数据用多模态大语言模型（MLLM）生成 2：描述数据的特征。-多属性对比学习</p>
<p><img src="/2024/12/03/15-10-34/QQ_1733210228765.png" alt=" "></p>
<p>方法：</p>
<ul>
<li>AesNet将基于图像的视觉特征与基于文本的属性特征融合，通过映射到不同的嵌入空间中，进行多属性对比学习。这种方法能够有效地区分相似和不相似的样本，从而获得更全面的美学表示。</li>
<li>通过引入语义亲和损失，本文解决了从通用视觉领域到美学领域过渡时的分布变化问题，提升了模型的泛化能力。</li>
</ul>
<p>语义亲和损失： 为了减轻从一般视觉领域到美学领域的分布转移，本文提出了语义亲和损失（Semantic Affinity Loss），通过约束图像的视觉信息与美学语义信息的一致性，帮助模型保留内容信息并增强其泛化能力。</p>
<h3 id="多属性对比学习"><a href="#多属性对比学习" class="headerlink" title="多属性对比学习"></a>多属性对比学习</h3><ul>
<li><strong>特征融合</strong>：通过视觉-属性聚合模块，将图像的视觉特征与七种美学属性的文本特征进行融合。这些融合特征被映射到七个不同的嵌入空间中，进行多属性对比学习，以学习到更加丰富的美学表示。</li>
<li><strong>对比学习损失</strong>：通过对比学习方法，模型在不同属性的嵌入空间中优化，拉近相似样本的距离，推远不相似样本的距离，最终提升美学特征的区分能力。 对比学习已经被证明能增强特征表示。</li>
<li><strong>语义亲和损失</strong>：为了缓解视觉信息和美学信息之间的分布差异，提出语义亲和损失，以确保模型能够保留更多的内容信息，从而增强其在实际美学任务中的泛化能力。</li>
</ul>
<p><img src="/2024/12/03/15-10-34/QQ_1733210248857.png" alt=" "></p>
<h1 id="AesExpert多模态美学专家模型"><a href="#AesExpert多模态美学专家模型" class="headerlink" title="AesExpert多模态美学专家模型"></a>AesExpert多模态美学专家模型</h1><p>大语言模型构建了一个数据集</p>
<p>8块A100训练。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>PECoP动作质量评估的参数预训练</title>
    <url>/2024/11/24/18-13-44/</url>
    <content><![CDATA[<h1 id="阅读笔记：PECoP-Parameter-Efficient-Continual-Pretraining-for-Action-Quality-Assessment"><a href="#阅读笔记：PECoP-Parameter-Efficient-Continual-Pretraining-for-Action-Quality-Assessment" class="headerlink" title="阅读笔记：PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment"></a>阅读笔记：<strong>PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment</strong></h1><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h3><ul>
<li><strong>动作质量评估（AQA）</strong>：<ul>
<li>AQA 任务需要评估视频中动作的细节和质量，广泛应用于体育比赛评分、医疗动作评估等领域。</li>
<li>其与动作分类任务的核心区别在于，AQA 更关注动作细节和流畅性，而不仅仅是类别判断。</li>
</ul>
</li>
<li><strong>现有方法的挑战</strong>：<ul>
<li><strong>迁移学习局限性</strong>：<ul>
<li>通常采用基于动作分类模型的迁移学习方法，但预训练任务与目标任务（AQA）之间存在领域差异，导致迁移效果有限。</li>
</ul>
</li>
<li><strong>算力和存储需求</strong>：<ul>
<li>全模型微调对计算资源和存储要求较高，尤其在资源受限的场景中难以部署。</li>
</ul>
</li>
</ul>
</li>
<li><strong>研究目标</strong>：<ul>
<li>提出一种<strong>参数高效的持续预训练方法</strong>，在参数量有限的情况下提高迁移性能，适配 AQA 任务。</li>
</ul>
</li>
</ul>
<h3 id="1-2-创新点"><a href="#1-2-创新点" class="headerlink" title="1.2 创新点"></a>1.2 创新点</h3><ul>
<li><strong>参数高效性</strong>：<ul>
<li>通过仅对少量参数进行优化，实现高效的迁移学习。</li>
</ul>
</li>
<li><strong>持续预训练</strong>：<ul>
<li>提出一种动态策略，将模型的预训练与下游任务训练紧密结合，使模型适应新的领域分布。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><h3 id="2-1-动作质量评估方法"><a href="#2-1-动作质量评估方法" class="headerlink" title="2.1 动作质量评估方法"></a>2.1 动作质量评估方法</h3><ul>
<li><strong>传统方法</strong>：<ul>
<li>通常基于手工设计的特征提取器（如 SIFT、HOG），精度较低且不适应多样化场景。</li>
</ul>
</li>
<li><strong>深度学习方法</strong>：<ul>
<li>利用 CNN 和 Transformer 模型提取视频特征。</li>
<li><strong>问题</strong>：大多依赖大规模预训练，且适应 AQA 任务的泛化性能有限。</li>
</ul>
</li>
</ul>
<h3 id="2-2-参数高效学习"><a href="#2-2-参数高效学习" class="headerlink" title="2.2 参数高效学习"></a>2.2 参数高效学习</h3><ul>
<li><strong>低秩分解和权重共享</strong>：<ul>
<li>通过分解模型权重矩阵或共享模块参数，减少训练所需的参数量。</li>
</ul>
</li>
<li><strong>提示学习（Prompting）</strong>：<ul>
<li>增加少量的任务提示参数，而非调整整个模型。</li>
<li>常用于 NLP 任务，在视觉领域的应用较少。</li>
</ul>
</li>
</ul>
<h3 id="2-3-持续学习"><a href="#2-3-持续学习" class="headerlink" title="2.3 持续学习"></a>2.3 持续学习</h3><ul>
<li><strong>主要方法</strong>：<ul>
<li>基于记忆回放（Memory Replay）和参数正则化。</li>
</ul>
</li>
<li><strong>在迁移学习中的应用</strong>：<ul>
<li>持续学习可缓解领域迁移中的遗忘问题，但需要设计适合 AQA 的策略。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><h3 id="3-1-模型架构"><a href="#3-1-模型架构" class="headerlink" title="3.1 模型架构"></a>3.1 模型架构</h3><ul>
<li><strong>主干网络</strong>：<ul>
<li>选择了轻量化的 Transformer 架构，便于集成额外的参数高效模块。</li>
</ul>
</li>
<li><strong>PECoP 模块</strong>：<ul>
<li>核心组件包括<strong>可学习的参数化模块</strong>，仅对特定层进行调整，减少整体参数开销。</li>
</ul>
</li>
</ul>
<h3 id="3-2-参数高效模块设计"><a href="#3-2-参数高效模块设计" class="headerlink" title="3.2 参数高效模块设计"></a>3.2 参数高效模块设计</h3><ul>
<li><strong>动态任务提示</strong>：<ul>
<li>根据输入视频的领域特征，动态调整提示参数，提升迁移性能。</li>
</ul>
</li>
<li><strong>低秩适配（LoRA）</strong>：<ul>
<li>在模型权重中加入低秩矩阵，以最小的参数变化实现域适配。</li>
</ul>
</li>
</ul>
<h3 id="3-3-持续预训练策略"><a href="#3-3-持续预训练策略" class="headerlink" title="3.3 持续预训练策略"></a>3.3 持续预训练策略</h3><ul>
<li><strong>阶段性更新</strong>：<ul>
<li>持续预训练分为多个阶段，每阶段逐步引入新任务数据以强化模型泛化能力。</li>
</ul>
</li>
<li><strong>正则化与回放</strong>：<ul>
<li>采用基于正则化的防遗忘策略，同时对关键样本进行回放，确保模型对旧任务的记忆。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-实验与分析"><a href="#4-实验与分析" class="headerlink" title="4. 实验与分析"></a>4. 实验与分析</h2><h3 id="4-1-数据集与实验设置"><a href="#4-1-数据集与实验设置" class="headerlink" title="4.1 数据集与实验设置"></a>4.1 数据集与实验设置</h3><ul>
<li><strong>数据集</strong>：<ul>
<li>在公开的 AQA 数据集上验证模型，包括 MTL-AQA 和 FineGym。</li>
</ul>
</li>
<li><strong>实验设置</strong>：<ul>
<li>与全模型微调方法进行对比，重点考察 PECoP 的参数效率和迁移性能。</li>
</ul>
</li>
</ul>
<h3 id="4-2-实验结果"><a href="#4-2-实验结果" class="headerlink" title="4.2 实验结果"></a>4.2 实验结果</h3><ul>
<li><strong>性能比较</strong>：<ul>
<li>在多个数据集上，PECoP 的性能优于全模型微调和其他参数高效方法。</li>
</ul>
</li>
<li><strong>参数效率</strong>：<ul>
<li>相较全模型微调，参数量减少了 90%以上，同时保持了竞争力性能。</li>
</ul>
</li>
</ul>
<h3 id="4-3-消融实验"><a href="#4-3-消融实验" class="headerlink" title="4.3 消融实验"></a>4.3 消融实验</h3><ul>
<li><strong>提示学习与低秩适配的效果</strong>：<ul>
<li>动态提示和低秩适配模块的结合显著提升了模型的泛化能力。</li>
</ul>
</li>
<li><strong>持续预训练策略的影响</strong>：<ul>
<li>持续引入新任务数据能有效缓解领域迁移中的遗忘问题。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="5-总结与未来工作"><a href="#5-总结与未来工作" class="headerlink" title="5. 总结与未来工作"></a>5. 总结与未来工作</h2><h3 id="5-1-贡献"><a href="#5-1-贡献" class="headerlink" title="5.1 贡献"></a>5.1 贡献</h3><ul>
<li>提出了针对 AQA 任务的参数高效持续预训练框架 PECoP。</li>
<li>通过动态提示和低秩适配模块，实现了迁移性能与参数效率的平衡。</li>
</ul>
<h3 id="5-2-局限性"><a href="#5-2-局限性" class="headerlink" title="5.2 局限性"></a>5.2 局限性</h3><ul>
<li>持续预训练的收敛速度较慢，需进一步优化。</li>
<li>模型在复杂动作场景中的表现还有提升空间。</li>
</ul>
<h3 id="5-3-未来方向"><a href="#5-3-未来方向" class="headerlink" title="5.3 未来方向"></a>5.3 未来方向</h3><ul>
<li>探索更轻量化的提示机制。</li>
<li>将 PECoP 扩展至其他领域任务（如医学图像分析、行为识别）。</li>
</ul>
<hr>
]]></content>
      <categories>
        <category>AQA</category>
      </categories>
      <tags>
        <tag>AQA</tag>
      </tags>
  </entry>
  <entry>
    <title>Else-Net基于骨架的连续动作识别弹性特征选择</title>
    <url>/2024/11/24/16-26-10/</url>
    <content><![CDATA[<h1 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h1><p>​    尽管作者已经为解决这个任务做出了努力，但他们采用了一种可扩展的架构，这种架构可以在每次出现新的类时为网络添加一个新的可学习模块。虽然这种技术有助于减轻灾难性遗忘，但模型的计算足迹逐渐增长，使得该方法耗费内存且可扩展性差。此外，它们的设置增加了与真实世界场景不同的约束。也就是说，他们在大多数训练实例上预训练网络，并且在增量阶段只保留几个类。他们使用NTU RGB + D 60的前50个类预训练他们的网络，并在10个任务上进行增量训练，每个任务聚焦于一个不同的类。我们认为这样的基准不同于经典的CL基准，因为它是简化的并且远离真实世界的场景。</p>
<h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><ul>
<li><p><strong>背景</strong>:<br>现有动作识别方法多为离线学习，需要所有类别数据同时可用。然而，实际应用中，如人机交互和安防监控，模型需处理连续流式的新动作数据，这对模型提出了持续学习的挑战。</p>
</li>
<li><p><strong>问题</strong>:<br>持续学习面临 <strong>灾难性遗忘 (Catastrophic Forgetting)</strong> 问题，新学习的动作会覆盖之前学到的知识。</p>
</li>
<li><p><strong>灵感来源</strong>:<br>人类大脑通过在多重新皮层区域搜索并巩固相关记忆，避免遗忘旧知识，同时学习新知识。</p>
</li>
<li><p><strong>核心思想</strong>:<br>提出 <strong>Elastic Semantic Network (Else-Net)</strong>，通过动态搜索最相关的学习块，并使用新块存储新知识，有效避免灾难性遗忘。</p>
</li>
</ul>
<hr>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><h3 id="2-1-持续学习"><a href="#2-1-持续学习" class="headerlink" title="2.1 持续学习"></a>2.1 持续学习</h3><ul>
<li>持续学习旨在模拟人类智能，持续学习新任务而不遗忘旧知识。</li>
<li>现有方法多集中在图像分类领域：<ul>
<li><strong>GEM (Gradient Episodic Memory)</strong>: 使用梯度记忆缓解遗忘。</li>
<li><strong>ReMind</strong>: 通过压缩表示回放以减少输入样本需求。</li>
</ul>
</li>
</ul>
<h3 id="2-2-骨架动作识别"><a href="#2-2-骨架动作识别" class="headerlink" title="2.2 骨架动作识别"></a>2.2 骨架动作识别</h3><ul>
<li>主流方法：基于 CNN、图卷积网络 (GCN)、时空图卷积网络 (ST-GCN)。</li>
<li>挑战：需设计能处理新任务且避免遗忘的网络。</li>
</ul>
<h3 id="2-3-动态网络架构"><a href="#2-3-动态网络架构" class="headerlink" title="2.3 动态网络架构"></a>2.3 动态网络架构</h3><ul>
<li>SkipNet 和 MutualNet 等动态网络可根据输入数据调整架构。</li>
<li>Else-Net 的动态学习块搜索功能受此启发，结合动作语义进行动态路径选择。</li>
</ul>
<hr>
<h2 id="3-弹性语义网络-Else-Net"><a href="#3-弹性语义网络-Else-Net" class="headerlink" title="3. 弹性语义网络 (Else-Net)"></a>3. 弹性语义网络 (Else-Net)</h2><h3 id="3-1-模块架构"><a href="#3-1-模块架构" class="headerlink" title="3.1 模块架构"></a>3.1 模块架构</h3><p><img src="/2024/11/24/16-26-10/QQ_1732510011895.png" alt=" "></p>
<p>假设 Else-Net 包含 $N$ 个弹性单元（Elastic Units），每个弹性单元 $n$（$n \in \{1, 2, \dots, N\}$）由若干学习块（Learning Blocks）和一个切换块（Switch Block）组成。学习块表示为：</p>
<script type="math/tex; mode=display">\{f_{\theta_{i,n}}(\cdot)\}_{i=1}^{B_n},</script><p>其中，$\theta_{i,n}$ 是第 $i$ 个学习块的参数，$B_n$ 是第 $n$ 个弹性单元中的学习块数量。为了捕获新知识，第 $n$ 个弹性单元临时添加一个新学习块 $f_{\theta_{B_n+1,n}}(\cdot)$，以便存储新知识。</p>
<p>输入特征 $x_n$ 被传递给第 $n$ 个弹性单元中的所有学习块，生成对应的潜在特征：</p>
<script type="math/tex; mode=display">z_{i,n} = f_{\theta_{i,n}}(x_n), \quad i \in \{1, \dots, B_n + 1\}.</script><p>这些特征通过切换块的门控模块 $g$ 和 Gumbel Softmax，生成一个 one-hot 匹配向量 $b_n$：</p>
<script type="math/tex; mode=display">b_n = \text{Gumbel Softmax}(g(z_{1,n}), g(z_{2,n}), \dots, g(z_{B_n+1,n})).</script><p>如果输入特征 $x_n$ 与第 $i$ 个学习块最匹配，则 $g(z_{i,n})$ 的输出值会高于其他块，并通过 Gumbel Softmax 将该块激活。</p>
<p><strong>Gumbel Softmax</strong>：</p>
<p>假设给定概率分布 $ p = (p_1, p_2, \dots, p_n) $，Gumbel Softmax 通过以下步骤生成一个平滑的离散随机变量：</p>
<ol>
<li><p><strong>生成 Gumbel 噪声：</strong><br>对于每个类别 $ i $，从 Gumbel 分布中采样 $ g_i \sim \text{Gumbel}(0, 1) $。</p>
</li>
<li><p><strong>计算加噪声的 logits：</strong>  </p>
<script type="math/tex; mode=display">
y_i = \frac{\log(p_i) + g_i}{\tau}</script><p>其中，$ p_i $ 是类别 $ i $ 的概率，$ g_i $ 是对应的 Gumbel 噪声，$ \tau $ 是温度参数。</p>
</li>
<li><p><strong>使用 Softmax 函数进行归一化：</strong>  </p>
<script type="math/tex; mode=display">
y_i = \frac{\exp\left( \frac{\log(p_i) + g_i}{\tau} \right)}{\sum_{j=1}^n \exp\left( \frac{\log(p_j) + g_j}{\tau} \right)}</script><p>这是一个平滑的概率分布，经过温度调节，可以用来近似离散采样。</p>
</li>
<li><p><strong>采样：</strong><br>通过这个平滑的概率分布，我们可以进行采样，得到一个类别的近似选择。</p>
</li>
</ol>
<ul>
<li><strong>可微性：</strong> Gumbel Softmax 使得离散变量的选择过程可导，可以通过反向传播进行梯度优化。</li>
<li><strong>平滑近似：</strong> 温度参数 $ \tau $ 控制了采样过程的离散度，当 $ \tau \to 0 $ 时，结果趋近于 one-hot 向量；而当 $ \tau \to \infty $ 时，结果变得平滑。</li>
</ul>
<p>Gumbel Softmax 广泛应用于以下领域：</p>
<ol>
<li><strong>生成模型：</strong> 如生成对抗网络（GANs）和变分自编码器（VAEs）中，生成离散数据时，使用 Gumbel Softmax 来近似离散采样。</li>
<li><strong>强化学习：</strong> 在强化学习中，使用 Gumbel Softmax 来处理离散动作空间，使得离散动作的选择可导。</li>
<li><strong>自然语言处理：</strong> 在文本生成等任务中，使用 Gumbel Softmax 来平滑采样。</li>
</ol>
<p>假设有三个类别的概率分布 $ p = [0.7, 0.2, 0.1] $，我们可以用 Gumbel Softmax 来选择一个类别：</p>
<ol>
<li>为每个类别生成 Gumbel 噪声 $ g_1, g_2, g_3 $。</li>
<li>计算加噪声的 logits $ y_1, y_2, y_3 $。</li>
<li>用 Softmax 函数归一化并生成平滑的概率分布。</li>
</ol>
<p>在给定温度 $ \tau = 0.5 $ 时，得到的 $ y_i $ 值接近于一个 one-hot 向量，表示选择某个类别。</p>
<p>Gumbel-Softmax 和 硬选择（Hardmax） 的区别</p>
<p>Gumbel-Softmax（$\tau \to 0$）:</p>
<ul>
<li><strong>概率分布</strong>：平滑的概率分布，温度趋近于 0 时，逐渐接近 one-hot 向量。</li>
<li><strong>随机性</strong>：具有随机性，即使在温度较低时，输出仍然是基于概率的。</li>
<li><strong>平滑性</strong>：随着温度的降低，逐渐过渡到硬选择，避免突然的离散化。</li>
<li><strong>训练过程</strong>：避免了梯度问题，支持平滑的梯度传递，适合连续的训练过程。</li>
<li><strong>适用场景</strong>：适合需要探索不同选择、避免过拟合的训练过程。</li>
</ul>
<p>硬选择（Hardmax）:</p>
<ul>
<li><strong>概率分布</strong>：直接选择最大概率类别，输出一个离散的 one-hot 向量。</li>
<li><strong>随机性</strong>：没有随机性，始终选择最大概率的类别。</li>
<li><strong>平滑性</strong>：没有平滑，直接进行硬选择，不能避免离散化过渡。</li>
<li><strong>训练过程</strong>：由于没有梯度，可能导致训练不稳定。</li>
<li><strong>适用场景</strong>：适用于需要明确选择类别且不需要平滑过渡的情况。</li>
</ul>
<p>关键区别</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>特点</strong></th>
<th><strong>Gumbel-Softmax（$\tau \to 0$）</strong></th>
<th><strong>硬选择（Hardmax）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>概率分布</strong></td>
<td>平滑的概率分布</td>
<td>直接产生一个 one-hot 向量</td>
</tr>
<tr>
<td><strong>是否具有随机性</strong></td>
<td>具有一定的随机性</td>
<td>没有随机性，固定选择</td>
</tr>
<tr>
<td><strong>平滑性</strong></td>
<td>在 $\tau$ 很低时趋近于 one-hot</td>
<td>没有平滑，直接选择最大值</td>
</tr>
<tr>
<td><strong>训练过程</strong></td>
<td>能够避免梯度问题</td>
<td>由于没有梯度，可能会导致训练不稳定</td>
</tr>
<tr>
<td><strong>实现复杂性</strong></td>
<td>需要计算 Gumbel 噪声并进行 softmax</td>
<td>直接取最大值</td>
</tr>
</tbody>
</table>
</div>
<p>总结</p>
<ul>
<li><strong>Gumbel-Softmax</strong> 提供了平滑的过渡，避免梯度问题，适合需要探索的训练过程。</li>
<li><strong>硬选择</strong> 是完全离散的，没有随机性和渐进的平滑过渡，适合需要明确类别选择的情况。</li>
</ul>
<p><a href="https://wmathor.com/index.php/archives/1595/">Gumbel-Softmax解析</a></p>
<p>对于参数更新，仅更新与当前输入最相关的学习块参数，其余块的参数保持冻结。更新规则如下：</p>
<script type="math/tex; mode=display">\theta_{i,n} \leftarrow \theta_{i,n} - \alpha \nabla_{\theta_{i,n}} [-b_{i,n} \cdot y_k \log \hat{y}_k],</script><p>其中，$y_k$ 和 $\hat{y}_k$ 分别是当前输入的真实标签和预测标签，$b_{i,n}$ 是匹配分数（$1$ 表示最佳匹配块，$0$ 表示其他块），$\alpha$ 是学习率。</p>
<p><strong>交叉熵解释</strong></p>
<p>该公式是交叉熵损失函数的一部分。在交叉熵损失函数中，每个类别的损失被计算为：<br>$\sum_{k} y_k \log \hat{y}_k$<br>这里，$(y_k)$是真实标签（one-hot 编码），而 $( \hat{y}_k )$ 是模型的预测值。<br>如果 ( k ) 是真实类别，则 ( y_k = 1 )，此时损失函数成为：<br>$\log \hat{y}_k$<br>这意味着，如果模型对正确类别的预测概率较高，损失较小；如果模型对正确类别的预测概率较低，损失会变大。</p>
<p>通过 $N$ 个弹性单元的最相关学习块连接起来，Else-Net 构建了一个最优的语义路径，利用已有知识高效地学习新动作。</p>
<hr>
<h2 id="3-2-Pathway-Construction-for-Body-Part-Branches"><a href="#3-2-Pathway-Construction-for-Body-Part-Branches" class="headerlink" title="3.2 Pathway Construction for Body Part Branches"></a>3.2 Pathway Construction for Body Part Branches</h2><p><img src="/2024/11/24/16-26-10/QQ_1732514917546.png" alt=" "></p>
<p><img src="/2024/11/24/16-26-10/QQ_1732515066509.png" alt=" "></p>
<p>考虑到全身动作可能与先前动作差异显著，而局部身体部分可能存在共享的特征，Else-Net 将输入特征分解为五个身体部位特征：</p>
<script type="math/tex; mode=display">\{x_{p_j}\}_{j=1}^5,</script><p>对应左臂、右臂、躯干、左腿和右腿。每个身体部位分支的架构与全身分支相同，每个分支包含 $N$ 个弹性单元。<br>每个分支独立搜索和构建与当前输入特征最匹配的语义路径，分别生成每个部位的潜在特征：</p>
<script type="math/tex; mode=display">\{x'_{p_j}\}_{j=1}^5.</script><p>最终，通过将这些部位特征拼接，得到完整的全身特征：</p>
<script type="math/tex; mode=display">x' = \text{concat}(x'_{p_1}, x'_{p_2}, x'_{p_3}, x'_{p_4}, x'_{p_5}).</script><p>这种结构设计能够有效利用局部共享特征，提升对新动作的学习能力，同时缓解遗忘问题。</p>
<blockquote>
<p>如何分解特征。</p>
</blockquote>
<hr>
<h2 id="3-3-Training-and-Testing"><a href="#3-3-Training-and-Testing" class="headerlink" title="3.3 Training and Testing"></a>3.3 Training and Testing</h2><p>训练阶段包括两个优化步骤：外部优化和内部优化。</p>
<h3 id="外部优化"><a href="#外部优化" class="headerlink" title="外部优化"></a>外部优化</h3><p>冻结所有学习块的参数，仅更新切换块的参数：</p>
<script type="math/tex; mode=display">\theta_g \leftarrow \theta_g - \alpha \nabla_{\theta_g} [-y_k \log \hat{y}_k].</script><h3 id="内部优化"><a href="#内部优化" class="headerlink" title="内部优化"></a>内部优化</h3><p>冻结切换块的参数，仅更新选中学习块的参数：</p>
<script type="math/tex; mode=display">\theta_m \leftarrow \theta_m - \alpha \nabla_{\theta_m} [-b \cdot y_k \log \hat{y}_k].</script><p>其中，$\theta_g$ 和 $\theta_m$ 分别是切换块和学习块的参数，$b$ 是学习块的匹配分数。</p>
<p>测试阶段，输入动作通过语义路径搜索最相关的学习块，并生成全局特征，用于动作分类。</p>
<hr>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><h3 id="4-1-数据集"><a href="#4-1-数据集" class="headerlink" title="4.1 数据集"></a>4.1 数据集</h3><ul>
<li><strong>NTU RGB+D</strong>: 60 类动作，56,880 视频，分为交叉视角和交叉主体两种评估协议。</li>
<li><strong>PKU-MMD</strong>: 51 类动作，1,076 视频，类似评估协议。</li>
</ul>
<h3 id="4-2-评估指标"><a href="#4-2-评估指标" class="headerlink" title="4.2 评估指标"></a>4.2 评估指标</h3><ul>
<li><strong>平均准确率 (ACC)</strong>: 学完所有任务后，对所有任务的平均识别准确率。<br>$\begin{aligned}ACC=\frac1T\sum_{q=1}^Ta_{T,q}\end{aligned}$</li>
<li><strong>遗忘度 (FM)</strong>: 表征模型对旧任务的遗忘程度，越低越好。<br>$\begin{aligned}FM&amp;=\frac{1}{T-1}\sum_{q=1}^{T-1}\max_{t\in\{1,2,…,T-1\}}\{a_{t,q}-a_{T,q}\}\end{aligned}$</li>
<li><strong>学习准确率 (LA)</strong>: 表征模型对当前任务的学习能力。<br>$\begin{aligned}LA=\frac1T\sum_{q=1}^Ta_{q,q}\end{aligned}$</li>
</ul>
<p><img src="/2024/11/24/16-26-10/QQ_1731649890669.png" alt=" "></p>
<h3 id="4-3-结果分析"><a href="#4-3-结果分析" class="headerlink" title="4.3 结果分析"></a>4.3 结果分析</h3><ul>
<li><strong>持续学习</strong>:<br>Else-Net 在两个数据集上均显著优于现有方法（如 GEM 和 ReMind）。</li>
<li><strong>离线学习</strong>:<br>Else-Net 在离线学习设置下也取得与最优方法相当的结果。</li>
</ul>
<h3 id="4-4-消融实验"><a href="#4-4-消融实验" class="headerlink" title="4.4 消融实验"></a>4.4 消融实验</h3><ul>
<li><strong>弹性单元数量</strong>: 增加单元数量可提升性能，但超过 3 层时增益减少。</li>
<li><strong>部件路径的作用</strong>: 使用语义分支显著降低遗忘度，提高学习性能。</li>
<li><strong>块搜索的作用</strong>: 动态块搜索避免混合噪声，提高知识保留能力。</li>
<li><strong>选择性更新</strong>: 仅更新最相关块有助于知识保留和有效学习。</li>
</ul>
<hr>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><ul>
<li>Else-Net 模拟人类大脑，动态选择最相关学习块学习新动作，同时保留旧知识。</li>
<li>通过分解语义路径的构建，能更有效地捕获人体局部同质特征，缓解灾难性遗忘问题。</li>
<li>实验结果验证了 Else-Net 在持续动作识别中的有效性。</li>
</ul>
]]></content>
      <categories>
        <category>skeleton</category>
      </categories>
      <tags>
        <tag>skeleton, AR</tag>
      </tags>
  </entry>
  <entry>
    <title>掩码与压缩：持续学习中基于骨架的高效动作识别</title>
    <url>/2024/11/24/11-18-59/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>CL</category>
      </categories>
      <tags>
        <tag>AR, CL, skeleton</tag>
      </tags>
  </entry>
  <entry>
    <title>持续学习的方法</title>
    <url>/2024/11/21/08-40-12/</url>
    <content><![CDATA[<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p><a href="https://github.com/Vision-Intelligence-and-Robots-Group/Best-Incremental-Learning">best incremental learning github</a></p>
<h1 id="持续学习（Continual-Learning）最新综述"><a href="#持续学习（Continual-Learning）最新综述" class="headerlink" title="持续学习（Continual Learning）最新综述"></a>持续学习（Continual Learning）最新综述</h1><h2 id="背景与挑战"><a href="#背景与挑战" class="headerlink" title="背景与挑战"></a>背景与挑战</h2><p>持续学习（Continual Learning, CL）旨在使模型在不断接收新任务的过程中，不遗忘旧任务的知识，同时能够高效地适应新任务。这一领域的关键挑战包括：</p>
<ul>
<li><strong>灾难性遗忘</strong>：模型在学习新任务时会覆盖旧任务的知识。</li>
<li><strong>任务间冲突</strong>：新旧任务目标函数的梯度冲突，导致模型难以协调优化。</li>
<li><strong>存储限制</strong>：部分场景无法存储历史数据，增加了设计的复杂性。</li>
</ul>
<p>为了解决这些问题，研究者提出了多种技术方案，包括特征回放、提示学习（Prompting）、正则化方法以及生成模型。</p>
<hr>
<h2 id="模型分类与最新进展"><a href="#模型分类与最新进展" class="headerlink" title="模型分类与最新进展"></a>模型分类与最新进展</h2><p>以下是最新的持续学习模型分类及其特点，包含特征回放、生成式模型、正则化方法等不同范式。</p>
<h3 id="1-特征回放（Feature-Replay）"><a href="#1-特征回放（Feature-Replay）" class="headerlink" title="1. 特征回放（Feature Replay）"></a>1. 特征回放（Feature Replay）</h3><p>通过保存历史任务的特征或中间表示，避免直接存储原始数据：</p>
<ul>
<li><p><strong>Experience Replay (ER)</strong><br>使用历史数据的特征与新任务共同训练，简单高效。</p>
</li>
<li><p><strong>ER-ACE</strong><br>优化了损失函数，避免回放数据对当前任务的干扰。</p>
</li>
<li><p><strong>Dark Experience Replay (DER)</strong> &amp; <strong>DER++</strong><br>保存网络输出的表征，用于提高特征一致性，特别是 <strong>DER++</strong> 进一步改进了模型泛化性能。</p>
</li>
<li><p><strong>Meta-Experience Replay (MER)</strong><br>基于元学习的方法优化特征回放的策略。</p>
</li>
<li><p><strong>iCaRL (Incremental Classifier and Representation Learning)</strong><br>保存每个类别的特征均值，结合最近邻分类器完成任务。</p>
</li>
<li><p><strong>LiDER 系列</strong><br>基于 DER++ 和 ER-ACE，优化了特征回放的选择策略。</p>
</li>
</ul>
<hr>
<h3 id="2-提示学习（Prompt-Learning）"><a href="#2-提示学习（Prompt-Learning）" class="headerlink" title="2. 提示学习（Prompt Learning）"></a>2. 提示学习（Prompt Learning）</h3><p>通过为不同任务设计动态提示，减少灾难性遗忘：</p>
<ul>
<li><p><strong>DualPrompt</strong><br>结合提示学习与特征回放，在任务切换时动态调整提示。</p>
</li>
<li><p><strong>L2P (Learning to Prompt)</strong><br>基于 Transformer，自动生成适应新任务的提示。</p>
</li>
<li><p><strong>CODA-Prompt</strong><br>解构任务并动态生成提示，无需保存历史数据。</p>
</li>
<li><p><strong>STAR-Prompt</strong><br>提出了两阶段动态提示，分别用于特征学习和分类任务。</p>
</li>
</ul>
<hr>
<h3 id="3-生成式模型（Generative-Replay）"><a href="#3-生成式模型（Generative-Replay）" class="headerlink" title="3. 生成式模型（Generative Replay）"></a>3. 生成式模型（Generative Replay）</h3><p>使用生成模型生成历史任务数据进行训练：</p>
<ul>
<li><p><strong>Continual Generative Training for Incremental Prompt-Learning (CGIL)</strong><br>基于生成模型对任务特征进行增量优化。</p>
</li>
<li><p><strong>Hindsight Anchor Learning (HAL)</strong><br>使用生成式方法生成任务锚点，降低任务间冲突。</p>
</li>
</ul>
<hr>
<h3 id="4-正则化方法（Regularization-Based-Methods）"><a href="#4-正则化方法（Regularization-Based-Methods）" class="headerlink" title="4. 正则化方法（Regularization-Based Methods）"></a>4. 正则化方法（Regularization-Based Methods）</h3><p>通过约束参数更新，减少灾难性遗忘：</p>
<ul>
<li><p><strong>EWC (Elastic Weight Consolidation)</strong><br>通过加权惩罚约束关键参数的更新。</p>
</li>
<li><p><strong>Synaptic Intelligence (SI)</strong><br>动态调整正则项的权重，以适应新任务。</p>
</li>
<li><p><strong>Function Distance Regularization (FDR)</strong><br>通过约束特征空间距离，减少特征漂移。</p>
</li>
</ul>
<hr>
<h3 id="5-其他方法"><a href="#5-其他方法" class="headerlink" title="5. 其他方法"></a>5. 其他方法</h3><ul>
<li><p><strong>GDumb</strong><br>简单地保存少量数据用于训练，作为强基线。</p>
</li>
<li><p><strong>X-DER (eXtended-DER)</strong><br>扩展了 DER 方法，结合正则化和生成式回放技术。</p>
</li>
<li><p><strong>CLIP &amp; AttriCLIP</strong><br>在多模态任务中，结合预训练模型实现持续学习。</p>
</li>
</ul>
<hr>
<h2 id="持续学习未来方向"><a href="#持续学习未来方向" class="headerlink" title="持续学习未来方向"></a>持续学习未来方向</h2><ol>
<li><strong>统一框架设计</strong>：整合特征回放、提示学习和生成式模型的优点。</li>
<li><strong>小样本持续学习</strong>：在数据有限的情况下提高模型性能。</li>
<li><strong>可解释性增强</strong>：研究参数更新和知识保存的可视化与分析。</li>
<li><strong>多模态持续学习</strong>：扩展到图像、文本、音频等多模态数据场景。</li>
</ol>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>持续学习领域的研究正在快速发展，各类方法在减少灾难性遗忘、提高新任务适应性方面各有特点。结合特征回放与提示学习的技术，例如 <strong>DualPrompt</strong> 和 <strong>LiDER 系列</strong>，表现出强大的泛化能力。未来的研究将继续朝着更加高效、通用的方向发展。</p>
<hr>
<h1 id="Function-Distance-Regularization-FDR-模型综述"><a href="#Function-Distance-Regularization-FDR-模型综述" class="headerlink" title="Function Distance Regularization (FDR) 模型综述"></a>Function Distance Regularization (FDR) 模型综述</h1><p><strong>Function Distance Regularization (FDR)</strong> 是持续学习中的一个关键方向，通过约束任务间特征空间的距离，减少新旧任务特征分布的漂移。以下是一些典型模型及其论文：</p>
<hr>
<h2 id="1-Hindsight-Anchor-Learning-HAL"><a href="#1-Hindsight-Anchor-Learning-HAL" class="headerlink" title="1. Hindsight Anchor Learning (HAL)"></a>1. <strong>Hindsight Anchor Learning (HAL)</strong></h2><ul>
<li><strong>特点</strong>：<br>HAL 为每个任务设置“锚点”（anchor），保持新旧任务特征分布一致性，用固定参考减少特征漂移。</li>
<li><strong>论文名称</strong>：<br><em>Hindsight Anchor Learning for Continual Learning</em>  </li>
<li><strong>技术细节</strong>：<br>计算当前任务特征与锚点的距离损失，优化特征空间。</li>
</ul>
<hr>
<h2 id="2-Gradient-Episodic-Memory-GEM"><a href="#2-Gradient-Episodic-Memory-GEM" class="headerlink" title="2. Gradient Episodic Memory (GEM)"></a>2. <strong>Gradient Episodic Memory (GEM)</strong></h2><ul>
<li><strong>特点</strong>：<br>使用梯度投影技术，在优化新任务时，确保特征更新不会破坏旧任务。</li>
<li><strong>论文名称</strong>：<br><em>Gradient Episodic Memory for Continual Learning</em>  </li>
<li><strong>技术细节</strong>：<br>约束新任务梯度方向，使其与旧任务梯度变化一致，从而优化特征分布。</li>
</ul>
<hr>
<h2 id="3-Meta-Experience-Replay-MER"><a href="#3-Meta-Experience-Replay-MER" class="headerlink" title="3. Meta-Experience Replay (MER)"></a>3. <strong>Meta-Experience Replay (MER)</strong></h2><ul>
<li><strong>特点</strong>：<br>结合经验回放和元学习，对齐任务间特征空间的变化，减少遗忘。</li>
<li><strong>论文名称</strong>：<br><em>Meta-Experience Replay for Continual Learning</em>  </li>
<li><strong>技术细节</strong>：<br>动态调整样本权重，优化新旧任务特征距离。</li>
</ul>
<hr>
<h2 id="4-Synaptic-Intelligence-SI"><a href="#4-Synaptic-Intelligence-SI" class="headerlink" title="4. Synaptic Intelligence (SI)"></a>4. <strong>Synaptic Intelligence (SI)</strong></h2><ul>
<li><strong>特点</strong>：<br>使用重要性权重约束任务特征更新，对旧任务重要的特征赋予更高正则化权重。</li>
<li><strong>论文名称</strong>：<br><em>Continual Learning Through Synaptic Intelligence</em>  </li>
<li><strong>技术细节</strong>：<br>计算参数更新的重要性，减少对旧任务关键特征的干扰。</li>
</ul>
<hr>
<h2 id="5-Regular-Polytope-Classifier-RPC"><a href="#5-Regular-Polytope-Classifier-RPC" class="headerlink" title="5. Regular Polytope Classifier (RPC)"></a>5. <strong>Regular Polytope Classifier (RPC)</strong></h2><ul>
<li><strong>特点</strong>：<br>强调特征点在几何空间中的一致性，利用正则化约束新旧任务特征在多边形空间中的投影。</li>
<li><strong>论文名称</strong>：<br><em>Regular Polytope Networks for Incremental Learning</em>  </li>
<li><strong>技术细节</strong>：<br>通过几何对齐优化特征分布，减少新旧任务分类界限偏移。</li>
</ul>
<hr>
<p>以上模型通过不同方式优化特征分布，减少新任务学习对旧任务的干扰，是 Function Distance Regularization 的典型实现。</p>
<h1 id="三种增量学习的区别"><a href="#三种增量学习的区别" class="headerlink" title="三种增量学习的区别"></a>三种增量学习的区别</h1><h2 id="模型明确知道当前评分属于某一个特定区间的含义"><a href="#模型明确知道当前评分属于某一个特定区间的含义" class="headerlink" title="模型明确知道当前评分属于某一个特定区间的含义"></a>模型明确知道当前评分属于某一个特定区间的含义</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a><strong>定义</strong></h3><p>当模型明确知道当前评分属于某一个特定区间时，表示在训练或测试过程中，每次输入数据时，模型会接收到一个“任务标记”或“区间信息”，明确告知模型当前任务的范围。</p>
<hr>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a><strong>特点</strong></h3><ol>
<li><p><strong>明确任务边界</strong></p>
<ul>
<li>模型知道当前任务对应的评分区间，例如 “0-20”。</li>
<li>模型只需处理这个范围内的数据，其他区间的数据对当前任务无影响。</li>
</ul>
</li>
<li><p><strong>任务切换</strong></p>
<ul>
<li>如果切换到 “20-40” 区间，则任务标记会更新。</li>
<li>模型能够根据任务标记进行专门的优化，无需处理全局关系。</li>
</ul>
</li>
<li><p><strong>简化学习</strong></p>
<ul>
<li>模型不需要自行判断数据属于哪个区间，任务标记提供了显式的指导。</li>
<li>减少了任务间的歧义和干扰。</li>
</ul>
</li>
<li><p><strong>典型应用</strong></p>
<ul>
<li><strong>任务增量学习（Task-Incremental Learning, TIL）</strong><ul>
<li>通过任务标记显式区分任务。</li>
<li>每个任务相对独立，模型在不同任务间的干扰较少。</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="域增量学习"><a href="#域增量学习" class="headerlink" title="域增量学习"></a><strong>域增量学习</strong></h3><ol>
<li><p><strong>定义</strong></p>
<ul>
<li>域增量学习（Domain-Incremental Learning, DIL）中，模型接收不同领域（如不同数据分布）的数据流，并试图学习适应这些变化的能力。</li>
<li>但模型无法通过显式任务标记识别当前数据来自哪个领域。</li>
</ul>
</li>
<li><p><strong>特点</strong></p>
<ul>
<li>模型必须对不同领域的特征分布自适应，而不依赖于任务提示。</li>
<li>比如动作质量评估中，评分数据可能来自不同相机的拍摄，域增量学习要求模型跨相机（领域）泛化。</li>
</ul>
</li>
<li><p><strong>对比</strong></p>
<ul>
<li>与任务增量不同，域增量学习不提供显式标记，强调跨领域一致性。</li>
<li>与类增量相似，需自主应对数据分布变化，但着重领域变化而非类别增加。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a><strong>示例</strong></h3><p>在动作质量评估（AQA）任务中：</p>
<ul>
<li><strong>任务增量学习</strong>：每个评分区间（如 <code>0-20</code>、<code>20-40</code>）是独立任务，提供区间标记。</li>
<li><strong>域增量学习</strong>：评分区间相同，但数据分布因拍摄设备、场景等变化，模型需跨领域泛化。</li>
<li><strong>类增量学习</strong>：评分范围内新增类别（如从动作类型 A 增加到类型 B），需对新类别进行学习。</li>
</ul>
<hr>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th>学习类别</th>
<th>特点</th>
<th>应用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>类增量学习</strong></td>
<td>增加新类别，模型需学习新知识，不忘旧知识</td>
<td>分类任务，如新增动作类型</td>
</tr>
<tr>
<td><strong>任务增量学习</strong></td>
<td>明确任务边界，任务间独立</td>
<td>评分区间划分，明确每段评分区间</td>
</tr>
<tr>
<td><strong>域增量学习</strong></td>
<td>数据分布变化，需跨领域适应</td>
<td>同评分范围内，不同设备、场景或环境的数据分布差异</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Mammoth框架"><a href="#Mammoth框架" class="headerlink" title="Mammoth框架"></a>Mammoth框架</h1><h2 id="配置解析"><a href="#配置解析" class="headerlink" title="配置解析"></a>配置解析</h2><h3 id="parse-args解析配置："><a href="#parse-args解析配置：" class="headerlink" title="parse_args解析配置："></a>parse_args解析配置：</h3><p><code>add_initial_args</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">--dataset</span><br><span class="line">--model</span><br><span class="line">--backbone</span><br><span class="line">--load_best_args</span><br></pre></td></tr></table></figure>
<p><code>add_configuration_args</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">--dataset_config:   </span><br><span class="line">  by choices=get_dataset_config_names(args.dataset)</span><br><span class="line">--model_config</span><br><span class="line">  models/config/&lt;model&gt;.yaml</span><br></pre></td></tr></table></figure>
<p>type=field_with_aliases({‘default’: [‘base’, ‘default’], ‘best’: [‘best’]}):别名连接</p>
<p><code>load_configs</code>:加载配置文件<br>get_model_class，返回数据集类型，dataset_name 就是datasets/{name}.py。<br>get_default_args_for_dataset(args.dataset)：加载数据集的参数，在数据集中定义。<br>如果模型参数中没有dataset_config，就使用数据集中的参数。<br>可以指定参数名称，在数据集文件夹下，否则使用default.yaml。<br>最后返回数据集，模型，基本参数。</p>
<p><code>add_dynamic_parsable_args(parser, args.dataset, backbone)</code>:动态加载参数。<br>如果参数是字典类型，当做参数解析出来。</p>
<p><code>add_management_args(parser)</code>:添加管理参数</p>
<p><code>add_experiment_args(parser)</code>: 添加实验参数</p>
<p><code>update_cli_defaults(parser, config)</code>：配置文件加载参数</p>
<p>get_dataset获取数据类：</p>
<p><code>extend_args(args, dataset</code>：加载数据类的额外参数<br>backbone = get_backbone(args)：</p>
<p>基础参数要在命令行中输入:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sys.argv = [&#x27;CAQA&#x27;,</span><br><span class="line">            &#x27;--dataset&#x27;,</span><br><span class="line">            &#x27;class_rg&#x27;,</span><br><span class="line">            &#x27;--model&#x27;,</span><br><span class="line">            &#x27;fea_gr&#x27;,</span><br><span class="line">            &#x27;--backbone&#x27;,</span><br><span class="line">            &#x27;resnet18&#x27;,</span><br><span class="line">            &#x27;--buffer_size&#x27;,</span><br><span class="line">            &#x27;10&#x27;</span><br><span class="line">            ]</span><br></pre></td></tr></table></figure>
<h2 id="datasets"><a href="#datasets" class="headerlink" title="datasets"></a>datasets</h2><h3 id="class-rg"><a href="#class-rg" class="headerlink" title="class_rg"></a>class_rg</h3><p>修改dataload，改为自己的数据加载器。</p>
]]></content>
      <categories>
        <category>CL</category>
      </categories>
      <tags>
        <tag>AQA, CL, CAQA</tag>
      </tags>
  </entry>
  <entry>
    <title>AGSG多类任务持续学习</title>
    <url>/2024/11/20/21-30-30/</url>
    <content><![CDATA[<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><h2 id="训练命令"><a href="#训练命令" class="headerlink" title="训练命令"></a>训练命令</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python run_net.py --exp_name your_exp_name \</span><br><span class="line">  --gpu 0 --seed 0 --approach g_e_graph \</span><br><span class="line">  --lambda_distill 9 --lambda_diff 0.7 \</span><br><span class="line">  --replay --replay_method group_replay --memory_size 30 \</span><br><span class="line">  --diff_loss \</span><br><span class="line">  --aug_approach aug-diff --aug_mode fs_aug --num_helpers 7 --aug_scale 0.3 \</span><br><span class="line">  --save_graph --g_e_graph --fix_graph_mode no_fix \</span><br><span class="line">  --save_ckpt \</span><br><span class="line">  --optim_mode new_optim --lr_decay --num_epochs 200 --batch-size 16 --alpha 0.8 </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>AQA</category>
      </categories>
      <tags>
        <tag>AQA, CAQA</tag>
      </tags>
  </entry>
  <entry>
    <title>AQA领域的相关实验室，研究者，仓库</title>
    <url>/2024/11/19/11-38-16/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="8ce85e62b5efd1efa748fb29abbdbae8e97e40eda91e6cb625e4b51ad0b3084e">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>AQA</category>
      </categories>
      <tags>
        <tag>AQA</tag>
      </tags>
  </entry>
  <entry>
    <title>AQA领域相关实验的说明</title>
    <url>/2024/11/19/11-37-55/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="9b3a3a58e3c8ca467339d1ad2512645a72a7b3d289fa5b6a80eeb847eaa83d34">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>AQA</category>
      </categories>
      <tags>
        <tag>AQA</tag>
      </tags>
  </entry>
  <entry>
    <title>post</title>
    <url>/2024/11/16/07-47-05/</url>
    <content><![CDATA[<p>IJCAI 2024</p>
<h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>the inherent struggle of these backbones to capture the subtle cues essential for AQA,gains of 5.49% and 3.55% on Rhythmic Gymnastics and Fis-V.</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>involves leveraging the backbone pre-trained on large-scale action recognition datasets to adapt the score regression requirements of small-scale AQA datasets.</p>
<p>two critical issues: domain shift and a severe risk of overfitting.</p>
<p><code>characterizing AQA as a coarse-to-fine classification task</code><br>using an MLP classifier to predict the coarse-grained grades, a fine-grained classification task to discern variations within each grade.</p>
<blockquote>
<p>ST-GCN, 预训练的模型存在差异。</p>
</blockquote>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>present an innovative solution—a novel pre-training alignment for AQA by aligning it with the pre-trained task.</p>
<p>Neural Collapse(fall): study AQA from the neural collapse perspective, has interpretability(understand, explain).</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h1 id><a href="#" class="headerlink" title=" "></a> </h1><p><img src="/2024/11/16/07-47-05/QQ_1731720481805.png" alt=" "></p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1>]]></content>
      <categories>
        <category>AQA</category>
      </categories>
      <tags>
        <tag>AQA</tag>
      </tags>
  </entry>
  <entry>
    <title>MAGR-CAQA持续学习</title>
    <url>/2024/11/14/07-37-25/</url>
    <content><![CDATA[<p>论文：MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment<br>code：<a href="https://github.com/ZhouKanglei/MAGR_CAQA">github</a></p>
<h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><p>面向FineDiving和LOGO(26个游泳项目),每个样本8名运动员</p>
<p>区别：传统持续学习，分类任务，AQA是回归任务。</p>
<p>两种回放方式：<br>经验回放：储存数据或特征。特征重放：有隐私保护的重放特征</p>
<p>创新点：新的特征重放方法-MAGR<br>使用的还是旧特征，还是一种经验回放。</p>
<p>有点类似：墙面背景变化去识别螺帽。</p>
<p>C3D骨干网络更新，适应新特征，存在与旧特征不匹配风险。</p>
<p>动态AQA：运动员在康复过程中的AQA，随着恢复而变化。</p>
<p>传统CL：主要在解决公式1。</p>
<p>改进点：有序均匀采样OUS，选择存储特征。</p>
<p>模拟真实世界的技能变化，我们提出了一种新颖的CAQA成绩递增设置：<br>按照分数分布提出不同的task，把数据集划分为不同分数的task。把AQA-7数据集里6个类别，<code>只用了跳水</code>，每个类别划分不同的task。每次训练只训练一个类别。</p>
<blockquote>
<p>是否可以交换顺序？，先学习高分，在学习低分。</p>
<p>不同任务如何设置task，所有类别一起训练</p>
</blockquote>
<p>MTL-AQA：是一起划分task：全是跳水，3m，10m，单人双人。</p>
<blockquote>
<p>为什么不分类别？</p>
</blockquote>
<p>每个task，少量样本训练，剩下样本微调，测试时对task所有样本评估。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>指标：aft好像是自己新建的。</p>
<p><strong>Memory-Free Methods</strong>利用模型结构、正则化和优化策略来减轻灾难性遗忘<br><strong>Memory-Based Approaches</strong>（基于记忆的方法）是<strong>持续学习</strong>中应对灾难性遗忘的一类重要策略。这类方法通过<strong>显式存储旧任务的样本或特征</strong>，在学习新任务时定期回顾</p>
<p>table-1：整体训练，用来证明模型有对所有数据学习的能力，改变流行数据后，因为遗忘导致能力下降。<br>在和持续学习对比，不是AQA对比。</p>
<p>fig-7： 每个task重放10个特征。</p>
<p>最后：<br>MAGR只适用于AQA任务，不知道和其他CL方法的对比是什么样的。从分类进入回归任务。</p>
<p>应该是属于持续学习领域的连续任务，而不是AQA领域的持续学习。</p>
<h2 id="持续学习"><a href="#持续学习" class="headerlink" title="持续学习"></a>持续学习</h2><p>主要方法：<br>正则化，元持续学习，记忆回放，在线和不平衡持续学习，方差减少技术。</p>
<p>图正则化：通过构建一个图，其中每个节点表示一个图像类别，边表示类别间的相似性（例如，猫和狗可能有相似的特征，可以有较强的边连接），模型在学习新类别（如马、鱼）时，通过图正则化避免过度更新与旧类别（如猫、狗）之间的边。</p>
<p>评价指标：</p>
<p><img src="/2024/11/14/07-37-25/QQ_1731649890669.png" alt=" "></p>
<h1 id="Mammoth库"><a href="#Mammoth库" class="headerlink" title="Mammoth库"></a>Mammoth库</h1><h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h2><p>使用的mammoth库，<a href="https://github.com/aimagelab/mammoth">github</a></p>
<p>代码结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mammoth/</span><br><span class="line">├── requirements.txt       # 项目依赖的库和版本</span><br><span class="line">├── main.py*                # 入口 </span><br><span class="line">├── backbone/              # 骨干网络</span><br><span class="line">    ├── test.yaml</span><br><span class="line">    └── train.taml</span><br><span class="line">├── datasets/              # 数据定义</span><br><span class="line">├── models/                # 各种数据模型？</span><br><span class="line">├── utils/                 # 辅助模块</span><br><span class="line">├── scripts/               # 脚本</span><br><span class="line">├── tests/                 # 测试模块</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ├── __init__.py</span><br><span class="line">    ├── main.py             # 源代码目录</span><br><span class="line">    ├── model/</span><br><span class="line">    │   ├── __init__.py    # 源代码目录</span><br><span class="line">    │   ├── model.py</span><br><span class="line">    │   └── layers.py</span><br><span class="line">    ├── utils/</span><br><span class="line">    │   ├── __init__.py</span><br><span class="line">    │   ├── data_loader.py</span><br><span class="line">    │   └── metrics.py</span><br><span class="line">    └── config.py</span><br></pre></td></tr></table></figure>
<p><a href="https://aimagelab.github.io/mammoth/getting_started/index.html">getting started</a></p>
<h2 id="getting-started"><a href="#getting-started" class="headerlink" title="getting started"></a>getting started</h2><h3 id="fist-steps"><a href="#fist-steps" class="headerlink" title="fist steps"></a>fist steps</h3><p>logs: under the <code>data/results</code>, change by <code>base_path</code> in CONF.<br>    organized: <em><setting>/<dataset>/<model>/logs.pyd.</model></dataset></setting></em></p>
<p>WandB:</p>
<p>use: <code>--wandb_project</code>, <code>--wandb_entity</code><br>add in the <strong>observe</strong>,  like:loss = loss1 + loss2</p>
<h3 id="Training-Validation-and-Testing"><a href="#Training-Validation-and-Testing" class="headerlink" title="Training, Validation, and Testing"></a>Training, Validation, and Testing</h3><p>the validation set is <strong>disabled</strong>,only test and train.</p>
<h3 id="Load-and-save-checkpoints"><a href="#Load-and-save-checkpoints" class="headerlink" title="Load and save checkpoints"></a>Load and save checkpoints</h3><p>training: <code>--savecheck</code>and <code>--loadcheck</code> arguments.<br><code>python utils/main.py --savecheck=last</code>:<br>last: save after last task<br>task: save after each task<br><code>python utils/main.py –loadcheck=&lt;path to checkpoint&gt;.pt</code></p>
<h3 id="Fast-training-amp-optimizations"><a href="#Fast-training-amp-optimizations" class="headerlink" title="Fast training &amp; optimizations"></a>Fast training &amp; optimizations</h3><p>use: <code>–code_optimization 2</code> or <code>-O 2</code></p>
<p>distrubuted training: use <code>–distributed=dp</code></p>
<h3 id="Scripts"><a href="#Scripts" class="headerlink" title="Scripts"></a>Scripts</h3><p>scripts/prepare_grid.py: contains a <code>grid_combinations</code> dictionary</p>
<p>scripts/local_launcher.py: launch on local machine</p>
<p>scripts/slurm_sbatcher.py: launch  on a SLURM cluster.</p>
<p>scripts/wandb_sync.py:  syncing the logs produced by WandB</p>
<h3 id="Registration-of-backbones-and-datasets"><a href="#Registration-of-backbones-and-datasets" class="headerlink" title="Registration of backbones and datasets"></a>Registration of backbones and datasets</h3><p>..</p>
<h2 id="contents"><a href="#contents" class="headerlink" title="contents"></a>contents</h2><h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h3><p>training and testing<br>named  <strong><model_name>.py</model_name></strong><br>implemented <strong>observe</strong> method</p>
<p>Evaluation<br><strong>forward</strong> method in the base class <strong>ContinualModel</strong> by default</p>
<h4 id="Attributes-and-utility-methods"><a href="#Attributes-and-utility-methods" class="headerlink" title="Attributes and utility methods"></a>Attributes and utility methods</h4><p>Automatic attributes: provides a few properties that are automatically set during the incremental training</p>
<h4 id="Module-attributes-and-functions"><a href="#Module-attributes-and-functions" class="headerlink" title="Module attributes and functions"></a>Module attributes and functions</h4><p>models.get_model(args, backbone, loss, transform,dataset):<br>PARAMETERS:<br>args (Namespace) – the arguments which contains the –model attribute<br>backbone (nn.Module) – the backbone of the model<br>loss – the loss function<br>transform – the transform function<br>dataset – the instance of the dataset</p>
<p>utils<br>models.utils.load_model_config(args, buffer_size=None):<br>Loads the configuration file for the model</p>
<h3 id="datasets"><a href="#datasets" class="headerlink" title="datasets"></a>datasets</h3><h3 id="backbones"><a href="#backbones" class="headerlink" title="backbones"></a>backbones</h3><p>Features and logits<br>returnt=’out’: the backbone returns the logits produced after the classification layer.<br>returnt=’features’: the backbone returns the features extracted immediately before the classification layer.<br>returnt=’both’: the backbone returns both the logits and the features (a tuple (logits, feature)).</p>
<h1 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h1><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">torch</span><br><span class="line">torchvision</span><br><span class="line">torchaudio</span><br><span class="line">numpy</span><br><span class="line">scipy</span><br><span class="line">tqdm</span><br><span class="line">pandas</span><br><span class="line">scikit-learn</span><br><span class="line">protobuf</span><br><span class="line">PyYAML</span><br></pre></td></tr></table></figure>
<h2 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h2><h3 id="OSError"><a href="#OSError" class="headerlink" title="OSError"></a>OSError</h3><p><code>OSError: image file is truncated</code></p>
<p>在class_seven.py中添加：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> ImageFile</span><br><span class="line">ImageFile.LOAD_TRUNCATED_IMAGES = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h2 id="预训练模型一直在被改变？"><a href="#预训练模型一直在被改变？" class="headerlink" title="预训练模型一直在被改变？"></a>预训练模型一直在被改变？</h2><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python main.py \</span><br><span class="line">  --config ./configs/rg.yaml \</span><br><span class="line">   --minibatch_size 2 --n_tasks 4 \</span><br><span class="line">    --fewshot False --buffer_size 10 \</span><br><span class="line">    --gpus 0</span><br><span class="line">    </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">同步</span></span><br><span class="line">wandb sync ./wandb/run</span><br></pre></td></tr></table></figure>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>loss_d_score</strong>: 分数损失。mse_loss</p>
<p><strong>loss_d_re</strong>: ListNetLoss排序损失。</p>
]]></content>
      <categories>
        <category>AQA</category>
      </categories>
      <tags>
        <tag>持续学习, AQA, CAQA</tag>
      </tags>
  </entry>
  <entry>
    <title>Self-supervised subaction Parsing Network for Semi-supervised Action Quality</title>
    <url>/2024/10/19/16-50-30/</url>
    <content><![CDATA[<p>论文题目：Self-supervised subaction Parsing Network for Semi-supervised Action Quality</p>
<p>作者：Kumie Gedamu, Yanli Ji*, Yang Yang, Jie Shao, Heng Tao Shen</p>
<p>单位：中山大学</p>
<p>期刊名称：IEEE Transactions on Image Processing</p>
<p>中科院分区：1区</p>
<h1 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h1><p>文章文风很好，写作方法可以学习，但是很多内容介绍不仔细，K-mean算法也有用。</p>
<h1 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>介绍完全监督AQA任务的局限性，手工标注成本很好，作者使用师生网络获取一致性的语义表示。</p>
<p>减少背景的依赖，</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729408370524.png" alt=" "></p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h2 id="提出的方法"><a href="#提出的方法" class="headerlink" title="提出的方法"></a>提出的方法</h2><p>整体结构：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729408411850.png" alt=" "></p>
<h3 id="A-初步定义"><a href="#A-初步定义" class="headerlink" title="A.初步定义"></a>A.初步定义</h3><p>标注数据集：</p>
<p>$X_\ell=\{x_i^\ell,y_i\}_{i=1}^{\mathbb{N}_\ell},x_i^\ell\in\mathbb{R}^{T\times H\times W\times C}$</p>
<p>$y_i$:标签的语义分数</p>
<p>未标注数据集：</p>
<p>$X_{u} = \{x_{i}^{u}\}_{i=1}^{\mathbb{N}_{u}}, x_{i}^{u} \in \mathbb{R}^{T\times H\times W\times C}$</p>
<p>$\mathbb{N}_u\gg{\mathbb{N}_\ell}$</p>
<p>教师模型生成预测区域，提取出特征：</p>
<p>$X_{a}=\{x_{i}^{a}\}_{i=1}^{\mathbb{N}_{u}}$</p>
<p>I3D作为骨干网络：共享参数$\theta$</p>
<p>$\mathcal{F}_u=E_\theta(X_u),\quad\mathcal{F}_a=E_\theta(X_a),\quad\mathcal{F}_\ell=E_\theta(X_\ell)$</p>
<h3 id="B-半监督AQA的师生网络"><a href="#B-半监督AQA的师生网络" class="headerlink" title="B. 半监督AQA的师生网络"></a>B. 半监督AQA的师生网络</h3><p>上两分支是学生网络，下面是教师网络。</p>
<p>teacher：先生成预测区域regions。</p>
<p>student：生成pseudo label，pseudo — prediction， 伪标签与预测的差异最小化。</p>
<p><strong>回归损失</strong>：</p>
<p>$<br>\begin{aligned}<br>L_{un}=-\frac{1}{\mathbb{N}_{u}}\sum_{i=1}^{\mathbb{N}_{u}}||R_{\vartheta}\left(\bar{\mathcal{F}}_{u}\right)-(1\max\left(R_{\vartheta}\left(\bar{\mathcal{F}}_{a}\right)\right)\geq\tau)||^{2}<br>\end{aligned}$</p>
<blockquote>
<p>$\max\left(R_{\vartheta}\left(\bar{\mathcal{F}}_{a}\right)\right)$ ：先求回归结果的最大值，超过阀值为1，否则为0</p>
<p> L2 范数的平方(欧几里得距离的平方)</p>
</blockquote>
<p>目的是：类似一致性规则化，让学生的预测接近伪标签。</p>
<h3 id="C-自监督子动作解析"><a href="#C-自监督子动作解析" class="headerlink" title="C. 自监督子动作解析"></a>C. 自监督子动作解析</h3><p>目标是识别子动作的独特模式及其时间依赖性，并学习动作的时空结构以实现更好的表示。</p>
<p>假设有$K$个子动作，作者希望识别视频的所有帧到每个子动作集。</p>
<p>模块由两部分组成：</p>
<p>一：Pseudo-label generation</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729580152322.png" alt=" "></p>
<p>使用子集选择来构建子动作解析的伪标签。使用$\mathcal{F}_{w}^{t}$，$w \in \{u,a\}$表示$t$时刻的输入特征。</p>
<p><strong>使用attention refinement(注意力优化或注意力精细化)</strong>：引导选择视频帧中特征最多的的区域：全局平均池化（GAP）每个通道，输入特征映射减少到 C × 1 × 1 向量，捕获每个通道内的整体信息。sigmoid激活函数，计算通道范围的通道注意力分数，选择分数最高的<strong>通道</strong>(通道级注意力)，特征减少之后怎么卷积。</p>
<p>然后进行逐通道乘法：(对特征进行通道放缩)，结果是$\hat{F}_w^t$，增强了子动作集之间的特征差异。</p>
<p><strong>伪标签生成</strong>：利用注意力特征和(潜在状态簇)clusters of latent states：先选择潜在状态簇集合，把他们作为子动作训练。然后将所有视频帧分配到这些子动作集中，子集选择过程将生成伪标签。</p>
<p>潜在状态簇$S$生成方法：k-means algorithm(k均值算法)，$M$个中心，输入为$\mathcal{F}_{w}^{t}$，基于聚类选择子集的过程涉及使用集合 $S$ 内的状态 $M$ 以及子集选择组件中的注意力优化特征 $\hat{F}_w^t$。</p>
<p>生成潜在特征$S=\{s_1,s_2,\cdots,s_M\}$,表示输入特征中的不同模式或聚类。</p>
<p>聚类集合选择公式定义为：</p>
<p>$<br>\begin{aligned}<br>Z(\hat{\mathcal{S}})\triangleq\frac1T\sum_{t=1}^T\min_{i=\{1,\cdots,M\}}\|\hat{\mathcal{F}}_w^t-s_i\|_2<br>\end{aligned}<br>$</p>
<blockquote>
<p>$Z(\hat{\mathcal{S}})$:最小化目标函数，时间序列上每一帧到最近的聚类中心的欧氏距离(L2范数)</p>
</blockquote>
<p>输出包含$\hat{K}$个选择状态，对应子动作序列，$\hat{K}$小于等于$M$，不一定每个子动作模式或阶段都重要，$M$代表动作序列的细粒度。</p>
<blockquote>
<p>因为假设有$K$个子动作集</p>
</blockquote>
<p>贪心算法优化过程：初始化$\Gamma$为空，迭代添加状态从$S$到$\Gamma$，选择$\Gamma$中已有元素是的损失函数最小的状态。继续这个过程直到获取最多$\hat{K}$个状态。表示为$P=\{p_1,p_2,\cdots,p_M\}$,就是接下来要用的伪标签。</p>
<blockquote>
<p>$P$集合有k个状态？ (one-hot vectors (0-1向量))</p>
</blockquote>
<p><img src="/2024/10/19/16-50-30/QQ_1729585075753.png" alt=" "></p>
<p>输入：</p>
<ol>
<li><p><strong>所有可能的状态集合</strong>：$ \{1, \cdots, \hat{K}\} $和输入特征 $\hat{\mathcal{F}}_w^t $，这些特征来自视频的每一帧。</p>
<blockquote>
<p>所有可能状态集合有$M$个</p>
</blockquote>
</li>
<li><p><strong>损失函数</strong>： $Z(\Gamma)$ 表示目标函数的值，所有帧与状态集合$\Gamma$的差异。</p>
<blockquote>
<p>状态集合与每一帧的最小差异</p>
</blockquote>
</li>
<li><p><strong>增益函数</strong>：$ \delta{\Gamma}(z^<em>) = Z(\Gamma) - Z(\Gamma \cup \{(z^</em>)\}) $，表示将状态$ z^* $包含到当前选定状态集$ \Gamma$ 后，损失函数的减少量。</p>
<blockquote>
<p>增加了一个状态后，差异会缩小，因为中心点多了。<br>增益函数为负值时，说明序列中的子动作小于$K$个。</p>
</blockquote>
</li>
</ol>
<p>输出：</p>
<p>​    <strong>最多</strong>选择 $\hat{K}$ 个状态作为子动作序列的伪标签。</p>
<p>步骤：</p>
<ol>
<li><p><strong>初始化活跃集</strong>：将活跃集 $\Gamma$ 设为空集。</p>
</li>
<li><p><strong>迭代选择状态</strong>（共 $\hat{K}$ 次）：</p>
<ul>
<li><p>在每次迭代中，从所有状态中选择一个当前不在$ \Gamma$ 中的状态 $z^*$ ，使得添加该状态后目标函数的值减少最多。</p>
<blockquote>
<p>贪心算法</p>
</blockquote>
</li>
<li><p>计算加入状态 $z^<em>$ 后的增益 $\delta_{\Gamma}(z^</em>) $。</p>
<blockquote>
<p>可能存在子动作数小于$K$</p>
</blockquote>
</li>
<li><p>将状态 $z^*$ 添加到活跃集 $\Gamma $中。</p>
</li>
</ul>
</li>
<li><p><strong>返回结果</strong>：最终的活跃集 $\Gamma$ 中包含最多 $\hat{K}$ 个状态，这些状态作为子动作解析中的伪标签。</p>
</li>
</ol>
<blockquote>
<p>? 伪标签不带时间顺序吗？</p>
</blockquote>
<p>二. subaction parsing:</p>
<ul>
<li>$M$ 是初始聚类生成的状态数，表示动作序列中的所有潜在子动作模式。</li>
<li>$K$ 是通过子集选择算法从 $M $个状态中挑选出的最重要的$ K$ 个状态，用于伪标签生成和最终子动作的解析。</li>
<li>在子动作解析模块中，模型仍然输入的是 $M$ 个状态，因为需要完整的潜在状态来进行解析，最终再聚焦于$ K $个关键状态</li>
</ul>
<p>每一帧生成一个M维的one-hot向量(0-1),代表可能的状态。每个子块在时间维度上扩展注意力优化特征，应用最大池化减少空间特征。MLP (多层感知器)投影到概率向量中，得到:$\begin{aligned}\mathcal{A}=\{a_1,\cdots,a_M\}\end{aligned}$,表示发生子动作的可能性。</p>
<p>投影公式为：$[a_1,\cdots,a_M]=\mathbb{F}_\emptyset(\hat{\mathcal{F}}_w^t)$</p>
<blockquote>
<p>整体变为TxM的向量。 转置了</p>
</blockquote>
<p>$\mathbb{F}$表示$\emptyset$参数下的自监督子动作解析</p>
<p>$a_M\in\mathbb{R}^T$是第 M 个潜在状态的预测概率。</p>
<blockquote>
<p>T维向量，转置了</p>
</blockquote>
<p>$\begin{aligned}<br>\bar{t}_k=\operatorname*{argmax}_{\frac TM(k-1)\leq t\leq\frac TMk}a_M(t)<br>\end{aligned}$</p>
<blockquote>
<p>argmax, 返回最大值索引</p>
<p>两个连续的帧，即t和t + 1具有不同的表示。在此之后，动作解析在第(t + 1)帧标记新的子动作实例的开始。</p>
<p>$M$个时间段内，每个$t$的最大概率实例，表示为这个段内的实例。子动作划分是否具有细粒度？</p>
</blockquote>
<p>其中：$p_M(t)$表示第$t$帧的伪标签的真实值。$a_M(t)$表示第$t$帧的预测概率分布。第$M^{th}$个序列的预测值是$\bar{t}_M$，充当给定序列的最好表示。</p>
<p>为了确保伪标签和$a_M(t)$ 预测的子动作解析之间的一致性，我们制定了二元交叉熵损失$L_{ubc}$，实现细粒度的高级内部结构及其时间依赖性。</p>
<p>$\begin{aligned}L_{ubc}&amp;=-\sum_tp_M(t)\log_M(t)+(1-p_M(t))\log(1-a_M(t))\end{aligned}$</p>
<h3 id="D-Group-contrastive-learning"><a href="#D-Group-contrastive-learning" class="headerlink" title="D. Group contrastive learning"></a>D. Group contrastive learning</h3><p>组对比学习，实现相同语义间差异最小化，不同语义子集间差异最大化。</p>
<p>MLP projection head， 时空全局均值化。</p>
<p>$h_u=h_\varphi(\bar{\mathcal{F}}_u)$,$h_a=h_\varphi(\bar{\mathcal{F}}_a)$</p>
<p>然后，为这些输入的子动作概率分布向量分配与具有最大激活和高语义相似度的类相对应的伪标签。具有语义相似伪标签的特征以小批量的形式分组在一起，</p>
<p>$G_w^p=\frac{\sum_{i=1}^B1_{\{p=h_w\}}\mathrm{g}(h_w)}{T_B}$</p>
<p>相同语义对：$(G_a^p,G_u^p)$</p>
<p>不同语义对：$(G_a^p,G_u^q)$</p>
<p>组对比学习损失：</p>
<p>$L_{gc}=-\log\frac{\mathcal{H}(G_a^p,G_u^p)/\iota}{\mathcal{H}(G_a^p,G_u^p)/\iota+\sum_{q=1,k}^S\mathbb{1}_{p\neq q}\mathcal{H}(G_a^p,G_w^q)/\iota}$</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>回归损失：$\bar{y}_i=\mathcal{R}_\vartheta\left(\bar{\mathcal{F}}_\ell\right)$</p>
<p>MSE(均方误差)损失：$L_{reg}=\frac1{\mathbb{N}_\ell}\sum_{i=1}^{\mathbb{N}_\ell}\left(\bar{y}_i-y_i\right)^2$</p>
<p>总损失：$L_{all}=L_{reg}+\lambda_1L_{ubc}+\lambda_2L_{un}+\lambda_3L_{gc}$</p>
<p>其中$\lambda_1$：超参数</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集和参数"><a href="#数据集和参数" class="headerlink" title="数据集和参数"></a>数据集和参数</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>MTL-AQA：</p>
<p>FineDiving：</p>
<p>Rhythmic Gymnastics</p>
<p>FineFS</p>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>评价指标(斯皮尔曼系数)：$\rho=\frac{\sum_i(y_i-y)(\bar{y}_i-\bar{y})}{\sqrt{\sum_i(y_i-y)^2\sum_i(\bar{y}_i-\bar{y})^2}}$</p>
<h3 id="消融实验-Ablation-Study"><a href="#消融实验-Ablation-Study" class="headerlink" title="消融实验 Ablation Study"></a>消融实验 Ablation Study</h3><p><img src="/2024/10/19/16-50-30/QQ_1729641986537.png" alt=" "></p>
<p><img src="/2024/10/19/16-50-30/QQ_1729641996950.png" alt=" "></p>
<p>预测值和真实值的可视化分布：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729642292266.png" alt=" "></p>
<p>对标记样本比例的敏感性：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729642276297.png" alt=" "></p>
<p>10%的标记数据达到了$S^4AQA$的40%标记样本的效果。</p>
<p>阈值的影响：FineDiving-15%</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729642516281.png" alt=" "></p>
<p>损失函数的评估：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729642730772.png" alt=" "></p>
<p>作者认为：$L_{gc}$和$L_{ubc}$在学习具有时间多样性的细粒度特征方面起着至关重要的作用。</p>
<p>对最优子动作数量$K$的评价：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729642888974.png" alt="QQ_1729642888974"></p>
<p>跳水三个阶段。</p>
<p>伪标签的演员中心区域：</p>
<p>替换为一个增强版本，在保持视频语义不变的情况下，从输入剪辑中改变像素级别的分布。</p>
<blockquote>
<p>应该是手动剪辑的意思</p>
</blockquote>
<p><img src="/2024/10/19/16-50-30/QQ_1729643125984.png" alt=" "></p>
<p>注意力细化对子动作解析的影响：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729643211428.png" alt=" "></p>
<p>w/o，没有注意力细化</p>
<h3 id="与最先进的方法对比"><a href="#与最先进的方法对比" class="headerlink" title="与最先进的方法对比"></a>与最先进的方法对比</h3><p>每个数据集上最先进的方法：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729643303767.png" alt=" "></p>
<p><img src="/2024/10/19/16-50-30/QQ_1729643341556.png" alt=" "></p>
<p><img src="/2024/10/19/16-50-30/QQ_1729643352697.png" alt=" "></p>
<p> <img src="/2024/10/19/16-50-30/QQ_1729643370901.png" alt=" "></p>
<h3 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h3><p>注意力细化和演员中心的效果：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729643457754.png" alt=" "></p>
<p>不同比例标记的散点图：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729643501306.png" alt=" "></p>
<p>失败样本：</p>
<p><img src="/2024/10/19/16-50-30/QQ_1729643556052.png" alt=" "></p>
<p><img src="/2024/10/19/16-50-30/QQ_1729643564108.png" alt=" "></p>
<p>误差较大的样本340，动作失误，被判0分。</p>
<blockquote>
<p>动作犯规后，评分不线性，0分</p>
</blockquote>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2>]]></content>
      <categories>
        <category>AQA</category>
      </categories>
      <tags>
        <tag>自监督, AQA, 子动作解析</tag>
      </tags>
  </entry>
  <entry>
    <title>基于骨架图卷积时空特征表示的AQA</title>
    <url>/2024/10/15/12-56-12/</url>
    <content><![CDATA[<h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><p>代码结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">AQAForFS/</span><br><span class="line">├── requirements.txt       # 项目依赖的库和版本</span><br><span class="line">├── main.py                # 入口 </span><br><span class="line">├── visualization.py       # 可视化</span><br><span class="line">├── config/                # 参数配置文件</span><br><span class="line">    ├── test.yaml</span><br><span class="line">    └── train.taml</span><br><span class="line">├── data/                  # 训练数据</span><br><span class="line">├── feeder/                # 数据喂养器</span><br><span class="line">├── models/                # 各种数据模型？</span><br><span class="line">├── net/                   # 网络模型</span><br><span class="line">    ├── __init__.py</span><br><span class="line">    ├── graph.py           # 图卷积定义</span><br><span class="line">    └── st_gcn.py					 # 主要模块</span><br><span class="line">├── utils/                 # 辅助模块</span><br><span class="line">├── src/                   # 源代码目录</span><br><span class="line">├── src/                   # 源代码目录</span><br><span class="line">    ├── __init__.py</span><br><span class="line">    ├── main.py             # 源代码目录</span><br><span class="line">    ├── model/</span><br><span class="line">    │   ├── __init__.py    # 源代码目录</span><br><span class="line">    │   ├── model.py</span><br><span class="line">    │   └── layers.py</span><br><span class="line">    ├── utils/</span><br><span class="line">    │   ├── __init__.py</span><br><span class="line">    │   ├── data_loader.py</span><br><span class="line">    │   └── metrics.py</span><br><span class="line">    └── config.py</span><br></pre></td></tr></table></figure>
<pre class="mermaid">graph TD


    %%main.p.get_parser--> ASESS_processor.load_arg         -->
    main.p--> Processor.__init__ --> Processor.load_arg --> ASESS_processor.get_parser --> Processor.get_parser --> Processor.init_environment --> ASESS_processor.load_model-->Processor.load_data-->ASESS_processor.load_optimizer-->main.p.start-->Processor.start
    Processor.start--trian-->ASESS_processor.train-->ASESS_processor.adjust_lr
    Processor.start--test-->ASESS_processor.test

    %%程序入口
    subgraph main.py

    subgraph main
      main.p.get_parser["p.get_parser()"]-.-main.p["p = Processor(sys.argv[2:])"]-.-main.p.start["p.start()"]

    end
    end

    %%评估类
    subgraph ASESS_processor.py

    subgraph class_ASESS_processor["class ASESS_processor"]

         ASESS_processor.get_parser["get_parser()"]-.-ASESS_processor.load_model["load_model(self)"]-.-ASESS_processor.load_optimizer["load_optimizer(self)"]-.-ASESS_processor.show_src["show_src(self, totalpred)"]
         ASESS_processor.adjust_lr["adjust_lr(self)"]
         ASESS_processor.test["test(self, evaluation=True)"]
         ASESS_processor.train["train(self)"]


    end
    end


       %%处理器类
       subgraph Processor.py

    subgraph class_Processor["class Processor"]

      Processor.__init__["__init__"]
      Processor.load_arg["load_arg"]-.-Processor.get_parser["get_parser(add_help=False)"]
      Processor.init_environment["init_environment"]-.-Processor.load_data["load_data"]-.-Processor.start{"start判断"}


    end
    end</pre>



<h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>```shell</p>
<p> python main.py assess —config config/train.yaml</p>
<p>```</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>```shell</p>
<p> python main.py assess —config config/test.yaml</p>
<p>```</p>
<h2 id="net"><a href="#net" class="headerlink" title="net"></a>net</h2><h3 id="st-gcn-py"><a href="#st-gcn-py" class="headerlink" title="st_gcn.py"></a>st_gcn.py</h3><h1 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h1><p>数据采样300帧。</p>
<h2 id="datainfo-MIT-npy"><a href="#datainfo-MIT-npy" class="headerlink" title="datainfo/MIT.npy"></a>datainfo/MIT.npy</h2><p>数据预览</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">字典类型</span><br><span class="line">dict_keys([&#x27;test_sample_name&#x27;, &#x27;video_num&#x27;, &#x27;test_video_num&#x27;, &#x27;test_index&#x27;, &#x27;video_label&#x27;])</span><br><span class="line"></span><br><span class="line"> test_sample_name:</span><br><span class="line"> &lt;class &#x27;list&#x27;&gt;</span><br><span class="line">[&#x27;59_7.json&#x27;, &#x27;106_1.json&#x27;, &#x27;106_10.json&#x27;, &#x27;106_11.json&#x27;, &#x27;106_12.json&#x27;]</span><br><span class="line"></span><br><span class="line">video_num: &lt;class &#x27;int&#x27;&gt; 171</span><br><span class="line"></span><br><span class="line">test_video_num: &lt;class &#x27;int&#x27;&gt; 71</span><br><span class="line"></span><br><span class="line">test_index: &lt;class &#x27;list&#x27;&gt;</span><br><span class="line">[5, 7, 13, 16, 17]</span><br><span class="line"></span><br><span class="line">应该是分数</span><br><span class="line">Fisv：</span><br><span class="line">对应的所有标签的分数，然后在按照下标从中间选。</span><br><span class="line">video_label: &lt;class &#x27;list&#x27;&gt;</span><br><span class="line">[49.74, 41.94, 43.84, 36.1, 52.16]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="datainfo-tes-train-data-npy"><a href="#datainfo-tes-train-data-npy" class="headerlink" title="datainfo/tes/train_data.npy"></a>datainfo/tes/train_data.npy</h2><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">type: &lt;class &#x27;numpy.ndarray&#x27;&gt;</span><br><span class="line">shape: (5710, 3, 300, 18, 2)</span><br><span class="line">        # N: 批量大小 (batch size)</span><br><span class="line">        # C: 通道数 (channels)</span><br><span class="line">        # T: 帧</span><br><span class="line">        # V: 节点数 (vertices)</span><br><span class="line">        # M: 一帧中的实例, 人体骨架数量 (可能存在多个人体骨架)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="datainfo-tes-train-label-pkl"><a href="#datainfo-tes-train-label-pkl" class="headerlink" title="datainfo/tes/train_label.pkl"></a>datainfo/tes/train_label.pkl</h2><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">是一个元组，有两个list元素</span><br><span class="line">info: &lt;class &#x27;tuple&#x27;&gt;</span><br><span class="line">Element 0: Type: &lt;class &#x27;list&#x27;&gt;, value: [&#x27;1_1.json&#x27;, &#x27;1_10.json&#x27;]</span><br><span class="line">Element 1: Type: &lt;class &#x27;list&#x27;&gt;, value: [28.71, 28.71]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>AQA</category>
      </categories>
      <tags>
        <tag>ST-GCN, AQA, LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>c++服务发现中心consul</title>
    <url>/2024/10/13/07-26-48/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="5a79ba45693a6178ba4fe2b6ed86f6efd3ab5846b75e8dde7fdb2d1becae7adf">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++项目, consul</tag>
      </tags>
  </entry>
  <entry>
    <title>经典卷积神经网络-ResNet</title>
    <url>/2024/10/08/15-15-24/</url>
    <content><![CDATA[<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="卷积层前向传播"><a href="#卷积层前向传播" class="headerlink" title="卷积层前向传播"></a>卷积层前向传播</h2><p><img src="/2024/10/08/15-15-24/convolution-mlp-mapping.png" alt></p>
<h3 id="梯度反向传播的卷积"><a href="#梯度反向传播的卷积" class="headerlink" title="梯度反向传播的卷积"></a>梯度反向传播的卷积</h3><p><img src="/2024/10/08/15-15-24/cnn_gradient_finger.png" alt=" "></p>
<p><img src="/2024/10/08/15-15-24/konwolucja.png" alt></p>
<p>反向传播使用了这种卷积操作，将权值矩阵旋转180度，而梯度矩阵会进行填充为：</p>
<p>$\left.\left[\begin{array}{llll}\mathbf{0}&amp;\mathbf{0}&amp;\mathbf{0}&amp;\mathbf{0}\\\mathbf{0}&amp;\delta_{11}^l&amp;\delta_{12}^l&amp;\mathbf{0}\\\mathbf{0}&amp;\delta_{21}^l&amp;\delta_{22}^l&amp;\mathbf{0}\\\mathbf{0}&amp;\mathbf{0}&amp;\mathbf{0}&amp;\mathbf{0}\end{array}\right.\right]$</p>
<p>下图是反向传播的更新梯度，(图片没有填充)：</p>
<p><img src="/2024/10/08/15-15-24/screenshot-from-2016-04-17-212043.png" alt></p>
<p>卷积将卷积核旋转180度，然后进行互相关操作。</p>
<h3 id="二维卷积公式"><a href="#二维卷积公式" class="headerlink" title="二维卷积公式"></a>二维卷积公式</h3><p>深度学习中的卷积是先旋转180度，然后互相关操作。</p>
<p>前馈神经网络中：神经元$j$的误差定义为：</p>
<p>$\delta_j^l=\frac{\partial C}{\partial z_j^l}$</p>
<p>其中：$z_j^l=\sum_kw_{kj}^la_k^{l-1}+b_k^l$</p>
<p>$a_j^l=\sigma(z_j^l)$，激活函数</p>
<p>其中$w_{kj}^l$:是第$l-1$与$l$层之间的输入$k$节点与输出$j$节点的权重，</p>
<p>对于CNN：</p>
<p>前向传播公式为：</p>
<p>$<br>\begin{aligned}<br>z_{x,y}^{l+1} = w^{l+1}*\sigma(z_{x,y}^l)+b_{x,y}^{l+1} = \sum_{a}\sum_{b}w_{a,b}^{l+1}\sigma(z_{x-a,y-b}^l) +b_{x,y}^{l+1}<br>\end{aligned}<br>$</p>
<p>其中$z_{x-a,y-b}^l$：相当于对核翻转。</p>
<p>定义误差梯度：</p>
<p>$\begin{aligned}\delta_{x,y}^l=\frac{\partial C}{\partial z_{x,y}^l}=\sum_{x^{\prime}}\sum_{y^{\prime}}\frac{\partial C}{\partial z_{x^{\prime},y^{\prime}}^{l+1}}\frac{\partial z_{x^{\prime},y^{\prime}}^{l+1}}{\partial z_{x,y}^l}\end{aligned}$</p>
<blockquote>
<p>其中$x^{\prime},y^{\prime}$，取决于前向传播卷积的示意图中，$l$层每个节点$x,y$，影响的$l+1$层的$x^{\prime},y^{\prime}$不同。</p>
</blockquote>
<p>将前向传播带入公式：</p>
<p>$\begin{aligned}<br>\frac{\partial C}{\partial z_{x,y}^l} &amp; =\sum_{x^{\prime}}\sum_{y^{\prime}}\frac{\partial C}{\partial z_{x^{\prime},y^{\prime}}^{l+1}}\frac{\partial z_{x^{\prime},y^{\prime}}^{l+1}}{\partial z_{x,y}^{l}} \\<br>&amp;=\sum_{x^{\prime} }\sum_{y^{\prime}}\delta_{x^{\prime},y^{\prime} }^{l+1}\frac{\partial(\displaystyle\sum_{a}\displaystyle\sum_{b}w_{a,b}^{l+1}\sigma(z_{x^{\prime}-a,y^{\prime}-b}^{l})+b_{x^{\prime},y^{\prime}}^{l+1}) }{\partial z_{x,y}^{l}}<br>\end{aligned}$</p>
<p>对公式右边进行展开，只有$x = x^{\prime} - a$和$y = y^{\prime} -b$时偏导不为0，故重新整理后得，将$a = x^{\prime} - x$和$b = y^{\prime} -y$代入：</p>
<p>$\begin{aligned}\sum_{x’}\sum_{y’}\delta_{x’,y’}^{l+1}w_{a,b}^{l+1}\sigma’(z_{x,y}^l)&amp;=\sum_{x’}\sum_{y’}\delta_{x’,y’}^{l+1}w_{x’-x,y’-y}^{l+1}\sigma’(z_{x,y}^l)\end{aligned}$</p>
<blockquote>
<p>上述式子中：$x’-x$不会超出范围，因为$x’$与$x$具有关联，所以一定会在$w$的矩阵范围内。</p>
</blockquote>
<p>而右边第一项就是梯度反向传播的公式：</p>
<p>$\begin{aligned}\delta^{l+1}*w_{-x,-y}^{l+1} = \sum_{x’}\sum_{y’}\delta_{x’,y’}^{l+1}w_{x’-x,y’-y}^{l+1}\end{aligned}$</p>
<p>理解上面这个公式：</p>
<p><img src="/2024/10/08/15-15-24/QQ_1728452268383.png" alt=" "></p>
<p>$\delta_{1,2}^{l}$，只影响了下一层的$\delta_{1,1}^{l+1}$$\delta_{1,2}^{l+1}$，所以$x’,y’$取值范围只有$(1,1),(1,2)$，得到的$w$下标${x’-x,y’-y}$，取值为:$(0,-1),(0,0)$</p>
<p>推导公式：</p>
<p>所以最终的反向传播误差公式：</p>
<p>$\delta_{x,y}^l=\delta^{l+1}*ROT180(w_{x,y}^{l+1})\sigma^{\prime}(z_{x,y}^l)$</p>
<p>成本函数的梯度：</p>
<p>$\frac{\partial C}{\partial w_{a,b}^l}=\delta_{a,b}^l*\sigma(ROT180(z_{a,b}^{l-1}))$</p>
<h3 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h3><p><a href="https://grzegorzgwardys.wordpress.com/2016/04/22/8/#unique-identifier2">卷积神经网络反向传播：从直觉到推导</a></p>
<p><a href="https://pavisj.medium.com/convolutions-and-backpropagations-46026a8f5d2c#6042">卷积和反向传播</a></p>
<p><a href="https://www.pycodemates.com/2023/07/backward-pass-in-convolutional-neural-network-explained.html">Derivation of Backpropagation in Convolutional Neural Network (CNN)</a></p>
<p><a href="https://zzutk.github.io/docs/reports/2016.10%20-%20Derivation%20of%20Backpropagation%20in%20Convolutional%20Neural%20Network%20(CNN">Derivation of Backpropagation in Convolutional Neural Network zhangzhifei</a>.pdf)</p>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="矩阵微分"><a href="#矩阵微分" class="headerlink" title="矩阵微分"></a>矩阵微分</h2><p>矩阵：$f(\boldsymbol{X}),\boldsymbol{X}_{m\times n}=(x_{ij})_{i=1,j=1}^{m,n}$</p>
<p>多元函数设为可微：<br>$<br>\begin{aligned}<br>\mathrm{d}f(\boldsymbol{X})&amp; =\frac{\partial f}{\partial x_{11}}\mathrm{d}x_{11}+\frac{\partial f}{\partial x_{12}}\mathrm{d}x_{12}+\cdots+\frac{\partial f}{\partial x_{1n}}\mathrm{d}x_{1n} \\<br>&amp;+\frac{\partial f}{\partial x_{21}}\mathrm{d}x_{21}+\frac{\partial f}{\partial x_{22}}\mathrm{d}x_{22}+\cdots+\frac{\partial f}{\partial x_{2n}}\mathrm{d}x_{2n} \\<br>&amp;+\ldots \\<br>&amp;+\frac{\partial f}{\partial x_{m1}}\mathrm{d}x_{m1}+\frac{\partial f}{\partial x_{m2}}\mathrm{d}x_{m2}+\cdots+\frac{\partial f}{\partial x_{mn}}\mathrm{d}x_{mn}<br>\end{aligned}<br>$</p>
<p>相当于：</p>
<p>$<br>\begin{aligned}<br>\mathrm{d}f(\boldsymbol{X})=\mathrm{tr}(\begin{bmatrix}\frac{\partial f}{\partial x_{11}}&amp;\frac{\partial f}{\partial x_{21}}&amp;\cdots&amp;\frac{\partial f}{\partial x_{m1}}\\\frac{\partial f}{\partial x_{12}}&amp;\frac{\partial f}{\partial x_{22}}&amp;\cdots&amp;\frac{\partial f}{\partial x_{m2}}\\\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\\frac{\partial f}{\partial x_{1n}}&amp;\frac{\partial f}{\partial x_{2n}}&amp;\cdots&amp;\frac{\partial f}{\partial x_{mn}}\end{bmatrix}_{n\times m}\begin{bmatrix}\mathrm{d}x_{11}&amp;\mathrm{d}x_{12}&amp;\cdots&amp;\mathrm{d}x_{1n}\\\mathrm{d}x_{21}&amp;\mathrm{d}x_{22}&amp;\cdots&amp;\mathrm{d}x_{2n}\\\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\\mathrm{d}x_{m1}&amp;\mathrm{d}x_{m2}&amp;\cdots&amp;\mathrm{d}x_{mn}\end{bmatrix}_{m\times n})<br>\end{aligned}<br>$</p>
<p>Jacobian矩阵形式:<br>即先把矩阵变元$X$进行转置，再对转置后的每个位置的元素逐个求偏导，结果布局和转置布局一样<br>$<br>\begin{aligned}<br>\operatorname{D}_\boldsymbol{X}f(\boldsymbol{X})&amp; =\frac{\partial f(\boldsymbol{X})}{\partial\boldsymbol{X}_{\boldsymbol{m}\times\boldsymbol{n}}^T} \\<br>&amp;=\begin{bmatrix}\frac{\partial f}{\partial x_{11}}&amp;\frac{\partial f}{\partial x_{21}}&amp;\cdots&amp;\frac{\partial f}{\partial x_{m1}}\\\frac{\partial f}{\partial x_{12}}&amp;\frac{\partial f}{\partial x_{22}}&amp;\cdots&amp;\frac{\partial f}{\partial x_{m2}}\\\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\\frac{\partial f}{\partial x_{1n}}&amp;\frac{\partial f}{\partial x_{2n}}&amp;\cdots&amp;\frac{\partial f}{\partial x_{mn}}\end{bmatrix}_{n\times m}<br>\end{aligned}<br>$</p>
<p>$\boldsymbol{X}_{m\times n}$的矩阵微分：<br>$<br>\begin{aligned}\mathrm{d}\boldsymbol{X}_{m\times n}=\begin{bmatrix}\mathrm{d}x_{11}&amp;\mathrm{d}x_{12}&amp;\cdots&amp;\mathrm{d}x_{1n}\\\mathrm{d}x_{21}&amp;\mathrm{d}x_{22}&amp;\cdots&amp;\mathrm{d}x_{2n}\\\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\\mathrm{d}x_{m1}&amp;\mathrm{d}x_{m2}&amp;\cdots&amp;\mathrm{d}x_{mn}\end{bmatrix}_{m\times n}\end{aligned}<br>$</p>
<p>于是，矩阵微分形式为：<br>$<br>\begin{aligned}\mathrm{d}f(\boldsymbol{X})=\mathrm{tr}(\frac{\partial f(\boldsymbol{X})}{\partial\boldsymbol{X}^T}\mathrm{d}\boldsymbol{X})<br>\end{aligned}<br>$</p>
<h3 id="链接-1"><a href="#链接-1" class="headerlink" title="链接"></a>链接</h3><p><a href="https://www.zhihu.com/column/c_1318542724966715392">Iterator的专栏「工科数学」</a></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习, 计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch相关语法，Python特性</title>
    <url>/2024/09/30/09-10-49/</url>
    <content><![CDATA[<h1 id="并行计算"><a href="#并行计算" class="headerlink" title="并行计算"></a>并行计算</h1><p><code>view()</code>: 只取值，不复制，所以数据还是在之前的设备上，不需要to。</p>
<h1 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h1><p>cProfile显示各部分运行时间，优化时间长，调用重复的代码。</p>
<h1 id="爱因斯坦求和-einsum"><a href="#爱因斯坦求和-einsum" class="headerlink" title="爱因斯坦求和 einsum"></a>爱因斯坦求和 einsum</h1><h2 id="口诀"><a href="#口诀" class="headerlink" title="口诀"></a>口诀</h2><blockquote>
<p>外部重复做乘积<br>内部重复把数取<br>从有到无要求和<br>重复默认要丢弃(ij, jk-&gt; ), 默认丢掉i，变成ik</p>
</blockquote>
<img src="/2024/09/30/09-10-49/v2-db1d287d5bf82068c30d7cfe76692dd9_r.jpg" class>
<img src="/2024/09/30/09-10-49/v2-7c04ed50c83ed07c0e41ec428b2b3591_r.jpg" class>
<img src="/2024/09/30/09-10-49/v2-02da0f28e1fbd40eb8273209a9e9d6b6_r.jpg" class>
<img src="/2024/09/30/09-10-49/v2-ce4ac6f159fa47935f4e679f5f2c1400_r.jpg" class>
<p>用循环实现einsum：<br><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>)</span><br><span class="line">b = torch.randn(<span class="number">11</span>,<span class="number">13</span>,<span class="number">3</span>,<span class="number">17</span>,<span class="number">5</span>)</span><br><span class="line"><span class="comment"># p = 2, q = 3, r = 5, s = 7</span></span><br><span class="line"><span class="comment"># t = 11, u = 13, v = 17, r = 5</span></span><br><span class="line">torch_ein_out = torch.einsum(<span class="string">&#x27;pqrs,tuqvr-&gt;pstuv&#x27;</span>, [a, b]).numpy()</span><br><span class="line">torch_org_out = torch.tensordot(a, b, dims=([<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">4</span>])).numpy()</span><br><span class="line"></span><br><span class="line">np_a = a.numpy()</span><br><span class="line">np_b = b.numpy()</span><br><span class="line"><span class="comment"># 循环展开实现</span></span><br><span class="line">np_out = np.empty((<span class="number">2</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">17</span>), dtype=np.float32)</span><br><span class="line"><span class="comment"># 自由索引外循环</span></span><br><span class="line"><span class="comment"># 这里就是 p,s,t,u和v</span></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">7</span>):</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">11</span>):</span><br><span class="line">            <span class="keyword">for</span> u <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">13</span>):</span><br><span class="line">                <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">17</span>):</span><br><span class="line">                    <span class="comment"># 求和索引内循环</span></span><br><span class="line">                    <span class="comment"># 这里是 q和r</span></span><br><span class="line">                    sum_result = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">                        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>):</span><br><span class="line">                            sum_result += np_a[p, q, r, s] * np_b[t, u, q, v, r]</span><br><span class="line">                    np_out[p, s, t, u, v] = sum_result</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;is np_out == torch_ein_out ?&quot;</span>, np.allclose(torch_ein_out, np_out, atol=<span class="number">1e-6</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;is torch_ein_out == torch_org_out ?&quot;</span>, np.allclose(torch_ein_out, torch_org_out, atol=<span class="number">1e-6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 终端打印输出</span></span><br><span class="line"><span class="comment"># is np_out == torch_ein_out ? True</span></span><br><span class="line"><span class="comment"># is torch_ein_out == torch_org_out ? True</span></span><br></pre></td></tr></table></figure><br>下图解释了张量相乘，类似GCN中的，AX，但是应用爱因斯坦求和(X,A)。<br><img src="/2024/09/30/09-10-49/main-qimg-1cd18200b07e2c0cf462c074c537958c-pjlq.jpeg" class></p>
<p><a href="https://zhuanlan.zhihu.com/p/361209187">一文学会 Pytorch 中的 einsum</a><br><a href="https://juejin.cn/post/7276345038958166052">案例. 四维张量乘三维张量</a><br><a href="https://zhuanlan.zhihu.com/p/672346603">知乎einsum文章</a><br><a href="https://www.cnblogs.com/qftie/p/16245124.html">优雅地实现多头自注意力——使用einsum（爱因斯坦求和）进行矩阵运算</a><br><a href="https://rockt.github.io/2018/04/30/einsum">einsum</a></p>
<h1 id="与c-底层的绑定方法"><a href="#与c-底层的绑定方法" class="headerlink" title="与c++底层的绑定方法"></a>与c++底层的绑定方法</h1><h2 id="PYBIND11-MODULE-宏绑定函数"><a href="#PYBIND11-MODULE-宏绑定函数" class="headerlink" title="PYBIND11_MODULE 宏绑定函数"></a>PYBIND11_MODULE 宏绑定函数</h2><p>该宏用于定义一个 PyBind11 模块，example 是python模块的名称，m 是模块对象，可以用于绑定类和函数，以便在 Python 中使用。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;pybind11/pybind11.h&gt;</span><br><span class="line"></span><br><span class="line">int add(int i, int j) &#123;</span><br><span class="line">    return i + j;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PYBIND11_MODULE(example, m) &#123;</span><br><span class="line">    m.def(&quot;add&quot;, &amp;add, &quot;A function which adds two numbers&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="py-class-绑定c-类"><a href="#py-class-绑定c-类" class="headerlink" title="py::class_ 绑定c++类"></a>py::class_ 绑定c++类</h2><p><code>pybind11::class_&lt;MyClass&gt;(m, &quot;MyClass&quot;)</code><br>“MyClass”: 是py中的新类名</p>
<p><MyClass>: 指的绑定c++的类名</MyClass></p>
<pre><code>- m.def 是用于将c++全局函数绑定到 Python 模块上的方法。
-    .def 是用于将类的方法绑定到 Python 上的接口。
</code></pre><h2 id="使用-PyTorch-的-C-扩展-TorchScript"><a href="#使用-PyTorch-的-C-扩展-TorchScript" class="headerlink" title="使用 PyTorch 的 C++ 扩展 (TorchScript)"></a>使用 PyTorch 的 C++ 扩展 (TorchScript)</h2><h1 id="内存泄漏检测"><a href="#内存泄漏检测" class="headerlink" title="内存泄漏检测"></a>内存泄漏检测</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tracemalloc</span><br><span class="line">tracemalloc.start()</span><br><span class="line"><span class="keyword">for</span> path, im, im0s, vid_cap, s <span class="keyword">in</span> dataset:</span><br><span class="line">    snapshot1 = tracemalloc.take_snapshot()</span><br><span class="line">    <span class="comment"># 内容</span></span><br><span class="line">    snapshot2 = tracemalloc.take_snapshot()</span><br><span class="line">    <span class="comment"># 比较内存快照</span></span><br><span class="line">    top_stats = snapshot2.compare_to(snapshot1, <span class="string">&#x27;lineno&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[ Top 10 memory usage ]&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> stat <span class="keyword">in</span> top_stats[:<span class="number">10</span>]:</span><br><span class="line">        <span class="built_in">print</span>(stat)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习, 计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>yolo学习笔记</title>
    <url>/2024/09/20/13-08-34/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="3595e2571886d1cbd820bf6e6050a872b8d2cec1d9e6ede22cf84ac68b242c80">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>yolo</category>
      </categories>
      <tags>
        <tag>深度学习, 机器学习, yolo</tag>
      </tags>
  </entry>
  <entry>
    <title>好用的的科研神器</title>
    <url>/2024/09/15/11-15-05/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="3671c6e8d2119b316f5930910be637adb062453ba3f911a50e51cee1e4565d46">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>深度学习, 计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习训练中的技巧</title>
    <url>/2024/09/13/14-56-06/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="5b8b17cf222f8267e1076fa0b787a0f6c5b5b6712f4c2298e49e3b5693267c34">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>深度学习, 神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>ST-GCN 时空图卷积神经网路</title>
    <url>/2024/09/13/10-54-10/</url>
    <content><![CDATA[<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<p>论文名称：Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</p>
<h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><img src="/2024/09/13/10-54-10/31983eeeb54690c58ff568866b3d2f13.png" class>
<p>在这里插入图片描述</p>
<ul>
<li>对视频进行姿态估计，在骨架序列上构造时空图；</li>
<li>在输入数据上应用多层时空图卷积(ST-GCN)，逐步在图上生成更高层次的特征图；</li>
<li>然后由标准 Softmax 分类器将其分类到相应的动作类别。</li>
<li>整个模型采用反向传播的端到端方式训练。</li>
</ul>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>传统的骨骼建模方法依赖于手工制作的部件或遍历规则，表达能力有限和泛化困难，模型很难推广到其他应用。<br>这项工作提出了动态骨架模型。</p>
<h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><ul>
<li>提出了 ST-GCN，这是一种基于图的动态骨骼建模通用公式，这是第一个将基于图的神经网络应用于该任务。</li>
<li>针对骨架建模的具体要求，提出了 ST-GCN 中卷积核的设计原则。</li>
<li>在基于骨骼的动作识别的两个大规模数据集上，与之前使用手工制作部件或遍历规则的方法相比，所提出的模型获得了更好的性能，在手工设计方面的工作量大大减少。ST-GCN 的代码和模型是公开的。</li>
</ul>
<h3 id="ST-GCN：基于图的动态骨骼建模通用公式"><a href="#ST-GCN：基于图的动态骨骼建模通用公式" class="headerlink" title="ST-GCN：基于图的动态骨骼建模通用公式"></a>ST-GCN：基于图的动态骨骼建模通用公式</h3><img src="/2024/09/13/10-54-10/c837aa026e5755538d5406dd26bdb6d2.png" class>
<p>分别为两种边：时间边，空间边。<br>ST-GCN 的层次特性消除了手工制作的部件分配或遍历规则的需要。</p>
<blockquote>
<p>传统的骨架模型方法中，根据预定义的部件分配和遍历规则对骨架进行建模。<br>具体规则：<br>1.人体骨架的构成方式和连接顺序。<br>2.节点对应的身体部位、编号和名称等信息。<br>3.骨架节点的搜索顺序和遍历方式。</p>
</blockquote>
<p><strong>STGCN可以动态学习现有数据的关系规则</strong></p>
<p>ST-GCN 在进行时空图卷积之前，仍然需要对骨架节点进行部件分配和遍历规则的设计。可以根据不同任务，设定不同规则。</p>
<h3 id="ST-GCN-模型中的组件"><a href="#ST-GCN-模型中的组件" class="headerlink" title="ST-GCN 模型中的组件"></a>ST-GCN 模型中的组件</h3><p>在具有 N 个关节和 T 个框架（帧）的骨架序列上构造了无向时空图 $G=(V,E)$</p>
<ul>
<li>节点集$V=\{v_{ti}\left|t=1, \ldots, T;i=1, \ldots, N\right\}$包括骨架序列中的所有关节,节点 $F(v_{ti}\mathrm{~})$ 上的特征向量由帧 $t$ 上第 $i$ 个关节的坐标向量和估计置信度组成。</li>
<li>边集 E:<br>骨架内连接，记为 $E_S=\{v_{ti} v_{tj} |(i,j)\in H\},$，其中 $H$ 为自然连接的人体关节集合<br>帧间边：$E_F=\{v_{ti}v_{(t+1)i}\}$</li>
</ul>
<h3 id="空间图卷积神经网络"><a href="#空间图卷积神经网络" class="headerlink" title="空间图卷积神经网络"></a>空间图卷积神经网络</h3><h4 id="单帧的图CNN模型"><a href="#单帧的图CNN模型" class="headerlink" title="单帧的图CNN模型"></a>单帧的图CNN模型</h4><p>在时间 $\tau$ 的单个帧上，将有 $N$ 个关节节点 $V_t$​，以及骨架边 $E_S\left(\tau\right)=\{v_{ti} v_{tj} |t=\tau , (i,j)\in H\}$。</p>
<p>给定核尺寸为 $K×K$ 的卷积算子，以及通道数量为 $c$ 的输入特征映射 $f_{in}$​。在空间位置 $x$ 处，单个通道的输出值可以写成：</p>
<p>$\displaystyle f_{out}\left(x\right)=\sum_{h=1}^K\sum_{w=1}^Kf_{in}\left(\mathbf{p}(\mathbf{x},h,w)\right)\cdotp\mathbf{w}(h,w)$</p>
<p><img src="/2024/09/13/10-54-10/image-20250227134832573.png" alt="image-20250227134832573"></p>
<p>抽样函数 $\mathbf{p}$：$Z^2\times Z^2\to Z^2$ 列举了空间位置 $x$ 的邻居的位置。在图像卷积，它也可以表示为 $\mathbf{p}(\mathbf{x},h,w)=\mathbf{x}+\mathbf{p}^{\prime}(h,w)$。权函数 $\mathbf{w}$:$Z^2\to\mathbb{R}^c$ 提供了一个 $c$ 维实空间中的权向量，用于计算与采样的 $c$ 维输入特征向量的内积。注意，权函数与输入位置 $x$ 无关。因此，在输入图像滤波器权重到处都是共享的。</p>
<blockquote>
<p>$\mathbf{p}^{\prime}(h,w)$：映射一个方向访问函数，类似象棋中的[0,-1],[1,0];</p>
<p>抽样函数 $\mathbf{p}$： 就是一个编码的定位函数，映射$x$位置附近的卷积核内对应的位置。</p>
</blockquote>
<p>从二维抽象到三维：<br>通过将上述公式扩展到输入特征映射位于空间图 $V_t$​ 上的情况，来定义图上的卷积操作。即特征映射 $f_{in}^t$​：$V_t​→R^c$ 在图的每个节点上都有一个向量。扩展的下一步是重新定义抽样函数 p 和权重函数 w。</p>
<h4 id="采样函数"><a href="#采样函数" class="headerlink" title="采样函数"></a>采样函数</h4><p>在图像上，采样函数 $\mathbf{p}(h,w)$ 是在相邻像素点关于中心位置 $x$ 上定义的。在图上，我们同样可以在节点 $v_{ti}$​ 的邻居集 $B(v_{ti})=\{v_{tj}|d(v_{tj},v_{ti})\leq D\}$ 上定义采样函数。这里 $d(v_{tj},v_{ti}\mathrm{~})$ 表示从 $v_{tj}$​ 到 $v_{ti}$​ 的任何路径的最小长度。因此，抽样函数 $\mathbf{p}:B(v_{ti})\to V$ 可以写成：<br>$\mathbf{p}(v_{ti},v_{tj})=v_{tj}$</p>
<blockquote>
<p>注意这里的 $v_{tj}$ 是节点 $v_{ti}$ 的邻居集 $B(v_{ti})$ 中的节点，即当 $D=1$ 时，采样函数 $\mathbf{p}$ 取的是邻接点。</p>
</blockquote>
<h4 id="权重函数"><a href="#权重函数" class="headerlink" title="权重函数"></a>权重函数</h4><p>主要是如何定义图的顺序，保证权重可以按顺序对邻接点作用。</p>
<p>将一个关节点 $v_{ti}$ 的邻居集 $B(v_{ti})$ 划分为固定数量的 $K$ 个子集，其中每个子集都有一个数字标签，从 $0$ 到 $K-1$。</p>
<p>映射 $l_{ti}: B(v_{ti} )\to\{0, \ldots, K-1\}$，<br>权重函数 $\mathbf{w}(v_{ti},v_{tj}):B(v_{ti})\to R^c$ 可以通过索引一个 $(c,K)$ 维张量或<br>$\mathbf{w}(v_{ti},v_{tj})=\mathbf{w}^{\prime}(l_{ti}\left(v_{tj}\right))$<br>来实现。</p>
<blockquote>
<p>即相同标签会使用同一个权重，保持权重函数不变，拓展到每个节点。</p>
<p>c：指通道数</p>
</blockquote>
<h4 id="空间图卷积"><a href="#空间图卷积" class="headerlink" title="空间图卷积"></a>空间图卷积</h4><p>有了改进的抽样函数和权函数，我们现在用图卷积重写公式(1)如下：<br>$\displaystyle f_{out}\left(v_{ti}\right)=\sum_{v_{tj}\in B(v_{ti})}\frac1{Z_{ti}\left(v_{tj}\right)}f_{in}\left(\mathbf{p}(v_{ti},v_{tj})\right)\cdotp\mathbf{w}(v_{ti},v_{tj})$<br>其中归一项 $Z_{ti}\left(v_{tj}\right.)=\left|v_{tk}\left|l_{ti}\left(v_{tk}\right.\right)=l_{ti}\left(v_{tj}\right.\right)|$ 等于相应子集的基数。为了平衡不同子集对输出的贡献。得到<br>$\displaystyle f_{out}\left(v_{ti}\right)=\sum_{v_{tj}\in B\left(v_{ti}\right)}\frac1{Z_{ti}\left(v_{tj}\right)}f_{in}\left(v_{tj}\right)\cdotp\mathbf{w}(l_{ti}\left(v_{tj}\right))$</p>
<h4 id="时空建模"><a href="#时空建模" class="headerlink" title="时空建模"></a>时空建模</h4><p>需要重新定义相邻位置的关系，增加时间相邻点。</p>
<blockquote>
<p>将2D图像增加时间维度，转换为3D图像，应用到图卷积神经网络。</p>
</blockquote>
<p>$\displaystyle B(v_{ti})=\{v_{qj}\left|d(v_{tj},v_{ti})\leq K,\left|q-t\right|\leq\left\lfloor\Gamma/2\right\rfloor\right\}$<br>该公式定义了每个节点的邻域 $B(v_{ti})$ 包括空间距离不超过 $K $个单位的所有节点和时间距离不超过 $\lfloor\Gamma/2\rfloor$ 个单位的所有节点。其中，参数 $\Gamma$ 控制时间上要包含在邻域图中的范围，因此可以称为时间内核大小。</p>
<p>重新修改映射标签：$\displaystyle l_{ST}\left(v_{qj}\right)=l_{ti}\left(v_{tj}\right)+\left(q-t+\left\lfloor\Gamma/2\right\rfloor\right)\times K$<br>其中 $l_{ti}\left(v_{tj}\right)$ 是在一个单帧 $t$ 上关节 $i$ 的邻接点 $j$ 的标签映射，而 $(q-t+\lfloor\Gamma/2\rfloor)\times K$ 用于在时间维度上对标签进行编码。</p>
<blockquote>
<p>对时间的前后正负，转移到正数轴上，乘$K$，将所有的数据转移到一维数组内的顺序。可以通过每一个下标，定位到第几个节点的第几个时间帧内。</p>
<p>注：时间顺序(-1,0,1)，变换映射到(0,1,2)的三组K维标签上。</p>
</blockquote>
<h3 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h3><p>标签映射：将相邻的接点映射到标签内。<br>标签划分：如何确定接点的属于哪个标签。</p>
<img src="/2024/09/13/10-54-10/2cd7d888fa671be10bd482949b831e2d.png" class>
<p>上图是构造卷积运算的划分策略。从左到右：</p>
<p>（a）输入骨架示例帧。身体关节用蓝点绘制。D=1 的过滤器的接受域用红色虚线圈绘制。</p>
<p>（b）单标签分区策略，即一个邻域内的所有节点都有相同的标签(绿色)。</p>
<p>（c）距离分区策略，这两个子集是距离为0的根节点本身(绿色)和距离为1的其他相邻点(蓝色)。</p>
<p>（d）空间结构分区策略，节点根据与骨架重心(黑色十字)的距离分为：根节点(绿色)的距离最近，向心节点的距离较短(蓝色)，而离心节点的距离较长(黄色)。</p>
<p>相同标签的子集，使用相同的权重。</p>
<p>Uni-labeling 单标签分区策略</p>
<p>所有节点一个标签，只有一种权重。</p>
<p>istance partitioning 距离分区策略</p>
<p>$D$ = 1,的情况下，只有两种权重，分为两个子集。</p>
<p>Spatial configuration partitioning 空间结构分区策略</p>
<p><strong>将节点集分为三个子集：</strong>  只有一种连接划分，没有节点划分<br>1)根节点；距离中心节点最近的节点</p>
<p>2)近节点：距离跟第二近的节点</p>
<p>3)否则远连接。其他节点</p>
<p><strong>将节点之间的连接分为三个子集：</strong><br>1)根连接；如果两个节点与中心节点跳距相同，视为根连接<br>具有对称性</p>
<p>2)近连接：一个节点比另一个节点距离中心节点更近<br>j&gt;i,则$adj[j][i]$属于近连接，看终点的距离 </p>
<p>3)否则远连接。</p>
<p>$\begin{aligned}<br>l_{ti}\left(v_{tj}\right)=<br>\begin{cases}<br>0, &amp; r_j=r_i \\<br>1, &amp; r_j &lt; r_i \\<br>2, &amp; r_j &gt; r_i<br>\end{cases}<br>\end{aligned}$</p>
<p>其中 $r_i$​ 是训练集中所有帧中从重心到关节 $i$ 的平均距离。</p>
<blockquote>
<p>会不会出现三种标签，只有两种：如端节点</p>
</blockquote>
<h3 id="可学习边缘重要性加权"><a href="#可学习边缘重要性加权" class="headerlink" title="可学习边缘重要性加权"></a>可学习边缘重要性加权</h3><p>我们在时空图卷积的每一层上都加上一个可学习的掩码 $M$。<br>掩码将根据 $E_s$​ 中每个空间图边的学习重要性权重，将节点的特征贡献扩展到其邻近节点。</p>
<p><strong>作者认为未来改进方向</strong></p>
<ul>
<li>也可以使用一个依赖于数据的注意力图，来提升空间特征的贡献扩展效果。</li>
</ul>
<h2 id="实现-ST-GCN"><a href="#实现-ST-GCN" class="headerlink" title="实现 ST-GCN"></a>实现 ST-GCN</h2><p>我们采用类似于(Kipf和Welling 2017)中的图卷积实现。单帧内关节的体内连接由表示自连接的邻接矩阵 $A$ 和单位矩阵 $I$ 表示。<br>在单帧情况下，采用第一个分区策略的 ST-GCN 可以用以下公式实现<br> $\mathbf{f}_{out}=\mathbf{\Lambda}^{ {-\frac{1}{2}} }(\mathbf{A}+\mathbf{I})\mathbf{\Lambda}^{ {-\frac{1}{2}} }\mathbf{f}_{in}\mathbf{W}$<br>解释：</p>
<ul>
<li>$\mathbf{f}_{in}$: 输入特征矩阵,每行对应一个节点的特征向量,是一个行向量，具有多个特征。</li>
<li>$\mathbf{\Lambda}$: 度矩阵</li>
<li>$\mathbf{\Lambda}^{ {-\frac{1}{2}} }$: 平方根的倒数，用于归一化</li>
<li>$\mathbf{A}$: 邻接矩阵, 行$i$是出发点，列$j$是连接点</li>
<li>$\mathbf{I}$: 单位矩阵，表示自环</li>
</ul>
<h3 id="公式的解析"><a href="#公式的解析" class="headerlink" title="公式的解析"></a>公式的解析</h3><p>(此公式来源于GCN中，ST-GCN没有做修改)</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727574536215.png" alt=" "></p>
<p><img src="/2024/09/13/10-54-10/QQ_1727574561853.png" alt=" "></p>
<p><img src="/2024/09/13/10-54-10/QQ_1727575199340.png" alt=" "></p>
<p>$D^{-1}$：代表了节点的度，用于平衡节点贡献。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727575217843.png" alt=" "></p>
<blockquote>
<p>用公式法展开算一遍，就能算出上面的公式。</p>
</blockquote>
<p><img src="/2024/09/13/10-54-10/QQ_1727575622534.png" alt=" "></p>
<p>$\begin{aligned}<br>\bar{\boldsymbol{x}}_i &amp;= \sum_{j=1}^n \tilde{a}_{i,j} \boldsymbol{x}_j  \\<br>&amp;= \sum_{j \in \text{Neigh}(i)} \tilde{a}_{i,j} \boldsymbol{x}_j  \\<br>&amp;= \sum_{j \in \text{Neigh}(i)} \frac{1}{\sqrt{d_{i,i} d_{j,j} } } \boldsymbol{x}_j<br>\end{aligned}$</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727576201509.png" alt=" "></p>
<blockquote>
<p>$W$:控制下一个layer的输入维度。做特征维度变换。</p>
<p>$A$:做特征聚合。</p>
</blockquote>
<p><img src="/2024/09/13/10-54-10/QQ_1727576424168.png" alt=" "></p>
<p>上图中：初始输入$x_i$，经过矩阵$A$，转变为下一层的输入向量。数学表示为：</p>
<p>$\begin{aligned}<br>\boldsymbol{H}_1 &amp;:= f_{\boldsymbol{W}_1}(\boldsymbol{X}, \boldsymbol{A}) \\ \boldsymbol{H}_2 &amp;:= f_{\boldsymbol{W}_2}(\boldsymbol{H}_1, \boldsymbol{A}) \\ \boldsymbol{H}_3 &amp;:= f_{\boldsymbol{W}_3}(\boldsymbol{H}_2, \boldsymbol{A})<br>\end{aligned}$</p>
<p>公式计算过程</p>
<ul>
<li><p>自环矩阵：$\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}$</p>
</li>
<li><p>对称归一化：$\tilde{\mathbf{A}}_{\mathrm{norm}}=\Lambda^{-\frac12}\tilde{\mathbf{A}}\Lambda^{-\frac12}$</p>
<blockquote>
<p>为什么不采均值归一化，高度节点在聚合过程中，与低度节点具有不同的作用，削弱高度节点的影响，防止多重传播信息爆炸。</p>
</blockquote>
<p>保持了数值的稳定性，防止特征信息在传播中过度缩放。<br>过程：</p>
<blockquote>
<p>左行右列：<br>左乘：是i行出发节点</p>
<p>右乘：是j列到达节点</p>
</blockquote>
<ol>
<li>先右乘$\Lambda^{-\frac12}$：对每个达到节点的邻接节点进行缩放,高度节点，影响被缩小了。</li>
<li>再左乘$\Lambda^{-\frac12}$：对出发节点的邻接节点进行缩放，保持传播平衡，避免单向影响。即添加了信息回传的平衡。</li>
</ol>
</li>
<li><p>输入特征与邻接矩阵传播: $\tilde{\mathbf{A}}_{\mathrm{norm}}\mathbf{f}_{\mathrm{in}}$每个节点将从它的邻居节点中接收特征并进行聚合。<br>过程：</p>
<p>$\begin{aligned}<br>\mathbf{f}_{\mathrm{in}}=\begin{bmatrix}\mathbf{f}_1 \\<br>\mathbf{f}_2 \\<br>\vdots \\<br>\mathbf{f}_N\end{bmatrix}\in\mathbb{R}^{N\times F_{\mathrm{in} } }<br>\end{aligned}$</p>
</li>
</ul>
<ul>
<li>特征传播公式：$\mathbf{f}_{\mathrm{prop}}=\mathbf{Af}_{\mathrm{in}}$</li>
<li>对于每个节点$i$，它的新特征$\displaystyle \mathbf{f}_{i,\mathrm{prop}}$，可以表示为：$\displaystyle \mathbf{f}_{i,\mathrm{prop}}=\sum_{j\in\mathcal{N}(i)}A_{ij}\mathbf{f}_j$<br>其中，$\mathcal{N}(i)$: 表示$i$的所有邻接节点</li>
<li>节点$i$的新特征向量是其所有邻居节点特征向量的加权和</li>
</ul>
<ul>
<li>假设$N$个节点，$d_{in}$维的输入特征: $\tilde{\mathbf{A}}_{\mathrm{norm}}f_{\mathrm{in}}$<br>表示将邻居节点的信息通过归一化邻接矩阵传播给每个节点。每个节点的新特征是根据其相邻节点的特征进行加权求和</li>
</ul>
<blockquote>
<p>使用自连接矩阵，是为了用邻接矩阵算度</p>
</blockquote>
<p> 其中$\Lambda^{ii}=\sum_j(A^{ij}+I^{ij})$，度矩阵。$\mathbf{W}$多个通道叠加的权重。<br>实际中在时空的情况下，输入维度为$(C,V,T)$的特征张量。</p>
<p>在有多个子集的分区中，将邻接矩阵分解为几个矩阵：例如距离划分策略中，$\mathbf{A}_0=\mathbf{I}$$   ，\mathbf{A}_1=\mathbf{A}$ 表达式带入得到：<br>$\displaystyle \mathbf{f}_{out}=\sum_j\boldsymbol{\Lambda}_j^{-\frac12}\mathbf{A}_j\boldsymbol{\Lambda}_j^{-\frac12}\mathbf{f}_{in}\mathbf{W}_j$<br>将$\displaystyle \Lambda_j^{ii}=\sum_k(A_j^{ik})+\alpha $，$\alpha = 0.001$,避免度矩阵出现空行。</p>
<p>科学系边缘重要性加权的实现：使用一个可学习掩码权重矩阵$\text{M}$，与矩阵进行一个对应位置相乘的算法。用$(\mathbf{A}+\mathbf{I})\otimes\mathbf{M}$代替$(\mathbf{A}+\mathbf{I})$，用$\mathbf{A}_j\otimes\mathbf{M}$代替$\mathbf{A}_j$。$\text{M}$被初始化为全1的矩阵。</p>
<h3 id="网络架构与训练"><a href="#网络架构与训练" class="headerlink" title="网络架构与训练"></a>网络架构与训练</h3><p>由于 ST-GCN 在不同节点上共享权重，因此在不同节点上保持输入数据的比例一致是很重要的。首先将输入骨架提供给批处理规范化层来规范化数据。</p>
<p>ST-GCN 模型由9层时空图卷积算子(ST-GCN 单元)组成。前三层有64个输出通道，接着三层有128个输出通道，最后三层有256个输出通道。这些层有9个时间内核大小。<strong>Resnet 机制应用于每个 ST-GCN 单元</strong>。</p>
<p>在每个 ST-GCN 单元后，我们<strong>以0.5概率随机剔除特征，以避免过拟合</strong>。第4和第7时序卷积层的步长设为2作为池化层。然后对得到的张量进行全局池化，得到每个序列的256维特征向量。最后，我们将它们输入 <strong>SoftMax</strong> 分类器。使用随机梯度下降学习模型，学习率为0.01。我们在每10个 epoch 之后将学习率衰减0.1。</p>
<p>为了避免过拟合，我们在动力学数据集上训练时执行<strong>两种增强</strong>来替换掉层(Kay et al 2017)。</p>
<p>首先，为了模拟相机运动，我们对所有帧的骨架序列执行随机仿射变换。特别是从第一帧到最后一帧，我们选取了几个固定的角度、平移和比例因子作为候选因子，然后随机采样三个因子的两个组合来生成仿射变换。这种转换被插入到中间帧中，以产生一种效果，就好像我们在回放过程中平滑地移动视点一样。我们把这种增强称为随机移动。</p>
<p>其次，我们在训练中从原始骨架序列中随机抽取片段，并在测试中使用所有帧。网络顶部的全局池使网络能够处理不确定长度的输入序列。</p>
<blockquote>
<p>不确定长度的输入序列，长视频？</p>
</blockquote>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>两个数据集：Kinetics 人类动作数据集(Kinetics) (Kay et al 2017)，NTURGB+D (Shahroudy et al 2016)。</p>
<p>首先对动力学数据集进行详细的<strong>消融研究</strong>，以检查所提出的模型组件对识别性能的贡献。</p>
<p>为了验证我们在无约束设置中获得的经验是否具有普遍性，我们对 NTURGB+D 上的约束设置进行了实验，并将 ST-GCN 与其他最先进的方法进行了比较。所有实验均在 PyTorch 深度学习框架上进行，并配有8个 TITANX 图形处理器。</p>
<h3 id="数据集与评估标准"><a href="#数据集与评估标准" class="headerlink" title="数据集与评估标准"></a>数据集与评估标准</h3><p><strong>Kinetics-skeleton数据集</strong></p>
<p>该数据集包含从YouTube检索的大约300000个视频片段。且划分240000个视频片段为训练集、20000个视频片段为验证集。这些视频涵盖了多达400个人类动作类，从日常活动、体育场景到复杂的互动动作。每个视频片段大约10秒。该数据集仅提供没有骨架数据的原始视频片段。</p>
<p>为了获得骨架数据，作者首先将所有视频的分辨率调整为$340×256$，并将帧速率转换为$30 FPS$。然后，作者使用OpenPose姿态估计工具来估计视频片段中的每个帧上的18个关节的位置。OpenPose给出了像素坐标系中的$2D$坐标$(X，Y)$和18个人体关节的置信度得分。因此，作者使用一个元组($X，Y，C)$来表示每个关节，一个骨架帧被记录为一个18元组的数组。对于多人情况，作者在每个片段中选择平均联合置信度最高的2个人。</p>
<h1 id="论文理解"><a href="#论文理解" class="headerlink" title="论文理解"></a>论文理解</h1><p>ST-GCN 这篇论文算是 GCN 在骨骼行为识别里面的开山之作了，他提供了一种新的思路和实现方式，解决了以前方法的局限性并取得了较好的效果。虽然他只是2018年发表的，但是这篇论文给了很详细的代码，2019年发表在 CVPR 上的 AS-GCN 和 2s-AGCN 都是在该代码的基础上改进的。</p>
<h2 id="空间特征的提取"><a href="#空间特征的提取" class="headerlink" title="空间特征的提取"></a>空间特征的提取</h2><h4 id="1-图卷积"><a href="#1-图卷积" class="headerlink" title="1. 图卷积"></a>1. 图卷积</h4><p>特征提取器，输入结点特征和图结构，输出结点最终的特征表达。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1726535732803.png" alt></p>
<p>主要的框架就是这样，图卷积相比于图像卷积，只是多左乘了一个邻接矩阵 <strong>A</strong>，后面就是一些细节。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1726535781943.png" alt></p>
<h4 id="2-感受野"><a href="#2-感受野" class="headerlink" title="2. 感受野"></a>2. 感受野</h4><p>一般图像二维卷积最小的卷积核就是 3×3 的卷积核，感受野就是一个中心点和周围八个元素共九个元素的组合。</p>
<p>这里和CNN相似，定义离中心点距离 <em>D</em>=1 ，也就是<strong>与中心点直接相连的点</strong>为一个卷积核的<strong>感受野</strong>。如图（a）所示：</p>
<p><img src="/2024/09/13/10-54-10/QQ_1726535863661.png" alt></p>
<h4 id="3-卷积核"><a href="#3-卷积核" class="headerlink" title="3. 卷积核"></a>3. 卷积核</h4><p>对应论文中的分区策略。选择的空间结构分区策略</p>
<p>选择合适的感受野和卷积核之后就能够像 CNN 那样<strong>一个点一个点的卷积计算</strong>了，<strong>卷积的过程就是提取特征的过程</strong>。</p>
<h4 id="4-注意力机制"><a href="#4-注意力机制" class="headerlink" title="4. 注意力机制"></a>4. 注意力机制</h4><p>就是增加的可学习权重掩码。</p>
<h2 id="时间特征的提取"><a href="#时间特征的提取" class="headerlink" title="时间特征的提取"></a>时间特征的提取</h2><p><img src="/2024/09/13/10-54-10/QQ_1726538726736.png" alt=" "></p>
<h2 id="整体网络架构"><a href="#整体网络架构" class="headerlink" title="整体网络架构"></a>整体网络架构</h2><p><img src="/2024/09/13/10-54-10/QQ_1728000528016.png" alt=" "></p>
<p>输出格式：</p>
<p>每个通道的数据为(T,V), 代表300帧的18个节点。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1728022424193.png" alt=" "></p>
<p>卷积：</p>
<p>空间卷积通过1x1卷积，时间卷积9x1，<code>指的是时间和空间维度，不是通道维度</code></p>
<h5 id="空间卷积：对所有时间和空间做1x1卷积。矩阵乘法会转置，图中没有转置"><a href="#空间卷积：对所有时间和空间做1x1卷积。矩阵乘法会转置，图中没有转置" class="headerlink" title="空间卷积：对所有时间和空间做1x1卷积。矩阵乘法会转置，图中没有转置"></a>空间卷积：对所有时间和空间做1x1卷积。<code>矩阵乘法会转置，图中没有转置</code></h5><blockquote>
<p>二维卷积，一个组卷积核，输出一个值。</p>
</blockquote>
<p><img src="/2024/09/13/10-54-10/QQ_1728352320911.png" alt=" "></p>
<p><img src="/2024/09/13/10-54-10/image-20250303145211970.png" alt="image-20250303145211970"></p>
<p>一层GCN中，输出的特征维度为$f_{in}*3$，3来源于子集划分。一共3组卷积核，一组卷积核有$C$个$(1,1,C)$的卷积核。</p>
<blockquote>
<p>所有子集特征叠加，不会出问题吗？ 不应该保留三种子集的特征吗？</p>
<p>不会出问题：三组卷积核处理不同的子集，最后叠加，不属于这个子集的，没有被处理。</p>
</blockquote>
<h5 id="时间卷积："><a href="#时间卷积：" class="headerlink" title="时间卷积："></a>时间卷积：</h5><p><img src="/2024/09/13/10-54-10/QQ_1726539633950.png" alt=" "></p>
<p><img src="/2024/09/13/10-54-10/1_8MwnBuSizz7-eUhrQcri5g.png" alt=" "><br>左边是向前传递函数，右边是ST-GCN网络。</p>
<ul>
<li>N：批量大小。</li>
<li>C：原始节点特征，即（坐标-X/坐标-Y/置信度）三元组。</li>
<li>T：时间步长。</li>
<li>V：图中的节点数。</li>
<li>M： 一个帧的数据记录中的骨骼数量。</li>
</ul>
<p>代码解释：</p>
<p><img src="/2024/09/13/10-54-10/1_ZxJrjNHXUCk0uhzvd5E97w.png" alt=" "></p>
<p>代码的实现是，先进行传递到下层，再进行信息聚合。</p>
<h2 id="爱因斯坦求和"><a href="#爱因斯坦求和" class="headerlink" title="爱因斯坦求和"></a>爱因斯坦求和</h2><p>口诀：</p>
<p>外部重复做乘积 ,</p>
<p>内部重复把数取 ,</p>
<p>从有到无要求和 ,</p>
<p>重复默认要丢弃.</p>
<blockquote>
<p>$\text{result}[n,c,t,w]=\sum_{k=1}^K\sum_{v=1}^Vx[n,k,c,t,v]\cdot A[k,v,w]$</p>
<p>对A进行广播，扩充$n$维，公式变为：</p>
<p>$\text{result}[c,t,w]=\sum_{k=1}^K\sum_{v=1}^Vx[k,c,t,v]\cdot A[k,v,w]$</p>
<p>$\text{result}[c,t,w]=\sum_{k=1}^K(\sum_{v=1}^Vx[k,c,t,v]\cdot A[k,v,w])$</p>
<p>对于内部：广播A，变成：$(\sum_{v=1}^Vx[k,c,t,v]\cdot A[k,c,v,w])$</p>
<p>即变为：$\text{result}^1[k,c,t,w]=(\sum_{v=1}^Vx[k,c,t,v]\cdot A[k,c,v,w])$</p>
<p>表示为第k，c维下的第$t$帧的$w$节点的特征，等于第$t$帧所有节点特征乘邻接矩阵$A$的第$w$列向量。(等于聚合了w节点的其他相邻节点特征)</p>
<p>$\text{result}[c,t,w]=\sum_{k=1}^K(\text{result}^1[k,c,t,w])$</p>
<p>表示对新的特征向量进行k重叠加。（把GCN中的输出通道拉长3倍，又消掉了）</p>
</blockquote>
<ul>
<li>$AX_1$ 与 $X_2A$ 的的区别：$A:<v,w>$,$X_1:<t,v,c>$$X_2:<c,t,v>$</c,t,v></t,v,c></v,w></li>
</ul>
<p>与输入对第t帧卷积不同，爱因斯坦求和约定，是对第c通道的第t帧进行的。</p>
<blockquote>
<p>也就是说公式计算的是正方体的上截面，而代码计算的是前截面。所以计算方式不同！！</p>
<p>左行右列：$X_2$的行向量是t向量，列向量是v向量，所以对列操作，聚合不同帧t下的节点。</p>
</blockquote>
<p>只是完成了公式中一个邻接矩阵聚合。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727597036491.png" alt=" "></p>
<p>在对节点进行空间卷积之后，它转向<em>对每个节点分别</em>进行时间卷积。我们注意到这里的元组<code>kernel=(temporal_ker, 1)</code>，其中<code>temporal_ker</code>是硬编码的<code>9</code>（9个时间步骤）。</p>
<p>主训练循环在文件“recognition.py”  中定义，其内容很典型。<em>虽然论文中没有讨论</em>，但实现利用了<em>每个 ST-GCN 块中的</em>残差连接（又名跳过连接）来处理梯度退化。由于输入输出通道不同，连接实际上是一个微型神经网络（而不是恒等连接）：</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727597188885.png" alt=" "></p>
<p>在 ST-GCN 块中，为了防止梯度爆炸并实现更稳定的收敛，在非线性层之前和之后使用 BatchNorm，这是经常建议的。DropOut 层也有助于减少过度拟合 。但代码使用 Dropout 的默认设置（意思是：概率<code>0.5</code>），而卷积层的推荐值为<code>0.1</code>或<code>0.2</code>。</p>
<h2 id="ST-GCN的缺点"><a href="#ST-GCN的缺点" class="headerlink" title="ST-GCN的缺点"></a>ST-GCN的缺点</h2><p>在上述ST-GCN网络模型的表述中，不难发现，虽然其拥有许多优点，但是ST-GCN的缺点也十分明显：</p>
<ul>
<li><p>ST-GCN中的人体骨架图是人为手工定义的，限制后面的网络学习到骨架图中的必要关联与新建连接，例如：在识别如“鼓掌”，“阅读”等与人为手工定义的人体骨架图空间上相距甚远的双手间的关系至关重要的动作时，网络很难学习到其中的关联，也很难对该原本不存在的边进行新建以建立连接，很大程度上降低了动作识别的精度；</p>
</li>
<li><p>除此之外，ST-GCN中的网络结构是分层结构，每一层都代表了不同层次的语义信息，然而作者所采用的是一个全局固定的网络拓扑结构图，导致该网络无法灵活的对各层中的语义进行建模，且由于固定的全局网络拓扑结构，其感受野野不够灵活，也无法提取多尺度的信息；</p>
</li>
<li><p>ST-GCN计算复杂度过高，且由于图卷积各节点之间的信息相互聚合，传统dropout对其无效，导致过拟合，根据此项工作，许多科研人员在此基础上做了大量的相关工作，由于手工定义图的局限性，2s-AGCN与MS-AAGCN引入了多种自适应的拓扑图，而对于分层结构，为方便对各层语义进行建模，SGCN提出了一个通道融合模块 (CAMM)；最后由于高计算复杂度shiftGCN将shift操作引入图卷积代替卷积的特征融合操作，除此之外DC-GCN_ADG提出了DC-GCN（DeCoupling Graph Convolutional Network）图卷积网络，可以在不增加计算量的同时增强图卷积的表达能力，并提出了ADG（attention-guided DropGraph）取代原始dropout，解决了图卷积过拟合的问题。</p>
</li>
</ul>
<h1 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h1><h2 id="源码结构"><a href="#源码结构" class="headerlink" title="源码结构"></a>源码结构</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config/              包含不同模型训练的配置文件</span><br><span class="line">data/                用于数据加载，处理骨架数据集。</span><br><span class="line">feeder/              数据预处理模块，读取并规范化骨架数据。</span><br><span class="line">models/              训练好的模型参数</span><br><span class="line">net/                包含ST-GCN网络的具体实现和其他模型文件</span><br><span class="line">--utils/            utilities实用工具包含一些通用函数，类或模块</span><br><span class="line">----graph.py        图的定义，节点分集</span><br><span class="line">----tgcn.py          时间图卷积网络</span><br><span class="line">----__init__.py      导入utils包</span><br><span class="line">----st_gcn.py     </span><br><span class="line">----st_gcn_twostream.py</span><br><span class="line">processor/          初始化相关模块</span><br><span class="line">----io.py            模型加载</span><br><span class="line">----processor.py    重载模型加载</span><br><span class="line">----recognition.py  识别模块</span><br><span class="line">resource/            辅助工具，如骨架数据可视化。</span><br><span class="line">tools/              训练、测试和评估的脚本。</span><br><span class="line">torchlight/  </span><br><span class="line">main.py              项目的主入口</span><br><span class="line">requirements.txt    项目所需软件包</span><br></pre></td></tr></table></figure>
<h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><p>需要先安装torchlight包</p>
<p>修改安装路径的方式：</p>
<ul>
<li><del>指定安装目录</del>：<br>pip安装的版本有问题，需要进入torchlight文件夹，运行stepup.py安装，与pip安装不同，setup.py将文件安装到了<code>/usr/lib/python3.8/site-packages/</code>,<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python setup.py install --prefix=/usr/local</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>进入torchlight文件夹，运行<code>pip install .</code>，可以安装成功</strong></p>
<p>远程同步排除文件夹：<code>./data</code>, <code>./work_dir</code></p>
<h2 id="net-utils-graph-py"><a href="#net-utils-graph-py" class="headerlink" title="net/utils/graph.py"></a>net/utils/graph.py</h2><p>图的定义函数：</p>
<p><img src="/2024/09/13/10-54-10/QQ_1726562068335.png" alt=" "></p>
<p>上图表示，openpose关节点邻接顺序。</p>
<p>矩阵的幂运算：<code>np.linalg.matrix_power(A, d)</code>:表示邻接矩阵的d跳可达关系。</p>
<h2 id="相关命令"><a href="#相关命令" class="headerlink" title="相关命令"></a>相关命令</h2><ul>
<li>测试命令：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python main.py recognition --phase test --work_dir ./work_dir/recognition/kinetics_skeleton --config ./config/kinetics.yaml --weights ./models/model.pth --test_batch_size 64 --device 0 --save_result True</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python main.py recognition -c config/st_gcn/kinetics-skeleton/test.yaml</span><br></pre></td></tr></table></figure>
<ul>
<li>训练命令</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python main.py recognition -c config/st_gcn/kinetics-skeleton/train.yaml</span><br></pre></td></tr></table></figure>
<p>测试命令<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python main.py recognition -c config/st_gcn/kinetics-skeleton/test.yaml --weights ./work_dir/recognition/kinetics_skeleton/ST_GCN/epoch50_model.pt</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>训练相关</p>
<p>train.yaml</p>
<ul>
<li><p>feeder 通常是指数据加载器（Data Feeder）</p>
</li>
<li><p>修改config文件夹内的，train.yaml，设置显卡编号为单显卡。</p>
</li>
<li>减少batch_size = 64</li>
</ul>
<h3 id="特征可视化演示"><a href="#特征可视化演示" class="headerlink" title="特征可视化演示"></a>特征可视化演示</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">python main.py demo_offline --video ./resource/media/skateboarding.mp4 --openpose /root/share/openpose/build</span><br><span class="line"></span><br><span class="line">python main.py demo_offline --video video_1.mp4 --openpose /root/share/openpose/build</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">特征可视化：</span><br><span class="line">python main.py feature_visualization  --video video_1.mp4 --openpose /root/share/openpose/build</span><br><span class="line">python main.py feature_visualization  --video video_2_1.mp4 --openpose /root/share/openpose/build</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">python main.py get_pose</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">torch源码</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># nn/conv.py</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"> def __init__(</span><br><span class="line">        self,</span><br><span class="line">        in_channels: int,</span><br><span class="line">        out_channels: int,</span><br><span class="line">        kernel_size: _size_2_t,</span><br><span class="line">        stride: _size_2_t = 1,</span><br><span class="line">        padding: Union[str, _size_2_t] = 0,</span><br><span class="line">        dilation: _size_2_t = 1,</span><br><span class="line">        groups: int = 1,</span><br><span class="line">        bias: bool = True,</span><br><span class="line">        padding_mode: str = &#x27;zeros&#x27;,  # TODO: refine this type</span><br><span class="line">        device=None,</span><br><span class="line">        dtype=None</span><br><span class="line">    ) -&gt; None:</span><br><span class="line">        factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125;</span><br><span class="line">        kernel_size_ = _pair(kernel_size)</span><br><span class="line">        stride_ = _pair(stride)</span><br><span class="line">        padding_ = padding if isinstance(padding, str) else _pair(padding)</span><br><span class="line">        dilation_ = _pair(dilation)</span><br><span class="line">        super().__init__(</span><br><span class="line">            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,</span><br><span class="line">            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)</span><br><span class="line">       </span><br></pre></td></tr></table></figure>
<p>参数解释：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">1.  in_channels: 输入通道数。这个参数定义输入数据的通道数量。例如，对于 RGB 图像，输入通道数为 3。</span><br><span class="line">2.  out_channels: 输出通道数。这个定义卷积操作后输出的通道数量。</span><br><span class="line">3.  kernel_size: _size_2_t: 卷积核的大小，表示在每个维度上卷积核的尺寸。_size_2_t 是一个表示二维尺寸的类型（例如 (height, width)），你可以输入一个单个整数或者一个二元组。</span><br><span class="line">4.  stride: _size_2_t = 1: 卷积步长，决定卷积核在输入上滑动的步幅。步长可以是单个整数或者二元组，默认为 1。</span><br><span class="line">5.  padding: Union[str, _size_2_t] = 0: 填充，控制输入在边界处的填充大小。可以是一个字符串（如 &#x27;same&#x27; 表示输出与输入大小相同，自动计算填充），或者一个二元组指定各维度的填充大小。</span><br><span class="line">6.  dilation: _size_2_t = 1: 膨胀率，表示在卷积核内插入的空洞数，默认为 1，表示没有膨胀。</span><br><span class="line">7.  groups: int = 1: 分组卷积的参数。groups 值为 1 表示标准卷积，值大于 1 时表示使用分组卷积，即将输入通道分为多个组，分别进行卷积。</span><br><span class="line">8.  bias: bool = True: 是否在卷积操作中使用偏置项。默认为 True，表示在卷积结果中加上一个偏置。</span><br><span class="line">9.  padding_mode: str = &#x27;zeros&#x27;: 填充的模式，默认为 &#x27;zeros&#x27;，表示在边界填充零。其他填充方式还包括 &#x27;reflect&#x27;、&#x27;replicate&#x27; 等。</span><br><span class="line">10.  device=None 和 dtype=None: 这两个参数用于指定设备（如 GPU 或 CPU）和数据类型（如 torch.float32）。</span><br></pre></td></tr></table></figure>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="kinetics-skeleton"><a href="#kinetics-skeleton" class="headerlink" title="kinetics-skeleton"></a>kinetics-skeleton</h3><p>是作者自己提取的数据，做了归一化0-1。<br>在转npy文件的时候继续做了预处理，转为[-0.5,0.5]，置信度没有变。</p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://arxiv.org/pdf/1801.07455.pdf">论文地址</a><br><a href="https://github.com/yysijie/st-gcn">论文代码</a></p>
<p>论文解读博客：</p>
<p><a href="https://zhuanlan.zhihu.com/p/672346603">爱因斯坦求和</a></p>
<p><a href="https://thachngoctran.medium.com/spatial-temporal-graph-convolutional-networks-st-gcn-explained-bf926c811330">时空图卷积网络（ST-GCN）——详解</a></p>
<p><a href="https://mbernste.github.io/posts/gcn/">图卷积神经网络</a></p>
<p><a href="https://blog.csdn.net/xiaoyuting999/article/details/130039164?spm=1001.2014.3001.5506">一杯水果茶！-ST-GCN 论文解读</a></p>
<p><a href="https://blog.csdn.net/qq_36756312/article/details/121351721?spm=1001.2014.3001.5506">Relissc_Cao-AAAI2018||ST-GCN：Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/409348916">yyxy-清晰图解，一图看懂图卷积GCN、时空图卷积ST-GCN</a></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习, 计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>GNN 图神经网络</title>
    <url>/2024/09/13/09-59-13/</url>
    <content><![CDATA[<p>博客链接：<a href="https://distill.pub/2021/gnn-intro/">GNN博客</a></p>
<h1 id="图是什么"><a href="#图是什么" class="headerlink" title="图是什么?"></a>图是什么?</h1><p>实体（顶点 node）之间的关系（边 edge）。A graph represents the relations (edges) between a colelction of entities(nodes). </p>
<img src="/2024/09/13/09-59-13/ef7de1a9117a2b8046f5962c517559369153e850.png@620w_!web-note.png" class>
<p>Q2: attributes in V E U 重要吗？</p>
<p>！关心图的整个架构</p>
<p>！更关心 每个顶点、每条边、和 整个图表示的信息</p>
<p>Q3: attribute 如何表示呢？</p>
<img src="/2024/09/13/09-59-13/49dd0d770ef736cee57d5d85b15e28e8dc1785df.png@620w_!web-note.png" class>
<p>Q4：数据如何表示成图？</p>
<p>图片如何表示成图？</p>
<p> 244 <em> 244 </em> 3通道，3维度的tensor</p>
<p>把图片看作一张图，一个像素是一个点；一个像素跟我是连接关系的话，像素之间连一条边。</p>
<img src="/2024/09/13/09-59-13/909d5904a2de4cc989ee7760cafe4143fcfd0f91.png@620w_!web-note.png" class>
<p>把图片上的像素 映射到 图上的每一个点</p>
<p>0-0 2-2</p>
<img src="/2024/09/13/09-59-13/74a28726ded8422863321b9a13f283272cd11c3e.png@620w_!web-note.png" class>
<p>蓝色点 表示 adjacent matrix，白色点表示无连接；通常是很大的sparse matrix</p>
<h2 id="什么可以表示为图"><a href="#什么可以表示为图" class="headerlink" title="什么可以表示为图"></a>什么可以表示为图</h2><p>文本的顺序：图的有向路</p>
<p>Q5：除了图片、文本，还有什么数据可以表示成图？</p>
<p>香料分子图、咖啡因的分子图、社交网络图、引用图(directed)</p>
<h2 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h2><p>图有几类信息需要表示？</p>
<p>nodes V, edges E, global-context U and connectivity</p>
<p>V E U 可以用 vector 表示，问题是 connectivity 如何表示？by adjacent matrix? </p>
<p>Q6：n * n 的 0-1 元素矩阵 表示 connectivity 可以吗？</p>
<p>矩阵很大，i.e., wikipedia数据集，12M nodes，矩阵会有12M行、12M列，无法存储。<br>边通常是稀疏的，存储 sparse matrix ✔；但稀疏矩阵在GPU上的高效运算，难❌<br>邻接矩阵的 行、列顺序交换，不会影响图</p>
<h1 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h1><p>输入一个高效存储、顺序无关的 Nodes, Edges, Adjacency List, Global 信息，如何用 NN 处理呢？—— Graph Neural Networks</p>
<h2 id="图神经网络的定义"><a href="#图神经网络的定义" class="headerlink" title="图神经网络的定义"></a>图神经网络的定义</h2><p>A GNN is  an optimizable transformation on all attributes of the graph (nodes, edges, global-contex ) that preserves graph symmetries (permuation invariance).</p>
<p>图上属性的 可以优化的变化，且保持图的对称信息。</p>
<p>对 attributes 进行变换时，图的结构不变化</p>
<p>Un 全局向量, Vn 顶点向量, En 边向量 分别构造一个MLP, MLP的输入输出的大小一致。</p>
<p>3个MLP f_Un, f_Vn, f_En 组成一个 GNN 的层。</p>
<p>graph-in, graph out</p>
<p>MLP f_Un, f_Vn, f_En  分别对输入的 Un 全局向量, Vn 顶点向量, En 边向量 计算，得到更新 Un+1 全局向量, Vn+1 顶点向量, En+1 边向量 。</p>
<img src="/2024/09/13/09-59-13/58b51ce69eb62a7a67d381ab5d1fe9e3989de776.png@620w_!web-note.png" class>
<h2 id="GNN-predictions-by-pooling-information"><a href="#GNN-predictions-by-pooling-information" class="headerlink" title="GNN predictions by pooling information"></a>GNN predictions by pooling information</h2><p>最后一层的输出，怎么得到预测值？</p>
<p>simplest: nodes prediction 顶点预测</p>
<p>分类预测: i.e., 空手道俱乐部喜欢 A 老师还是 B 老师</p>
<p>和 NN 类似，node embeddings 向量 接入 输出维度为 2 or n 的全连接层 + 一个 softmax，得到分类结果；输出维度为 1，得到回归结果。</p>
<p>最后一层的顶点进入 图中的 C_V_{i,n} classification 全连接网络，得到顶点的分类。</p>
<img src="/2024/09/13/09-59-13/3222c2a50c031b6fe61c40eea4cb6a85a552c347.png@620w_!web-note.png" class>
<p>Note: 所有顶点共享一个全连接层 C_V_{i,n} 的参数。</p>
<p>GNN 之前的层，不论图有多大，一层里面只有 3 个MLP</p>
<ul>
<li>所有顶点共享一个 MLP</li>
<li>所有边共享一个 MLP</li>
<li>所有的全局 U（哈哈哈，全局只有一个）不用共享。</li>
</ul>
<p><strong>complex: node predictions without node embeddings</strong></p>
<p>对某个顶点分类预测，但是没有这个顶点的向量。</p>
<p>pooling 汇聚 （似 CNN 的 pooling）</p>
<p>与缺失点连接的边的向量 4个 + 全局向量 1 个 == 代表这个缺失点的向量，再做一个全连接层的预测输出。</p>
<p>假设：所有顶点、边、全局向量的维度一致；不一致，需要做投影。</p>
<img src="/2024/09/13/09-59-13/71411099ddffd12967a5d1255744c79cf70bbd72.png@620w_!web-note.png" class>
<p>别的缺失点，连接关系不一样，最后的向量也不一样。</p>
<img src="/2024/09/13/09-59-13/83d55b86bb0e6bd45f2992eddd76b16659f7cab1.png@620w_!web-note.png" class>
<p>V_n 是只有边、没有顶点的向量</p>
<p>E_n 是边的向量</p>
<p>Rho_{E_n —&gt; V_n} 通过 pooling 汇聚从 边 +全局 到 顶点的信息， 进入顶点共享的分类层 C_V_n 之前，每个顶点都有自己的向量 embedding</p>
<p>Q: 只有 node embeddings 没有 边的向量，怎么办？</p>
<p>Rho_{V_n —&gt; E_n} 把 node embeddings 汇聚到 vertex 边上。</p>
<p>一条边，连接两个顶点，2个顶点向量相加 （+ 全局向量）== 得到 边的向量，然后进入边共享的一个 MLP 预测分类网络，得到边的预测输出。</p>
<p>Q: 没有全局向量 U, 有 node embeddings, 对整个图做预测？</p>
<img src="/2024/09/13/09-59-13/83d32576fb6bd47389a7892c2a77125e5f00686a.png@620w_!web-note.png" class>
<p>把所有的 node embeddings 加起来，得到一个全局的向量，进入全局的MLP，得到一个全局的预测输出。</p>
<p><strong>总结：不论缺少哪一类 V E U attributes，pooling 汇聚得到缺失值 embeddings，进入MLP，得到预测值。</strong></p>
<h2 id="最简单的-GNN-示意图"><a href="#最简单的-GNN-示意图" class="headerlink" title="最简单的 GNN 示意图"></a>最简单的 GNN 示意图</h2><p>input graph 经过 GNN blocks （每一个 block 里面有 3个 MLP对 V顶点 E边 U全局 attributes 更新）得到 一个同结构的 transformed graph, 但 V E U 属性值已被更新。</p>
<p>(if 某类 embeddings 缺失，通过其它 embeddings pooling 汇聚而来)</p>
<p>最后根据 V E U 某类属性做预测，接一个 MLP classification layer 输出预测信息。</p>
<img src="/2024/09/13/09-59-13/QQ20240913-103019.png" class>
<p>Q: simplest GNN 有什么问题？</p>
<p>GNN blocks 没有使用图结构信息：使用 MLP 更新属性值时，没有看到 V 顶点 E 边 的交互信息，只是 V 进 MLP_V, E 进 MLP_E, U 进 MLP_U，忽略了点边之间的连接信息。</p>
<p>overlook information: 一个顶点与哪些边相连，一个顶点与哪些顶点相连；一条边与哪些顶点相连，一条边与哪些边相连</p>
<p>GNN blocks 没有合理的将整个图的信息更新到属性值里，最后的结果不能 leverage 图的信息。</p>
<p>Q: 如何改进 GNN blocks 以考虑图的结构信息？</p>
<p>信息传递<br>Passing messages between parts of the graph</p>
<p>类似 pooling 汇聚<br>顶点的向量和邻居的向量相加，进行MLP变换。</p>
<p>对图中顶点进行更新，<br><img src="/2024/09/13/09-59-13/QQ20240913-103203.png" class></p>
<p>和卷积的不同，卷积的核权重每一个都不一样，GNN权重都是一样的，直接相加。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>GNN 对超参数比较敏感：</p>
<p>多少层、attribute的embedding的维度、汇聚使用什么操作max average、怎样传递消息</p>
<h2 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h2><h3 id="除-GNN-外，还有别的图吗？"><a href="#除-GNN-外，还有别的图吗？" class="headerlink" title="除 GNN 外，还有别的图吗？"></a>除 GNN 外，还有别的图吗？</h3><p>Multigraph</p>
<p>一张图中的多种边：有向边、无向边；分层图，一些顶点是子图的<br><img src="/2024/09/13/09-59-13/QQ20240913-103857.png" class></p>
<p>不同的图结构 影响 message passing 的操作</p>
<h3 id="Sampling-graphs-and-batching-in-GNNs"><a href="#Sampling-graphs-and-batching-in-GNNs" class="headerlink" title="Sampling graphs and batching in GNNs"></a>Sampling graphs and batching in GNNs</h3><p>Why sample graphs？</p>
<p>i.e., 一个有很多层、很大的图</p>
<p>最后一层的顶点，即使只看每一层的一阶邻居，根据消息传递，最后一层的顶点能看到一个很大的图，甚至是全图的信息（如果图的connectivity可以的话）</p>
<p>计算梯度需要 forward 过程中全部的中间变量。因为最后一层的顶点要看整个图的话，对最后一层的顶点计算梯度，需要把整个图的计算中间结果都保存下来，—&gt; 计算难 /(ㄒoㄒ)/~~ </p>
<p>—&gt; sample graph   采样</p>
<blockquote>
<p>类似候选框</p>
</blockquote>
<h3 id="sample-graph-的方法有哪些呢？"><a href="#sample-graph-的方法有哪些呢？" class="headerlink" title="sample graph 的方法有哪些呢？"></a>sample graph 的方法有哪些呢？</h3><ol>
<li>random node sampling</li>
</ol>
<p>采样 4 个黄色nodes，得到 1-degree neighbor 红色点<br>只在 sample 出来的子图上做计算，避免图特别大、内存存不了<br><img src="/2024/09/13/09-59-13/QQ20240913-104144.png" class></p>
<ol>
<li>random walk sampling </li>
</ol>
<p>随机游走<br>从某一个顶点开始，随机找一条边，然后沿着这条边走到下一个节点<br>规定最多随机走多少步，得到一个sample子图<br><img src="/2024/09/13/09-59-13/QQ20240913-104206.png" class></p>
<ol>
<li>1 + 2 </li>
</ol>
<p>随机走三步，走到的节点的邻居也考虑进来<br><img src="/2024/09/13/09-59-13/QQ20240913-104231.png" class></p>
<ol>
<li>BFS</li>
</ol>
<p>取一个点及其一阶、二阶邻居，然后再往前走 k 步，得到子图<br><img src="/2024/09/13/09-59-13/QQ20240913-104255.png" class></p>
<p><strong>4 种 sample graphs 的方法取决于数据集的形式</strong></p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>卷积神经网络：假设空间变换不变性<br>循环神经网络：假设时序延续性<br>图神经网络：假设保持图的对称性：不管怎么交换顶点顺序，GNN的作用都不变。</p>
<p>GCN:图卷积神经网络，图神经网络做了汇聚操作。<br>GCN的汇聚操作，是如何把相邻节点的特征权重$W$或者$X$，通过邻接矩阵汇聚更新到自己。<br><img src="/2024/09/13/09-59-13/QQ20241004-094759.png" class></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习, 计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>科研文章相关笔记内容</title>
    <url>/2024/09/12/07-50-15/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="2347eaffe0ebe6e0b27e96920febd59bfe96a38bc8fd1605ff67e2d17c6f70b3">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>草稿</category>
      </categories>
  </entry>
  <entry>
    <title>研一课程笔记内容</title>
    <url>/2024/09/12/07-49-36/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="b01595e4d77454862ab446cdc83538f999c489443b3aafa7bc811a7dfb543399">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>草稿</tag>
      </tags>
  </entry>
  <entry>
    <title>R-C3D-高效实时的3D活动检测网络</title>
    <url>/2024/09/12/07-46-49/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>时序动作预测</category>
      </categories>
      <tags>
        <tag>论文阅读, 视频检测, TAL</tag>
      </tags>
  </entry>
  <entry>
    <title>docker常用命令以及pycharm连接服务器</title>
    <url>/2024/09/11/18-31-30/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="2ad29bb8c0dcf6ea20a29bcd182a40e1487ff39f1d37a1615b9f8befdf980d61">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>行为识别基础模型总结</title>
    <url>/2024/08/08/07-02-12/</url>
    <content><![CDATA[<h1 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h1><h2 id="Two-Stream-NIPS2014"><a href="#Two-Stream-NIPS2014" class="headerlink" title="Two-Stream(NIPS2014)"></a>Two-Stream(NIPS2014)</h2><p>Two-Stream Convolutional Networks for Action Recognition in Videos (NIPS 2014)<br>视频理解的开山之作</p>
<h3 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h3><p><a href="https://mna12478.github.io/action_3/">mna12478-论文阅读笔记</a><br><a href="https://www.bilibili.com/video/BV1mq4y1x7RU/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=9a9f9a00848a88972d0fcfd341e9e738">李沐老师论文解读</a><br><a href="https://blog.csdn.net/ybacm/article/details/125300863">ybacm-论文阅读笔记</a></p>
<h2 id="SCNN"><a href="#SCNN" class="headerlink" title="SCNN"></a>SCNN</h2><h3 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//挂载google硬盘</span></span><br><span class="line">from google.colab import drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/drive/&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>配置环境：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">!apt-get update</span><br><span class="line">!apt-get install -y --no-install-recommends \</span><br><span class="line">    build-essential \</span><br><span class="line">    cmake \</span><br><span class="line">    git \</span><br><span class="line">    libgoogle-glog-dev \</span><br><span class="line">    libgflags-dev \</span><br><span class="line">    libprotobuf-dev \</span><br><span class="line">    protobuf-compiler \</span><br><span class="line">    libhdf5-dev \</span><br><span class="line">    libleveldb-dev \</span><br><span class="line">    liblmdb-dev \</span><br><span class="line">    libsnappy-dev \</span><br><span class="line">    libopencv-dev \</span><br><span class="line">    libopenblas-dev \</span><br><span class="line">    libatlas-base-dev \</span><br><span class="line">    libboost-all-dev \</span><br><span class="line">    libgflags-dev \</span><br><span class="line">    libgoogle-glog-dev \</span><br><span class="line">    liblmdb-dev \</span><br><span class="line">    python3-dev \</span><br><span class="line">    python3-numpy \</span><br><span class="line">    python3-pip</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h3 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h3><ul>
<li><a href="https://www.ee.columbia.edu/ln/dvmm/researchProjects/cdc/scnn.html">作者博文</a></li>
<li><a href="https://www.youtube.com/watch?v=2JwRX3umGKE">作者汇报</a></li>
</ul>
<h1 id="经典方法"><a href="#经典方法" class="headerlink" title="经典方法"></a>经典方法</h1><h2 id="iDT算法"><a href="#iDT算法" class="headerlink" title="iDT算法"></a>iDT算法</h2><p><a href="https://github.com/PKunicor/improved_trajectory_code">improved_trajectory_code仓库</a></p>
<p>来源：<a href="https://blog.csdn.net/qq_21872981/article/details/106936451">行为识别基础模型总结</a></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>论文阅读, 计算机视觉, 行为识别</tag>
      </tags>
  </entry>
  <entry>
    <title>Dense trajectories and motion boundary descriptors for action recognition - 密集轨迹DT算法</title>
    <url>/2024/07/22/12-24-41/</url>
    <content><![CDATA[<p>论文：Dense trajectories and motion boundary descriptors for action recognition</p>
<h1 id="密集轨迹算法（DT算法）"><a href="#密集轨迹算法（DT算法）" class="headerlink" title="密集轨迹算法（DT算法）"></a>密集轨迹算法（DT算法）</h1><p>1.1算法基本框架</p>
<p>如图所示即为算法的基本框架，包括密集采样特征点，特征点轨迹跟踪和基于轨迹的特征提取几个部分。后续的特征编码和分类过程则没有在图中画出。下面分别介绍一下这几个部分：</p>
<p>密集采样特征点：<br>DT方法通过网格划分的方式在图片的多个尺度上分别密集采样特征点。在多个空间尺度上采样能保证采样的特征点覆盖了所有空间位置和尺度，通常8个空间尺度已经非常足够了，若图像很大，可以适当增加。后续的特征提取也是在各个尺度上分别进行的。特征点采样的间隔（即网格的大小）W根据经验通常取W=5（其实W=5是可以通用的一个比较好的值）。</p>
<p>由于下一步的目标是在时间序列上跟踪这些特征点，但在没有任何结构的均匀区域（比如一块白色墙壁中间的点，或者一件黑色大衣中的点）中跟踪特征点是无法实现的。因此在进行特征点跟踪前要先去除一些这样的特征点。</p>
<p>此处的方法是计算每个像素点自相关矩阵的特征值，并设置阈值去除低于阈值的特征点。阈值由下式决定，式中0.001是经验值，(λ1i,λ2i)是图像I中像素点<br>i 的特征值：</p>
<p>应用上式，下图即为密集采样的一个示例效果图片，可以看出，同质区域的大部分采样点已被删除。</p>
<p>特征点轨迹跟踪：<br>得到了密集采样后的特征点，现在我们要对这些点进行跟踪得到轨迹，对轨迹的追踪是通过光流。设上一步中密集采样到的某个特征点的坐标为Pt=(xt,yt),则我们用下式来计算该特征点在下一帧图像中的位置，这样该点在后续连续帧的位置连起来就形成了一个轨迹。</p>
<p>式中ωt=(ut,vt)为密集光流场，是由It和It+1计算得到的，u和v分别代表光流的水平和垂直分量。而M则代表中值滤波器，尺寸为3*3。也就是说该式是通过计算特征点邻域内的光流中值来得到特征点的运动方向的。</p>
<p>另外作者提到，由于特征点的跟踪得到的轨迹随着时间会存在漂移现象，也就是从初始位置移动到很远的地方，故长时间的跟踪是不可靠的，所以每L帧要重新密集采样一次特征点，重新进行跟踪。在DT/iDT算法中，选取L=15。这个漂移现象我觉得解释应该是说，跟踪本身就不是绝对的，因为要计算光流场，也取决于中值滤波器的平滑操作，越跟踪，跟踪得到的轨迹可能与原始绝对轨迹之间存在更大误差，进而导致作者所说的长时间的跟踪是不可靠的，需要L帧重新采样一次再跟踪。</p>
<p>对于每个帧，如果在W×W邻域中找不到跟踪点，则会采样一个新点并将其添加到跟踪过程中，以确保轨迹的密集覆盖。至于为什么要在没有跟踪点的W <em> W邻域添加新点然后跟踪它？我认为就是要保证每一帧的W </em> W邻域都要有跟踪点，这样来实现密集轨迹，后续在得到众多轨迹后，会删除静态的轨迹和突出增大位移的轨迹，保留正常轨迹。</p>
<p>轨迹描述子<br>（1）轨迹形状描述子：</p>
<p>轨迹本身也可以构成轨迹形状特征描述子。对于一个长度为L的轨迹，其形状可以用<br>来描述，其中位移矢量ΔPt=(Pt+1−Pt)=<br>。在进行正则化后就可以得到轨迹特征描述子了。正则化方式为：</p>
<p>最终得到的轨迹特征为15*2=30维向量。</p>
<p>除了轨迹形状信息，我们还设计了描述子来嵌入外观 ( appearence ) 和运动 ( Motion ) 信息。</p>
<p>下图展示了HOG/HOF/MBH描述子捕获信息的图示：摄像机从右向左移动，而该人正从摄像机中走开。渐变/流动方向由颜色（色调）和饱和度大小表示。通过光流计算得到的HOF光流信息（第一行第二列这个图），在背景中显示出恒定的运动，（可以看出光流整体向右流动）说明这是由于相机的运动，但是作者觉得还不够完善，因为我们知道有相机的运动，但同时人也在从车里走出，和背景之间本身也会有一个相对运动，为了在一定程度上克服相机运动的影响，我们主要应该看的其实就是背景和前景之间的相对运动，于是作者使用了MBH描述子（不是作者提出的）。运动边界MBH（最右边这一列的两张图）编码人与背景之间的相对运动。</p>
<p>（2）在1.1节中的算法框架中右边就是对轨迹描述子的可视化，包括HOG/HOF/MBH。首先在1.1节这个图中，是在NNL（实验设置N：32像素、L：15帧）的时空体范围内去计算描述子的。而描述子的计算范围是nσ <em> nσ </em> nτ（σ：2 、τ：3）这样一个细分的时空网格。</p>
<h1 id="基础知识-特征描述符"><a href="#基础知识-特征描述符" class="headerlink" title="基础知识-特征描述符"></a>基础知识-特征描述符</h1><h2 id="HOG-histograms-of-oriented-gradients-：方向梯度直方图"><a href="#HOG-histograms-of-oriented-gradients-：方向梯度直方图" class="headerlink" title="HOG (histograms of oriented gradients) ：方向梯度直方图"></a>HOG (histograms of oriented gradients) ：方向梯度直方图</h2><p>目标检测的图像特征提取之（一）HOG特征<br>HOG特征描述子的主要思想是：在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。<br>HOG的实现方法：首先将图像分成小的连通区域，我们把它叫细胞单元cell。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。</p>
<img src="/2024/07/22/12-24-41/v2-db0e86f06f7145d93b56851ea0649562_r.jpg" class>
<p><a href="https://blog.csdn.net/zouxy09/article/details/7929348/">HOG相关链接</a></p>
<h2 id="HOF-histograms-of-optical-flow-：光流直方图。"><a href="#HOF-histograms-of-optical-flow-：光流直方图。" class="headerlink" title="HOF (histograms of optical flow)：光流直方图。"></a>HOF (histograms of optical flow)：光流直方图。</h2><p>同样参考下面这篇博客：HOF特征<br>HOF(Histogramsof Oriented Optical Flow)与HOG类似，是对光流方向进行加权统计，得到光流方向信息直方图。通常用于动作识别中。由于目标的尺寸会随着时间发生变化，相应的光流特征描述子的维度也会变化，同时，光流的计算对背景噪声、尺度变化以及运动方向都较敏感，因此需要寻找一种基于光流的既能表征时域动作信息，又对尺度和运动方向不敏感的特征。HOF则是基于此需求提出来的。</p>
<p>MBH（Motion boundary histograms）：作者认为如果只计算光流信息来描述运动信息的话，光流表示两帧之间的绝对运动，其中包含来自多个来源的运动，即前景对象运动和背景相机运动。 如果将摄影机运动视为动作运动，则可能会破坏动作分类。在真实视频中可以观察到各种类型的相机运动，例如缩放、倾斜、旋转等。在许多情况下，相机运动是局部平移的，并且在图像平面上平滑变化。（这是作者在这篇文章考虑的相机运动情况，被称为DT算法，但是后面作者又提出了改进克服相机运动的考虑，更好的克服了相机运动的影响，因此改进的DT被称为iDT算法。）<br>由于MBH代表光流的梯度，因此去除了局部恒定的摄像机运动，并保留了关于流场变化（即运动边界）的信息。MBH对摄像机运动的鲁棒性比光流更强，因此对动作识别更具辨别力。</p>
<p>小结：DT算法介绍了一种基于密集轨迹和运动边界直方图描述子的高效视频描述方法。<br>解决了上一篇博客的疑问：</p>
<p>如何对输入视频获得密集轨迹：密集采样特征点，通过计算密集光流场对采样得到的特征点进行密集跟踪（同时也解决了另外的一个疑问：为什么在基于时空体方法做车辆异常检测的论文中需要两个输入，一个是stvv时空视频体，一个是对应的光流场？因为得到密集轨迹对它跟踪需要计算密集光流场）<br>对获得的密集轨迹进行外观和运动的描述：HOG、MBH描述子</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>论文阅读, 计算机视觉, 行为识别</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉 卷积神经网络</title>
    <url>/2024/05/17/13-57-26/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.17：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/05/06/10-28-51/" title="eecs498 ML-DL-CV 笔记汇总">计算机视觉-笔记汇总</a>
</li>
</ul>
<p>内容列表：</p>
<ul>
<li><strong>结构概述</strong></li>
<li><p><strong>用来构建卷积神经网络的各种层</strong>  </p>
<ul>
<li>卷积层</li>
<li>汇聚层</li>
<li>归一化层</li>
<li>全连接层</li>
<li>将全连接层转化成卷积层</li>
</ul>
</li>
<li><strong>卷积神经网络的结构</strong><ul>
<li>层的排列规律</li>
<li>层的尺寸设置规律</li>
<li>案例学习（LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet）</li>
<li>计算上的考量</li>
</ul>
</li>
<li><strong>拓展资源</strong></li>
</ul>
<h1 id="卷积神经网络（CNN-ConvNet）"><a href="#卷积神经网络（CNN-ConvNet）" class="headerlink" title="卷积神经网络（CNN / ConvNet）"></a>卷积神经网络（CNN / ConvNet）</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>卷积神经网络和上一章讲的常规神经网络非常相似：它们都是由神经元组成，神经元中有具有学习能力的权重和偏差。每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出是不同类别的评分。在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或Softmax），并且在神经网络中我们实现的各种技巧和要点依旧适用于卷积神经网络。</p>
<p>那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。</p>
<p>或者说，原来的普通神经网络，实际上假设了模式之间不存在结构和交互，然后把图像展开为向量进行输入的，这样就丢失了图像中的空间信息，所以我们需要一种可以采集空间信息的方式 </p>
<h2 id="结构概述"><a href="#结构概述" class="headerlink" title="结构概述"></a>结构概述</h2><p>在上一章中，神经网络的输入是一个向量，然后在一系列的隐藏层中对它做变换。每个隐层都是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接。但是在一个隐层中，神经元相互独立不进行任何连接。最后的全连接层被称为”输出层”，在分类问题中，它输出的值被看做是不同类别的评分值。</p>
<p>_常规神经网络对于大尺寸图像效果不尽人意_。在CIFAR-10中，图像的尺寸是32x32x3（宽高均为32像素，3个颜色通道），因此，对应的的常规神经网络的第一个隐层中，每一个单独的全连接神经元就有32x32x3=3072个权重。这个数量看起来还可以接受，但是很显然这个全连接的结构不适用于更大尺寸的图像。举例说来，一个尺寸为200x200x3的图像，会让神经元包含200x200x3=120,000个权重值。而网络中肯定不止一个神经元，那么参数的量就会快速增加！显而易见，这种全连接方式效率低下，大量的参数也很快会导致网络过拟合。</p>
<p>神经元的三维排列。卷积神经网络针对输入全部是图像的情况，将结构调整得更加合理，获得了不小的优势。与常规神经网络不同，卷积神经网络的各层中的神经元是3维排列的：<strong>宽度</strong>、<strong>高度</strong>和<strong>深度</strong>（这里的<strong>深度</strong>指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）。举个例子，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是32x32x3（宽度，高度和深度）。我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的。下面是例子：</p>
<img src="/2024/05/17/13-57-26/2ef08bb4cf60805d726b2d6db39dd985_b.jpg" class>
<p>左边是一个3层的神经网络。右边是一个卷积神经网络，图例中网络将它的神经元都排列成3个维度（宽、高和深度）。卷积神经网络的每一层都将3D的输入数据变化为神经元3D的激活数据并输出。在这个例子中，红色的输入层装的是图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是3（代表了红、绿、蓝3种颜色通道）。</p>
<blockquote>
<p>卷积神经网络是由层组成的。每一层都有一个简单的API：用一些含或者不含参数的可导的函数，将输入的3D数据变换为3D的输出数据。</p>
</blockquote>
<h2 id="用来构建卷积网络的各种层"><a href="#用来构建卷积网络的各种层" class="headerlink" title="用来构建卷积网络的各种层"></a>用来构建卷积网络的各种层</h2><p>一个简单的卷积神经网络是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。卷积神经网络主要由三种类型的层构成：<strong>卷积层</strong>，<strong>汇聚（Pooling）层</strong>和<strong>全连接层</strong>（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。</p>
<p>网络结构例子：这仅仅是个概述，下面会更详解的介绍细节。一个用于CIFAR-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下：</p>
<ul>
<li>输入[32x32x3]存有图像的原始像素值，本例中图像宽高均为32，有3个颜色通道。  </li>
<li>卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积<strong>（这个内积作为元素组成下一张特征图，其含义为与模版匹配的程度）</strong>，同时每个卷积核都有一个偏差项。卷积层会计算所有神经元的输出。如果我们使用12个滤波器（也叫作核），得到的输出数据体的维度就是[32x32x12]。  </li>
<li>ReLU层将会逐个元素地进行激活函数操作，比如使用以0为阈值的$max(0,x)$作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。  </li>
<li>汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。  </li>
<li>全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。</li>
</ul>
<p>由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值。其中有的层含有参数，有的没有。具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。</p>
<p><strong>小结</strong>：</p>
<ul>
<li>简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）。  </li>
<li>卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）。  </li>
<li>每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据。  </li>
<li>有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。</li>
<li>有的层有额外的超参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）。</li>
</ul>
<img src="/2024/05/17/13-57-26/d9259be829b1cdb3d98a399ebc56defa_b.jpg" class>
<p>一个卷积神经网络的激活输出例子。左边的输入层存有原始图像像素，右边的输出层存有类别分类评分。在处理流程中的每个激活数据体是铺成一列来展示的。因为对3D数据作图比较困难，我们就把每个数据体切成层，然后铺成一列显示。最后一层装的是针对不同类别的分类得分，这里只显示了得分最高的5个评分值和对应的类别。本例中的结构是一个小的VGG网络，VGG网络后面会有讨论。</p>
<p>现在讲解不同的层，层的超参数和连接情况的细节。</p>
<h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量。</p>
<h5 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h5><p>一次卷积操作，示意图如下</p>
<img src="/2024/05/17/13-57-26/15.jpg" class>
<p>我们使用一个卷积核或者过滤器，进行卷积操作，并且将输出结果组成一个特征图，这个过程是线性的，一个核就是一个模板，输出结果就是这块图像与模版的匹配程度</p>
<p>实际上，一个卷积核是不够的，会存在多个卷积核，并且以多维张量的形式表示</p>
<img src="/2024/05/17/13-57-26/17.jpg" class>
<p>这里有六个卷积核，所以卷积核为一个四维张量，每个卷积核大小为3*5*5</p>
<img src="/2024/05/17/13-57-26/23.jpg" class>
<p>如果我们进行多次卷积操作，那么一张3*32*32的图片，经过大小为6*3*5*5的卷积核卷积，就得到六张特征图，大小为6*28*28</p>
<p>但是，我们并不能直接这样操作，因为实际上卷积是线性操作，多个卷积仍然是线性网络，所以我们需要加上激活函数，这样才可以更好的学习</p>
<img src="/2024/05/17/13-57-26/25.jpg" class>
<p>卷积神经网络的核心依然是学习模板，只不过是分层学习，不同层的网络学习不同的模板，第一层学习一些局部的模板（比如说特定边缘），</p>
<p><strong>概述和直观介绍</strong>：首先讨论的是，在没有大脑和生物意义上的神经元之类的比喻下，卷积层到底在计算什么。卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案。</p>
<p>在每个卷积层上，我们会有一整个集合的滤波器（比如12个），每个都会生成一个不同的二维激活图。将这些激活映射在深度方向上层叠起来就生成了输出数据。</p>
<p><strong>以大脑做比喻</strong>：如果你喜欢用大脑和生物神经元来做比喻，那么输出的3D数据中的每个数据项可以被看做是神经元的一个输出，而该神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。现在开始讨论神经元的连接，它们在空间中的排列，以及它们参数共享的模式。  </p>
<p><strong>局部连接</strong>：在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的<strong>感受野（receptive field）</strong>，它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。</p>
<p>_例1_：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。</p>
<p>_例2_：假设输入数据体的尺寸是[16x16x20]，感受野尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20=180个连接。再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）。</p>
<img src="/2024/05/17/13-57-26/ba9dcfa847a71cb695c2653230ea9147_b.jpg" class>
<p><strong>左边</strong>：红色的是输入数据体（比如CIFAR-10中的图像），蓝色的部分是第一个卷积层中的神经元。卷积层中的每个神经元都只是与输入数据体的一个局部在空间上相连，但是与输入数据体的所有深度维度全部相连（所有颜色通道）。在深度方向上有多个神经元（本例中5个），它们都接受输入数据的同一块区域（<strong>感受野</strong>相同）。至于深度列的讨论在下文中有。</p>
<p><strong>右边</strong>：神经网络章节中介绍的神经元保持不变，它们还是计算权重和输入的内积，然后进行激活函数运算，只是它们的连接被限制在一个局部空间。</p>
<p><strong>空间排列</strong>：上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：<strong>深度（depth），步长（stride）</strong>和<strong>零填充（zero-padding）</strong>。下面是对它们的讨论：</p>
<ol>
<li>首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为<strong>深度列（depth column）</strong>，也有人使用纤维（fibre）来称呼它们。</li>
<li>其次，在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。</li>
<li>在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的。这个<strong>零填充（zero-padding）</strong>的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。  </li>
</ol>
<p>输出数据体在空间上的尺寸可以通过输入数据体尺寸（W），卷积层中神经元的感受野尺寸（F），步长（S）和零填充的数量（P）的函数来计算。（这里假设输入数组的空间形状是正方形，即高度和宽度相等）输出数据体的空间尺寸为(W-F +2P)/S+1。比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出。如果步长为2，输出就是3x3。下面是例子：</p>
<img src="/2024/05/17/13-57-26/90af0bd67ba498239688c81fd61bbc66_b.jpg" class>
<p>空间排列的图示。在本例中只有一个空间维度（x轴），神经元的感受野尺寸F=3，输入尺寸W=5，零填充P=1。左边：神经元使用的步长S=1，所以输出尺寸是(5-3+2)/1+1=5。右边：神经元的步长S=2，则输出尺寸是(5-3+2)/2+1=3。注意当步长S=3时是无法使用的，因为它无法整齐地穿过数据体。从等式上来说，因为(5-3+2)=4是不能被3整除的。</p>
<p>本例中，神经元的权重是[1,0,-1]，显示在图的右上角，偏差值为0。这些权重是被所有黄色的神经元共享的（参数共享的内容看下文相关内容）。</p>
<h5 id="使用零填充"><a href="#使用零填充" class="headerlink" title="使用零填充"></a>使用零填充</h5><p>在上面左边例子中，注意输入维度是5，输出维度也是5。之所以如此，是因为感受野是3并且使用了1的零填充。如果不使用零填充，则输出数据体的空间维度就只有3，因为这就是滤波器整齐滑过并覆盖原始数据需要的数目。一般说来，当步长$S=1$时，零填充的值是$P=(F-1)/2$，这样就能保证输入和输出数据体有相同的空间尺寸。这样做非常常见，在介绍卷积神经网络的结构的时候我们会详细讨论其原因。  </p>
<p>_步长的限制_：注意这些空间排列的超参数之间是相互限制的。举例说来，当输入尺寸$W=10$，不使用零填充则$P=0$，滤波器尺寸$F=3$，这样步长$S=2$就行不通，因为$(W-F+2P)/S+1=(10-3+0)/2+1=4.5$，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体。因此，这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，或者修改零填充值来让设置合理，或者修改输入数据体尺寸来让设置合理，或者其他什么措施。在后面的卷积神经网络结构小节中，读者可以看到合理地设置网络的尺寸让所有的维度都能正常工作，这件事可是相当让人头痛的。而使用零填充和遵守其他一些设计策略将会有效解决这个问题。  </p>
<p>_真实案例_：Krizhevsky构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227x227x3]。在第一个卷积层，神经元使用的感受野尺寸$F=11$，步长$S=4$，不使用零填充$P=0$。因为(227-11)/4+1=55，卷积层的深度$K=96$，则卷积层的输出数据体尺寸为[55x55x96]。55x55x96个神经元中，每个都和输入数据体中一个尺寸为[11x11x3]的区域全连接。在深度列上的96个神经元都是与输入数据体中同一个[11x11x3]区域连接，但是权重不同。有一个有趣的细节，在原论文中，说的输入图像尺寸是224x224，这是肯定错误的，因为(224-11)/4+1的结果不是整数。这件事在卷积神经网络的历史上让很多人迷惑，而这个错误到底是怎么发生的没人知道。我的猜测是Alex忘记在论文中指出自己使用了尺寸为3的额外的零填充。  </p>
<p><strong>参数共享</strong>：在卷积层中使用参数共享是用来控制参数的数量。就用上面的例子，在第一个卷积层就有55x55x96=290,400个神经元，每个有11x11x3=364个参数和1个偏差。将这些合起来就是290400x364=105,705,600个参数。单单第一层就有这么多参数，显然这个数目是非常大的。</p>
<p>作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量。换言之，就是将深度维度上一个单独的2维切片看做<strong>深度切片（depth slice）</strong>，比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]。在每个深度切片上的神经元都使用同样的权重和偏差。在这样的参数共享下，例子中的第一个卷积层就只有96个不同的权重集了，一个权重集对应一个深度切片，共有96x11x11x3=34,848个不同的权重，或34,944个参数（+96个偏差）。在每个深度切片中的55x55个权重使用的都是同样的参数。在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。</p>
<p>注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的<strong>卷积</strong>（这就是”卷积层”名字由来）。这也是为什么总是将这些权重集合称为<strong>滤波器（filter）</strong>（或<strong>卷积核（kernel）</strong>），因为它们和输入进行了卷积。</p>
<img src="/2024/05/17/13-57-26/dd62e1d75bda9b592dabb91627d68aa6_b.jpg" class>
<p>Krizhevsky等学习到的滤波器例子。这96个滤波器的尺寸都是[11x11x3]，在一个深度切片中，每个滤波器都被55x55个神经元共享。注意参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也会同样是有用的，这是因为图像结构具有平移不变性。所以在卷积层的输出数据体的55x55个不同位置中，就没有必要重新学习去探测一个水平边界了。</p>
<p>注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为<strong>局部连接层</strong>（Locally-Connected Layer）。</p>
<p><strong>Numpy例子</strong>：为了让讨论更加的具体，我们用代码来展示上述思路。假设输入数据体是numpy数组<strong>X</strong>。那么：</p>
<ul>
<li>一个位于<strong>(x,y)</strong>的深度列（或纤维）将会是<strong>X[x,y,:]</strong>。  </li>
<li>在深度为<strong>d</strong>处的深度切片，或激活图应该是<strong>X[:,:,d]</strong>。  </li>
</ul>
<p>_卷积层例子_：假设输入数据体<strong>X</strong>的尺寸<strong>X.shape:(11,11,4)</strong>，不使用零填充（$P=0$），滤波器的尺寸是$F=5$，步长$S=2$。那么输出数据体的空间尺寸就是(11-5)/2+1=4，即输出数据体的宽度和高度都是4。那么在输出数据体中的激活映射（称其为<strong>V</strong>）看起来就是下面这样（在这个例子中，只有部分元素被计算）：</p>
<ul>
<li><strong>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0  
</strong></li>
<li><strong>V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0  
</strong></li>
<li><strong>V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0  
</strong></li>
<li><strong>V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</strong>  </li>
</ul>
<p>在numpy中，<strong>*</strong>操作是进行数组间的逐元素相乘。权重向量<strong>W0</strong>是该神经元的权重，<strong>b0</strong>是其偏差。在这里，<strong>W0</strong>被假设尺寸是<strong>W0.shape: (5,5,4)</strong>，因为滤波器的宽高是5，输入数据量的深度是4。注意在每一个点，计算点积的方式和之前的常规神经网络是一样的。同时，计算内积的时候使用的是同一个权重和偏差（因为参数共享），在宽度方向的数字每次上升2（因为步长为2）。要构建输出数据体中的第二张激活图，代码应该是：</p>
<ul>
<li><strong>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1  
</strong></li>
<li><strong>V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1  
</strong></li>
<li><strong>V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1  
</strong></li>
<li><strong>V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</strong>  </li>
<li><strong>V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1 </strong>（在y方向上）  </li>
<li><strong>V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1 </strong>（或两个方向上同时）</li>
</ul>
<p>我们访问的是<strong>V</strong>的深度维度上的第二层（即index1），因为是在计算第二个激活图，所以这次试用的参数集就是<strong>W1</strong>了。在上面的例子中，为了简洁略去了卷积层对于输出数组<strong>V</strong>中其他部分的操作。还有，要记得这些卷积操作通常后面接的是ReLU层，对激活图中的每个元素做激活函数运算，这里没有显示。</p>
<p><strong>小结</strong>： 我们总结一下卷积层的性质：</p>
<ul>
<li>输入数据体的尺寸为$W_1times H_1times D_1$</li>
<li>4个超参数：<ul>
<li>滤波器的数量$K$</li>
<li>滤波器的空间尺寸$F$</li>
<li>步长$S$</li>
<li>零填充数量$P$</li>
</ul>
</li>
<li>输出数据体的尺寸为$W_2 \times H_2 \times D_2$ ，其中：<br>$W_2=(W_1-F+2P)/S+1$<ul>
<li>$H_2=(H_1-F+2P)/S+1$ （宽度和高度的计算方法相同）<br>$D_2=K$</li>
</ul>
</li>
<li>由于参数共享，每个滤波器包含$F \cdot F \cdot D_1$个权重，卷积层一共有$F \cdot F \cdot D_1 \cdot K$个权重和$K$个偏置。</li>
<li>在输出数据体中，第$d$个深度切片（空间尺寸是$W_2 \times H_2$），用第$d$个滤波器和输入数据进行有效卷积运算的结果（使用步长$S$），最后在加上第$d$个偏差。</li>
</ul>
<p>对这些超参数，常见的设置是$F=3$，$S=1$，$P=1$。同时设置这些超参数也有一些约定俗成的惯例和经验，可以在下面的卷积神经网络结构章节中查看。</p>
<p>卷积层演示：下面是一个卷积层的运行演示。因为3D数据难以可视化，所以所有的数据（输入数据体是蓝色，权重数据体是红色，输出数据体是绿色）都采取将深度切片按照列的方式排列展现。输入数据体的尺寸是$W_1=5,H_1=5,D_1=3$，卷积层参数$K=2,F=3,S=2,P=1$。就是说，有2个滤波器，滤波器的尺寸是$3 \cdot 3$，它们的步长是2.因此，输出数据体的空间尺寸是(5-3+2)/2+1=3。注意输入数据体使用了零填充$P=1$，所以输入数据体外边缘一圈都是0。下面的例子在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来。  </p>
<img src="/2024/05/17/13-57-26/333077b83ed421d6bd53eb7a44fd5799_b.jpg" class>
<p><strong>用矩阵乘法实现</strong>：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法：</p>
<ol>
<li><p>输入图像的局部区域被<strong>im2col</strong>操作拉伸为列。比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3=363的列向量。重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)/4+1=55，所以得到_im2col_操作的输出矩阵<strong>X_col</strong>的尺寸是[363x3025]，其中每列是拉伸的感受野，共有55x55=3,025个。注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。  </p>
</li>
<li><p>卷积层的权重也同样被拉伸成行。举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵<strong>W_row</strong>，尺寸为[96x363]。  </p>
</li>
<li><p>现在卷积的结果和进行一个大矩阵乘<strong>np.dot(W_row, X_col)</strong>是等价的了，能得到每个滤波器和每个感受野间的点积。在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出。  </p>
</li>
<li><p>结果最后必须被重新变为合理的输出尺寸[55x55x96]。</p>
</li>
</ol>
<p>这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在<strong>X_col</strong>中被复制了多次。但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的BLAS API）。还有，同样的_im2col_思路可以用在汇聚操作中。</p>
<p>反向传播：卷积操作的反向传播（同时对于数据和权重）还是一个卷积（但是是和空间上翻转的滤波器）。使用一个1维的例子比较容易演示。  </p>
<p><strong>1x1卷积</strong>：一些论文中使用了1x1的卷积，这个方法最早是在论文Network in Network中出现。人们刚开始看见这个1x1卷积的时候比较困惑，尤其是那些具有信号处理专业背景的人。因为信号是2维的，所以1x1卷积就没有意义。但是，在卷积神经网络中不是这样，因为这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）。  </p>
<p><strong>扩张卷积</strong>：最近一个研究（Fisher Yu和Vladlen Koltun的论文）给卷积层引入了一个新的叫_扩张（dilation）_的超参数。到目前为止，我们只讨论了卷积层滤波器是连续的情况。但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张。举例，在某个维度上滤波器<strong>w</strong>的尺寸是3，那么计算输入<strong>x</strong>的方式是：<strong>w[0]<em>x[0] + w[1]</em>x[1] + w[2]*x[2]</strong>，此时扩张为0。如果扩张为1，那么计算为： <strong>w[0]<em>x[0] + w[1]</em>x[2] + w[2]*x[4]</strong>。换句话说，操作中存在1的间隙。在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为在很少的层数内更快地汇集输入图片的大尺度特征。比如，如果上下重叠2个3x3的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中5x5的区域（可以成这些神经元的_有效感受野_是5x5）。如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长。  </p>
<h4 id="汇聚层"><a href="#汇聚层" class="headerlink" title="汇聚层"></a>汇聚层</h4><p>通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。汇聚层的一些公式：</p>
<ul>
<li>输入数据体尺寸$W_1 \cdot H_1 \cdot D_1$  </li>
<li><p>有两个超参数：  </p>
<ul>
<li>空间大小$F$</li>
<li>步长$S$</li>
</ul>
</li>
<li><p>输出数据体尺寸$W_2 \cdot H_2 \cdot D_2$，其中<br>$W_2=(W_1-F)/S+1$<br>$H_2=(H_1-F)/S+1$<br>$D_2=D_1$</p>
</li>
<li><p>因为对输入进行的是固定函数计算，所以没有引入参数  </p>
</li>
<li>在汇聚层中很少使用零填充  </li>
</ul>
<p>在实践中，最大汇聚层通常只有两种形式：一种是$F=3,S=2$，也叫重叠汇聚（overlapping pooling），另一个更常用的是$F=2,S=2$。对更大感受野进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性。</p>
<p><strong>普通汇聚（General Pooling）</strong>：除了最大汇聚，汇聚单元还可以使用其他的函数，比如_平均_汇聚_（average pooling）_或_L-2范式_汇聚_（L2-norm pooling）_。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。</p>
<img src="/2024/05/17/13-57-26/641c8846abcb02d35938660cf96cef1b_b.jpg" class>
<p>汇聚层在输入数据体的每个深度切片上，独立地对其进行空间上的降采样。左边：本例中，输入数据体尺寸[224x224x64]被降采样到了[112x112x64]，采取的滤波器尺寸是2，步长为2，而深度不变。右边：最常用的降采样操作是取最大值，也就是最大汇聚，这里步长为2，每个取最大值操作是从4个数字中选取（即2x2的方块区域中）。</p>
<p><strong>反向传播：</strong>回顾一下反向传播的内容，其中$max(x,y)$函数的反向传播可以简单理解为将梯度只沿最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大元素的索引记录下来（有时这个也叫作<strong>道岔（switches）</strong>），这样在反向传播的时候梯度的路由就很高效。</p>
<p><strong>不使用汇聚层</strong>：很多人不喜欢汇聚操作，认为可以不使用它。比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，无汇聚层的结构不太可能扮演重要的角色。</p>
<h4 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h4><p>归一化方式的作用，就是使得神经网络更容易训练（或者说可以使得输出具有零均值和单位方差），常见的方式称为批量归一化，即Batch Normalization</p>
<p>这种方式减少了一种称为内部协变偏移的东西，有利于优化</p>
<p>从下图可以看到，加上归一化层之后，更有利于模型收敛</p>
<img src="/2024/05/17/13-57-26/87.jpg" class>
<p>这是因为，每一层看到的都是上一层的输出，但是上一层的权重是会进行学习的，上一次输出的分布也是会改变的，某种意义上不利于优化，所以需要标准化所有层</p>
<p>在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。对于不同类型的归一化层，可以看看Alex Krizhevsky的关于cuda-convnet library API的讨论。  </p>
<h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看_神经网络_章节。</p>
<h2 id="把全连接层转化成卷积层"><a href="#把全连接层转化成卷积层" class="headerlink" title="把全连接层转化成卷积层"></a>把全连接层转化成卷积层</h2><p>全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：</p>
<ul>
<li>对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块（这是因为有局部连接），其余部分都是零。而在其中大部分块中，元素都是相等的（因为参数共享）。  </li>
<li>相反，任何全连接层都可以被转化为卷积层。比如，一个$K=4096$的全连接层，输入数据体的尺寸是$7 \times 7 \times 512$，这个全连接层可以被等效地看做一个$F=7,P=0,S=1,K=4096$的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成$1 \times 1 \times 4096$，这个结果就和使用初始的那个全连接层一样了。  </li>
</ul>
<p><strong>全连接层转化为卷积层</strong>：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：  </p>
<ul>
<li>针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为$F=7$，这样输出数据体就为[1x1x4096]了。  </li>
<li>针对第二个全连接层，令其滤波器尺寸为$F=1$，这样输出数据体为[1x1x4096]。  </li>
<li>对最后一个全连接层也做类似的，令其$F=1$，最终输出为[1x1x1000]  </li>
</ul>
<p>实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（_<strong>译者注</strong>：即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分_），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。</p>
<p>举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！（_<strong>译者注</strong>：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解_）</p>
<blockquote>
<p>面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。</p>
</blockquote>
<p>自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。</p>
<p>最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。（_<strong>译者注</strong>：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解_）</p>
<ul>
<li>Net Surgery上一个使用Caffe演示如何在进行变换的IPython Note教程。  </li>
</ul>
<h3 id="卷积神经网络的结构"><a href="#卷积神经网络的结构" class="headerlink" title="卷积神经网络的结构"></a><strong>卷积神经网络的结构</strong></h3><p>卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。</p>
<h4 id="层的排列规律"><a href="#层的排列规律" class="headerlink" title="层的排列规律"></a>层的排列规律</h4><p>卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：</p>
<p><strong>INPUT -&gt; [[CONV -&gt; RELU]<em>N -&gt; POOL?]</em>M -&gt; [FC -&gt; RELU]*K -&gt; FC</strong></p>
<p>其中<strong>*</strong>指的是重复次数，<strong>POOL?</strong>指的是一个可选的汇聚层。其中<strong>N &gt;=0</strong>,通常<strong>N&lt;=3</strong>,<strong>M&gt;=0</strong>,<strong>K&gt;=0</strong>,通常<strong>K&lt;3</strong>。例如，下面是一些常见的网络结构规律：</p>
<ul>
<li><strong>INPUT -&gt; FC</strong>,实现一个线性分类器，此处<strong>N = M = K = 0</strong>。  </li>
<li><strong>INPUT -&gt; CONV -&gt; RELU -&gt; FC</strong>  </li>
<li><strong>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC</strong>。此处在每个汇聚层之间有一个卷积层。  </li>
<li><strong>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]<em>3 -&gt; [FC -&gt; RELU]</em>2 -&gt; FC</strong>。此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。  </li>
</ul>
<p>_几个小滤波器卷积层的组合比一个大滤波器卷积层好_：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7，但是就有一些缺点。首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有$C$个通道，那么单独的7x7卷积层将会包含$C \times (7 \times 7 \times C)=49C^2$个参数，而3个3x3的卷积层的组合仅有$3 \times (C \times (3 \times 3 \times C))=27C^2$个参数。直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。</p>
<p>最新进展：传统的将层按照线性进行排列的方法已经受到了挑战，挑战来自谷歌的Inception结构和微软亚洲研究院的残差网络（Residual Net）结构。这两个网络（下文案例学习小节中有细节）的特征更加复杂，连接结构也不同。</p>
<h4 id="层的尺寸设置规律"><a href="#层的尺寸设置规律" class="headerlink" title="层的尺寸设置规律"></a>层的尺寸设置规律</h4><p>到现在为止，我们都没有提及卷积神经网络中每层的超参数的使用。现在先介绍设置结构尺寸的一般性规则，然后根据这些规则进行讨论：</p>
<p><strong>输入层</strong>（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。</p>
<p><strong>卷积层</strong>应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长$S=1$。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当$F=3$，那就使用$P=1$来保持输入尺寸。当$F=5,P=2$，一般对于任意$F$，当$P=(F-1)/2$的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上。</p>
<p><strong>汇聚层</strong>负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野（即$F=2$）的最大值汇聚，步长为2（$S=2$）。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。</p>
<p>_减少尺寸设置的问题_：上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。</p>
<p>_为什么在卷积层使用1的步长_？在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。</p>
<p>_为何使用零填充_？使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。</p>
<p>_因为内存限制所做的妥协_：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。</p>
<h4 id="案例学习"><a href="#案例学习" class="headerlink" title="案例学习"></a>案例学习</h4><p>下面是卷积神经网络领域中比较有名的几种结构：</p>
<ul>
<li><strong>LeNet</strong>： 第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的。当然，最著名还是被应用在识别数字和邮政编码等的LeNet结构。</li>
<li><strong>AlexNet</strong>：AlexNet卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的ImageNet ILSVRC 竞赛中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。</li>
<li><strong>ZF Net</strong>：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为 ZFNet（Zeiler &amp; Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。</li>
<li><strong>GoogLeNet</strong>：ILSVRC 2014的胜利者是谷歌的Szeged等实现的卷积神经网络。它主要的贡献就是实现了一个_奠基模块_，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了。GooLeNet还有几种改进的版本，最新的一个是Inception-v4。</li>
<li><strong>VGGNet</strong>：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为VGGNet。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的预训练模型是可以在网络上获得并在Caffe中使用的。VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。</li>
<li><strong>ResNet</strong>：残差网络（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的_跳跃链接_，大量使用了批量归一化（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲，以及一些使用Torch重现网络的实验。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，可以看论文Identity Mappings in Deep Residual Networks，2016年3月发表。</li>
</ul>
<p><strong>VGGNet的细节：</strong>我们进一步对VGGNet的细节进行分析学习。整个VGGNet中的卷积层都是以步长为1进行3x3的卷积，使用了1的零填充，汇聚层都是以步长为2进行了2x2的最大值汇聚。可以写出处理过程中每一步数据体尺寸的变化，然后对数据尺寸和整体权重的数量进行查看：</p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">INPUT: [224x224x3]        memory:  <span class="number">224</span>*<span class="number">224</span>*<span class="number">3</span>=150K   weights: <span class="number">0</span></span><br><span class="line">CONV3-<span class="number">64</span>: [224x224x64]  memory:  <span class="number">224</span>*<span class="number">224</span>*<span class="number">64</span>=<span class="number">3.2</span>M   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">3</span>)*<span class="number">64</span> = <span class="number">1</span>,<span class="number">728</span></span><br><span class="line">CONV3-<span class="number">64</span>: [224x224x64]  memory:  <span class="number">224</span>*<span class="number">224</span>*<span class="number">64</span>=<span class="number">3.2</span>M   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">64</span>)*<span class="number">64</span> = <span class="number">36</span>,<span class="number">864</span></span><br><span class="line">POOL2: [112x112x64]  memory:  <span class="number">112</span>*<span class="number">112</span>*<span class="number">64</span>=800K   weights: <span class="number">0</span></span><br><span class="line">CONV3-<span class="number">128</span>: [112x112x128]  memory:  <span class="number">112</span>*<span class="number">112</span>*<span class="number">128</span>=<span class="number">1.6</span>M   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">64</span>)*<span class="number">128</span> = <span class="number">73</span>,<span class="number">728</span></span><br><span class="line">CONV3-<span class="number">128</span>: [112x112x128]  memory:  <span class="number">112</span>*<span class="number">112</span>*<span class="number">128</span>=<span class="number">1.6</span>M   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">128</span>)*<span class="number">128</span> = <span class="number">147</span>,<span class="number">456</span></span><br><span class="line">POOL2: [56x56x128]  memory:  <span class="number">56</span>*<span class="number">56</span>*<span class="number">128</span>=400K   weights: <span class="number">0</span></span><br><span class="line">CONV3-<span class="number">256</span>: [56x56x256]  memory:  <span class="number">56</span>*<span class="number">56</span>*<span class="number">256</span>=800K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">128</span>)*<span class="number">256</span> = <span class="number">294</span>,<span class="number">912</span></span><br><span class="line">CONV3-<span class="number">256</span>: [56x56x256]  memory:  <span class="number">56</span>*<span class="number">56</span>*<span class="number">256</span>=800K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">256</span>)*<span class="number">256</span> = <span class="number">589</span>,<span class="number">824</span></span><br><span class="line">CONV3-<span class="number">256</span>: [56x56x256]  memory:  <span class="number">56</span>*<span class="number">56</span>*<span class="number">256</span>=800K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">256</span>)*<span class="number">256</span> = <span class="number">589</span>,<span class="number">824</span></span><br><span class="line">POOL2: [28x28x256]  memory:  <span class="number">28</span>*<span class="number">28</span>*<span class="number">256</span>=200K   weights: <span class="number">0</span></span><br><span class="line">CONV3-<span class="number">512</span>: [28x28x512]  memory:  <span class="number">28</span>*<span class="number">28</span>*<span class="number">512</span>=400K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">256</span>)*<span class="number">512</span> = <span class="number">1</span>,<span class="number">179</span>,<span class="number">648</span></span><br><span class="line">CONV3-<span class="number">512</span>: [28x28x512]  memory:  <span class="number">28</span>*<span class="number">28</span>*<span class="number">512</span>=400K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">CONV3-<span class="number">512</span>: [28x28x512]  memory:  <span class="number">28</span>*<span class="number">28</span>*<span class="number">512</span>=400K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">POOL2: [14x14x512]  memory:  <span class="number">14</span>*<span class="number">14</span>*<span class="number">512</span>=100K   weights: <span class="number">0</span></span><br><span class="line">CONV3-<span class="number">512</span>: [14x14x512]  memory:  <span class="number">14</span>*<span class="number">14</span>*<span class="number">512</span>=100K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">CONV3-<span class="number">512</span>: [14x14x512]  memory:  <span class="number">14</span>*<span class="number">14</span>*<span class="number">512</span>=100K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">CONV3-<span class="number">512</span>: [14x14x512]  memory:  <span class="number">14</span>*<span class="number">14</span>*<span class="number">512</span>=100K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">POOL2: [7x7x512]  memory:  <span class="number">7</span>*<span class="number">7</span>*<span class="number">512</span>=25K  weights: <span class="number">0</span></span><br><span class="line">FC: [1x1x4096]  memory:  <span class="number">4096</span>  weights: <span class="number">7</span>*<span class="number">7</span>*<span class="number">512</span>*<span class="number">4096</span> = <span class="number">102</span>,<span class="number">760</span>,<span class="number">448</span></span><br><span class="line">FC: [1x1x4096]  memory:  <span class="number">4096</span>  weights: <span class="number">4096</span>*<span class="number">4096</span> = <span class="number">16</span>,<span class="number">777</span>,<span class="number">216</span></span><br><span class="line">FC: [1x1x1000]  memory:  <span class="number">1000</span> weights: <span class="number">4096</span>*<span class="number">1000</span> = <span class="number">4</span>,096,<span class="number">000</span></span><br><span class="line"></span><br><span class="line">TOTAL memory: 24M * <span class="number">4</span> <span class="built_in">bytes</span> ~= 93MB / image (only forward! ~*<span class="number">2</span> <span class="keyword">for</span> bwd)</span><br><span class="line">TOTAL params: 138M parameters</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>注意，大部分的内存和计算时间都被前面的卷积层占用，大部分的参数都用在后面的全连接层，这在卷积神经网络中是比较常见的。在这个例子中，全部参数有140M，但第一个全连接层就包含了100M的参数。</p>
<h2 id="计算上的考量"><a href="#计算上的考量" class="headerlink" title="计算上的考量"></a>计算上的考量</h2><p>在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈。大部分现代GPU的内存是3/4/6GB，最好的GPU大约有12GB的内存。要注意三种内存占用来源：</p>
<ul>
<li>来自中间数据体尺寸：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）。通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）。在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到。但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量。  </li>
<li>来自参数尺寸：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存。因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多。  </li>
<li>卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等。  </li>
</ul>
<p>一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位。把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量。如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉 反向传播</title>
    <url>/2024/05/17/13-29-46/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.17：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/05/06/10-28-51/" title="eecs498 ML-DL-CV 笔记汇总">计算机视觉-笔记汇总</a>
</li>
</ul>
<p>内容列表：</p>
<ul>
<li>简介</li>
<li>简单表达式和理解梯度</li>
<li>复合表达式，链式法则，反向传播</li>
<li>直观理解反向传播</li>
<li>模块：Sigmoid例子</li>
<li>反向传播实践：分段计算</li>
<li>回传流中的模式</li>
<li>用户向量化操作的梯度</li>
<li>小结</li>
</ul>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>目标</strong>：本节将帮助读者对<strong>反向传播</strong>形成直观而专业的理解。反向传播是利用<strong>链式法则</strong>递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常<strong>关键</strong>。  </p>
<p><strong>问题陈述</strong>：这节的核心问题是：给定函数$f(x)$, 其中$x$是输入数据的向量，需要计算函数$f$关于$x$的梯度，也就是$\nabla f(x)$。</p>
<p><strong>目标</strong>：之所以关注上述问题，是因为在神经网络中$f$对应的是损失函数（$L$），输入$x$里面包含训练数据和神经网络的权重。举个例子，损失函数可以是SVM的损失函数，输入则包含了训练数据$(x_i,y_i),i=1…N$、权重$W$和偏差$b$。注意训练集是给定的（在机器学习中通常都是这样），而权重是可以控制的变量。因此，即使能用反向传播计算输入数据$x_i$ 上的梯度，但在实践为了进行参数更新，通常也只计算参数（比如$W,b$）的梯度。然而$x_i$的梯度有时仍然是有用的：比如将神经网络所做的事情可视化便于直观理解的时候，就能用上。</p>
<p>如果读者之前对于利用链式法则计算偏微分已经很熟练，仍然建议浏览本篇笔记。因为它呈现了一个相对成熟的反向传播视角，在该视角中能看见基于实数值回路的反向传播过程，而对其细节的理解和收获将帮助读者更好地通过本课程。  </p>
<h2 id="简单表达式和理解梯度"><a href="#简单表达式和理解梯度" class="headerlink" title="简单表达式和理解梯度"></a>简单表达式和理解梯度</h2><p>从简单表达式入手可以为复杂表达式打好符号和规则基础。先考虑一个简单的二元乘法函数$f(x,y)=xy$。对两个输入变量分别求偏导数还是很简单的：  </p>
<p>$\displaystyle f(x,y)=xy \to \frac {df}{dx}=y \quad \frac {df}{dy}=x$  </p>
<p><strong>解释</strong>：牢记这些导数的意义：函数变量在某个点周围的极小区域内变化，而导数就是变量变化导致的函数在该方向上的变化率。  </p>
<p>$\displaystyle \frac{df(x)}{dx}= \lim_{h \to 0}\frac{f(x+h)-f(x)}{h}$</p>
<p>注意等号左边的分号和等号右边的分号不同，不是代表分数。相反，这个符号表示操作符$\frac{d}{dx}$被应用于函数$f$，并返回一个不同的函数（导数）。对于上述公式，可以认为$h$值非常小，函数可以被一条直线近似，而导数就是这条直线的斜率。换句话说，每个变量的导数指明了整个表达式对于该变量的值的敏感程度。比如，若$x=4,y=-3$，则$f(x,y)=-12$，$x$的导数$\frac{\partial f}{\partial x}=-3$。这就说明如果将变量$x$的值变大一点，整个表达式的值就会变小（原因在于负号），而且变小的量是$x$变大的量的三倍。通过重新排列公式可以看到这一点（$f(x+h)=f(x)+h \frac{df(x)}{dx}$）。同样，因为$\frac{\partial f}{\partial y}=4$，可以知道如果将$y$的值增加$h$，那么函数的输出也将增加（原因在于正号），且增加量是$4h$。  </p>
<p>&gt; 函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度。  </p>
<p>如上所述，梯度$\nabla f$是偏导数的向量，所以有$\nabla f(x)=[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]=[y,x]$。即使是梯度实际上是一个向量，仍然通常使用类似”x上的梯度”的术语，而不是使用如”x的偏导数”的正确说法，原因是因为前者说起来简单。</p>
<p>我们也可以对加法操作求导：  </p>
<p>$\displaystyle f(x,y)=x+y \to \frac {df}{dx}=1 \quad \frac {df}{dy}=1$</p>
<p>这就是说，无论其值如何，$x,y$的导数均为1。这是有道理的，因为无论增加$x,y$中任一个的值，函数$f$的值都会增加，并且增加的变化率独立于$[x,y]$的具体值（情况和乘法操作不同）。取最大值操作也是常常使用的：<br>$\displaystyle f(x,y)=max(x,y) \to \frac {df}{dx}=1 (x&gt;=y) \quad \frac {df}{dy}=1 (y&gt;=x)$ </p>
<p>上式是说，如果该变量比另一个变量大，那么梯度是1，反之为0。例如，若$x=4,y=2$，那么$max$是4，所以函数对于$y$就不敏感。也就是说，在$y$上增加$h$，函数还是输出为4，所以梯度是0：因为对于函数输出是没有效果的。当然，如果给$y$增加一个很大的量，比如大于2，那么函数$f$的值就变化了，但是导数并没有指明输入量有巨大变化情况对于函数的效果，他们只适用于输入量变化极小时的情况，因为定义已经指明：$\displaystyle \lim_{h \to 0}$。  </p>
<h2 id="使用链式法则计算复合表达式"><a href="#使用链式法则计算复合表达式" class="headerlink" title="使用链式法则计算复合表达式"></a>使用链式法则计算复合表达式</h2><p>现在考虑更复杂的包含多个函数的复合函数，比如$f(x,y,z)=(x+y)z$。虽然这个表达足够简单，可以直接微分，但是在此使用一种有助于读者直观理解反向传播的方法。将公式分成两部分：$q=x+y$和$f=qz$。在前面已经介绍过如何对这分开的两个公式进行计算，因为$f$是$q$和$z$相乘，所以$\displaystyle \frac{\partial f}{\partial q}=z,\frac{\partial f}{\partial z}=q$，又因为$q$是$x$加$y$，所以$\displaystyle \frac{\partial q}{\partial x}=1,\frac{\partial q}{\partial y}=1$。然而，并不需要关心中间量$q$的梯度，因为$\frac{\partial f}{\partial q}$没有用。相反，函数$f$关于$x,y,z$的梯度才是需要关注的。<strong>链式法则</strong>指出将这些梯度表达式链接起来的正确方式是相乘，比如$\displaystyle \frac{\partial f}{\partial x}=\frac{\partial f}{\partial q}\frac{\partial q}{\partial x}$。在实际操作中，这只是简单地将两个梯度数值相乘，示例代码如下：</p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置输入值</span></span><br><span class="line">x = -<span class="number">2</span>; y = <span class="number">5</span>; z = -<span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">q = x + y <span class="comment"># q becomes 3</span></span><br><span class="line">f = q * z <span class="comment"># f becomes -12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行反向传播:</span></span><br><span class="line"><span class="comment"># 首先回传到 f = q * z</span></span><br><span class="line">dfdz = q <span class="comment"># df/dz = q, 所以关于z的梯度是3</span></span><br><span class="line">dfdq = z <span class="comment"># df/dq = z, 所以关于q的梯度是-4</span></span><br><span class="line"><span class="comment"># 现在回传到q = x + y</span></span><br><span class="line">dfdx = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dx = 1. 这里的乘法是因为链式法则</span></span><br><span class="line">dfdy = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dy = 1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>最后得到变量的梯度[<strong>dfdx, dfdy, dfdz]</strong>，它们告诉我们函数<strong>f</strong>对于变量[<strong>x, y, z]</strong>的敏感程度。这是一个最简单的反向传播。一般会使用一个更简洁的表达符号，这样就不用写<strong>df</strong>了。这就是说，用<strong>dq</strong>来代替<strong>dfdq</strong>，且总是假设梯度是关于最终输出的。</p>
<p>这次计算可以被可视化为如下计算线路图像：  </p>
<img src="/2024/05/17/13-29-46/213da7f66594510b45989bd134fc2d8b_b.jpg" class>
<p>上图的真实值计算线路展示了计算的视觉化过程。<strong>前向传播</strong>从输入计算到输出（绿色），<strong>反向传播</strong>从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。</p>
<h2 id="反向传播的直观理解"><a href="#反向传播的直观理解" class="headerlink" title="反向传播的直观理解"></a>反向传播的直观理解</h2><p>反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：1. 这个门的输出值，和2.其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。</p>
<p>&gt; 这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等。  </p>
<p>下面通过例子来对这一过程进行理解。加法门收到了输入[-2, 5]，计算输出是3。既然这个门是加法操作，那么对于两个输入的局部梯度都是+1。网络的其余部分计算出最终值为-12。在反向传播时将递归地使用链式法则，算到加法门（是乘法门的输入）的时候，知道加法门的输出的梯度是-4。如果网络如果想要输出值更高，那么可以认为它会想要加法门的输出更小一点（因为负号），而且还有一个4的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让-4乘以<strong>x</strong>和<strong>y</strong>的局部梯度，x和y的局部梯度都是1，所以最终都是-4）。可以看到得到了想要的效果：如果<strong>x，y减小</strong>（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。  </p>
<p>因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高。  </p>
<h2 id="模块化：Sigmoid例子"><a href="#模块化：Sigmoid例子" class="headerlink" title="模块化：Sigmoid例子"></a>模块化：Sigmoid例子</h2><p>上面介绍的门是相对随意的。任何可微分的函数都可以看做门。可以将多个门组合成一个门，也可以根据需要将一个函数分拆成多个门。现在看看一个表达式：  </p>
<p>$\displaystyle f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$</p>
<p>在后面的课程中可以看到，这个表达式描述了一个含输入<strong>x</strong>和权重<strong>w</strong>的2维的神经元，该神经元使用了_sigmoid激活_函数。但是现在只是看做是一个简单的输入为$x$和$w$，输出为一个数字的函数。这个函数是由多个门组成的。除了上文介绍的加法门，乘法门，取最大值门，还有下面这4种：</p>
<p>$\displaystyle f(x)=frac{1}{x} \to \frac{df}{dx}=-1/x^2$<br>$\displaystyle f_c(x)=c+x \to \frac{df}{dx}=1$<br>$\displaystyle f(x)=e^x \to \frac{df}{dx}=e^x$<br>$\displaystyle f_a(x)=ax \to \frac{df}{dx}=a$</p>
<p>其中，函数$f_c$使用对输入值进行了常量$c$的平移，$f_a$将输入值扩大了常量$a$倍。它们是加法和乘法的特例，但是这里将其看做一元门单元，因为确实需要计算常量$c,a$的梯度。整个计算线路如下：</p>
<img src="/2024/05/17/13-29-46/0799b3d6e5e92245ee937db3c26d1b80_b.png" class>
<p>使用sigmoid激活函数的2维神经元的例子。输入是[x0, x1]，可学习的权重是[w0, w1, w2]。一会儿会看见，这个神经元对输入数据做点积运算，然后其激活数据被sigmoid函数挤压到0到1之间。  </p>
<p>在上面的例子中可以看见一个函数操作的长链条，链条上的门都对<strong>w</strong>和<strong>x</strong>的点积结果进行操作。该函数被称为sigmoid函数$\sigma (x)$。sigmoid函数关于其输入的求导是可以简化的(使用了在分子上先加后减1的技巧)：</p>
<p>$\displaystyle \sigma(x)=\frac{1}{1+e^{-x}}$<br>$\displaystyle \to \frac{d \sigma(x)}{dx}=\frac{e^{-x}}{(1+e^{-x})^2}=(\frac{1+e^{-x}-1}{1+e^{-x}})(\frac{1}{1+e^{-x}})=(1-\sigma(x))\sigma(x)$</p>
<p>可以看到梯度计算简单了很多。举个例子，sigmoid表达式输入为1.0，则在前向传播中计算出输出为0.73。根据上面的公式，局部梯度为(1-0.73)*0.73~=0.2，和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。因此，在实际的应用中将这些操作装进一个单独的门单元中将会非常有用。该神经元反向传播的代码实现如下：  </p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">w = [<span class="number">2</span>,-<span class="number">3</span>,-<span class="number">3</span>] <span class="comment"># 假设一些随机数据和权重</span></span><br><span class="line">x = [-<span class="number">1</span>, -<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">dot = w[<span class="number">0</span>]*x[<span class="number">0</span>] + w[<span class="number">1</span>]*x[<span class="number">1</span>] + w[<span class="number">2</span>]</span><br><span class="line">f = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-dot)) <span class="comment"># sigmoid函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对神经元反向传播</span></span><br><span class="line">ddot = (<span class="number">1</span> - f) * f <span class="comment"># 点积变量的梯度, 使用sigmoid函数求导</span></span><br><span class="line">dx = [w[<span class="number">0</span>] * ddot, w[<span class="number">1</span>] * ddot] <span class="comment"># 回传到x</span></span><br><span class="line">dw = [x[<span class="number">0</span>] * ddot, x[<span class="number">1</span>] * ddot, <span class="number">1.0</span> * ddot] <span class="comment"># 回传到w</span></span><br><span class="line"><span class="comment"># 完成！得到输入的梯度</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><br><strong>实现提示：分段反向传播</strong>。上面的代码展示了在实际操作中，为了使反向传播过程更加简洁，把向前传播分成不同的阶段将是很有帮助的。比如我们创建了一个中间变量<strong>dot</strong>，它装着<strong>w</strong>和<strong>x</strong>的点乘结果。在反向传播的时，就可以（反向地）计算出装着<strong>w</strong>和<strong>x</strong>等的梯度的对应的变量（比如<strong>ddot</strong>，<strong>dx</strong>和<strong>dw</strong>）。</p>
<p>本节的要点就是展示反向传播的细节过程，以及前向传播过程中，哪些函数可以被组合成门，从而可以进行简化。知道表达式中哪部分的局部梯度计算比较简洁非常有用，这样他们可以”链”在一起，让代码量更少，效率更高。  </p>
<h2 id="反向传播实践：分段计算"><a href="#反向传播实践：分段计算" class="headerlink" title="反向传播实践：分段计算"></a>反向传播实践：分段计算</h2><p>看另一个例子。假设有如下函数：  </p>
<p>$\displaystyle f(x,y)=\frac{x+\sigma(y)}{\sigma(x)+(x+y)^2}$</p>
<p>首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子，需要强调的是，如果对$x$或$y$进行微分运算，运算结束后会得到一个巨大而复杂的表达式。然而做如此复杂的运算实际上并无必要，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码模式：  </p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = <span class="number">3</span> <span class="comment"># 例子数值</span></span><br><span class="line">y = -<span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">sigy = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-y)) <span class="comment"># 分子中的sigmoi          #(1)</span></span><br><span class="line">num = x + sigy <span class="comment"># 分子                                    #(2)</span></span><br><span class="line">sigx = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-x)) <span class="comment"># 分母中的sigmoid         #(3)</span></span><br><span class="line">xpy = x + y                                              <span class="comment">#(4)</span></span><br><span class="line">xpysqr = xpy**<span class="number">2</span>                                          <span class="comment">#(5)</span></span><br><span class="line">den = sigx + xpysqr <span class="comment"># 分母                                #(6)</span></span><br><span class="line">invden = <span class="number">1.0</span> / den                                       <span class="comment">#(7)</span></span><br><span class="line">f = num * invden <span class="comment"># 搞定！                                 #(8)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>到了表达式的最后，就完成了前向传播。注意在构建代码s时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。这样计算反向传播就简单了：我们对前向传播时产生每个变量(<strong>sigy, num, sigx, xpy, xpysqr, den, invden</strong>)进行回传。我们会有同样数量的变量，但是都以<strong>d</strong>开头，用来存储对应变量的梯度。注意在反向传播的每一小块中都将包含了表达式的局部梯度，然后根据使用链式法则乘以上游梯度。对于每行代码，我们将指明其对应的是前向传播的哪部分。</p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 回传 f = num * invden</span></span><br><span class="line">dnum = invden <span class="comment"># 分子的梯度                                         #(8)</span></span><br><span class="line">dinvden = num                                                     <span class="comment">#(8)</span></span><br><span class="line"><span class="comment"># 回传 invden = 1.0 / den </span></span><br><span class="line">dden = (-<span class="number">1.0</span> / (den**<span class="number">2</span>)) * dinvden                                <span class="comment">#(7)</span></span><br><span class="line"><span class="comment"># 回传 den = sigx + xpysqr</span></span><br><span class="line">dsigx = (<span class="number">1</span>) * dden                                                <span class="comment">#(6)</span></span><br><span class="line">dxpysqr = (<span class="number">1</span>) * dden                                              <span class="comment">#(6)</span></span><br><span class="line"><span class="comment"># 回传 xpysqr = xpy**2</span></span><br><span class="line">dxpy = (<span class="number">2</span> * xpy) * dxpysqr                                        <span class="comment">#(5)</span></span><br><span class="line"><span class="comment"># 回传 xpy = x + y</span></span><br><span class="line">dx = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line">dy = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line"><span class="comment"># 回传 sigx = 1.0 / (1 + math.exp(-x))</span></span><br><span class="line">dx += ((<span class="number">1</span> - sigx) * sigx) * dsigx <span class="comment"># Notice += !! See notes below  #(3)</span></span><br><span class="line"><span class="comment"># 回传 num = x + sigy</span></span><br><span class="line">dx += (<span class="number">1</span>) * dnum                                                  <span class="comment">#(2)</span></span><br><span class="line">dsigy = (<span class="number">1</span>) * dnum                                                <span class="comment">#(2)</span></span><br><span class="line"><span class="comment"># 回传 sigy = 1.0 / (1 + math.exp(-y))</span></span><br><span class="line">dy += ((<span class="number">1</span> - sigy) * sigy) * dsigy                                 <span class="comment">#(1)</span></span><br><span class="line"><span class="comment"># 完成! 嗷~~</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>需要注意的一些东西：</p>
<p><strong>对前向传播变量进行缓存</strong>：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。</p>
<p><strong>在不同分支的梯度要相加</strong>：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用 <strong>+=</strong> 而不是 <strong>=</strong> 来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的_多元链式法则_，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。</p>
<h2 id="回传流中的模式"><a href="#回传流中的模式" class="headerlink" title="回传流中的模式"></a>回传流中的模式</h2><p>一个有趣的现象是在多数情况下，反向传播中的梯度可以被很直观地解释。例如神经网络中最常用的加法、乘法和取最大值这三个门单元，它们在反向传播过程中的行为都有非常简单的解释。先看下面这个例子：  </p>
<img src="/2024/05/17/13-29-46/39162d0c528144362cc09f1965d710d1_b.jpg" class>
<p>一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。  </p>
<p>从上例可知：</p>
<p><strong>加法门单元</strong>把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门把梯度2.00不变且相等地路由给了两个输入。</p>
<p><strong>取最大值门单元</strong>对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了<strong>z</strong>变量，因为<strong>z</strong>的值比<strong>w</strong>高，于是<strong>w</strong>的梯度保持为0。</p>
<p><strong>乘法门单元</strong>相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，<strong>x</strong>的梯度是-4.00x2.00=-8.00。<br>$f=x*y\ \frac{\partial f}{\partial x}=y$<br>_非直观影响及其结果_。注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，那么乘法门的操作将会不是那么直观：它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。在线性分类器中，权重和输入是进行点积$w^Tx_i$，这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本$x_i$乘以1000，那么权重的梯度将会增大1000倍，这样就必须降低学习率来弥补。这就是为什么数据预处理关系重大，它即使只是有微小变化，也会产生巨大影响。对于梯度在计算线路中是如何流动的有一个直观的理解，可以帮助读者调试网络。</p>
<h2 id="用向量化操作计算梯度"><a href="#用向量化操作计算梯度" class="headerlink" title="用向量化操作计算梯度"></a>用向量化操作计算梯度</h2><p>上述内容考虑的都是单个变量情况，但是所有概念都适用于矩阵和向量操作。然而，在操作的时候要注意关注维度和转置操作。</p>
<p>关于向量的求导中维度的变化，公式如下<br><img src="/2024/05/17/13-29-46/69-1682754437671-1.jpg" class></p>
<p>得到的实际上是雅克比矩阵，这个矩阵可以描述每个输出对输入的影响</p>
<p><strong>矩阵相乘的梯度</strong>：可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作：  </p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">W = np.random.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">X = np.random.randn(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">D = W.dot(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们得到了D的梯度</span></span><br><span class="line">dD = np.random.randn(*D.shape) <span class="comment"># 和D一样的尺寸</span></span><br><span class="line">dW = dD.dot(X.T) <span class="comment">#.T就是对矩阵进行转置</span></span><br><span class="line">dX = W.T.dot(dD)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>_提示：要分析维度！_注意不需要去记忆<strong>dW</strong>和<strong>dX</strong>的表达，因为它们很容易通过维度推导出来。例如，权重的梯度dW的尺寸肯定和权重矩阵W的尺寸是一样的，而这又是由<strong>X</strong>和<strong>dD</strong>的矩阵乘法决定的（在上面的例子中<strong>X</strong>和<strong>W</strong>都是数字不是矩阵）。总有一个方式是能够让维度之间能够对的上的。例如，<strong>X</strong>的尺寸是[10x3]，<strong>dD</strong>的尺寸是[5x3]，如果你想要dW和W的尺寸是[5x10]，那就要<strong>dD.dot(X.T)</strong>。</p>
<p><strong>使用小而具体的例子</strong>：有些读者可能觉得向量化操作的梯度计算比较困难，建议是写出一个很小很明确的向量化例子，在纸上演算梯度，然后对其一般化，得到一个高效的向量化操作形式。  </p>
<p><strong>矩阵相乘反向传播的通用策略</strong></p>
<p>那么在实际操作中，如何实现反向传播呢？下面这个是一个例子，我们从接收到的上游梯度（实际上是一个N*M的矩阵）<br>$<br>\frac{\mathrm{d} L }{\mathrm{d} y}:[N\times M]<br>$<br>这个矩阵可以告诉我们，y对L的影响是什么样的</p>
<p>我们需要找到一种隐式的计算的方式<br><img src="/2024/05/17/13-29-46/88.jpg" class></p>
<p>我们先从输入的一个元素开始考虑，我们假设这个元素为<br>$<br>x_{11}<br>$<br>我们从这个元素开始考虑，这时候有<br>$<br>\frac{\mathrm{d} L }{\mathrm{d} x_{11}}=\frac{\mathrm{d} y }{\mathrm{d} x_{11}}\cdot \frac{\mathrm{d} L }{\mathrm{d} y}<br>$<br>其中上流梯度一支，我们只需求y对x的梯度即可</p>
<img src="/2024/05/17/13-29-46/92.jpg" class>
<p>然后我们根据矩阵乘法可知，y的第一行第一列的元素是x第一行与w第一列的乘积，对x_11求导的结果就是w_11</p>
<p>类似的可以推广到y的所有元素，然后就可以得到y对x_11的导数以及y对x的导数</p>
<img src="/2024/05/17/13-29-46/6-105.jpg" class>
<p>矩阵乘法例子下的隐式雅克比矩阵计算，计算稀疏矩阵的有效方法</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。</li>
<li>讨论了<strong>分段计算</strong>在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其”链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。</li>
</ul>
<p>在下节课中，将会开始定义神经网络，而反向传播使我们能高效计算神经网络各个节点关于损失函数的梯度。换句话说，我们现在已经准备好训练神经网络了，本课程最困难的部分已经过去了！ConvNets相比只是向前走了一小步。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉 神经网络</title>
    <url>/2024/05/07/12-17-20/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.07：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/05/06/10-28-51/" title="eecs498 ML-DL-CV 笔记汇总">计算机视觉-笔记汇总</a>
</li>
</ul>
<h2 id="特征变换"><a href="#特征变换" class="headerlink" title="特征变换"></a>特征变换</h2><p>我们从几何学的角度看到了这一点，记得从线性分类器的几何学角度来看，我们认为线性分类器是在画高维超平面，将这个高维的欧几里得空间分割成两块</p>
<p>当我们从视觉角度考虑线性分类器时，我们认为线性分类器只为每个类别学习一个模板，因此它们无法代表同一对象类别的多种模式。</p>
<p>总之，模板是一种用于表示目标对象特征的数据结构，可以采用多种形式。在模板匹配和模板学习任务中，模板被用来在输入数据中搜索和识别目标对象</p>
<p>对于某些线性不可分的数据集，我们可以提出一种特征变换的方式（feature transform），将这些数据映射到特征空间去进行分类，或者说找到一种合适分类的映射方式来完成分类任务，这样就可以克服线性分类器的一些缺点；在这里我们是在特征空间中训练分类器</p>
<img src="/2024/05/07/12-17-20/image-20230425220806741.png" class>
<p>这就是一个将笛卡尔坐标系变换到极坐标系下的例子，这样的话，原本在笛卡尔坐标系下线性不可分的数据集，在极坐标系下线性可分了，也就是说我们可以在特征空间下构建线性分类器</p>
<p>特征变换的思想在计算机视觉中应用广泛，一种就是颜色直方图的概念</p>
<p>我们可以将图像中的颜色空间的RGB光谱进行分解与统计，然后得到颜色直方图，这种方式丢弃了图像中的空间信息，只关心图像中的颜色分布，更具有空间不变性（对不同位置的相同特征可以很好的处理）</p>
<h2 id="图像特征：定向梯度直方图"><a href="#图像特征：定向梯度直方图" class="headerlink" title="图像特征：定向梯度直方图"></a>图像特征：定向梯度直方图</h2><p>还有一种进行图像特征提取的方式就是定向梯度直方图</p>
<img src="/2024/05/07/12-17-20/10.jpg" class>
<p>这种方式可以通过一系列处理得到图像的局部梯度信息，具有很好的区分性和鲁棒性，然后，这些特征可以被用于训练分类器</p>
<h2 id="图像特征：词袋模型（数据驱动）"><a href="#图像特征：词袋模型（数据驱动）" class="headerlink" title="图像特征：词袋模型（数据驱动）"></a>图像特征：词袋模型（数据驱动）</h2><p>数据驱动，实际上是由我们在训练集中看到的数据驱动的，而数据驱动的特征转换的一个例子就是视觉词袋模型</p>
<p>想法是有一个大型训练数据集，可以提取大量不同比例和大小的随机补丁（也就是从图像上随机切下一小块），然后我们对这些随机补丁进行聚类，就获得了所谓的密码本或者一组视觉词，他们可以表示图像中倾向于出现哪种特征，如果这些补丁在训练集中大量出现，那么我们希望可以学习某种视觉词表示，它可以识别你的训练集中的每一个常见特征</p>
<p>之后的步骤是构建视觉词的代码本，使用学习到的视觉词的代码本来编码你的图像，来表示每个视觉词在单个输入图像中出现了多少次</p>
<img src="/2024/05/07/12-17-20/16.jpg" class>
<p>实际上，2011年的ImageNet挑战赛里面的冠军方案，就是基于这种模型；他们的方案分为两部分：特征提取器和可学习分类器</p>
<h2 id="神经网络概述"><a href="#神经网络概述" class="headerlink" title="神经网络概述"></a>神经网络概述</h2><p>在不诉诸大脑的类比的情况下，依然是可以对神经网络算法进行介绍的。在线性分类一节中，在给出图像的情况下，是使用$s=Wx$来计算不同视觉类别的评分，其中$W$是一个矩阵，$x$是一个输入列向量，它包含了图像的全部像素数据。在使用数据库CIFAR-10的案例中，$x$是一个[3072x1]的列向量，$W$是一个[10x3072]的矩阵，所以输出的评分是一个包含10个分类评分的向量。</p>
<p>在之前我们使用线性分类器进行分类，但是接下来我们使用一种两层神经网络来进行学习</p>
<p>神经网络算法不同于线性网络，它的计算公式是$s=W_2max(0,W_1x)$。其中$W_1$的含义是这样的：举个例子来说，它可以是一个[100x3072]的矩阵，其作用是将图像转化为一个100维的过渡向量。函数$max(0,-)$是非线性的，它会作用到每个元素。这个非线性函数有多种选择，后续将会学到。但这个形式是一个最常用的选择，它就是简单地设置阈值，将所有小于0的值变成0。最终，矩阵$W_2$的尺寸是[10x100]，因此将得到10个数字，这10个数字可以解释为是分类的评分。注意非线性函数在计算上是至关重要的，如果略去这一步，那么两个矩阵将会合二为一，对于分类的评分计算将重新变成关于输入的线性函数。这个非线性函数就是改变的关键点。参数$W_1,W_2$将通过随机梯度下降来学习到，他们的梯度在反向传播过程中，通过链式法则来求导计算得出。</p>
<p>一个三层的神经网络可以类比地看做$s=W_3max(0,W_2max(0,W_1x))$，其中$W_1,W_2,W_3$是需要进行学习的参数。中间隐层的尺寸是网络的超参数，后续将学习如何设置它们。现在让我们先从神经元或者网络的角度理解上述计算</p>
<p>之前的线性函数为<br>$<br>\begin{split}<br>f=Wx<br>\end{split}<br>$<br>现在使用的两层神经网络为<br>$<br>\begin{split}<br>f=W_2\max(0,W_1x)\\<br>W_2\in \mathbb{R}^{C\times H}\quad<br>W_1\in \mathbb{R}^{H\times D}\quad<br>x\in \mathbb{R}^{D}<br>\end{split}<br>$<br>其中包括两个可学习的权重矩阵$W_1,W_2$，当然在实践过程中，这些向量乘法都会隐式的包含一个可学习的偏差项</p>
<p>当然我们也可以使用这种图形化的形式来表示神经网络，因为其中的每个部分都相互连接，所以也被称为全连接神经网络</p>
<img src="/2024/05/07/12-17-20/24.jpg" class>
<p>这个激活函数的功能就是可以拓宽神经网络的拟合能力，因为如果有多个线性分类器组合，那么结果还是一个线性分类器，多个分类器等于一个分类器</p>
<h2 id="快速简介"><a href="#快速简介" class="headerlink" title="快速简介"></a>快速简介</h2><h2 id="神经网络的几何解释"><a href="#神经网络的几何解释" class="headerlink" title="神经网络的几何解释"></a>神经网络的几何解释</h2><p>面对一个线性可分的数据集，线性分类器的功能就是找到一个或者多个超平面，可以对不同的数据进行分割，但是存在某些线性不可分的数据集，这个时候，就无法使用超平面进行分割了，或者说找不到一种线性方式进行分割，比如说下面的数据集</p>
<img src="/2024/05/07/12-17-20/55.jpg" class>
<p>我们发现，找不到一个合适的变换矩阵$W$来分割这个数据集，所以就需要找到一种非线性方式来分割，也就是激活函数的方式，这样就可以完成某些无法线性分割的数据集的分割，比如说下面这种方式，就是使用ReLu函数进行特征变换，然后可以看到，BCD三个区间都并不对应新空间下的三个象限，而是分别落在坐标轴和原点上</p>
<img src="/2024/05/07/12-17-20/60-1682664488898-3.jpg" class>
<p>这样，就可以完成某些数据集的分割了，比如说下图</p>
<img src="/2024/05/07/12-17-20/64.jpg" class>
<h1 id="单个神经元建模"><a href="#单个神经元建模" class="headerlink" title="单个神经元建模"></a>单个神经元建模</h1><p>神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好效果。然而，讨论将还是从对生物系统的一个高层次的简略描述开始，因为神经网络毕竟是从这里得到了启发。  </p>
<h2 id="生物动机与连接"><a href="#生物动机与连接" class="headerlink" title="生物动机与连接"></a>生物动机与连接</h2><p>大脑的基本计算单位是<strong>神经元（neuron）</strong>。人类的神经系统中大约有860亿个神经元，它们被大约$10^{14}-10^{15}$个<strong>突触（synapses）</strong>连接起来。下面图表的左边展示了一个生物学的神经元，右边展示了一个常用的数学模型。每个神经元都从它的<strong>树突</strong>获得输入信号，然后沿着它唯一的<strong>轴突（axon）</strong>产生输出信号。轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。</p>
<p>在神经元的计算模型中，沿着轴突传播的信号（比如$x_0$）将基于突触的突触强度（比如$w_0$），与其他神经元的树突进行乘法交互（比如$w_0x_0$）。其观点是，突触的强度（也就是权重$w$），是可学习的且可以控制一个神经元对于另一个神经元的影响强度（还可以控制影响方向：使其兴奋（正权重）或使其抑制（负权重））。在基本模型中，树突将信号传递到细胞体，信号在细胞体中相加。如果最终之和高于某个阈值，那么神经元将会激活，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号的频率在交流信息。基于这个速率编码的观点，将神经元的激活率建模为<strong>激活函数（activation function）$f$</strong>，它表达了轴突上激活信号的频率。由于历史原因，激活函数常常选择使用<strong>sigmoid函数$\sigma$</strong>，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。在本节后面部分会看到这些激活函数的各种细节。</p>
<img src="/2024/05/07/12-17-20/d0cbce2f2654b8e70fe201fec2982c7d_b.png" class>
<p>左边是生物神经元，右边是数学模型。  </p>
<p>一个神经元前向传播的实例代码如下：  </p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="comment"># ... </span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">inputs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 假设输入和权重是1-D的numpy数组，偏差是一个数字 &quot;&quot;&quot;</span></span><br><span class="line">    cell_body_sum = np.<span class="built_in">sum</span>(inputs * self.weights) + self.bias</span><br><span class="line">    firing_rate = <span class="number">1.0</span> / (<span class="number">1.0</span> + math.exp(-cell_body_sum)) <span class="comment"># sigmoid激活函数</span></span><br><span class="line">    <span class="keyword">return</span> firing_rate</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>换句话说，每个神经元都对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数（或称为激活函数）。本例中使用的是sigmoid函数$\sigma(x)=1/(1+e^{-x})$。在本节的末尾部分将介绍不同激活函数的细节。</p>
<p><strong>粗糙模型</strong>：要注意这个对于生物神经元的建模是非常粗糙的：在实际中，有很多不同类型的神经元，每种都有不同的属性。生物神经元的树突可以进行复杂的非线性计算。突触并不就是一个简单的权重，它们是复杂的非线性动态系统。很多系统中，输出的峰值信号的精确时间点非常重要，说明速率编码的近似是不够全面的。鉴于所有这些已经介绍和更多未介绍的简化，如果你画出人类大脑和神经网络之间的类比，有神经科学背景的人对你的板书起哄也是非常自然的。</p>
<h2 id="作为线性分类器的单个神经元"><a href="#作为线性分类器的单个神经元" class="headerlink" title="作为线性分类器的单个神经元"></a>作为线性分类器的单个神经元</h2><p>神经元模型的前向计算数学公式看起来可能比较眼熟。就像在线性分类器中看到的那样，神经元有能力”喜欢”（激活函数值接近1），或者不喜欢（激活函数值接近0）输入空间中的某些线性区域。因此，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。</p>
<p><strong>二分类Softmax分类器</strong>。举例来说，可以把$\displaystyle\sigma(\sigma_iw_ix_i+b)$看做其中一个分类的概率$P(y_i=1|x_i;w)$，其他分类的概率为$P(y_i=0|x_i;w)=1-P(y_i=1|x_i;w)$，因为它们加起来必须为1。根据这种理解，可以得到交叉熵损失，这个在线性分一节中已经介绍。然后将它最优化为二分类的Softmax分类器（也就是逻辑回归）。因为sigmoid函数输出限定在0-1之间，所以分类器做出预测的基准是神经元的输出是否大于0.5。  </p>
<p><strong>二分类SVM分类器</strong>。或者可以在神经元的输出外增加一个最大边界折叶损失（max-margin hinge loss）函数，将其训练成一个二分类的支持向量机。  </p>
<p><strong>理解正则化</strong>。在SVM/Softmax的例子中，正则化损失从生物学角度可以看做逐渐遗忘，因为它的效果是让所有突触权重$w$在参数更新过程中逐渐向着0变化。  </p>
<p>&gt; 一个单独的神经元可以用来实现一个二分类分类器，比如二分类的Softmax或者SVM分类器。  </p>
<h2 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h2><p>激活函数是一种很有用的方式，可以增强模型的拟合能力，来完成对各种数据的拟合</p>
<p>每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数：  </p>
<img src="/2024/05/07/12-17-20/677187e96671a4cac9c95352743b3806_b.png" class>
<p>左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。  </p>
<p><strong>Sigmoid。</strong>sigmoid非线性函数的数学公式是$\displaystyle\sigma(x)=1/(1+e^{-x})$，函数图像如上图的左边所示。在前一节中已经提到过，它输入实数值并将其”挤压”到0到1范围内。更具体地说，很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和（<strong>saturated</strong>）的激活（1）。然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点：</p>
<ul>
<li>_ Sigmoid函数饱和使梯度消失_。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地”杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。</li>
<li>_Sigmoid函数的输出不是零中心的_。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在$f=w^Tx+b$中每个元素都$x&gt;0$），那么关于$w$的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式$f$而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。</li>
</ul>
<p><strong>Tanh。</strong>tanh非线性函数图像如上图右边所示。它将实数值压缩到[-1,1]之间。和sigmoid神经元一样，它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，_tanh非线性函数比sigmoid非线性函数更受欢迎_。注意tanh神经元是一个简单放大的sigmoid神经元，具体说来就是：$tanh(x)=2\sigma(2x)-1$。  </p>
<img src="/2024/05/07/12-17-20/83682a138f6224230f5b0292d9c01bd2_b.png" class>
<p>左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当$x=0$时函数值为0。当$x&gt;0$函数的斜率为1。</p>
<p><strong>ReLU。</strong>在近些年ReLU变得非常流行。它的函数公式是$f(x)=max(0,x)$。换句话说，这个激活函数就是一个关于0的阈值（如上图左侧）。使用ReLU有以下一些优缺点：</p>
<ul>
<li>优点：相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用。据称这是由它的线性，非饱和的公式导致的。</li>
<li>优点：sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。</li>
<li>缺点：在训练的时候，ReLU单元比较脆弱并且可能”死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。</li>
</ul>
<p><strong>Leaky ReLU。</strong>Leaky ReLU是为解决”ReLU死亡”问题的尝试。ReLU中当x&lt;0时，函数值为0。而Leaky ReLU则是给出一个很小的负数梯度值，比如0.01。所以其函数公式为$f(x)=max(0,x)+\alpha\cdot min(0,x)$其中$\alpha$是一个小的常量。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。Kaiming He等人在2015年发布的论文中介绍了一种新方法PReLU，把负区间上的斜率当做每个神经元中的一个参数。然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰。</p>
<p><strong>Maxout。</strong>一些其他类型的单元被提了出来，它们对于权重和数据的内积结果不再使用$f(w^Tx+b)$函数形式。一个相关的流行选择是Maxout神经元。Maxout是对ReLU和leaky ReLU的一般化归纳，它的函数是：$max(w^T_1x+b_1,w^T_2x+b_2)$。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当$w_1,b_1=0$的时候）。这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。</p>
<p>以上就是一些常用的神经元及其激活函数。最后需要注意一点：在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有什么根本性问题来禁止这样做。  </p>
<p><strong>一句话</strong>：”那么该用那种呢？”用ReLU非线性函数。注意设置好学习率，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。  </p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉 最优化</title>
    <url>/2024/05/07/11-38-48/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.07：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/05/06/10-28-51/" title="eecs498 ML-DL-CV 笔记汇总">计算机视觉-笔记汇总</a>
</li>
</ul>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在上一节中，我们介绍了图像分类任务中的两个关键部分：</p>
<ol>
<li>基于参数的<strong>评分函数。</strong>该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。</li>
<li><strong>损失函数</strong>。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。</li>
</ol>
<p>上节中，线性函数的形式是$f(x_i, W)=Wx_i$，而SVM实现的公式是：  </p>
<p>$L=\frac1N\sum_i\sum_{j\neq y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)]+\alpha R(W)$</p>
<p>对于图像数据$x_i$，如果基于参数集$W$做出的分类预测与真实情况比较一致，那么计算出来的损失值$L$就很低。现在介绍第三个，也是最后一个关键部分：<strong>最优化Optimization</strong>。最优化是寻找能使得损失函数值最小化的参数$W$的过程。</p>
<p><strong>铺垫</strong>：一旦理解了这三个部分是如何相互运作的，我们将会回到第一个部分（基于参数的函数映射），然后将其拓展为一个远比线性函数复杂的函数：首先是神经网络，然后是卷积神经网络。而损失函数和最优化过程这两个部分将会保持相对稳定。</p>
<h2 id="损失函数可视化"><a href="#损失函数可视化" class="headerlink" title="损失函数可视化"></a>损失函数可视化</h2><p>本课中讨论的损失函数一般都是定义在高维度的空间中（比如，在CIFAR-10中一个线性分类器的权重矩阵大小是[10x3073]，就有30730个参数），这样要将其可视化就很困难。然而办法还是有的，在1个维度或者2个维度的方向上对高维空间进行切片，就能得到一些直观感受。例如，随机生成一个权重矩阵$W$，该矩阵就与高维空间中的一个点对应。然后沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向$W_1$并且沿着此方向计算损失值，计算方法是根据不同的$a$值来计算$L(W+aW_1)$。这个过程将生成一个图表，其$x$轴是$a$值，$y$轴是损失函数值。同样的方法还可以用在两个维度上，通过改变$a,b$来计算损失值$L(W+aW_1+bW_2)$，从而给出二维的图像。在图像中，$a,b$可以分别用x和y轴表示，而损失函数的值可以用颜色变化表示：</p>
<img src="/2024/05/07/11-38-48/94dd0714f65ef94b3cbfff4780b1988d_b.png" class>
<p>一个无正则化的多类SVM的损失函数的图示。左边和中间只有一个样本数据，右边是CIFAR-10中的100个数据。<strong>左</strong>：a值变化在某个维度方向上对应的的损失值变化。<strong>中和右</strong>：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。注意损失函数的分段线性结构。多个样本的损失值是总体的平均值，所以右边的碗状结构是很多的分段线性结构的平均（比如中间这个就是其中之一）。</p>
<p>我们可以通过数学公式来解释损失函数的分段线性结构。对于一个单独的数据，有损失函数的计算公式如下：  </p>
<p>$\large Li=\sum_{j\neq y_i}[max(0,w_j^Tx_i-w_{y_i}^Tx_i+1)]$</p>
<p>通过公式可见，每个样本的数据损失值是以$W$为参数的线性函数的总和（零阈值来源于$max(0,-)$函数）。$W$的每一行（即$w_j$），有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有3个只有1个维度的点，数据集数据点有3个类别。那么完整的无正则化SVM的损失值计算如下：</p>
<p>$L_0=max(0,w^T_1x_0-w^T_0x_0+1)+max(0,w^T_2x_0-w^T_0x_0+1)$<br>$L_1=max(0,w^T_0x_1-w^T_1x_1+1)+max(0,w^T_2x_1-w^T_1x_1+1)$<br>$L_2=max(0,w^T_0x_2-w^T_2x_2+1)+max(0,w^T_1x_2-w^T_2x_2+1)$<br>$L=(L_0+L_1+L_2)/3$</p>
<p>因为这些例子都是一维的，所以数据$x_i$和权重$w_j$都是数字。观察$w_0$，可以看到上面的式子中一些项是$w_0$的线性函数，且每一项都会与0比较，取两者的最大值。可作图如下：</p>
<img src="/2024/05/07/11-38-48/3f6fbcd487b1c214e8fea1ea66eb413e_b.png" class>
<p>从一个维度方向上对数据损失值的展示。x轴方向就是一个权重，y轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与0阈值的比较。完整的SVM数据损失就是这个形状的30730维版本。  </p>
<p>需要多说一句的是，你可能根据SVM的损失函数的碗状外观猜出它是一个凸函数。。但是一旦我们将$f$函数扩展到神经网络，目标函数就就不再是凸函数了，图像也不会像上面那样是个碗状，而是凹凸不平的复杂地形形状。  </p>
<p>不可导的损失函数。作为一个技术笔记，你要注意到：由于max操作，损失函数中存在一些不可导点（kinks），这些点使得损失函数不可微，因为在这些不可导点，梯度是没有定义的。但是次梯度（subgradient）依然存在且常常被使用。在本课中，我们将交换使用次梯度和梯度两个术语。  </p>
<h2 id="最优化-Optimization"><a href="#最优化-Optimization" class="headerlink" title="最优化 Optimization"></a>最优化 Optimization</h2><p>重申一下：损失函数可以量化某个具体权重集<strong>W</strong>的质量。而最优化的目标就是找到能够最小化损失函数值的<strong>W</strong> 。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于有一些经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM 损失函数）是一个凸函数问题。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。  </p>
<p><strong>策略#1：一个差劲的初始方案：随机搜索</strong></p>
<p>既然确认参数集<strong>W</strong>的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。过程如下：  </p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">    <span class="comment"># 假设X_train的每一列都是一个数据样本（比如3073 x 50000）</span></span><br><span class="line">    <span class="comment"># 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）</span></span><br><span class="line">    <span class="comment"># 假设函数L对损失函数进行评价</span></span><br><span class="line">    </span><br><span class="line">    bestloss = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="comment"># Python assigns the highest possible float value</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">      W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.0001</span> <span class="comment"># generate random parameters</span></span><br><span class="line">      loss = L(X_train, Y_train, W) <span class="comment"># get the loss over the entire training set</span></span><br><span class="line">      <span class="keyword">if</span> loss &amp;lt; bestloss: <span class="comment"># keep track of the best solution</span></span><br><span class="line">        bestloss = loss</span><br><span class="line">        bestW = W</span><br><span class="line">      <span class="built_in">print</span> <span class="string">&#x27;in attempt %d the loss was %f, best %f&#x27;</span> % (num, loss, bestloss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出:</span></span><br><span class="line">    <span class="comment"># in attempt 0 the loss was 9.401632, best 9.401632</span></span><br><span class="line">    <span class="comment"># in attempt 1 the loss was 8.959668, best 8.959668</span></span><br><span class="line">    <span class="comment"># in attempt 2 the loss was 9.044034, best 8.959668</span></span><br><span class="line">    <span class="comment"># in attempt 3 the loss was 9.278948, best 8.959668</span></span><br><span class="line">    <span class="comment"># in attempt 4 the loss was 8.857370, best 8.857370</span></span><br><span class="line">    <span class="comment"># in attempt 5 the loss was 8.943151, best 8.857370</span></span><br><span class="line">    <span class="comment"># in attempt 6 the loss was 8.605604, best 8.605604</span></span><br><span class="line">    <span class="comment"># ... (trunctated: continues for 1000 lines)</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">在上面的代码中，我们尝试了若干随机生成的权重矩阵**W**，其中某些的损失值较小，而另一些的损失值大些。我们可以把这次随机搜索中找到的最好的权重**W**取出，然后去跑测试集：  </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]</span></span><br><span class="line">    scores = Wbest.dot(Xte_cols) <span class="comment"># 10 x 10000, the class scores for all test examples</span></span><br><span class="line">    <span class="comment"># 找到在每列中评分值最大的索引（即预测的分类）</span></span><br><span class="line">    Yte_predict = np.argmax(scores, axis = <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 以及计算准确率</span></span><br><span class="line">    np.mean(Yte_predict == Yte)</span><br><span class="line">    <span class="comment"># 返回 0.1555</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>验证集上表现最好的权重<strong>W</strong>跑测试集的准确率是<strong>15.5%，</strong>而完全随机猜的准确率是10%，如此看来，这个准确率对于这样一个不经过大脑的策略来说，还算不错嘛！  </p>
<p><strong>核心思路：迭代优化</strong>。当然，我们肯定能做得更好些。核心思路是：虽然找到最优的权重<strong>W</strong>非常困难，甚至是不可能的（尤其当<strong>W</strong>中存的是整个神经网络的权重的时候），但如果问题转化为：对一个权重矩阵集<strong>W</strong>取优，使其损失值稍微减少。那么问题的难度就大大降低了。换句话说，我们的方法从一个随机的<strong>W</strong>开始，然后对其迭代取优，每次都让它的损失值变得更小一点。  </p>
<p>&gt; 我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。  </p>
<p><strong>蒙眼徒步者的比喻</strong>：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是30730维的（因为<strong>W</strong>是3073x10）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。  </p>
<p><strong>策略#2：随机本地搜索</strong></p>
<p>第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机$W$开始，然后生成一个随机的扰动$\delta W$ ，只有当$W+\delta W$的损失值变低，我们才会更新。</p>
<img src="/2024/05/07/11-38-48/4_1.jpg" class>
<blockquote>
<p>加入扰动后的梯度计算如图所示<br>这个过程的具体代码如下：  </p>
</blockquote>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 生成随机初始W</span></span><br><span class="line">bestloss = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  step_size = <span class="number">0.0001</span></span><br><span class="line">  Wtry = W + np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * step_size</span><br><span class="line">  loss = L(Xtr_cols, Ytr, Wtry)</span><br><span class="line">  <span class="keyword">if</span> loss &amp;lt; bestloss:</span><br><span class="line">    W = Wtry</span><br><span class="line">    bestloss = loss</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;iter %d loss is %f&#x27;</span> % (i, bestloss)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>使用同样的数据（1000），这个方法可以得到<strong>21.4%</strong>的分类准确率。这个比策略一好，但是依然过于浪费计算资源。  </p>
<p><strong>策略#3：跟随梯度</strong>  </p>
<p>前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的<strong>梯度（gradient）</strong>。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。</p>
<p>在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数<strong>derivatives</strong>）。对一维函数的求导公式如下：</p>
<p>$\frac{df(x)}{dx}=\displaystyle\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$</p>
<p>当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。</p>
<h2 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h2><p>计算梯度有两种方法：一个是缓慢的近似方法（<strong>数值梯度法</strong>），但实现相对简单。另一个方法（<strong>分析梯度法</strong>）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对两种方法进行介绍：</p>
<p><strong>利用有限差值计算梯度</strong></p>
<p>上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数<strong>f</strong>和向量<strong>x，</strong>计算<strong>f</strong>的梯度的通用函数，它返回函数<strong>f</strong>在点<strong>x处</strong>的梯度：</p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">eval_numerical_gradient</span>(<span class="params">f, x</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;  </span></span><br><span class="line"><span class="string">  一个f在x处的数值梯度法的简单实现</span></span><br><span class="line"><span class="string">  - f是只有一个参数的函数</span></span><br><span class="line"><span class="string">  - x是计算梯度的点</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line">  fx = f(x) <span class="comment"># 在原点计算函数值</span></span><br><span class="line">  grad = np.zeros(x.shape)</span><br><span class="line">  h = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 对x中所有的索引进行迭代</span></span><br><span class="line">  it = np.nditer(x, flags=[<span class="string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="string">&#x27;readwrite&#x27;</span>])</span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算x+h处的函数值</span></span><br><span class="line">    ix = it.multi_index</span><br><span class="line">    old_value = x[ix]</span><br><span class="line">    x[ix] = old_value + h <span class="comment"># 增加h</span></span><br><span class="line">    fxh = f(x) <span class="comment"># 计算f(x + h)</span></span><br><span class="line">    x[ix] = old_value <span class="comment"># 存到前一个值中 (非常重要)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算偏导数</span></span><br><span class="line">    grad[ix] = (fxh - fx) / h <span class="comment"># 坡度</span></span><br><span class="line">    it.iternext() <span class="comment"># 到下个维度</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> grad</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>根据上面的梯度公式，代码对所有维度进行迭代，在每个维度上产生一个很小的变化$h$，通过观察函数值变化，计算函数在该维度上的偏导数。最后，所有的梯度存储在变量$grad$中。</p>
<p><strong>实践考量</strong>：注意在数学公式中，$h$的取值是趋近于0的，然而在实际中，用一个很小的数值（比如例子中的1e-5）就足够了。在不产生数值计算出错的理想前提下，你会使用尽可能小的$h$。还有，实际中用<strong>中心差值公式（centered difference formula）</strong>$[f(x+h)-f(x-h)]/2h$效果较好。</p>
<p>可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10损失函数的梯度：</p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要使用上面的代码我们需要一个只有一个参数的函数</span></span><br><span class="line"><span class="comment"># (在这里参数就是权重)所以也包含了X_train和Y_train</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CIFAR10_loss_fun</span>(<span class="params">W</span>):</span><br><span class="line">  <span class="keyword">return</span> L(X_train, Y_train, W)</span><br><span class="line"></span><br><span class="line">W = np.random.rand(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 随机权重向量</span></span><br><span class="line">df = eval_numerical_gradient(CIFAR10_loss_fun, W) <span class="comment"># 得到梯度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>梯度告诉我们损失函数在每个维度上的斜率，以此来进行更新：  </p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">loss_original = CIFAR10_loss_fun(W) <span class="comment"># 初始损失值</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;original loss: %f&#x27;</span> % (loss_original, )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同步长的效果</span></span><br><span class="line"><span class="keyword">for</span> step_size_log <span class="keyword">in</span> [-<span class="number">10</span>, -<span class="number">9</span>, -<span class="number">8</span>, -<span class="number">7</span>, -<span class="number">6</span>, -<span class="number">5</span>,-<span class="number">4</span>,-<span class="number">3</span>,-<span class="number">2</span>,-<span class="number">1</span>]:</span><br><span class="line">  step_size = <span class="number">10</span> ** step_size_log</span><br><span class="line">  W_new = W - step_size * df <span class="comment"># 权重空间中的新位置</span></span><br><span class="line">  loss_new = CIFAR10_loss_fun(W_new)</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;for step size %f new loss: %f&#x27;</span> % (step_size, loss_new)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># original loss: 2.200718</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-10 new loss: 2.200652</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-09 new loss: 2.200057</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-08 new loss: 2.194116</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-07 new loss: 2.135493</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-06 new loss: 1.647802</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-05 new loss: 2.844355</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-04 new loss: 25.558142</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-03 new loss: 254.086573</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-02 new loss: 2539.370888</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-01 new loss: 25392.214036</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p><strong>在梯度负方向上更新</strong>：在上面的代码中，为了计算$W_new$，要注意我们是向着梯度$df$的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。</p>
<p><strong>步长的影响</strong>：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。在后续的课程中可以看到，选择步长（也叫作_习率）将会是神经网络训练中最重要的超参数设定之一。还是用蒙眼徒步者下山的比喻，这就好比我们可以感觉到脚朝向的不同方向上，地形的倾斜程度不同。但是该跨出多长的步长呢？不确定。如果谨慎地小步走，情况可能比较稳定但是进展较慢（这就是步长较小的情况）。相反，如果想尽快下山，那就大步走吧，但结果也不一定尽如人意。在上面的代码中就能看见反例，在某些点如果步长过大，反而可能越过最低点导致更高的损失值。</p>
<img src="/2024/05/07/11-38-48/d8b52b9b9ca31e2132c436c39af2943c_b.jpg" class>
<p>将步长效果视觉化的图例。从某个具体的点W开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为<strong>学习率</strong>）将会是我们在调参中最重要的超参数之一。  </p>
<p><strong>效率问题</strong>：你可能已经注意到，计算数值梯度的复杂性和参数的量线性相关。在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度。现代神经网络很容易就有上千万的参数，因此这个问题只会越发严峻。显然这个策略不适合大规模数据，我们需要更好的策略。</p>
<h3 id="微分分析计算梯度"><a href="#微分分析计算梯度" class="headerlink" title="微分分析计算梯度"></a>微分分析计算梯度</h3><p>使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于$h$值是选取了一个很小的数值，但真正的梯度定义中$h$趋向0的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做<strong>梯度检查</strong>。</p>
<p>用SVM的损失函数在某个数据点上的计算来举例：</p>
<p>$L_i=\displaystyle\sum_{j\not =y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+\delta)]$ </p>
<p>可以对函数进行微分。比如，对$w_{y_i}$进行微分得到：</p>
<p>$\displaystyle\nabla_{w_{y_i}}L_i=-(\sum_{j\not=y_i}\mathbb{1}(w^T_jx_i-w^T_{y_i}x_i+\delta&gt;0))x_i$</p>
<p>其中$\mathbb{1}$是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以$x_i$就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些$j\not =y_i$行的梯度是：  </p>
<p>$\displaystyle\nabla_{w_j}L_i=\mathbb{1}(w^T_jx_i-w^T_{y_i}x_i+\delta&gt;0)x_i$  </p>
<p>一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。  </p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为梯度下降，他的<strong>普通</strong>版本是这样的：  </p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 普通的梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 进行梯度更新</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>这个简单的循环在所有的神经网络核心库中都有。虽然也有其他实现最优化的方法（比如LBFGS），但是到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。</p>
<p><strong>小批量数据梯度下降（Mini-batch gradient descent）</strong>：在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的<strong>小批量（batches）</strong>数据。例如，在目前最高水平的卷积神经网络中，一个典型的小批量包含256个例子，而整个训练集是多少呢？一百二十万个。这个小批量数据就用来实现一个参数更新：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 普通的小批量数据梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  data_batch = sample_training_data(data, <span class="number">256</span>) <span class="comment"># 256个数据</span></span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 参数更新</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个方法之所以效果不错，是因为训练集中的数据都是相关的。要理解这一点，可以想象一个极端情况：在ILSVRC中的120万个图像是1000张不同图片的复制（每个类别1张图片，每张图片有1200张复制）。那么显然计算这1200张复制图像的梯度就应该是一样的。对比120万张图片的数据损失的均值与只计算1000张的子集的数据损失均值时，结果应该是一样的。实际情况中，数据集肯定不会包含重复图像，那么小批量数据的梯度就是对整个数据集梯度的一个近似。因此，在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。</p>
<p>小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为<strong>随机梯度下降（Stochastic Gradient Descent 简称SGD）</strong>，有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据 比100次计算1个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。</p>
<p><strong>两个潜在问题：</strong></p>
<ul>
<li>学习率如果过大，那么就会出现震荡的情况，学习率过小的话，虽然没有震荡情况出现，但是学习速度会很慢，这个问题在技术上有时候被称为具有高条件数的问题</li>
<li>存在局部最小点和鞍点，在某些维度上可能无法优化，也就是高维度下优化难题，这样模型无法达到最佳</li>
</ul>
<p>或者可以使用动量法进行优化，尝试越过鞍点和局部最小点，会产生很不错的效果，同时还可以抑制震荡，是一种很有用的方法</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><img src="/2024/05/07/11-38-48/03b3eccf18ee3760e219f9f95ec14305_b.png" class>
<p>信息流的总结图例。数据集中的$(x,y)$是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量$f$中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分$f$和实际标签$y$之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。</p>
<p>在本节课中：</p>
<ul>
<li>将损失函数比作了一个<strong>高维度的最优化地形</strong>，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。</li>
<li>提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。</li>
<li>函数的<strong>梯度</strong>给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是$h$，用来计算数值梯度）。</li>
<li>参数更新需要有技巧地设置<strong>步长</strong>。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。</li>
<li>讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用<strong>梯度检查</strong>来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。</li>
</ul>
<ul>
<li>介绍了<strong>梯度下降</strong>算法，它在循环中迭代地计算梯度并更新参数。</li>
</ul>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉 线性分类器</title>
    <url>/2024/05/06/15-22-39/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.06：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/05/06/10-28-51/" title="eecs498 ML-DL-CV 笔记汇总">计算机视觉-笔记汇总</a>
</li>
</ul>
<h1 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>上一篇笔记介绍了图像分类问题。图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。我们还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：</p>
<ul>
<li>分类器必须记住所有训练数据并将其存储起来，以便于未来测试数据用于比较。这在存储空间上是低效的，数据集的大小很容易就以GB计。</li>
<li>对一个测试图像进行分类需要和所有训练图像作比较，算法计算资源耗费高。</li>
</ul>
<p>所有我们使用了一种更广泛的构建机器学习模型的方法，那就是参数方法，这种方法的想法是参数可学习</p>
<p><strong>概述</strong>：我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是<strong>评分函数（score function）</strong>，它是原始图像数据到类别分值的映射。另一个是<strong>损失函数（loss function）</strong>，它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。</p>
<h2 id="从图像到标签分值的参数化映射"><a href="#从图像到标签分值的参数化映射" class="headerlink" title="从图像到标签分值的参数化映射"></a>从图像到标签分值的参数化映射</h2><p>该方法的第一部分就是定义一个评分函数，这个函数将图像的像素值映射为各个分类类别的得分，得分高低代表图像属于该类别的可能性高低。下面会利用一个具体例子来展示该方法。现在假设有一个包含很多图像的训练集$x_{i}\in R^{D}$，每个图像都有一个对应的分类标签$y_{i}$。这里$y=1,2,3…N$并且$y_{i}\in 1…K$。这就是说，我们有<strong>N</strong>个图像样例，每个图像的维度是<strong>D</strong>，共有<strong>K</strong>种不同的分类。</p>
<p>举例来说，在CIFAR-10中，我们有一个<strong>N</strong>=50000的训练集，每个图像有<strong>D</strong>=32x32x3=3072个像素，而<strong>K</strong>=10，这是因为图片被分为10个不同的类别（狗，猫，汽车等）。我们现在定义评分函数为：$f:R^{D}\rightarrow R^{K}$，该函数是原始图像像素到分类分值的映射。</p>
<p><strong>线性分类器</strong>：在本模型中，我们从最简单的概率函数开始，单变量线性映射：  </p>
<p>$f(x_{i},W,b)=Wx_{i}+b$</p>
<p>在上面的公式中，假设每个图像数据都被拉长为一个长度为D的列向量，大小为[D x 1]。其中大小为[K x D]的矩阵<strong>W</strong>和大小为[K x 1]列向量<strong>b</strong>为该函数的<strong>参数（parameters）</strong>。还是以CIFAR-10为例，$x_i$就包含了第i个图像的所有像素信息，这些信息被拉成为一个[3072 x 1]的列向量，<strong>W</strong>大小为[10x3072]，<strong>b</strong>的大小为[10x1]。因此，3072个数字（原始像素数值）输入函数，函数输出10个数字（不同分类得到的分值）。参数<strong>W</strong>被称为<strong>权重（weights）</strong>。<strong>b</strong>被称为<strong>偏差向量（bias vector）</strong>，这是因为它影响输出数值，但是并不和原始数据$x_i$产生关联。在实际情况中，人们常常混用<strong>权重</strong>和<strong>参数</strong>这两个术语。</p>
<p>需要注意的几点：</p>
<ul>
<li>首先，一个单独的矩阵乘法$W_{x_{i}}$就高效地并行评估10个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是W的一个行向量。</li>
<li>注意我们认为输入数据$(x_{i},y_{i})$是给定且不可改变的，但参数<strong>W</strong>和<strong>b</strong>是可控制改变的。我们的目标就是通过设置这些参数，使得计算出来的分类分值情况和训练集中图像数据的真实类别标签相符。在接下来的课程中，我们将详细介绍如何做到这一点，但是目前只需要直观地让正确分类的分值比错误分类的分值高即可。</li>
<li>该方法的一个优势是训练数据是用来学习到参数<strong>W</strong>和<strong>b</strong>的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。这是因为一个测试图像可以简单地输入函数，并基于计算出的分类分值来进行分类。</li>
<li>最后，注意只需要做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比k-NN中将测试图像和所有训练数据做比较的方法快多了。</li>
<li>当然，我们这里只是将偏差作为一个单独的可学习参数，我们也可以进行合并，将其合并到权重矩阵之中，这样的方式无非就是给权重矩阵扩张一下列向量，并且给输入向量增加一个常量维度</li>
</ul>
<p>&gt; 预告：卷积神经网络映射图像像素值到分类分值的方法和上面一样，但是映射 <strong>(f)</strong> 就要复杂多了，其包含的参数也更多。</p>
<h2 id="理解线性分类器"><a href="#理解线性分类器" class="headerlink" title="理解线性分类器"></a>理解线性分类器</h2><p>线性分类器计算图像中3个颜色通道中所有像素的值与权重的矩阵乘，从而得到分类分值。根据我们对权重设置的值，对于图像中的某些位置的某些颜色，函数表现出喜好或者厌恶（根据每个权重的符号而定）。举个例子，可以想象”船”分类就是被大量的蓝色所包围（对应的就是水）。那么”船”分类器在蓝色通道上的权重就有很多的正权重（它们的出现提高了”船”分类的分值），而在绿色和红色通道上的权重为负的就比较多（它们的出现降低了”船”分类的分值）。</p>
<img src="/2024/05/06/15-22-39/7c204cd1010c0af1e7b50000bfff1d8e_b.jpg" class>
<p>一个将图像映射到分类分值的例子。为了便于可视化，假设图像只有4个像素（都是黑白像素，这里不考虑RGB通道），有3个分类（红色代表猫，绿色代表狗，蓝色代表船，注意，这里的红、绿和蓝3种颜色仅代表分类，和RGB通道没有关系）。首先将图像像素拉伸为一个列向量，与W进行矩阵乘，然后得到各个分类的分值。需要注意的是，这个W一点也不好：猫分类的分值非常低。从上图来看，算法倒是觉得这个图像是一只狗。  </p>
<p><strong>将图像看做高维度的点</strong>：既然图像被伸展成为了一个高维度的列向量，那么我们可以把图像看做这个高维度空间中的一个点（即每张图像是3072维空间中的一个点）。整个数据集就是一个点的集合，每个点都带有1个分类标签。</p>
<p>既然定义每个分类类别的分值是权重和图像的矩阵乘，那么每个分类类别的分数就是这个空间中的一个线性函数的函数值。我们没办法可视化3072维空间中的线性函数，但假设把这些维度挤压到二维，那么就可以看看这些分类器在做什么了：</p>
<img src="/2024/05/06/15-22-39/cfcb46408daa5353c38cb37e9bb6eb01_b.jpg" class>
<p>图像空间的示意图。其中每个图像是一个点，有3个分类器。以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。  </p>
<p>从上面可以看到，<strong>W</strong>的每一行都是一个分类类别的分类器。对于这些数字的几何解释是：如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转。而偏差<strong>b</strong>，则允许分类器对应的直线平移。需要注意的是，如果没有偏差，无论权重如何，在$x_i=0$时分类分值始终为0。这样所有分类器的线都不得不穿过原点。</p>
<p><strong>将线性分类器看做模板匹配</strong>：关于权重<strong>W</strong>的另一个解释是<strong>它</strong>的每一行对应着一个分类的模板（有时候也叫作原型）。一张图像对应不同分类的得分，是通过使用内积（也叫点积）来比较图像和模板，然后找到和哪个模板最相似。从这个角度来看，线性分类器就是在利用学习到的模板，针对图像做模板匹配。从另一个角度来看，可以认为还是在高效地使用k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），而且我们会使用（负）内积来计算向量间的距离，而不是使用L1或者L2距离。</p>
<img src="/2024/05/06/15-22-39/13e72e4ce83c11b49d36bbbb51d29ab4_b.jpg" class>
<p>将课程进度快进一点。这里展示的是以CIFAR-10为训练集，学习结束后的权重的例子。注意，船的模板如期望的那样有很多蓝色像素。如果图像是一艘船行驶在大海上，那么这个模板利用内积计算图像将给出很高的分数。  </p>
<p>可以看到马的模板看起来似乎是两个头的马，这是因为训练集中的马的图像中马头朝向各有左右造成的。线性分类器将这两种情况融合到一起了。类似的，汽车的模板看起来也是将几个不同的模型融合到了一个模板中，并以此来分辨不同方向不同颜色的汽车。这个模板上的车是红色的，这是因为CIFAR-10中训练集的车大多是红色的。线性分类器对于不同颜色的车的分类能力是很弱的，但是后面可以看到神经网络是可以完成这一任务的。神经网络可以在它的隐藏层中实现中间神经元来探测不同种类的车（比如绿色车头向左，蓝色车头向前等）。而下一层的神经元通过计算不同的汽车探测器的权重和，将这些合并为一个更精确的汽车分类分值。</p>
<p><strong>偏差和权重的</strong>合并技巧：在进一步学习前，要提一下这个经常使用的技巧。它能够将我们常用的参数W和b合二为一。回忆一下，分类评分函数定义为：</p>
<p>$f(x_{i},W,b)=Wx_{i}+b$</p>
<p>分开处理这两个参数（权重参数<strong>W</strong>和偏差参数<strong>b</strong>）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时$x_{i}$向量就要增加一个维度，这个维度的数值是常量1，这就是默认的<em>偏差维度</em>。这样新的公式就简化成下面这样：</p>
<p>$f(x_{i},W)=W{x_{i}}$</p>
<p>还是以CIFAR-10为例，那么$x_i$的大小就变成 <strong>[3073x1]</strong>，而不是 <strong>[3072x1]</strong> 了，多出了包含常量1的1个维度）。$W$大小就是 <strong>[10x3073]</strong> 了。$W$中多出来的这一列对应的就是偏差值b，具体见下图：</p>
<img src="/2024/05/06/15-22-39/3c69a5c87a43bfb07e2b59bfcbd2f149_b.jpg" class>
<p>偏差技巧的示意图。左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加1个含常量1的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的。通过右边这样做，我们就只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵了。</p>
<p><strong>图像数据预处理</strong>：在上面的例子中，所有图像都是使用的原始像素值（从0到255）。在机器学习中，对于输入的特征做归一化（normalization）处理是常见的套路。而在图像分类的例子中，图像上的每个像素可以看做一个特征。在实践中，对每个特征减去平均值来<strong>中心化</strong>数据是非常重要的。在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127, 127]之间了。下一个常见步骤是，让所有数值分布的区间变为[-1, 1]。<strong>零均值的中心化</strong>是很重要的，等我们理解了梯度下降后再来详细解释。</p>
<h2 id="损失函数-Loss-function"><a href="#损失函数-Loss-function" class="headerlink" title="损失函数 Loss function"></a>损失函数 Loss function</h2><p>在上一节定义了从图像像素值到所属类别的评分函数（score function），该函数的参数是权重矩阵$W$。在函数中，数据$(x_i,y_i)$是给定的，不能修改。但是我们可以调整权重矩阵这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p>
<p>回到之前那张猫的图像分类例子，它有针对”猫”，”狗”，”船”三个类别的分数。我们看到例子中权重值非常差，因为猫分类的得分非常低（-96.8），而狗（437.9）和船（61.95）比较高。我们将使用<strong>损失函数（Loss Function）</strong>-样本误差（有时也叫<strong>代价函数</strong>Cost Function-单个误差或<strong>目标函数</strong>Objective）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。</p>
<p>同时，损失函数有多种类型，不同类型有不同偏好，我们在不同任务中可以选择不同类型的损失函数。</p>
<h2 id="多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss"><a href="#多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss" class="headerlink" title="多类支持向量机损失 Multiclass Support Vector Machine Loss"></a>多类支持向量机损失 Multiclass Support Vector Machine Loss</h2><p>损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值$\delta$。我们可以把损失函数想象成一个人，这位SVM先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么SVM就更加喜欢它。</p>
<p>让我们更精确一些。回忆一下，第i个数据中包含图像$x_i$的像素和代表正确类别的标签$y_i$。评分函数输入像素数据，然后通过公式 $f(x_i,W)$来计算不同分类类别的分值。这里我们将分值简写为$s$。比如，针对第j个类别的得分就是第j个元素：$s_{j}=f(x_{i},W)$。针对第i个数据的多类SVM的损失函数定义如下：</p>
<p>$L_i=\sum_{j\not=y_i}\max(0,s_j-s_{y_i}+\Delta)$</p>
<p><strong>举例</strong>：用一个例子演示公式是如何计算的。假设有3个分类，并且得到了分值$s = [13,-7,11]$。其中第一个类别是正确类别，即$y_i=0$。同时假设$\Delta$是10（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（$j\not=y_i$）加起来，所以我们得到两个部分：  </p>
<p>$L_i=\max(0,-7-13+10)+\max(0,11-13+10)$</p>
<p>可以看到第一个部分结果是0，这是因为[-7-13+10]得到的是负数，经过$\max(0,-)$函数处理后得到0。这一对类别分数和标签的损失值是0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10。而SVM只关心差距至少要大于10，更大的差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13&gt;11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。</p>
<p>那么在这次的模型中，我们面对的是线性评分函数($f(x_i,W)=Wx_i$)，所以我们可以将损失函数的公式稍微改写一下：</p>
<p>$L_i=\sum_{j\not=y_i}\max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)$</p>
<p>其中$w_j$是权重$W$的第j行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数$f$公式，这样做就不是必须的了。</p>
<p>在结束这一小节前，还必须提一下的属于是关于0的阀值：$\max(0,-)$函数，它常被称为<strong>折叶损失（hinge loss）</strong>。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是$\max(0,-)^2$，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。</p>
<p>&gt; 我们对于预测训练集数据分类标签的情况总有一些不满意的，而损失函数就能将这些不满意的程度量化。  </p>
<img src="/2024/05/06/15-22-39/%E5%A4%9A%E7%B1%BBsvm_scores.jpg" class>
<p>多类SVM”想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出delta的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。  </p>
<p><strong>正则化（Regularization）：</strong>上面损失函数有一个问题。假设有一个数据集和一个权重集<strong>W</strong>能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有$L_i=0$）。问题在于这个<strong>W</strong>并不唯一：可能有很多相似的<strong>W</strong>都能正确地分类所有的数据。一个简单的例子：如果<strong>W</strong>能够正确分类所有数据，即对于每个数据，损失值都是0。那么当 $\lambda &gt;1$时，任何数乘$\lambda W$都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对<strong>W</strong>乘以2将使得差距变成30。</p>
<p>换句话说，我们希望能向某些特定的权重<strong>W</strong>添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个<strong>正则化惩罚（regularization penalty）</strong>$R(W)$部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：</p>
<p>$R(W)=\sum_k\sum_lW^2_{k,l}$</p>
<p>上面的表达式中，将$W$中所有元素平方后求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数了，它由两个部分组成：<strong>数据损失（data loss）</strong>，即所有样例的的平均损失$L_i$，以及<strong>正则化损失（regularization loss）</strong>。完整公式如下所示：  </p>
<p>$ L= \underbrace{ \frac{1}{N}\sum_i L_i}_{data   loss}+\underbrace{\lambda R(W)}_{regularization  loss} $</p>
<p>将其展开完整公式是：</p>
<p>$ L= \frac{1}{N}\sum_i\sum_{j\not=y_i} \left[ \max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+\Delta )\right] + \lambda\sum_k\sum_l W_{k,l}^2$</p>
<p>其中，$N$是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数$\lambda$来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。</p>
<p>除了上述理由外，引入正则化惩罚还带来很多良好的性质，这些性质大多会在后续章节介绍。比如引入了L2惩罚后，SVM们就有了<strong>最大边界（max margin）</strong>这一良好性质。（如果感兴趣，可以查看CS229课程）。</p>
<p>其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。举个例子，假设输入向量$x = [1,1,1,1]$，两个权重向量 $w_1=[1,0,0,0]$，$w_2=[0.25,0.25,0.25,0.25]$。那么 $w^T_1x=w^T_2=1$，两个权重向量都得到同样的内积，但是$w_1$的L2惩罚是1.0，而$w_2$的L2惩罚是0.25。因此，根据L2惩罚来看，$w_2$更好，因为它的正则化损失更小。从直观上来看，这是因为$w_2$的权重值更小且更分散。既然L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免<strong>过拟合</strong>。</p>
<p>需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常只对权重$W$正则化，而不正则化偏差$b$。在实际操作中，可发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当$W=0$的特殊情况下，才能得到损失值为0。</p>
<p><strong>代码</strong>：下面是一个无正则化部分的损失函数的Python实现，有非向量化和半向量化两个形式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_i</span>(<span class="params">x, y, W</span>):</span><br><span class="line">     <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">     unvectorized version. Compute the multiclass svm loss for a single example (x,y)</span></span><br><span class="line"><span class="string">     - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)</span></span><br><span class="line"><span class="string">       with an appended bias dimension in the 3073-rd position (i.e. bias trick)</span></span><br><span class="line"><span class="string">     - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)</span></span><br><span class="line"><span class="string">     - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)</span></span><br><span class="line"><span class="string">     &quot;&quot;&quot;</span></span><br><span class="line">     delta = <span class="number">1.0</span> <span class="comment"># see notes about delta later in this section</span></span><br><span class="line">     scores = W.dot(x) <span class="comment"># scores becomes of size 10 x 1, the scores for each class</span></span><br><span class="line">     correct_class_score = scores[y]</span><br><span class="line">     D = W.shape[<span class="number">0</span>] <span class="comment"># number of classes, e.g. 10</span></span><br><span class="line">     loss_i = <span class="number">0.0</span></span><br><span class="line">     <span class="keyword">for</span> j <span class="keyword">in</span> xrange(D): <span class="comment"># iterate over all wrong classes</span></span><br><span class="line">       <span class="keyword">if</span> j == y:</span><br><span class="line">         <span class="comment"># skip for the true class to only loop over incorrect classes</span></span><br><span class="line">         <span class="keyword">continue</span></span><br><span class="line">       <span class="comment"># accumulate loss for the i-th example</span></span><br><span class="line">       loss_i += <span class="built_in">max</span>(<span class="number">0</span>, scores[j] - correct_class_score + delta)</span><br><span class="line">     <span class="keyword">return</span> loss_i</span><br><span class="line">   </span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">L_i_vectorized</span>(<span class="params">x, y, W</span>):</span><br><span class="line">     <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">     A faster half-vectorized implementation. half-vectorized</span></span><br><span class="line"><span class="string">     refers to the fact that for a single example the implementation contains</span></span><br><span class="line"><span class="string">     no for loops, but there is still one loop over the examples (outside this function)</span></span><br><span class="line"><span class="string">     &quot;&quot;&quot;</span></span><br><span class="line">     delta = <span class="number">1.0</span></span><br><span class="line">     scores = W.dot(x)</span><br><span class="line">     <span class="comment"># compute the margins for all classes in one vector operation</span></span><br><span class="line">     margins = np.maximum(<span class="number">0</span>, scores - scores[y] + delta)</span><br><span class="line">     <span class="comment"># on y-th position scores[y] - scores[y] canceled and gave delta. We want</span></span><br><span class="line">     <span class="comment"># to ignore the y-th position and only consider margin on max wrong class</span></span><br><span class="line">     margins[y] = <span class="number">0</span></span><br><span class="line">     loss_i = np.<span class="built_in">sum</span>(margins)</span><br><span class="line">     <span class="keyword">return</span> loss_i</span><br><span class="line">   </span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">L</span>(<span class="params">X, y, W</span>):</span><br><span class="line">     <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">     fully-vectorized implementation :</span></span><br><span class="line"><span class="string">     - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)</span></span><br><span class="line"><span class="string">     - y is array of integers specifying correct class (e.g. 50,000-D array)</span></span><br><span class="line"><span class="string">     - W are weights (e.g. 10 x 3073)</span></span><br><span class="line"><span class="string">     &quot;&quot;&quot;</span></span><br><span class="line">     <span class="comment"># evaluate loss over all examples in X without using any for loops</span></span><br><span class="line">     <span class="comment"># left as exercise to reader in the assignment</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在本小节的学习中，一定要记得SVM损失采取了一种特殊的方法，使得能够衡量对于训练数据预测分类和实际分类标签的一致性。还有，对训练集中数据做出准确分类预测和让损失值最小化这两件事是等价的。  </p>
<p>&gt; 接下来要做的，就是找到能够使损失值最小化的权重了。  </p>
<h2 id="实际考虑"><a href="#实际考虑" class="headerlink" title="实际考虑"></a>实际考虑</h2><p><strong>设置$\delta$</strong>：你可能注意到上面的内容对超参数$\delta$及其设置是一笔带过，那么它应该被设置成什么值？需要通过交叉验证来求得吗？现在看来，该超参数在绝大多数情况下设为$\delta=1.0$都是安全的。超参数$\delta$和$\lambda$看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重$W$的大小对于分类分值有直接影响（当然对他们的差异也有直接影响）：当我们将$W$中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如$\delta=1$或$\delta=100$）从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度$\lambda$来控制）。</p>
<p><strong>与二元支持向量机（Binary Support Vector Machine）的关系</strong>：在学习本课程前，你可能对于二元支持向量机有些经验，它对于第i个数据的损失计算公式是：</p>
<p>$ L_i=C\max(0,1-y_iw^Tx_i)+R(W)$</p>
<p>其中，$C$是一个超参数，并且$y_i\in{\left\{-1,1\right\}}$。可以认为本章节介绍的SVM公式包含了上述公式，上述公式是多类支持向量机公式只有两个分类类别的特例。也就是说，如果我们要分类的类别只有两个，那么公式就化为二元SVM公式。这个公式中的$C$和多类SVM公式中的$\lambda$都控制着同样的权衡，而且它们之间的关系是$C\propto\frac{1}{\lambda}$</p>
<p><strong>备注：在初始形式中进行最优化</strong>。如果在本课程之前学习过SVM，那么对kernels，duals，SMO算法等将有所耳闻。在本课程（主要是神经网络相关）中，损失函数的最优化的始终在非限制初始形式下进行。很多这些损失函数从技术上来说是不可微的（比如当$x=y$时，$\max(x,y)$函数就不可微分），但是在实际操作中并不存在问题，因为通常可以使用次梯度。</p>
<p><strong>备注：其他多类SVM公式</strong>。需要指出的是，本课中展示的多类SVM只是多种SVM公式中的一种。另一种常用的公式是_One-Vs-All_（OVA）SVM，它针对每个类和其他类训练一个独立的二元分类器。还有另一种更少用的叫做_All-Vs-All_（AVA）策略。我们的公式是按照[Weston and Watkins 1999 (pdf)]版本，比OVA性能更强（在构建有一个多类数据集的情况下，这个版本可以在损失值上取到0，而OVA就不行。感兴趣的话在论文中查阅细节）。最后一个需要知道的公式是Structured SVM，它将正确分类的分类分值和非正确分类中的最高分值的边界最大化。理解这些公式的差异超出了本课程的范围。本课程笔记介绍的版本可以在实践中安全使用，而被论证为最简单的OVA策略在实践中看起来也能工作的同样出色（在 Rikin等人2004年的论文[In Defense of One-Vs-All Classification (pdf)中可查）。</p>
<h2 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h2><p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，</strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出$f(x_i,W)$作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射$f(x_i;W)=Wx_i$保持不变，其输出是线性分类器预测的原始分数，但将这些评分值视为每个分类的未归一化的对数概率（或者叫做非标准化的对数概率），进行指数计算之后，所有的分数转化为正数，并且将<strong>折叶损失（hinge loss）</strong>替换为<strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong>。公式如下：</p>
<p>$Li=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})$ 或等价的 $L_i=-f_{y_i}+log(\sum_je^{f_j})$</p>
<p>在上式中，使用$f_j$来表示分类评分向量$f$中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值$L_i$的均值与正则化损失$R(W)$之和。其中函数$f_j(z)=\frac{e^{z_j}}{\sum_ke^{z_k}}$被称作<strong>softmax 函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（$z$中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。  </p>
<p>因为原始的线性分类器输出的是原始分数，可以看做是非标准化/未归一化的对数概率，然后取对数，得到非标准化/未归一化概率，然后进行归一化，就可以得到所有类的离散概率分布了</p>
<p><strong>信息理论视角</strong>：在”真实”分布$p$和估计分布$q$之间的<strong>交叉熵</strong>定义如下：  </p>
<p>$H(p,q)=-\sum_xp(x) logq(x)$  </p>
<p>因此，Softmax分类器所做的就是最小化在估计分类概率（ 就是上面的$\frac{e^{f_{y_i} } }{\sum_{j}e^{f_j} }$）和”真实”分布之间的交叉熵，在这个解释中，”真实”分布就是所有概率密度都分布在正确的类别上（比如：$p=[0,\ldots1,\ldots,0]$中在$y_i$的位置就有一个单独的1）。还有，既然交叉熵可以写成熵和相对熵（Kullback-Leibler divergence）$H(p,q)=H(p)+D_{KL}(p||q)$，并且$\delta$函数$p$的熵是0，那么就能等价的看做是对两个分布之间的相对熵做最小化操作。换句话说，交叉熵损失函数”想要”预测分布的所有<strong>概率密度</strong>都在正确分类上。</p>
<p><strong>注</strong>：Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。</p>
<p><strong>概率论解释</strong>：先看下面的公式：  </p>
<p>$P(y_i|x_i,W)=\frac{e^{f_{y_i}}}{\sum_je^{f_j}}$</p>
<p>可以解释为是给定图像数据$x_i$，以$W$为参数，分配给正确分类标签$y_i$的归一化概率。为了理解这点，请回忆一下Softmax分类器将输出向量$f$中的评分值解释为没有归一化的<strong>对数概率</strong>。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行了归一化处理，使得这些概率的和为1。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行<strong>最大似然估计</strong>（MLE）。该解释的另一个好处是，损失函数中的正则化部分$R(W)$可以被看做是权重矩阵$W$的高斯先验，这里进行的是最大后验估计（MAP）而不是最大似然估计。提及这些解释只是为了让读者形成直观的印象，具体细节就超过本课程范围了。</p>
<p><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项$e^{f_{y_i}}$和$\sum_j e^{f_j}$因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数$C$，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p>
<p>$\frac{e^{f_{y_i}}}{\sum_je^{f_j}}=\frac{Ce^{f_{y_i}}}{C\sum_je^{f_j}}=\frac{e^{f_{y_i}+logC}}{\sum_je^{f_j+logC}}$ </p>
<p>$C$的值可自由选择，不会影响计算结果，通过使用这个技巧可以提高计算中的数值稳定性。通常将$C$设为$logC=-max_jf_j$。该技巧简单地说，就是应该将向量$f$中的数值进行平移，使得最大值为0。代码实现如下：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.<span class="built_in">sum</span>(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.<span class="built_in">max</span>(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.<span class="built_in">sum</span>(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<strong>折叶损失（hinge loss）</strong>，有时候又被称为<strong>最大边界损失（max-margin loss）</strong>。Softmax分类器使用的是<strong>交叉熵损失（corss-entropy loss）</strong>。Softmax分类器的命名是从<strong>softmax函数</strong>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说”softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p>
<h2 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h2><p>下图有助于区分这 Softmax和SVM这两种分类器：</p>
<img src="/2024/05/06/15-22-39/a90ce9e0ff533f3efee4747305382064_b.png" class>
<p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p>
<p><strong>Softmax分类器为每个分类提供了”可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类”猫”，”狗”，”船”。而softmax分类器可以计算出这三个标签的”可能性”是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性”上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：</p>
<p>$[1,-2,0]\to[e^1,e^{-2},e^0]=[2.71,0.14,1]\to[0.7,0.04,0.26]$  </p>
<p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：  </p>
<p>$[0.5,-1,0]\to[e^{0.5},e^{-1},e^0]=[1.65,0.73,1]\to[0.55,0.12,0.33]$  </p>
<p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p>
<p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加”局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（$\delta =1$）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p>
<p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>
<h2 id="交互式的网页Demo"><a href="#交互式的网页Demo" class="headerlink" title="交互式的网页Demo"></a>交互式的网页Demo</h2><img src="/2024/05/06/15-22-39/a68bbfd4465689c6d65b3eae9c24c934_b.jpg" class>
<p>我们实现了一个交互式的网页原型，来帮助读者直观地理解线性分类器。原型将损失函数进行可视化，画面表现的是对于2维数据的3种类别的分类。原型在课程进度上稍微超前，展现了最优化的内容，最优化将在下一节课讨论。  </p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>总结如下：</p>
<ul>
<li>定义了从图像像素映射到不同类别的分类评分的评分函数。在本节中，评分函数是一个基于权重<strong>W</strong>和偏差<strong>b</strong>的线性函数。</li>
<li>与k-NN分类器不同，<strong>参数方法</strong>的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重<strong>W</strong>进行一个矩阵乘法运算。</li>
<li>介绍了偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。</li>
<li>定义了损失函数（介绍了SVM和Softmax线性分类器最常用的2个损失函数）。损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。</li>
</ul>
<p>现在我们知道了如何基于参数，将数据集中的图像映射成为分类的评分，也知道了两种不同的损失函数，它们都能用来衡量算法分类预测的质量。但是，如何高效地得到能够使损失值最小的参数呢？这个求得最优参数的过程被称为最优化，将在下节课中进行介绍。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉 图像分类</title>
    <url>/2024/05/06/14-24-17/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.06：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/05/06/10-28-51/" title="eecs498 ML-DL-CV 笔记汇总">计算机视觉-笔记汇总</a>
</li>
</ul>
<h1 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><strong>目标</strong>：这一节我们将介绍图像分类问题。所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像，或者简单的说，就是对于一个给定的图像，预测它属于哪个类别（或者给出属于一系列不同标签的可能性）。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。在这里我们面临的挑战就是语义鸿沟。在后面的课程中，我们可以看到计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。</p>
<p><strong>例子</strong>：以下图为例，图像分类模型读取该图片，并生成该图片属于集合 {cat, dog, hat, mug}中各个标签的概率。需要注意的是，对于计算机来说，图像是一个由数字组成的巨大的3维数组（在深度学习工具中，图像就是一个三维张量）。在下图这个例子中，猫的图像大小是宽248像素，高400像素，有3个颜色通道，分别是红、绿和蓝（简称RGB）。</p>
<p>如此，该图像就包含了248X400X3=297600个数字，每个数字都是在范围0-255之间的整型，其中0表示全黑，255表示全白。我们的任务就是把这些上百万的数字变成一个简单的标签，比如”猫”。或者说，我们需要借助某种方法将这个原始数字网络转变为相应的有意义的语义——比如说“猫”标签</p>
<img src="/2024/05/06/14-24-17/baab9e4b97aceb77ec70abeda6be022d_b.png" class>
<h2 id="困难和挑战"><a href="#困难和挑战" class="headerlink" title="困难和挑战"></a>困难和挑战</h2><p>对于人来说，识别出一个像”猫”一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。我们在下面列举了计算机视觉算法在图像识别方面遇到的一些困难</p>
<ul>
<li><strong>视角变化（</strong>Viewpoint variation<strong>）</strong>：同一个物体，摄像机可以从多个角度来展现，尽管可能角度的变化很轻微，但是可能使得这些数字发生不直观的某种改变</li>
<li><strong>类内差异（</strong>Intra-class variation<strong>）</strong>：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。比如说猫就是一种很会变形的生物</li>
<li><strong>相似类（</strong>Fine-Grained Categories<strong>）</strong>：不同类物体的个体之间的外形差异小</li>
<li><strong>背景干扰（</strong>Background clutter<strong>）</strong>：物体可能混入背景之中，使之难以被辨认</li>
<li><strong>光照条件（</strong>Illumination conditions<strong>）</strong>：在像素层面上，光照的影响非常大，比如说光照和昏暗情况下图像会有不同情况</li>
<li><strong>形变（</strong>Deformation<strong>）</strong>：很多东西的形状并非一成不变，会有很大变化。</li>
<li><strong>遮挡（</strong>Occlusion<strong>）</strong>：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的，比如说猫隐藏在草丛中，并不明显</li>
<li><strong>大小变化（</strong>Scale variation<strong>）</strong>：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。</li>
</ul>
<p>面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。  </p>
<img src="/2024/05/06/14-24-17/1ee9457872f773d671dd5b225647ef45_b.jpg" class>
<h2 id="目标检测任务"><a href="#目标检测任务" class="headerlink" title="目标检测任务"></a>目标检测任务</h2><p>当然，计算机视觉不止有图像分类，还有另一个相关的任务——目标检测，这个任务我们需要将图像中的目标对象圈出来</p>
<p>结果证明，图像分类本来就是一个基础，可以用来构建更多更复杂的应用程序，比如说目标检测等</p>
<img src="/2024/05/06/14-24-17/object_detection.jpg" class>
<p>—————————————————————————————————————————</p>
<p><strong>想法</strong>：如何实现图像分类？</p>
<p>根据之前所了解到的方法，我们可能首先想到通过对照片进行<strong>边缘检测</strong>来提取特征，如何尝试找到角点或者其他类型的可解释模式，比如说猫有三角形尖耳朵，所以可以通过检测这方面的边缘信息，或者我们知道猫有胡须，所以我们可以提取胡须的边缘信息，我们根据这些信息来写一个算法来检测他们</p>
<p>当然，这并不是一个很好的方法，比如说会有没有胡须的猫，会有没有尖耳朵的猫，或者有时候边缘检测器会失效从而无法正常提交所需的边缘，而且这很难进行迁移——当我们可以成功识别猫的时候，如果我们想将其用到其他方面，比如说识别狗，那么之前的工作将毫无意义，所以我们需要找到一种具有可扩展性的算法</p>
<h2 id="数据驱动方法"><a href="#数据驱动方法" class="headerlink" title="数据驱动方法"></a>数据驱动方法</h2><p>如何写一个图像分类的算法呢？这和写个排序算法可是大不一样。怎么写一个从图像中认出猫的算法？搞不清楚。因此，与其在代码中直接写明各类物体到底看起来是什么样的，倒不如说我们采取的方法和教小孩儿看图识物类似：给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。这种方法，就是_数据驱动方法_。也就是使用拥有从数据中学习如何识别不同类型对象与图像的算法。既然该方法的第一步就是收集大量已经做好分类标注的图片来作为训练集，那么下面就看看数据集到底长什么样：  </p>
<p>—————————————————————————————————————————</p>
<img src="/2024/05/06/14-24-17/bbbfd2e6878d6f5d2a82f8239addbbc0_b.jpg" class>
<p>这是一个有4个视觉类别的训练集，尽管这个数据集非常简陋。在实际中，我们可能有上千的分类，每个分类都有成千上万的图像。</p>
<p><strong>图像分类流程</strong>。在课程视频中已经学习过，<strong>图像分类</strong>就是输入一个元素为像素值的数组，然后给它分配一个分类标签。完整流程如下：</p>
<ul>
<li><strong>输入</strong>：输入是包含N个图像的集合，每个图像的标签是K种分类标签中的一种。这个集合称为训练集。</li>
<li><strong>学习</strong>：这一步的任务是使用训练集来学习每个类到底长什么样。一般该步骤叫做<strong>训练分类器</strong>或者学习一个模型。</li>
<li><strong>评价</strong>：让分类器来预测它未曾见过的图像的分类标签，并以此来评价分类器的质量。我们会把分类器预测的标签和图像真正的分类标签对比。毫无疑问，如果分类器预测的分类标签和图像真正的分类标签一致，那就是好事，这样的情况越多越好。</li>
</ul>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="MNIST数据集：计算机视觉中的果蝇"><a href="#MNIST数据集：计算机视觉中的果蝇" class="headerlink" title="MNIST数据集：计算机视觉中的果蝇"></a>MNIST数据集：计算机视觉中的果蝇</h3><p>MNIST数据集是一种手写数字数据集，其中的每张图片都包括一个不同的手写数字，大小统一为28 $\times$ 28，十个类别，有五万张作为训练集和一万张作为测试集</p>
<p>它更像一种玩具数据集，或者也被称为计算机视觉的果蝇，可以做很多测试，因为这个数据集很小而且简单，可以快速验证新想法</p>
<h3 id="CIFAR数据集"><a href="#CIFAR数据集" class="headerlink" title="CIFAR数据集"></a>CIFAR数据集</h3><p><strong>CIFAR-10</strong>：一个非常流行的图像分类数据集是CIFAR-10。这个数据集包含了60000张32X32的小图像。每张图像都有10种分类标签中的一种。这60000张图像被分为包含50000张图像的训练集和包含10000张图像的测试集。</p>
<p>在下图中你可以看见10个类的10张随机图片。</p>
<img src="/2024/05/06/14-24-17/fff49fd8cec00f77f657a4c4a679b030_b.jpg" class>
<p><strong>左边</strong>：从数据库来的样本图像。<strong>右边</strong>：第一列是测试图像，然后第一列的每个测试图像右边是使用Nearest Neighbor算法，根据像素差异，从训练集中选出的10张最类似的图片。  </p>
<p>此外还有CIFAR100数据集（100类）</p>
<h3 id="ImageNet：黄金数据集"><a href="#ImageNet：黄金数据集" class="headerlink" title="ImageNet：黄金数据集"></a>ImageNet：黄金数据集</h3><p>这是一个非常大的数据，有一千种类别，同时有一百三十万张图片（每个类别1300张），五万张验证集，十万张测试集，性能标准是Top5准确率，图像大小不统一</p>
<h2 id="Nearest-Neighbor分类器"><a href="#Nearest-Neighbor分类器" class="headerlink" title="Nearest Neighbor分类器"></a>Nearest Neighbor分类器</h2><p>作为课程介绍的第一个方法，我们来实现一个<strong>Nearest Neighbor分类器</strong>。虽然这个分类器和卷积神经网络没有任何关系，实际中也极少使用而且其非常简单，但通过实现它，可以让读者对于解决图像分类问题的方法有个基本的认识，也就是机器学习系统的两个基本部分——训练、预测。</p>
<p>其中，训练函数，就是记住所有的数据和标签（或者说进行学习），预测函数，就是预测出图像最可能的标签</p>
<img src="/2024/05/06/14-24-17/fff49fd8cec00f77f657a4c4a679b030_b.jpg" class>
<p>假设现在我们有CIFAR-10的50000张图片（每种分类5000张）作为训练集，我们希望将余下的10000作为测试集并给他们打上标签。Nearest Neighbor算法将会拿着测试图片和训练集中每一张图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。上面右边的图片就展示了这样的结果。请注意上面10个分类中，只有3个是准确的。比如第8行中，马头被分类为一个红色的跑车，原因在于红色跑车的黑色背景非常强烈，所以这匹马就被错误分类为跑车了。</p>
<p>那么具体如何比较两张图片的相似程度呢（或者可以将相似程度理解为距离，距离越近，图片越相似）？在本例中，就是比较32x32x3的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量$I_1$和$I_2$，然后计算他们的<strong>L1距离（曼哈顿距离）：</strong></p>
<p>$displaystyle d_1(I_1,I_2)=sum_p|I^p_1-I^p_2|$</p>
<p>这里的求和是针对所有的像素。下面是整个比较流程的图例：  </p>
<img src="/2024/05/06/14-24-17/95cfe7d9efb83806299c218e0710a6c5_b.jpg" class>
<p>以图片中的一个颜色通道为例来进行说明。两张图片使用L1距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么L1距离为0，但是如果两张图片很是不同，那L1值将会非常大。</p>
<p>下面，让我们看看如何用代码来实现这个分类器。首先，我们将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，<strong>Xtr</strong>（大小是50000x32x32x3）存有训练集中所有的图像，<strong>Ytr</strong>是对应的长度为50000的1维数组，存有图像对应的分类标签（从0到9）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class="string">&#x27;data/cifar10/&#x27;</span>) <span class="comment"># a magifunction we provide</span></span><br><span class="line"><span class="comment"># flatten out all images to be one-dimensional</span></span><br><span class="line">Xtr_rows = Xtr.reshape(Xtr.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xtr_rowbecomes 50000 x 3072</span></span><br><span class="line">Xte_rows = Xte.reshape(Xte.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xte_rowbecomes 10000 x 3072</span></span><br></pre></td></tr></table></figure>
<p>现在我们得到所有的图像数据，并且把他们拉长成为行向量了。接下来展示如何训练并评价一个分类器：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    nn = NearestNeighbor() <span class="comment"># create a Nearest Neighbor classifier class</span></span><br><span class="line">​    nn.train(Xtr_rows, Ytr) <span class="comment"># train the classifier on the training images and labels</span></span><br><span class="line">​    Yte_predict = nn.predict(Xte_rows) <span class="comment"># predict labels on the test images</span></span><br><span class="line">​    <span class="comment"># and now print the classification accuracy, which is the average number</span></span><br><span class="line">​    <span class="comment"># of examples that are correctly predicted (i.e. label matches)</span></span><br><span class="line">​    <span class="built_in">print</span> <span class="string">&#x27;accuracy: %f&#x27;</span> % ( np.mean(Yte_predict == Yte) )</span><br></pre></td></tr></table></figure>
<p>作为评价标准，我们常常使用<strong>准确率</strong>，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：<strong>train(X, y)</strong>函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个<strong>predict(X)</strong>函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用L1距离的Nearest Neighbor分类器的实现套路：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">​    </span><br><span class="line">​    <span class="keyword">class</span> <span class="title class_">NearestNeighbor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">​      <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">​        <span class="keyword">pass</span></span><br><span class="line">​    </span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class="line">        self.Xtr = X</span><br><span class="line">        self.ytr = y</span><br><span class="line">    </span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span></span><br><span class="line">        num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># lets make sure that the output type matches the input type</span></span><br><span class="line">        Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># loop over all test rows</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">          <span class="comment"># find the nearest training image to the i&#x27;th test image</span></span><br><span class="line">          <span class="comment"># using the L1 distance (sum of absolute value differences)</span></span><br><span class="line">          distances = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(self.Xtr - X[i,:]), axis = <span class="number">1</span>)</span><br><span class="line">          min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></span><br><span class="line">          Ypred[i] = self.ytr[min_index] <span class="comment"># predict the label of the nearest example</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> Ypred</span><br></pre></td></tr></table></figure>
<p>如果你用这段代码跑CIFAR-10，你会发现准确率能达到<strong>38.6%</strong>。这比随机猜测的10%要好，但是比人类识别的水平（据研究推测是94%）和卷积神经网络能达到的95%还是差多了。</p>
<p><strong>距离选择</strong>：计算向量间的距离有很多种方法，另一个常用的方法是<strong>L2距离</strong>，从几何学的角度，可以理解为它在计算两个向量间的欧式距离。L2距离的公式如下：  </p>
<p>$displaystyle d_2(I_1,I_2)=sqrt{ sum_p(I^p_1-I^p_2)^2}$</p>
<img src="/2024/05/06/14-24-17/60.jpg" class>
<p>换句话说，我们依旧是在计算像素间的差值，只是先求其平方，然后把这些平方全部加起来，最后对这个和开方。在Numpy中，我们只需要替换上面代码中的1行代码就行：  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">distances = np.sqrt(np.<span class="built_in">sum</span>(np.square(self.Xtr - X[i,:]), axis = <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>注意在这里使用了<strong>np.sqrt</strong>，但是在实际中可能不用。因为求平方根函数是一个单调函数，它对不同距离的绝对值求平方根虽然改变了数值大小，但依然保持了不同距离大小的顺序。所以用不用它，都能够对像素差异的大小进行正确比较。如果你在CIFAR-10上面跑这个模型，正确率是<strong>35.4%</strong>，比刚才低了一点。</p>
<p><strong>L1和L2比较</strong>。比较这两个度量方式是挺有意思的。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在<a href="https://planetmath.org/vectorpnorm">p-norm__</a>常用的特殊形式。</p>
<img src="/2024/05/06/14-24-17/60.jpg" class>
<h2 id="k-Nearest-Neighbor分类器"><a href="#k-Nearest-Neighbor分类器" class="headerlink" title="k-Nearest Neighbor分类器"></a>k-Nearest Neighbor分类器</h2><p>你可能注意到了，为什么只用最相似的1张图片的标签来作为测试图像的标签呢？这不是很奇怪吗！是的，使用<strong>k-Nearest Neighbor分类器</strong>就能做得更好。它的思想很简单：与其只找最相近的那1个图片的标签，我们找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。  </p>
<img src="/2024/05/06/14-24-17/237615075-bdd1fa05-cb3c-4dc5-b24f-940987cdb225.jpg" class>
<p>上面示例展示了Nearest Neighbor分类器和5-Nearest Neighbor分类器的区别。例子使用了2维的点来表示，分成3类（红、蓝和绿）。不同颜色区域代表的是使用L2距离的分类器的<strong>决策边界</strong>。白色的区域是分类模糊的例子（即图像与两个以上的分类标签绑定）。需要注意的是，在NN分类器中，异常的数据点（比如：在蓝色区域中的绿点）制造出一个不正确预测的孤岛。5-NN分类器将这些不规则都平滑了，使得它针对测试数据的<strong>泛化（generalization）</strong>能力更好（例子中未展示）。注意，5-NN中也存在一些灰色区域，这些区域是因为近邻标签的最高票数相同导致的（比如：2个邻居是红色，2个邻居是蓝色，还有1个是绿色)。</p>
<p>在实际中，大多使用k-NN分类器。但是k值如何确定呢？接下来就讨论这个问题。  </p>
<h2 id="用于超参数调优的验证集"><a href="#用于超参数调优的验证集" class="headerlink" title="用于超参数调优的验证集"></a>用于超参数调优的验证集</h2><p>k-NN分类器需要设定k值，那么选择哪个k值最合适的呢？我们可以选择不同的距离函数，比如L1范数和L2范数等，那么选哪个好？还有不少选择我们甚至连考虑都没有考虑到（比如：点积）。所有这些选择，被称为<strong>超参数（hyperparameter）</strong>。在基于数据进行学习的机器学习算法设计中，超参数是很常见的。一般说来，这些超参数具体怎么设置或取值并不是显而易见的。</p>
<p>你可能会建议尝试不同的值，看哪个值表现最好就选哪个。好主意！我们就是这么做的，但这样做的时候要非常细心。特别注意：<strong>决不能使用测试集来进行调优</strong>。当你在设计机器学习算法的时候，应该把测试集看做非常珍贵的资源，不到最后一步，绝不使用它。如果你使用测试集来调优，而且算法看起来效果不错，那么真正的危险在于：算法实际部署后，性能可能会远低于预期。这种情况，称之为算法对测试集<strong>过拟合</strong>。从另一个角度来说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实是过于乐观了，实际部署起来效果就会差很多。所以，最终测试的时候再使用测试集，可以很好地近似度量你所设计的分类器的泛化性能（在接下来的课程中会有很多关于泛化性能的讨论）。</p>
<p>&gt; 测试数据集只使用一次，即在训练完成后评价最终的模型时使用。</p>
<p>但是只有训练集和测试集会有一个问题，那就是不知道算法在新数据上表现如何。 </p>
<p>好在我们有不用测试集调优的方法。其思路是：从训练集中取出一部分数据用来调优，我们称之为<strong>验证集（validation set）</strong>。以CIFAR-10为例，我们可以用49000个图像作为训练集，用1000个图像作为验证集。验证集其实就是作为假的测试集来调优。下面就是代码：</p>
<p>​<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span></span><br><span class="line"><span class="comment"># recall Xtr_rows is 50,000 x 3072 matrix</span></span><br><span class="line">Xval_rows = Xtr_rows[:<span class="number">1000</span>, :] <span class="comment"># take first 1000 for validation</span></span><br><span class="line">Yval = Ytr[:<span class="number">1000</span>]</span><br><span class="line">Xtr_rows = Xtr_rows[<span class="number">1000</span>:, :] <span class="comment"># keep last 49,000 for train</span></span><br><span class="line">Ytr = Ytr[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># find hyperparameters that work best on the validation set</span></span><br><span class="line">validation_accuracies = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># use a particular value of k and evaluation on validation data</span></span><br><span class="line">  nn = NearestNeighbor()</span><br><span class="line">  nn.train(Xtr_rows, Ytr)</span><br><span class="line">  <span class="comment"># here we assume a modified NearestNeighbor class that can take a k as input</span></span><br><span class="line">  Yval_predict = nn.predict(Xval_rows, k = k)</span><br><span class="line">  acc = np.mean(Yval_predict == Yval)</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;accuracy: %f&#x27;</span> % (acc,)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># keep track of what works on the validation set</span></span><br><span class="line">  validation_accuracies.append((k, acc))</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>程序结束后，我们会作图分析出哪个k值表现最好，然后用这个k值来跑真正的测试集，并作出对算法的评价。  </p>
<p>&gt; 把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果。</p>
<p><strong>交叉验证</strong>。有时候，训练集数量较小（因此验证集的数量更小），人们会使用一种被称为<strong>交叉验证</strong>的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。</p>
<img src="/2024/05/06/14-24-17/6a3ceec60cc0a379b4939c37ee3e89e8_b.png" class>
<p>这就是5份交叉验证对k值调优的例子。针对每个k值，得到5个准确率结果，取其平均值，然后对不同k值的平均表现画线连接。本例中，当k=7的时算法表现最好（对应图中的准确率峰值）。如果我们将训练集分成更多份数，直线一般会更加平滑（噪音更少）。</p>
<p><strong>实际应用</strong>。在实际情况下，人们不是很喜欢用交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照50%-90%的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成3、5和10份。  </p>
<img src="/2024/05/06/14-24-17/cc88207c6c3c5e91df8b6367368f6450_b.jpg" class>
<p>常用的数据分割模式。给出训练集和测试集后，训练集一般会被均分。这里是分成5份。前面4份用来训练，黄色那份用作验证集调优。如果采取交叉验证，那就各份轮流作为验证集。最后模型训练完毕，超参数都定好了，让模型跑一次（而且只跑一次）测试集，以此测试结果评价算法。</p>
<h2 id="Nearest-Neighbor分类器的优劣"><a href="#Nearest-Neighbor分类器的优劣" class="headerlink" title="Nearest Neighbor分类器的优劣"></a>Nearest Neighbor分类器的优劣</h2><p>现在对Nearest Neighbor分类器的优缺点进行思考。首先，Nearest Neighbor分类器易于理解，实现简单。其次，算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率。其实，我们后续要学习的卷积神经网络在这个权衡上走到了另一个极端：虽然训练花费很多时间，但是一旦训练完成，对新的测试数据进行分类非常快。这样的模式就符合实际使用需求。</p>
<p>Nearest Neighbor分类器的计算复杂度研究是一个活跃的研究领域，若干<strong>Approximate Nearest Neighbor </strong>(ANN)算法和库的使用可以提升Nearest Neighbor分类器在数据上的计算速度(比如：FLANN)。这些算法可以在准确率和时空复杂度之间进行权衡，并通常依赖一个预处理/索引过程，这个过程中一般包含kd树的创建和k-means算法的运用。</p>
<p>Nearest Neighbor分类器在某些特定情况（比如数据维度较低）下，可能是不错的选择。但是在实际的图像分类工作中，很少使用。因为图像都是高维度数据（他们通常包含很多像素），而高维度向量之间的距离通常是反直觉的。下面的图片展示了基于像素的相似和基于感官的相似是有很大不同的：</p>
<img src="/2024/05/06/14-24-17/fd42d369eebdc5d81c89593ec1082e32_b.png" class>
<p>在高维度数据上，基于像素的的距离和感官上的非常不同。上图中，右边3张图片和左边第1张原始图片的L2距离是一样的。很显然，基于像素比较的相似和感官上以及语义上的相似是不同的。  </p>
<p>这里还有个视觉化证据，可以证明使用像素差异来比较图像是不够的。这是一个叫做t-SNE的可视化技术，它将CIFAR-10中的图片按照二维方式排布，这样能很好展示图片之间的像素差异值。在这张图片中，排列相邻的图片L2距离就小。  </p>
<img src="/2024/05/06/14-24-17/0f4980edb8710eaba0f3e661b1cbb830_b.jpg" class>
<p>上图使用t-SNE的可视化技术将CIFAR-10的图片进行了二维排列。排列相近的图片L2距离小。可以看出，图片的排列是被背景主导而不是图片语义内容本身主导。  </p>
<p>具体说来，这些图片的排布更像是一种颜色分布函数，或者说是基于背景的，而不是图片的语义主体。比如，狗的图片可能和青蛙的图片非常接近，这是因为两张图片都是白色背景。从理想效果上来说，我们肯定是希望同类的图片能够聚集在一起，而不被背景或其他不相关因素干扰。为了达到这个目的，我们不能止步于原始像素比较，得继续前进。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>简要说来：</p>
<ul>
<li>介绍了<strong>图像分类</strong>问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。</li>
<li>介绍了一个简单的图像分类器：<strong>最近邻分类器(Nearest Neighbor classifier)</strong>。分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。</li>
<li>选取超参数的正确方法是：将原始训练集分为训练集和<strong>验证集</strong>，我们在验证集上尝试不同的超参数，最后保留表现最好那个。</li>
<li>如果训练数据量不够，使用<strong>交叉验证</strong>方法，它能帮助我们在选取最优超参数的时候减少噪音。</li>
<li>一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。</li>
<li>最近邻分类器能够在CIFAR-10上得到将近40%的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。</li>
<li>最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。</li>
</ul>
<p>在接下来的课程中，我们将专注于解决这些问题和挑战，并最终能够得到超过90%准确率的解决方案。该方案能够在完成学习就丢掉训练集，并在一毫秒之内就完成一张图片的分类。</p>
<h2 id="小结：实际应用k-NN"><a href="#小结：实际应用k-NN" class="headerlink" title="小结：实际应用k-NN"></a>小结：实际应用k-NN</h2><p>如果你希望将k-NN分类器用到实处（最好别用到图像上，若是仅仅作为练手还可以接受），那么可以按照以下流程：</p>
<ol>
<li>预处理你的数据：对你数据中的特征进行归一化（normalize），让其具有零平均值（zero mean）和单位方差（unit variance）。在后面的小节我们会讨论这些细节。本小节不讨论，是因为图像中的像素都是同质的，不会表现出较大的差异分布，也就不需要标准化处理了。</li>
<li>如果数据是高维数据，考虑使用降维方法，比如PCA或随机投影。</li>
<li>将数据随机分入训练集和验证集。按照一般规律，70%-90% 数据作为训练集。这个比例根据算法中有多少超参数，以及这些超参数对于算法的预期影响来决定。如果需要预测的超参数很多，那么就应该使用更大的验证集来有效地估计它们。如果担心验证集数量不够，那么就尝试交叉验证方法。如果计算资源足够，使用交叉验证总是更加安全的（份数越多，效果越好，也更耗费计算资源）。</li>
<li>在验证集上调优，尝试足够多的k值，尝试L1和L2两种范数计算方式。</li>
<li>如果分类器跑得太慢，尝试使用Approximate Nearest Neighbor库（比如FLANN）来加速这个过程，其代价是降低一些准确率。</li>
<li>对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上运行并再次训练呢？因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。在实践中，<strong>不要这样做</strong>。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。<strong>直接使用测试集来测试用最优参数设置好的最优模型</strong>，得到测试集数据的分类准确率，并以此作为你的kNN分类器在该数据上的性能表现。</li>
</ol>
<h1 id="数据集代码实现"><a href="#数据集代码实现" class="headerlink" title="数据集代码实现"></a>数据集代码实现</h1><p>我们这里介绍了一系列的数据集和验证集的概念，但是我们肯定不会手动去进行计算和验证，必须依靠计算机来实现，所以我们应该如何使用代码去准备一个数据集呢？这就需要靠一些框架来完成这个操作</p>
<p>大家在算数的时候，会使用计算器来加快计算过程，在这里计算器只是一个工具，核心是你怎么去计算和要计算什么，在深度学习中也是，代码和相关的框架只不过是工具，可以起到计算器的作用，让你免于手动计算和自己搭建代码的困境，使你可以快速完成模型的学习</p>
<p>目前常用的深度学习框架是PyTorch、TensorFlow等，在学术界，PyTorch是主流的框架，而且简单易学，我们在这里使用PyTorch来完成一系列深度学习的代码任务</p>
<h2 id="数据集类——Dataset"><a href="#数据集类——Dataset" class="headerlink" title="数据集类——Dataset"></a>数据集类——Dataset</h2><p>在PyTorch中，Dataset是一个抽象的数据集类，可以用来自定义一些数据集类，用来实现从磁盘中读取数据集、读取标签和样本等操作，借助pytorch中dataset类，你可以创建适应任意模型的数据集接口</p>
<p>所谓数据集，无非就是一组{x:y}的集合吗，你只需要在这个类里说明“有一组{x:y}的集合”就可以了。x是样本的数据内容，y是样本标签</p>
<p>对于图像分类任务，图像+分类</p>
<p>对于目标检测任务，图像+bbox、分类</p>
<p>对于超分辨率任务，低分辨率图像+超分辨率图像</p>
<p>对于文本分类任务，文本+分类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;An abstract class representing a Dataset.</span></span><br><span class="line"><span class="string">    All other datasets should subclass it. All subclasses should override</span></span><br><span class="line"><span class="string">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</span></span><br><span class="line"><span class="string">    supporting integer indexing in range from 0 to len(self) exclusive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="keyword">return</span> ConcatDataset([self, other])</span><br></pre></td></tr></table></figure>
<p>上面的代码是pytorch给出的官方代码，其中__getitem__和__len__是子类必须继承的，或者说pytorch给出的官方代码限制了标准，你要按照它的标准进行数据集建立，其中__getitem__函数就是获取样本对（内容+标签），__len__就是获取数据集大小（或者说长度、样本数量）</p>
<p>比如说，我有一个图像分类的数据集，我想训练一个分类器，那么我就可以构建一个这样的数据集，它只需要满足下面的条件</p>
<ol>
<li>可以通过序号，得到图片和对应的分类标签</li>
<li>可以知道图片的数量（也就是数据集大小，从0开始计数）</li>
</ol>
<p>只要满足这两个条件，那么这个类就可以实例化为一个符合要求的数据集对象</p>
<p>我们考虑一个实际情况，图像分类的数据集放在文件夹D中，有三个类别的样本，分别在D下的a、b、c三个子文件夹中，子文件夹的名称就是类别的名称，那么我们如何自制一个数据集类来读取呢？</p>
<p>首先我们要告诉程序，数据集在哪里（或者说告诉路径），然后说明要对数据做什么变换，明确这是训练数据集还是验证数据集等等，所以我们需要在__init__函数中读取这些</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataSet</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_dir, dataset_type, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        image_dir: 路径</span></span><br><span class="line"><span class="string">        dataset_type: [&#x27;train&#x27;, &#x27;test&#x27;]</span></span><br><span class="line"><span class="string">        transform：各种处理</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line">        self.dataset_path = image_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.dataset_type = dataset_type</span><br><span class="line">        self.class_name=[]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> img_dir,class_name,_ <span class="keyword">in</span> os.walk(self.dataset_path) :</span><br><span class="line">            self.class_name.append(class_name)</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            self.sample_list.append(line.strip())</span><br><span class="line">        f.close()</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        item = self.sample_list[index]</span><br><span class="line">        <span class="comment"># img = cv2.imread(item.split(&#x27; _&#x27;)[0])</span></span><br><span class="line">        img = Image.<span class="built_in">open</span>(item.split(<span class="string">&#x27; _&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        label = <span class="built_in">int</span>(item.split(<span class="string">&#x27; _&#x27;</span>)[-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.sample_list)</span><br></pre></td></tr></table></figure>
<h2 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><p>下面是一些你可能感兴趣的拓展阅读链接：</p>
<ul>
<li><a href="http://homes.cs.washington.edu/%7Epedrod/papers/cacm12.pdf">A Few Useful Things to Know about Machine Learning__</a>，文中第6节与本节相关，但是整篇文章都强烈推荐。  </li>
<li><a href="http://people.csail.mit.edu/torralba/shortCourseRLOC/index.html">Recognizing and Learning Object Categories__</a>，ICCV 2005上的一节关于物体分类的课程。  </li>
</ul>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>eecs498 ML-DL-CV 笔记汇总</title>
    <url>/2024/05/06/10-28-51/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.06：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/05/06/10-28-38/" title="计算机视觉 Python常用库">计算机视觉笔记-第一章 Python常用库</a></li>
<li><a href="/2024/05/06/14-24-17/" title="计算机视觉 图像分类">计算机视觉笔记-第二章 图像分类</a></li>
<li><a href="/2024/05/06/15-22-39/" title="计算机视觉 线性分类器">计算机视觉笔记-第三章 线性分类器</a></li>
<li><a href="/2024/05/07/11-38-48/" title="计算机视觉 最优化">计算机视觉笔记-第四章 最优化</a></li>
<li><a href="/2024/05/07/12-17-20/" title="计算机视觉 神经网络">计算机视觉笔记-第五章 神经网络</a></li>
<li><a href="/2024/05/17/13-29-46/" title="计算机视觉 反向传播">计算机视觉笔记-第六章 反向传播</a></li>
<li><a href="/2024/05/17/13-57-26/" title="计算机视觉 卷积神经网络">计算机视觉笔记-第七章 卷积神经网络</a>
</li>
</ul>
<h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><p>笔记来源：<a href="https://github.com/Momordicin/ML_DL_CV_with_pytorch">Momordicin</a><br>课程视频：<a href="https://www.bilibili.com/video/BV1zg411a7Wi/?p=1&amp;vd_source=d5a3522259c17126f1b623c977d04c3e">bilibili</a></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉 Python常用库</title>
    <url>/2024/05/06/10-28-38/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.06：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/05/06/10-28-51/" title="eecs498 ML-DL-CV 笔记汇总">计算机视觉-笔记汇总</a>
</li>
</ul>
<p>我们将使用Python编程语言来完成本课程的所有作业。Python是一门伟大的通用编程语言，在一些常用库（numpy, scipy, matplotlib）的帮助下，它又会变成一个强大的科学计算环境。</p>
<p>内容列表：</p>
<ul>
<li>Python<ul>
<li>基本数据类型</li>
<li>容器</li>
<li>函数</li>
<li>类</li>
</ul>
</li>
<li>Numpy<ul>
<li>数组</li>
<li>访问数组</li>
<li>数据类型</li>
<li>数组计算</li>
<li>广播</li>
</ul>
</li>
<li>SciPy<ul>
<li>图像操作</li>
<li>MATLAB文件</li>
<li>点之间的距离</li>
</ul>
</li>
<li>Matplotlib<ul>
<li>绘制图形</li>
<li>绘制多个图形</li>
<li>图像</li>
</ul>
</li>
</ul>
<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p>Python是一种高级的，动态类型的多范型编程语言。很多时候，大家会说Python看起来简直和伪代码一样，这是因为你能够通过很少行数的代码表达出很有力的思想。举个例子，下面是用Python实现的经典的quicksort算法例子：  </p>
<pre><code class="lang-py">    def quicksort(arr):
        if len(arr) &lt;= 1:
            return arr
        pivot = arr[len(arr) / 2]
        left = [x for x in arr if x &lt; pivot]
        middle = [x for x in arr if x == pivot]
        right = [x for x in arr if x &gt; pivot]
        return quicksort(left) + middle + quicksort(right)

    print quicksort([3,6,8,10,1,2,1])
    # Prints &quot;[1, 1, 2, 3, 6, 8, 10]&quot;
</code></pre>
<h2 id="Python版本"><a href="#Python版本" class="headerlink" title="Python版本"></a>Python版本</h2><p>Python有两个支持的版本，分别是2.7和3.4。这有点让人迷惑，3.0向语言中引入了很多不向后兼容的变化，2.7下的代码有时候在3.4下是行不通的。在这个课程中，我们使用的是2.7版本。</p>
<p>如何查看版本呢？使用<strong>python —version</strong>命令。</p>
<h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><p>和大多数编程语言一样，Python拥有一系列的基本数据类型，比如整型、浮点型、布尔型和字符串等。这些类型的使用方式和在其他语言中的使用方式是类似的。</p>
<p><strong>数字</strong>：整型和浮点型的使用与其他语言类似。</p>
<pre><code class="lang-py">    x = 3
    print type(x) # Prints &quot;&quot;
    print x       # Prints &quot;3&quot;
    print x + 1   # Addition; prints &quot;4&quot;
    print x - 1   # Subtraction; prints &quot;2&quot;
    print x * 2   # Multiplication; prints &quot;6&quot;
    print x ** 2  # Exponentiation; prints &quot;9&quot;
    x += 1
    print x  # Prints &quot;4&quot;
    x *= 2
    print x  # Prints &quot;8&quot;
    y = 2.5
    print type(y) # Prints &quot;&quot;
    print y, y + 1, y * 2, y ** 2 # Prints &quot;2.5 3.5 5.0 6.25&quot;
</code></pre>
<p>需要注意的是，Python中没有 x++ 和 x— 的操作符。</p>
<p><strong>布尔型</strong>：Python实现了所有的布尔逻辑，但用的是英语，而不是我们习惯的操作符（比如&amp;&amp;和||等）。</p>
<pre><code class="lang-py">    t = True
    f = False
    print type(t) # Prints &quot;&quot;
    print t and f # Logical AND; prints &quot;False&quot;
    print t or f  # Logical OR; prints &quot;True&quot;
    print not t   # Logical NOT; prints &quot;False&quot;
    print t != f  # Logical XOR; prints &quot;True&quot;
</code></pre>
<p><strong>字符串</strong>：Python对字符串的支持非常棒。  </p>
<pre><code class="lang-py">    hello = &#39;hello&#39;   # String literals can use single quotes
    world = &quot;world&quot;   # or double quotes; it does not matter.
    print hello       # Prints &quot;hello&quot;
    print len(hello)  # String length; prints &quot;5&quot;
    hw = hello + &#39; &#39; + world  # String concatenation
    print hw  # prints &quot;hello world&quot;
    hw12 = &#39;%s %s %d&#39; % (hello, world, 12)  # sprintf style string formatting
    print hw12  # prints &quot;hello world 12&quot;
</code></pre>
<p>字符串对象有一系列有用的方法，比如：  </p>
<pre><code class="lang-py">    s = &quot;hello&quot;
    print s.capitalize()  # Capitalize a string; prints &quot;Hello&quot;
    print s.upper()       # Convert a string to uppercase; prints &quot;HELLO&quot;
    print s.rjust(7)      # Right-justify a string, padding with spaces; prints &quot;  hello&quot;
    print s.center(7)     # Center a string, padding with spaces; prints &quot; hello &quot;
    print s.replace(&#39;l&#39;, &#39;(ell)&#39;)  # Replace all instances of one substring with another;
                                   # prints &quot;he(ell)(ell)o&quot;
    print &#39;  world &#39;.strip()  # Strip leading and trailing whitespace; prints &quot;world&quot;
</code></pre>
<h2 id="容器Containers"><a href="#容器Containers" class="headerlink" title="容器Containers"></a>容器Containers</h2><p><strong>译者注</strong>：有知友建议container翻译为复合数据类型，供读者参考。</p>
<p>Python有以下几种容器类型：列表（lists）、字典（dictionaries）、集合（sets）和元组（tuples）。</p>
<h2 id="列表Lists"><a href="#列表Lists" class="headerlink" title="列表Lists"></a>列表Lists</h2><p>列表就是Python中的数组，但是列表长度可变，且能包含不同类型元素。</p>
<pre><code class="lang-py">    xs = [3, 1, 2]   # Create a list
    print xs, xs[2]  # Prints &quot;[3, 1, 2] 2&quot;
    print xs[-1]     # Negative indices count from the end of the list; prints &quot;2&quot;
    xs[2] = &#39;foo&#39;    # Lists can contain elements of different types
    print xs         # Prints &quot;[3, 1, &#39;foo&#39;]&quot;
    xs.append(&#39;bar&#39;) # Add a new element to the end of the list
    print xs         # Prints 
    x = xs.pop()     # Remove and return the last element of the list
    print x, xs      # Prints &quot;bar [3, 1, &#39;foo&#39;]&quot;
</code></pre>
<p><strong>切片Slicing</strong>：为了一次性地获取列表中的元素，Python提供了一种简洁的语法，这就是切片。  </p>
<pre><code class="lang-py">    nums = range(5)    # range is a built-in function that creates a list of integers
    print nums         # Prints &quot;[0, 1, 2, 3, 4]&quot;
    print nums[2:4]    # Get a slice from index 2 to 4 (exclusive); prints &quot;[2, 3]&quot;
    print nums[2:]     # Get a slice from index 2 to the end; prints &quot;[2, 3, 4]&quot;
    print nums[:2]     # Get a slice from the start to index 2 (exclusive); prints &quot;[0, 1]&quot;
    print nums[:]      # Get a slice of the whole list; prints [&quot;0, 1, 2, 3, 4]&quot;
    print nums[:-1]    # Slice indices can be negative; prints [&quot;0, 1, 2, 3]&quot;
    nums[2:4] = [8, 9] # Assign a new sublist to a slice
    print nums         # Prints &quot;[0, 1, 8, 8, 4]&quot;
</code></pre>
<p>在Numpy数组的内容中，我们会再次看到切片语法。</p>
<p><strong>循环Loops</strong>：我们可以这样遍历列表中的每一个元素：</p>
<pre><code class="lang-py">    animals = [&#39;cat&#39;, &#39;dog&#39;, &#39;monkey&#39;]
    for animal in animals:
        print animal
    # Prints &quot;cat&quot;, &quot;dog&quot;, &quot;monkey&quot;, each on its own line.
</code></pre>
<p>如果想要在循环体内访问每个元素的指针，可以使用内置的<strong>enumerate</strong>函数  </p>
<pre><code class="lang-py">    animals = [&#39;cat&#39;, &#39;dog&#39;, &#39;monkey&#39;]
    for idx, animal in enumerate(animals):
        print &#39;#%d: %s&#39; % (idx + 1, animal)
    # Prints &quot;#1: cat&quot;, &quot;#2: dog&quot;, &quot;#3: monkey&quot;, each on its own line
</code></pre>
<p><strong>列表推导List comprehensions</strong>：在编程的时候，我们常常想要将一种数据类型转换为另一种。下面是一个简单例子，将列表中的每个元素变成它的平方。  </p>
<pre><code class="lang-py">    nums = [0, 1, 2, 3, 4]
    squares = []
    for x in nums:
        squares.append(x ** 2)
    print squares   # Prints [0, 1, 4, 9, 16]
</code></pre>
<p>使用列表推导，你就可以让代码简化很多：  </p>
<pre><code class="lang-py">    nums = [0, 1, 2, 3, 4]
    squares = [x ** 2 for x in nums]
    print squares   # Prints [0, 1, 4, 9, 16]
</code></pre>
<p>列表推导还可以包含条件：  </p>
<pre><code class="lang-py">    nums = [0, 1, 2, 3, 4]
    even_squares = [x ** 2 for x in nums if x % 2 == 0]
    print even_squares  # Prints &quot;[0, 4, 16]&quot;
</code></pre>
<h3 id="字典Dictionaries"><a href="#字典Dictionaries" class="headerlink" title="字典Dictionaries"></a>字典Dictionaries</h3><p>字典用来储存（键, 值）对，这和Java中的Map差不多。你可以这样使用它：</p>
<pre><code class="lang-py">    d = &#123;&#39;cat&#39;: &#39;cute&#39;, &#39;dog&#39;: &#39;furry&#39;&#125;  # Create a new dictionary with some data
    print d[&#39;cat&#39;]       # Get an entry from a dictionary; prints &quot;cute&quot;
    print &#39;cat&#39; in d     # Check if a dictionary has a given key; prints &quot;True&quot;
    d[&#39;fish&#39;] = &#39;wet&#39;    # Set an entry in a dictionary
    print d[&#39;fish&#39;]      # Prints &quot;wet&quot;
    # print d[&#39;monkey&#39;]  # KeyError: &#39;monkey&#39; not a key of d
    print d.get(&#39;monkey&#39;, &#39;N/A&#39;)  # Get an element with a default; prints &quot;N/A&quot;
    print d.get(&#39;fish&#39;, &#39;N/A&#39;)    # Get an element with a default; prints &quot;wet&quot;
    del d[&#39;fish&#39;]        # Remove an element from a dictionary
    print d.get(&#39;fish&#39;, &#39;N/A&#39;) # &quot;fish&quot; is no longer a key; prints &quot;N/A&quot;
</code></pre>
<p><strong>循环Loops</strong>：在字典中，用键来迭代更加容易。</p>
<pre><code class="lang-py">    d = &#123;&#39;person&#39;: 2, &#39;cat&#39;: 4, &#39;spider&#39;: 8&#125;
    for animal in d:
        legs = d[animal]
        print &#39;A %s has %d legs&#39; % (animal, legs)
    # Prints &quot;A person has 2 legs&quot;, &quot;A spider has 8 legs&quot;, &quot;A cat has 4 legs&quot;
</code></pre>
<p>如果你想要访问键和对应的值，那就使用<strong>iteritems</strong>方法：  </p>
<pre><code class="lang-py">    d = &#123;&#39;person&#39;: 2, &#39;cat&#39;: 4, &#39;spider&#39;: 8&#125;
    for animal, legs in d.iteritems():
        print &#39;A %s has %d legs&#39; % (animal, legs)
    # Prints &quot;A person has 2 legs&quot;, &quot;A spider has 8 legs&quot;, &quot;A cat has 4 legs&quot;
</code></pre>
<p><strong>字典推导Dictionary comprehensions</strong>：和列表推导类似，但是允许你方便地构建字典。  </p>
<pre><code class="lang-py">    nums = [0, 1, 2, 3, 4]
    even_num_to_square = &#123;x: x ** 2 for x in nums if x % 2 == 0&#125;
    print even_num_to_square  # Prints &quot;&#123;0: 0, 2: 4, 4: 16&#125;&quot;
</code></pre>
<h3 id="集合Sets"><a href="#集合Sets" class="headerlink" title="集合Sets"></a>集合Sets</h3><p>集合是独立不同个体的无序集合。示例如下：  </p>
<pre><code class="lang-py">    animals = &#123;&#39;cat&#39;, &#39;dog&#39;&#125;
    print &#39;cat&#39; in animals   # Check if an element is in a set; prints &quot;True&quot;
    print &#39;fish&#39; in animals  # prints &quot;False&quot;
    animals.add(&#39;fish&#39;)      # Add an element to a set
    print &#39;fish&#39; in animals  # Prints &quot;True&quot;
    print len(animals)       # Number of elements in a set; prints &quot;3&quot;
    animals.add(&#39;cat&#39;)       # Adding an element that is already in the set does nothing
    print len(animals)       # Prints &quot;3&quot;
    animals.remove(&#39;cat&#39;)    # Remove an element from a set
    print len(animals)       # Prints &quot;2&quot;
</code></pre>
<p><strong>循环Loops</strong>：在集合中循环的语法和在列表中一样，但是集合是无序的，所以你在访问集合的元素的时候，不能做关于顺序的假设。</p>
<pre><code class="lang-py">    animals = &#123;&#39;cat&#39;, &#39;dog&#39;, &#39;fish&#39;&#125;
    for idx, animal in enumerate(animals):
        print &#39;#%d: %s&#39; % (idx + 1, animal)
    # Prints &quot;#1: fish&quot;, &quot;#2: dog&quot;, &quot;#3: cat&quot;
</code></pre>
<p><strong>集合推导**</strong>Set comprehensions**：和字典推导一样，可以很方便地构建集合：</p>
<pre><code class="lang-py">    from math import sqrt
    nums = &#123;int(sqrt(x)) for x in range(30)&#125;
    print nums  # Prints &quot;set([0, 1, 2, 3, 4, 5])&quot;
</code></pre>
<h3 id="元组Tuples"><a href="#元组Tuples" class="headerlink" title="元组Tuples"></a>元组Tuples</h3><p>元组是一个值的有序列表（不可改变）。从很多方面来说，元组和列表都很相似。和列表最重要的不同在于，元组可以在字典中用作键，还可以作为集合的元素，而列表不行。例子如下：  </p>
<pre><code class="lang-py">    d = &#123;(x, x + 1): x for x in range(10)&#125;  # Create a dictionary with tuple keys
    print d
    t = (5, 6)       # Create a tuple
    print type(t)    # Prints &quot;&quot;
    print d[t]       # Prints &quot;5&quot;
    print d[(1, 2)]  # Prints &quot;1&quot;
</code></pre>
<h2 id="函数Functions"><a href="#函数Functions" class="headerlink" title="函数Functions"></a>函数Functions</h2><p>Python函数使用def来定义函数：  </p>
<pre><code class="lang-py">    def sign(x):
        if x &gt; 0:
            return &#39;positive&#39;
        elif x &lt; 0:
            return &#39;negative&#39;
        else:
            return &#39;zero&#39;

    for x in [-1, 0, 1]:
        print sign(x)
    # Prints &quot;negative&quot;, &quot;zero&quot;, &quot;positive&quot;
</code></pre>
<p>我们常常使用可选参数来定义函数：  </p>
<pre><code class="lang-py">    def hello(name, loud=False):
        if loud:
            print &#39;HELLO, %s&#39; % name.upper()
        else:
            print &#39;Hello, %s!&#39; % name

    hello(&#39;Bob&#39;) # Prints &quot;Hello, Bob&quot;
    hello(&#39;Fred&#39;, loud=True)  # Prints &quot;HELLO, FRED!&quot;
</code></pre>
<h2 id="类Classes"><a href="#类Classes" class="headerlink" title="类Classes"></a>类Classes</h2><p>Python对于类的定义是简单直接的：</p>
<pre><code class="lang-py">    class Greeter(object):

        # Constructor
        def __init__(self, name):
            self.name = name  # Create an instance variable

        # Instance method
        def greet(self, loud=False):
            if loud:
                print &#39;HELLO, %s!&#39; % self.name.upper()
            else:
                print &#39;Hello, %s&#39; % self.name

    g = Greeter(&#39;Fred&#39;)  # Construct an instance of the Greeter class
    g.greet()            # Call an instance method; prints &quot;Hello, Fred&quot;
    g.greet(loud=True)   # Call an instance method; prints &quot;HELLO, FRED!&quot;
</code></pre>
<p>Numpy是Python中用于科学计算的核心库。它提供了高性能的多维数组对象，以及相关工具。</p>
<h2 id="数组Arrays"><a href="#数组Arrays" class="headerlink" title="数组Arrays"></a>数组Arrays</h2><p>一个numpy数组是一个由不同数值组成的网格。网格中的数据都是同一种数据类型，可以通过非负整型数的元组来访问。维度的数量被称为数组的阶，数组的大小是一个由整型数构成的元组，可以描述数组不同维度上的大小。</p>
<p>我们可以从列表创建数组，然后利用方括号访问其中的元素：</p>
<pre><code class="lang-py">    import numpy as np

    a = np.array([1, 2, 3])  # Create a rank 1 array
    print type(a)            # Prints &quot;&quot;
    print a.shape            # Prints &quot;(3,)&quot;
    print a[0], a[1], a[2]   # Prints &quot;1 2 3&quot;
    a[0] = 5                 # Change an element of the array
    print a                  # Prints &quot;[5, 2, 3]&quot;

    b = np.array([[1,2,3],[4,5,6]])   # Create a rank 2 array
    print b                           # 显示一下矩阵b
    print b.shape                     # Prints &quot;(2, 3)&quot;
    print b[0, 0], b[0, 1], b[1, 0]   # Prints &quot;1 2 4&quot;
</code></pre>
<p>Numpy还提供了很多其他创建数组的方法：  </p>
<pre><code class="lang-py">    import numpy as np

    a = np.zeros((2,2))  # Create an array of all zeros
    print a              # Prints &quot;[[ 0.  0.]
                         #          [ 0.  0.]]&quot;

    b = np.ones((1,2))   # Create an array of all ones
    print b              # Prints &quot;[[ 1.  1.]]&quot;

    c = np.full((2,2), 7) # Create a constant array
    print c               # Prints &quot;[[ 7.  7.]
                          #          [ 7.  7.]]&quot;

    d = np.eye(2)        # Create a 2x2 identity matrix
    print d              # Prints &quot;[[ 1.  0.]
                         #          [ 0.  1.]]&quot;

    e = np.random.random((2,2)) # Create an array filled with random values
    print e                     # Might print &quot;[[ 0.91940167  0.08143941]
                                #               [ 0.68744134  0.87236687]]&quot;
</code></pre>
<h2 id="访问数组"><a href="#访问数组" class="headerlink" title="访问数组"></a>访问数组</h2><p>Numpy提供了多种访问数组的方法。</p>
<p><strong>切片</strong>：和Python列表类似，numpy数组可以使用切片语法。因为数组可以是多维的，所以你<strong>必须</strong>为每个维度指定好切片。</p>
<pre><code class="lang-py">    import numpy as np

    # Create the following rank 2 array with shape (3, 4)
    # [[ 1  2  3  4]
    #  [ 5  6  7  8]
    #  [ 9 10 11 12]]
    a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])

    # Use slicing to pull out the subarray consisting of the first 2 rows
    # and columns 1 and 2; b is the following array of shape (2, 2):
    # [[2 3]
    #  [6 7]]
    b = a[:2, 1:3]

    # A slice of an array is a view into the same data, so modifying it
    # will modify the original array.
    print a[0, 1]   # Prints &quot;2&quot;
    b[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1]
    print a[0, 1]   # Prints &quot;77&quot;
</code></pre>
<p>你可以同时使用整型和切片语法来访问数组。但是，这样做会产生一个比原数组低阶的新数组。需要注意的是，这里和MATLAB中的情况是不同的：  </p>
<pre><code class="lang-py">    import numpy as np

    # Create the following rank 2 array with shape (3, 4)
    # [[ 1  2  3  4]
    #  [ 5  6  7  8]
    #  [ 9 10 11 12]]
    a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])

    # Two ways of accessing the data in the middle row of the array.
    # Mixing integer indexing with slices yields an array of lower rank,
    # while using only slices yields an array of the same rank as the
    # original array:
    row_r1 = a[1, :]    # Rank 1 view of the second row of a  
    row_r2 = a[1:2, :]  # Rank 2 view of the second row of a
    print row_r1, row_r1.shape  # Prints &quot;[5 6 7 8] (4,)&quot;
    print row_r2, row_r2.shape  # Prints &quot;[[5 6 7 8]] (1, 4)&quot;

    # We can make the same distinction when accessing columns of an array:
    col_r1 = a[:, 1]
    col_r2 = a[:, 1:2]
    print col_r1, col_r1.shape  # Prints &quot;[ 2  6 10] (3,)&quot;
    print col_r2, col_r2.shape  # Prints &quot;[[ 2]
                                #          [ 6]
                                #          [10]] (3, 1)&quot;
</code></pre>
<p><strong>整型数组访问</strong>：当我们使用切片语法访问数组时，得到的总是原数组的一个子集。整型数组访问允许我们利用其它数组的数据构建一个新的数组：  </p>
<pre><code class="lang-py">    import numpy as np

    a = np.array([[1,2], [3, 4], [5, 6]])

    # An example of integer array indexing.
    # The returned array will have shape (3,) and 
    print a[[0, 1, 2], [0, 1, 0]]  # Prints &quot;[1 4 5]&quot;

    # The above example of integer array indexing is equivalent to this:
    print np.array([a[0, 0], a[1, 1], a[2, 0]])  # Prints &quot;[1 4 5]&quot;

    # When using integer array indexing, you can reuse the same
    # element from the source array:
    print a[[0, 0], [1, 1]]  # Prints &quot;[2 2]&quot;

    # Equivalent to the previous integer array indexing example
    print np.array([a[0, 1], a[0, 1]])  # Prints &quot;[2 2]&quot;
</code></pre>
<p>整型数组访问语法还有个有用的技巧，可以用来选择或者更改矩阵中每行中的一个元素：  </p>
<pre><code class="lang-py">    import numpy as np

    # Create a new array from which we will select elements
    a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])

    print a  # prints &quot;array([[ 1,  2,  3],
             #                [ 4,  5,  6],
             #                [ 7,  8,  9],
             #                [10, 11, 12]])&quot;

    # Create an array of indices
    b = np.array([0, 2, 0, 1])

    # Select one element from each row of a using the indices in b
    print a[np.arange(4), b]  # Prints &quot;[ 1  6  7 11]&quot;

    # Mutate one element from each row of a using the indices in b
    a[np.arange(4), b] += 10

    print a  # prints &quot;array([[11,  2,  3],
             #                [ 4,  5, 16],
             #                [17,  8,  9],
             #                [10, 21, 12]])
</code></pre>
<p><strong>布尔型数组访问</strong>：布尔型数组访问可以让你选择数组中任意元素。通常，这种访问方式用于选取数组中满足某些条件的元素，举例如下：  </p>
<pre><code class="lang-py">    import numpy as np

    a = np.array([[1,2], [3, 4], [5, 6]])

    bool_idx = (a &gt; 2)  # Find the elements of a that are bigger than 2;
                        # this returns a numpy array of Booleans of the same
                        # shape as a, where each slot of bool_idx tells
                        # whether that element of a is &gt; 2.

    print bool_idx      # Prints &quot;[[False False]
                        #          [ True  True]
                        #          [ True  True]]&quot;

    # We use boolean array indexing to construct a rank 1 array
    # consisting of the elements of a corresponding to the True values
    # of bool_idx
    print a[bool_idx]  # Prints &quot;[3 4 5 6]&quot;

    # We can do all of the above in a single concise statement:
    print a[a &gt; 2]     # Prints &quot;[3 4 5 6]&quot;
</code></pre>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>每个Numpy数组都是数据类型相同的元素组成的网格。Numpy提供了很多的数据类型用于创建数组。当你创建数组的时候，Numpy会尝试猜测数组的数据类型，你也可以通过参数直接指定数据类型，例子如下：</p>
<pre><code class="lang-py">    import numpy as np

    x = np.array([1, 2])  # Let numpy choose the datatype
    print x.dtype         # Prints &quot;int64&quot;

    x = np.array([1.0, 2.0])  # Let numpy choose the datatype
    print x.dtype             # Prints &quot;float64&quot;

    x = np.array([1, 2], dtype=np.int64)  # Force a particular datatype
    print x.dtype                         # Prints &quot;int64&quot;
</code></pre>
<h2 id="数组计算"><a href="#数组计算" class="headerlink" title="数组计算"></a>数组计算</h2><p>基本数学计算函数会对数组中元素逐个进行计算，既可以利用操作符重载，也可以使用函数方式：  </p>
<pre><code class="lang-py">    import numpy as np

    x = np.array([[1,2],[3,4]], dtype=np.float64)
    y = np.array([[5,6],[7,8]], dtype=np.float64)

    # Elementwise sum; both produce the array
    # [[ 6.0  8.0]
    #  [10.0 12.0]]
    print x + y
    print np.add(x, y)

    # Elementwise difference; both produce the array
    # [[-4.0 -4.0]
    #  [-4.0 -4.0]]
    print x - y
    print np.subtract(x, y)

    # Elementwise product; both produce the array
    # [[ 5.0 12.0]
    #  [21.0 32.0]]
    print x * y
    print np.multiply(x, y)

    # Elementwise division; both produce the array
    # [[ 0.2         0.33333333]
    #  [ 0.42857143  0.5       ]]
    print x / y
    print np.divide(x, y)

    # Elementwise square root; produces the array
    # [[ 1.          1.41421356]
    #  [ 1.73205081  2.        ]]
    print np.sqrt(x)
</code></pre>
<p>和MATLAB不同，*是元素逐个相乘，而不是矩阵乘法。在Numpy中使用dot来进行矩阵乘法：  </p>
<pre><code class="lang-py">    import numpy as np

    x = np.array([[1,2],[3,4]])
    y = np.array([[5,6],[7,8]])

    v = np.array([9,10])
    w = np.array([11, 12])

    # Inner product of vectors; both produce 219
    print v.dot(w)
    print np.dot(v, w)

    # Matrix / vector product; both produce the rank 1 array [29 67]
    print x.dot(v)
    print np.dot(x, v)

    # Matrix / matrix product; both produce the rank 2 array
    # [[19 22]
    #  [43 50]]
    print x.dot(y)
    print np.dot(x, y)
</code></pre>
<p>Numpy提供了很多计算数组的函数，其中最常用的一个是<strong>sum</strong>：  </p>
<pre><code class="lang-py">    import numpy as np

    x = np.array([[1,2],[3,4]])

    print np.sum(x)  # Compute sum of all elements; prints &quot;10&quot;
    print np.sum(x, axis=0)  # Compute sum of each column; prints &quot;[4 6]&quot;
    print np.sum(x, axis=1)  # Compute sum of each row; prints &quot;[3 7]&quot;
</code></pre>
<p>除了计算，我们还常常改变数组或者操作其中的元素。其中将矩阵转置是常用的一个，在Numpy中，使用<strong>T</strong>来转置矩阵：</p>
<pre><code class="lang-py">    import numpy as np

    x = np.array([[1,2], [3,4]])
    print x    # Prints &quot;[[1 2]
               #          [3 4]]&quot;
    print x.T  # Prints &quot;[[1 3]
               #          [2 4]]&quot;

    # Note that taking the transpose of a rank 1 array does nothing:
    v = np.array([1,2,3])
    print v    # Prints &quot;[1 2 3]&quot;
    print v.T  # Prints &quot;[1 2 3]&quot;
</code></pre>
<h2 id="广播Broadcasting"><a href="#广播Broadcasting" class="headerlink" title="广播Broadcasting"></a>广播Broadcasting</h2><p>广播是一种强有力的机制，它让Numpy可以让不同大小的矩阵在一起进行数学计算。我们常常会有一个小的矩阵和一个大的矩阵，然后我们会需要用小的矩阵对大的矩阵做一些计算。</p>
<p>举个例子，如果我们想要把一个向量加到矩阵的每一行，我们可以这样做：</p>
<pre><code class="lang-py">    import numpy as np

    # We will add the vector v to each row of the matrix x,
    # storing the result in the matrix y
    x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
    v = np.array([1, 0, 1])
    y = np.empty_like(x)   # Create an empty matrix with the same shape as x

    # Add the vector v to each row of the matrix x with an explicit loop
    for i in range(4):
        y[i, :] = x[i, :] + v

    # Now y is the following
    # [[ 2  2  4]
    #  [ 5  5  7]
    #  [ 8  8 10]
    #  [11 11 13]]
    print y
</code></pre>
<p>这样是行得通的，但是当x矩阵非常大，利用循环来计算就会变得很慢很慢。我们可以换一种思路：  </p>
<pre><code class="lang-py">    import numpy as np

    # We will add the vector v to each row of the matrix x,
    # storing the result in the matrix y
    x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
    v = np.array([1, 0, 1])
    vv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other
    print vv                 # Prints &quot;[[1 0 1]
                             #          [1 0 1]
                             #          [1 0 1]
                             #          [1 0 1]]&quot;
    y = x + vv  # Add x and vv elementwise
    print y  # Prints &quot;[[ 2  2  4
             #          [ 5  5  7]
             #          [ 8  8 10]
             #          [11 11 13]]&quot;
</code></pre>
<p>Numpy广播机制可以让我们不用创建vv，就能直接运算，看看下面例子：  </p>
<pre><code class="lang-py">    import numpy as np

    # We will add the vector v to each row of the matrix x,
    # storing the result in the matrix y
    x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
    v = np.array([1, 0, 1])
    y = x + v  # Add v to each row of x using broadcasting
    print y  # Prints &quot;[[ 2  2  4]
             #          [ 5  5  7]
             #          [ 8  8 10]
             #          [11 11 13]]&quot;
</code></pre>
<p>对两个数组使用广播机制要遵守下列规则：</p>
<ol>
<li>如果数组的秩不同，使用1来将秩较小的数组进行扩展，直到两个数组的尺寸的长度都一样。</li>
<li>如果两个数组在某个维度上的长度是一样的，或者其中一个数组在该维度上长度为1，那么我们就说这两个数组在该维度上是<strong>相容</strong>的。</li>
<li>如果两个数组在所有维度上都是相容的，他们就能使用广播。</li>
<li>如果两个输入数组的尺寸不同，那么注意其中较大的那个尺寸。因为广播之后，两个数组的尺寸将和那个较大的尺寸一样。</li>
<li>在任何一个维度上，如果一个数组的长度为1，另一个数组长度大于1，那么在该维度上，就好像是对第一个数组进行了复制。</li>
</ol>
<p>支持广播机制的函数是全局函数。</p>
<p>下面是一些广播机制的使用：</p>
<pre><code class="lang-py">    import numpy as np

    # Compute outer product of vectors
    v = np.array([1,2,3])  # v has shape (3,)
    w = np.array([4,5])    # w has shape (2,)
    # To compute an outer product, we first reshape v to be a column
    # vector of shape (3, 1); we can then broadcast it against w to yield
    # an output of shape (3, 2), which is the outer product of v and w:
    # [[ 4  5]
    #  [ 8 10]
    #  [12 15]]
    print np.reshape(v, (3, 1)) * w

    # Add a vector to each row of a matrix
    x = np.array([[1,2,3], [4,5,6]])
    # x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),
    # giving the following matrix:
    # [[2 4 6]
    #  [5 7 9]]
    print x + v

    # Add a vector to each column of a matrix
    # x has shape (2, 3) and w has shape (2,).
    # If we transpose x then it has shape (3, 2) and can be broadcast
    # against w to yield a result of shape (3, 2); transposing this result
    # yields the final result of shape (2, 3) which is the matrix x with
    # the vector w added to each column. Gives the following matrix:
    # [[ 5  6  7]
    #  [ 9 10 11]]
    print (x.T + w).T

    # Another solution is to reshape w to be a row vector of shape (2, 1);
    # we can then broadcast it directly against x to produce the same
    # output.
    print x + np.reshape(w, (2, 1))

    # Multiply a matrix by a constant:
    # x has shape (2, 3). Numpy treats scalars as arrays of shape ();
    # these can be broadcast together to shape (2, 3), producing the
    # following array:
    # [[ 2  4  6]
    #  [ 8 10 12]]
    print x * 2
</code></pre>
<p>广播机制能够让你的代码更简洁更迅速，能够用的时候请尽量使用！  </p>
<h2 id="Numpy文档"><a href="#Numpy文档" class="headerlink" title="Numpy文档"></a>Numpy文档</h2><p>这篇教程涉及了你需要了解的numpy中的一些重要内容，但是numpy远不止如此。</p>
<p>Numpy提供了高性能的多维数组，以及计算和操作数组的基本工具。SciPy基于Numpy，提供了大量的计算和操作数组的函数，这些函数对于不同类型的科学和工程计算非常有用。</p>
<p>熟悉SciPy的最好方法就是阅读文档。我们会强调对于本课程有用的部分。</p>
<h2 id="图像操作"><a href="#图像操作" class="headerlink" title="图像操作"></a>图像操作</h2><p>SciPy提供了一些操作图像的基本函数。比如，它提供了将图像从硬盘读入到数组的函数，也提供了将数组中数据写入的硬盘成为图像的函数。下面是一个简单的例子：</p>
<pre><code class="lang-py">    from scipy.misc import imread, imsave, imresize

    # Read an JPEG image into a numpy array
    img = imread(&#39;assets/cat.jpg&#39;)
    print img.dtype, img.shape  # Prints &quot;uint8 (400, 248, 3)&quot;

    # We can tint the image by scaling each of the color channels
    # by a different scalar constant. The image has shape (400, 248, 3);
    # we multiply it by the array [1, 0.95, 0.9] of shape (3,);
    # numpy broadcasting means that this leaves the red channel unchanged,
    # and multiplies the green and blue channels by 0.95 and 0.9
    # respectively.
    img_tinted = img * [1, 0.95, 0.9]

    # Resize the tinted image to be 300 by 300 pixels.
    img_tinted = imresize(img_tinted, (300, 300))

    # Write the tinted image back to disk
    imsave(&#39;assets/cat_tinted.jpg&#39;, img_tinted)
</code></pre>
<img src="/2024/05/06/10-28-38/ff5f35bdb1a53c5e8dd5a16b391f63df_b.png" class>
<p>左边是原始图片，右边是变色和变形的图片。</p>
<h2 id="MATLAB文件"><a href="#MATLAB文件" class="headerlink" title="MATLAB文件"></a>MATLAB文件</h2><p>函数<strong>scipy.io.loadmat</strong>和<strong>scipy.io.savemat</strong>能够让你读和写MATLAB文件。</p>
<h2 id="点之间的距离"><a href="#点之间的距离" class="headerlink" title="点之间的距离"></a>点之间的距离</h2><p>SciPy定义了一些有用的函数，可以计算集合中点之间的距离。</p>
<p>函数<strong>scipy.spatial.distance.pdist</strong>能够计算集合中所有两点之间的距离：</p>
<pre><code class="lang-py">    import numpy as np
    from scipy.spatial.distance import pdist, squareform

    # Create the following array where each row is a point in 2D space:
    # [[0 1]
    #  [1 0]
    #  [2 0]]
    x = np.array([[0, 1], [1, 0], [2, 0]])
    print x

    # Compute the Euclidean distance between all rows of x.
    # d[i, j] is the Euclidean distance between x[i, :] and x[j, :],
    # and d is the following array:
    # [[ 0.          1.41421356  2.23606798]
    #  [ 1.41421356  0.          1.        ]
    #  [ 2.23606798  1.          0.        ]]
    d = squareform(pdist(x, &#39;euclidean&#39;))
    print d
</code></pre>
<p>函数<strong>scipy.spatial.distance.cdist</strong>可以计算不同集合中点的距离，## Matplotlib</p>
<p>Matplotlib是一个作图库。这里简要介绍<strong>matplotlib.pyplot</strong>模块，功能和MATLAB的作图功能类似。</p>
<h3 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h3><p>matplotlib库中最重要的函数是<strong>Plot</strong>。该函数允许你做出2D图形，如下：  </p>
<pre><code class="lang-py">    import numpy as np
    import matplotlib.pyplot as plt

    # Compute the x and y coordinates for points on a sine curve
    x = np.arange(0, 3 * np.pi, 0.1)
    y = np.sin(x)

    # Plot the points using matplotlib
    plt.plot(x, y)
    plt.show()  # You must call plt.show() to make graphics appear.
</code></pre>
<p>运行上面代码会产生下面的作图：  </p>
<img src="/2024/05/06/10-28-38/47169e8280088396107e0c62d7689e07_b.png" class>
<p>只需要少量工作，就可以一次画不同的线，加上标签，坐标轴标志等。  </p>
<pre><code class="lang-py">    import numpy as np
    import matplotlib.pyplot as plt

    # Compute the x and y coordinates for points on sine and cosine curves
    x = np.arange(0, 3 * np.pi, 0.1)
    y_sin = np.sin(x)
    y_cos = np.cos(x)

    # Plot the points using matplotlib
    plt.plot(x, y_sin)
    plt.plot(x, y_cos)
    plt.xlabel(&#39;x axis label&#39;)
    plt.ylabel(&#39;y axis label&#39;)
    plt.title(&#39;Sine and Cosine&#39;)
    plt.legend([&#39;Sine&#39;, &#39;Cosine&#39;])
    plt.show()
</code></pre>
<img src="/2024/05/06/10-28-38/955a7bcd45981728e91693961c21fbae_b.png" class>
<h2 id="绘制多个图像"><a href="#绘制多个图像" class="headerlink" title="绘制多个图像"></a>绘制多个图像</h2><p>可以使用<strong>subplot</strong>函数来在一幅图中画不同的东西：  </p>
<pre><code class="lang-py">    import numpy as np
    import matplotlib.pyplot as plt

    # Compute the x and y coordinates for points on sine and cosine curves
    x = np.arange(0, 3 * np.pi, 0.1)
    y_sin = np.sin(x)
    y_cos = np.cos(x)

    # Set up a subplot grid that has height 2 and width 1,
    # and set the first such subplot as active.
    plt.subplot(2, 1, 1)

    # Make the first plot
    plt.plot(x, y_sin)
    plt.title(&#39;Sine&#39;)

    # Set the second subplot as active, and make the second plot.
    plt.subplot(2, 1, 2)
    plt.plot(x, y_cos)
    plt.title(&#39;Cosine&#39;)

    # Show the figure.
    plt.show()
</code></pre>
<img src="/2024/05/06/10-28-38/c2abf551074a0db7445067f460417a08_b.png" class>
<h2 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h2><p>你可以使用<strong>imshow</strong>函数来显示图像，如下所示：  </p>
<pre><code class="lang-py">    import numpy as np
    from scipy.misc import imread, imresize
    import matplotlib.pyplot as plt

    img = imread(&#39;assets/cat.jpg&#39;)
    img_tinted = img * [1, 0.95, 0.9]

    # Show the original image
    plt.subplot(1, 2, 1)
    plt.imshow(img)

    # Show the tinted image
    plt.subplot(1, 2, 2)

    # A slight gotcha with imshow is that it might give strange results
    # if presented with data that is not uint8. To work around this, we
    # explicitly cast the image to uint8 before displaying it.
    plt.imshow(np.uint8(img_tinted))
    plt.show()
</code></pre>
<img src="/2024/05/06/10-28-38/81197033afe9b507dea565ed558a6239_b.png" class>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理, 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉概论 光反射成像，亮度，阴影</title>
    <url>/2024/05/03/12-11-15/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.03：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/27/08-42-03/" title="Ud810 Intro-to-cv 笔记汇总">计算机视觉概率-笔记汇总</a>
</li>
</ul>
<h2 id="图像的成像"><a href="#图像的成像" class="headerlink" title="图像的成像"></a>图像的成像</h2><h3 id="表面纹理"><a href="#表面纹理" class="headerlink" title="表面纹理"></a>表面纹理</h3><img src="/2024/05/03/12-11-15/QQ20240505-101936@2x.png" class>
<p>图像由：一小块表面法线，表面折射率，照明组成。</p>
<h4 id="辐射率"><a href="#辐射率" class="headerlink" title="辐射率"></a>辐射率</h4><img src="/2024/05/03/12-11-15/QQ20240505-102048@2x.png" class>
<p>辐射率：单位面积上吸收的能量<br>单位：瓦每平方米 $Wm^{-2}Sr^{-1}$</p>
<p>到达表面的能量：E<br>单位：$Wm^{-2}$</p>
<img src="/2024/05/03/12-11-15/QQ20240505-102428@2x.png" class>
<h4 id="双向反射分布函数-BRDF"><a href="#双向反射分布函数-BRDF" class="headerlink" title="双向反射分布函数 BRDF"></a>双向反射分布函数 BRDF</h4><p>入射，反射<br><img src="/2024/05/03/12-11-15/QQ20240505-102527@2x.png" class><br><img src="/2024/05/03/12-11-15/QQ20240505-102622@2x.png" class></p>
<p>BRDF公式：$f(\theta_i,\varphi_i;\theta_r,\varphi_r)=\frac{L^{surface}(\theta_r,\varphi_r)}{E^{surface}(\theta_i,\varphi_i)}$<br>反射与入射的比例</p>
<p>重要属性：<br>交换光源和照相机，比例不变：<br>$f(\theta_i,\varphi_i;\theta_r,\varphi_r)=f(\theta_r,\varphi_r;\theta_i,\varphi_i)$</p>
<p>旋转对称性：<br>$f(\theta_i,\varphi_i;\theta_r,\varphi_r)=f(\theta_i,\theta_r,\varphi_i-\varphi_r)$<br>表明公式与绝对角度无关，只有他们之间的相对角度有关。</p>
<h3 id="反射模型"><a href="#反射模型" class="headerlink" title="反射模型"></a>反射模型</h3><p>漫反射<br><img src="/2024/05/03/12-11-15/QQ20240505-103332@2x.png" class></p>
<p>镜面反射<br><img src="/2024/05/03/12-11-15/QQ20240505-103403@2x.png" class></p>
<p>反射光强度</p>
<img src="/2024/05/03/12-11-15/QQ20240505-103443@2x.png" class>
<h4 id="Lambertian-BRDF"><a href="#Lambertian-BRDF" class="headerlink" title="Lambertian BRDF"></a>Lambertian BRDF</h4><p>所有方向看起来一样亮</p>
<img src="/2024/05/03/12-11-15/QQ20240505-103722@2x.png" class>
<p>光的反射是按照角度衰减的。<br>为什么看到的光亮度相同？<br>因为垂直角度看到的面积小，但亮度高，接近平行的面积大，亮度低，总体看起来亮度相同。</p>
<img src="/2024/05/03/12-11-15/QQ20240505-104119@2x.png" class>
<p>光的反射率是一个常数<br>$albedo$:$f\left(\theta_i,\varphi_i;\theta_r,\varphi_r\right)=\rho_d$</p>
<p>表面辐射：$L=\rho_d I\cos\theta_i=\rho_d I ( \vec{n} \cdot\vec{s})$</p>
<p>镜面BRDF：<br><img src="/2024/05/03/12-11-15/QQ20240505-104452@2x.png" class><br>BRDF:$f(\theta_i,\phi_i;\theta_v,\phi_v)=\rho_s\delta(\theta_i-\theta_v)\delta(\phi_i+\pi-\phi_v)$<br>表面辐射：$L=I\rho_S\delta(\theta_i-\theta_v)\delta(\varphi_i+\pi-\varphi_v)$</p>
<p>另一种写法：$L=I\rho_s\delta(\vec{m}-\vec{v})\mathrm{~or~}I\rho_s\delta(\vec{n}-\vec{h})$<br>$\vec{h}$：半角</p>
<h4 id="光泽BRDF-Glossy"><a href="#光泽BRDF-Glossy" class="headerlink" title="光泽BRDF  Glossy"></a>光泽BRDF  Glossy</h4><img src="/2024/05/03/12-11-15/QQ20240505-104912@2x.png" class>
<p>表面辐射：$L=I\rho_s(\vec{m}\cdot\vec{v})^k$</p>
<h3 id="Phong-反射模型"><a href="#Phong-反射模型" class="headerlink" title="Phong 反射模型"></a>Phong 反射模型</h3><img src="/2024/05/03/12-11-15/QQ20240505-105128@2x.png" class>
<h2 id="亮度"><a href="#亮度" class="headerlink" title="亮度"></a>亮度</h2><h4 id="亮度假设"><a href="#亮度假设" class="headerlink" title="亮度假设"></a>亮度假设</h4><img src="/2024/05/03/12-11-15/QQ20240505-105533@2x.png" class>
<img src="/2024/05/03/12-11-15/QQ20240505-105617@2x.png" class>
<p>实际上两个方块的亮度是相同的，但大脑一直在补偿阴影。</p>
<img src="/2024/05/03/12-11-15/QQ20240505-105812@2x.png" class>
<img src="/2024/05/03/12-11-15/QQ20240505-105812@2x.png" class>
<p>亮度：等于反射率乘能量<br><img src="/2024/05/03/12-11-15/QQ20240505-110117@2x.png" class></p>
<h3 id="蒙德里安世界-The-Mondrian-world"><a href="#蒙德里安世界-The-Mondrian-world" class="headerlink" title="蒙德里安世界 The Mondrian world"></a>蒙德里安世界 The Mondrian world</h3><p>计算机的假设：</p>
<ol>
<li>光是缓慢变化的</li>
<li>反射率是常数</li>
<li>物体之间的反射率变化急剧<img src="/2024/05/03/12-11-15/QQ20240505-110352@2x.png" class>
</li>
</ol>
<p>强度恒定的斑块：<br><img src="/2024/05/03/12-11-15/QQ20240505-110431@2x.png" class></p>
<p>假设光照强度是低频的，缓慢变化。<br>边缘的反射率是恒定的。<br>图像相加？：<br><img src="/2024/05/03/12-11-15/QQ20240505-110637@2x.png" class></p>
<p>如何恢复反射率：<br><img src="/2024/05/03/12-11-15/QQ20240505-110754@2x.png" class></p>
<h4 id="视网膜色素-Land’s-Retinex-Theory"><a href="#视网膜色素-Land’s-Retinex-Theory" class="headerlink" title="视网膜色素 Land’s Retinex Theory"></a>视网膜色素 Land’s Retinex Theory</h4><p>展示了人类如何感知不同色素<br>目标：消除缓慢变化<br>$\log(L(x,y))~=~\log(R(x,y))~+~\log(E(x,y))$<br>高通滤波保留高波，去掉阀值。</p>
<p>一维亮度：<br><img src="/2024/05/03/12-11-15/QQ20240505-111110@2x.png" class><br>处理阀值，整合成反射率，但是多一个常数。<br><img src="/2024/05/03/12-11-15/QQ20240505-111242@2x.png" class></p>
<p>例子：彩色视网膜<br><img src="/2024/05/03/12-11-15/QQ20240505-111441@2x.png" class><br>重新校准强度</p>
<h4 id="颜色恒定率，强度恒定率"><a href="#颜色恒定率，强度恒定率" class="headerlink" title="颜色恒定率，强度恒定率"></a>颜色恒定率，强度恒定率</h4><img src="/2024/05/03/12-11-15/QQ20240505-111924@2x.png" class>
<p>颜色恒定率：某种颜色，在不用光照下，仍然可以看到颜色<br>强度恒定率：在不同环境，可以感知相同的强度。</p>
<h2 id="阴影"><a href="#阴影" class="headerlink" title="阴影"></a>阴影</h2><h3 id="阴影的形状"><a href="#阴影的形状" class="headerlink" title="阴影的形状"></a>阴影的形状</h3><p>阴影作为恢复形状的提示</p>
<h4 id="反射率图"><a href="#反射率图" class="headerlink" title="反射率图"></a>反射率图</h4><img src="/2024/05/03/12-11-15/QQ20240506-093432@2x.png" class>
<img src="/2024/05/03/12-11-15/QQ20240506-093505@2x.png" class>
<p>曲线法向量：$\mathbf{n}=\frac N{\left\|N\right\|}=\frac{t_x\times t_y}{\left\|t_x\times t_y\right\|}=\frac1{\sqrt{p^2+q^2+1}}\left(p,q,1\right)^T$</p>
<h4 id="高斯球和梯度空间投影"><a href="#高斯球和梯度空间投影" class="headerlink" title="高斯球和梯度空间投影"></a>高斯球和梯度空间投影</h4><p>存在从所有法线到高斯球的映射：<br><img src="/2024/05/03/12-11-15/QQ20240506-093838@2x.png" class><br><img src="/2024/05/03/12-11-15/QQ20240506-093909@2x.png" class><br><img src="/2024/05/03/12-11-15/QQ20240506-093921@2x.png" class></p>
<h4 id="源向量梯度空间和法向量梯度空间"><a href="#源向量梯度空间和法向量梯度空间" class="headerlink" title="源向量梯度空间和法向量梯度空间"></a>源向量梯度空间和法向量梯度空间</h4><img src="/2024/05/03/12-11-15/QQ20240506-094225@2x.png" class>
<p>单位法向量：$\mathbf{n=\frac N{|N|}=\frac{(p,q,1)}{\sqrt{p^2+q^2+1}}}$</p>
<p>单位源向量：$\mathbf{s}=\frac{\mathbf{S}}{|\mathbf{S}|}=\frac{(p_S,q_S,1)}{\sqrt{p_S^2+q_S^2+1}}$</p>
<p>向量夹角：$\cos\theta_i=\mathbf{n}\cdot\mathbf{s}=\frac{(pp_S+qq_S+1)}{\sqrt{p^2+q^2+1}\sqrt{p_S^2+q_S^2+1}}$</p>
<h3 id="阴影形状的定义"><a href="#阴影形状的定义" class="headerlink" title="阴影形状的定义"></a>阴影形状的定义</h3><img src="/2024/05/03/12-11-15/QQ20240506-094504@2x.png" class>
<h4 id="朗伯案例"><a href="#朗伯案例" class="headerlink" title="朗伯案例"></a>朗伯案例</h4><img src="/2024/05/03/12-11-15/QQ20240506-094718@2x.png" class>
<p>反射图：<br><img src="/2024/05/03/12-11-15/QQ20240506-094852@2x.png" class></p>
<p>Iso 亮度轮廓：<br><img src="/2024/05/03/12-11-15/QQ20240506-094911@2x.png" class></p>
<h3 id="光度立体"><a href="#光度立体" class="headerlink" title="光度立体"></a>光度立体</h3><p>相同的物体，不同的光照<br><img src="/2024/05/03/12-11-15/QQ20240506-095328@2x.png" class></p>
<img src="/2024/05/03/12-11-15/QQ20240506-095430@2x.png" class>
<img src="/2024/05/03/12-11-15/QQ20240506-095508@2x.png" class>
<img src="/2024/05/03/12-11-15/QQ20240506-095523@2x.png" class>
<p>最终pq空间图：<br><img src="/2024/05/03/12-11-15/QQ20240506-095554@2x.png" class></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉概论 图像特征</title>
    <url>/2024/05/02/13-20-59/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.05.02：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/27/08-42-03/" title="Ud810 Intro-to-cv 笔记汇总">计算机视觉概率-笔记汇总</a>
</li>
</ul>
<h2 id="特征检测"><a href="#特征检测" class="headerlink" title="特征检测"></a>特征检测</h2><p>一种寻找对应关系的方法</p>
<h4 id="图像点匹配问题"><a href="#图像点匹配问题" class="headerlink" title="图像点匹配问题"></a>图像点匹配问题</h4><p>Local Features：局部特征<br>目标：在其他图像中找到点的精确位置</p>
<p>特征匹配的过程：</p>
<ol>
<li>检测一些兴趣点(特征)</li>
<li>匹配两张图中的特征点</li>
<li>使用对应点，对齐图像</li>
</ol>
<img src="/2024/05/02/13-20-59/QQ20240502-132909@2x.png" class>
<p>特征匹配的问题：</p>
<ol>
<li>独立的特征， 需要重复检测器</li>
<li>重复相似匹配，使用特征描述符descriptor</li>
</ol>
<h4 id="好的特征"><a href="#好的特征" class="headerlink" title="好的特征"></a>好的特征</h4><p>可重复性：Repeatability/Precision<br>好的特征应该在同一场景的不同图片中被检测出来</p>
<p>显著性/可匹配性：Saliency/Matchability<br>有不同的区分描述</p>
<p>紧凑型和效率：Compactness and efficiency<br>比图像像素更少的特征</p>
<p>局部性：Locality<br>描述具有好的局部性,可以解决遮挡</p>
<h2 id="寻找角点"><a href="#寻找角点" class="headerlink" title="寻找角点"></a>寻找角点</h2><p>一个白墙上有一个黑色方块：<br>黑块不是一个好特征，无法定位。<br>边缘也不是好特征，角是一个好特征。</p>
<img src="/2024/05/02/13-20-59/QQ20240502-142106@2x.png" class>
<p>角是一个梯度变换的地方。</p>
<h4 id="哈里斯角-Harris-Corners"><a href="#哈里斯角-Harris-Corners" class="headerlink" title="哈里斯角 Harris Corners:"></a>哈里斯角 Harris Corners:</h4><p>$E\left(u,\nu\right)=\sum_{x,y}w\left(x,y\right)\left[I\left(x+u,y+\nu\right)-I\left(x,y\right)\right]^{2}$</p>
<ul>
<li>$w\left(x,y\right)$ 是一个窗口<img src="/2024/05/02/13-20-59/QQ20240502-142537@2x.png" class>
</li>
</ul>
<p>哈里斯角图：</p>
<img src="/2024/05/02/13-20-59/QQ20240502-142639@2x.png" class>
<ul>
<li>误差为0，是黑色</li>
<li>左图中红色方块是角图中心，误差为0，绿色移动后，出现误差</li>
</ul>
<p>哈里斯角在很小的移动会产生误差：<br>二阶泰勒展开：<br>$F\left(\delta x\right)\approx F\left(0\right)+\delta x\cdot\frac{dF\left(0\right)}{dx}+\frac12\delta x^2\cdot\frac{d^2F\left(0\right)}{dx^2}$</p>
<p>二维泰勒展开：<br><img src="/2024/05/02/13-20-59/QQ20240502-143009@2x.png" class></p>
<p>推导过程：<br>$E_u\left(u,\nu\right)=\sum_{x,y}2w\left(x,y\right)\left[I\left(x+u,y+\nu\right)-I\left(x,y\right)\right]I_x\left(x+u,y+v\right)$</p>
<p>$\begin{split}<br>E_{\textit{ u }\nu}\left(u,\nu\right)&amp; =\sum_{x,y}2w(x,y)I_y(x+u,y+v)I_x(x+u,y+v) \\<br>&amp;+\sum_{x,y}2w(x,y)\Big[I(x+u,y+v)-I(x,y)\Big]I_{xy}(x+u,y+v)<br>\end{split}$</p>
<p>$\begin{split}E_{u\nu}(u,\nu)&amp;=\sum_{x,y}2w(x,y)I_y(x+u,y+v)I_x(x+u,y+v)\\&amp;+\sum_{x,y}2w(x,y)\Big[I(x+u,y+v)-I(x,y)\Big]I_{xy}(x+u,y+v)\end{split}$</p>
<p>带入点$(0,0)$，消去得：</p>
<p>$E\left(u ,\nu\right) \approx \begin{bmatrix}u&amp;\nu\end{bmatrix} M\quad\begin{bmatrix}u\\\nu\end{bmatrix}$</p>
<p>$M=\sum_{x,y}w(x,y)\begin{bmatrix}I_x^2&amp;&amp;I_xI_y\\I_xI_y&amp;&amp;I_y^2\end{bmatrix}$</p>
<p>当没有权重$w(x,y)$时：</p>
<p>$M=\begin{bmatrix}\sum I_xI_x&amp;\sum I_xI_y\\\sum I_xI_y&amp;\sum I_yI_y\end{bmatrix}=\sum\left(\begin{bmatrix}I_x\\I_y\end{bmatrix}-\begin{bmatrix}I_x&amp;I_y\end{bmatrix}\right)=\sum\nabla I\left(\nabla I\right)^T$</p>
<p>其中$\nabla I$：是一个秩为1的列向量，所有等式右边是一个秩为1的二阶方阵。</p>
<p>? 必须让窗口内所有方向都有梯度，才可以求和后得到一个满秩矩阵？？</p>
<h4 id="解释二阶近似方程"><a href="#解释二阶近似方程" class="headerlink" title="解释二阶近似方程"></a>解释二阶近似方程</h4><p>代数表达式：椭圆方程<br>$\sum I_x^2u^2+2\sum I_xI_yu\nu+\sum I_y^2\nu^2=k$</p>
<p>椭圆图：<br><img src="/2024/05/02/13-20-59/QQ20240502-145419@2x.png" class></p>
<p>考虑，窗口内的梯度永远是水平或垂直，所有$I_xI_y$为0。<br>化简：<br>$M=\sum_{x,y}w(x,y)\begin{bmatrix}I_x^2&amp;&amp;I_xI_y\\I_xI_y&amp;&amp;I_y^2\end{bmatrix}=\begin{bmatrix}\lambda&amp;0\\0&amp;\lambda_2\end{bmatrix}$</p>
<p>$M$可以被相似对角化：<br>$M~=~R^{-1}\left[\begin{array}{cc}\lambda&amp;0\\0&amp;\lambda_2\end{array}\right]R$</p>
<p>特征值的作用：<br><img src="/2024/05/02/13-20-59/QQ20240502-145313@2x.png" class><br>在短边处：变换很快，长边处变换速度很慢，需要移动很多才有变化。</p>
<h4 id="解释特征值"><a href="#解释特征值" class="headerlink" title="解释特征值"></a>解释特征值</h4><p>计算哈里斯矩阵：<br><img src="/2024/05/02/13-20-59/QQ20240502-150536@2x.png" class></p>
<ul>
<li>平坦区域：flat<br>梯度为0，怎么移动都不会变化。</li>
<li>如果一个特征值特别大<br>朝这个方向的边缘移动，变换很快</li>
<li>两个特征值大小相似<br>角：在任何方向变化相同</li>
</ul>
<h4 id="哈里斯响应函数："><a href="#哈里斯响应函数：" class="headerlink" title="哈里斯响应函数："></a>哈里斯响应函数：</h4><p>$R=\det(M^2)-\alpha\mathrm{~trace}(M)^2=\lambda_1\lambda_2-\alpha(\lambda_1+\lambda_2)^2$<br>$\alpha$:特别小 0.04 to 0.06</p>
<p>$R$取决于特征值，但不需要计算特征值，<br>原因：</p>
<ul>
<li>角：两个特征值都很大，$R$正的大数</li>
<li>边：一个特征值很大,$R$是负的大数</li>
<li>平坦：$R$ 的绝对值很小</li>
</ul>
<h4 id="哈里斯函数如何工作"><a href="#哈里斯函数如何工作" class="headerlink" title="哈里斯函数如何工作"></a>哈里斯函数如何工作</h4><p>低纹理区域：<br><img src="/2024/05/02/13-20-59/QQ20240502-151651@2x.png" class></p>
<p>边缘区域：<br><img src="/2024/05/02/13-20-59/QQ20240502-151730@2x.png" class></p>
<p>高纹理区域：<br><img src="/2024/05/02/13-20-59/QQ20240502-151825@2x.png" class></p>
<h4 id="哈里斯检测器算法："><a href="#哈里斯检测器算法：" class="headerlink" title="哈里斯检测器算法："></a>哈里斯检测器算法：</h4><img src="/2024/05/02/13-20-59/QQ20240502-152016@2x.png" class>
<p>例子：<br><img src="/2024/05/02/13-20-59/QQ20240502-152156@2x.png" class></p>
<p>响应函数图：<br><img src="/2024/05/02/13-20-59/QQ20240502-152223@2x.png" class></p>
<p>非极大值抑制：<br><img src="/2024/05/02/13-20-59/QQ20240502-152309@2x.png" class></p>
<p>局部最大值的像素点：<br><img src="/2024/05/02/13-20-59/QQ20240502-152430@2x.png" class></p>
<p>在原图中的位置：<br><img src="/2024/05/02/13-20-59/QQ20240502-152454@2x.png" class></p>
<p>最终发现很多相同的特征点。</p>
<h2 id="尺度不变性-Scale-invariance"><a href="#尺度不变性-Scale-invariance" class="headerlink" title="尺度不变性  Scale invariance"></a>尺度不变性  Scale invariance</h2><h4 id="哈里斯检测器的一些特性"><a href="#哈里斯检测器的一些特性" class="headerlink" title="哈里斯检测器的一些特性"></a>哈里斯检测器的一些特性</h4><ul>
<li>对于旋转不变</li>
<li>图像强度：加法乘法不改变<br>加法：导数不变<br>乘法：导数整体变化</li>
<li>缩放，会改变</li>
</ul>
<h4 id="尺度不变性的检测"><a href="#尺度不变性的检测" class="headerlink" title="尺度不变性的检测"></a>尺度不变性的检测</h4><img src="/2024/05/02/13-20-59/QQ20240502-153839@2x.png" class>
<p>增大窗口，保持尺度不变</p>
<p>领域大小对函数的影响：<br><img src="/2024/05/02/13-20-59/QQ20240502-154050@2x.png" class><br>$S_1$, $S_2$，具有相同的缩放比例</p>
<h4 id="一个好的尺寸检测函数"><a href="#一个好的尺寸检测函数" class="headerlink" title="一个好的尺寸检测函数"></a>一个好的尺寸检测函数</h4><img src="/2024/05/02/13-20-59/QQ20240502-154434@2x.png" class>
<img src="/2024/05/02/13-20-59/QQ20240502-154635@2x.png" class>
<p>拉普拉斯差和高斯差几乎相同，选择高斯简化计算。</p>
<h4 id="关键点定位"><a href="#关键点定位" class="headerlink" title="关键点定位"></a>关键点定位</h4><p><strong>SIFT</strong>: Scale Invariant Feature Transform<br>尺度不变特征变换<br><img src="/2024/05/02/13-20-59/QQ20240502-154917@2x.png" class><br>取不同尺寸的图像，不断模糊图像，两两相减，计算他们的高斯图像差异。<br>像素与周围九个像素比较，以及不同比例邻居上的比较，的看是不是极值。</p>
<img src="/2024/05/02/13-20-59/QQ20240502-155145@2x.png" class>
<p>例子：<br><img src="/2024/05/02/13-20-59/QQ20240502-155601@2x.png" class></p>
<p><strong>哈里斯-拉普拉斯算法：</strong><br>先试用哈里斯角点检测<br>然后在看尺度方向上的拉普拉斯算子，找空间极值</p>
<h2 id="特征描述"><a href="#特征描述" class="headerlink" title="特征描述"></a>特征描述</h2><p>描述符：对领域进行描述</p>
<ul>
<li>独特：不同的点有不同的描述符</li>
<li>可区分</li>
<li>几乎相同</li>
</ul>
<h3 id="SIFT-Scale-Invariant-Feature-Detection"><a href="#SIFT-Scale-Invariant-Feature-Detection" class="headerlink" title="SIFT: Scale Invariant Feature Detection"></a>SIFT: Scale Invariant Feature Detection</h3><p>尺度不变特征变换</p>
<p>思想：</p>
<ul>
<li>图像内容是一组特征：对于平移，旋转,放缩等图像处理操作是不变的</li>
<li>描述符是稳健的</li>
</ul>
<img src="/2024/05/02/13-20-59/QQ20240503-094607@2x.png" class>
<h4 id="总体SIFT识别过程"><a href="#总体SIFT识别过程" class="headerlink" title="总体SIFT识别过程"></a>总体SIFT识别过程</h4><ol>
<li>确定关键点尺寸</li>
<li>定位关键点</li>
<li>找到领域的局部的方向</li>
<li>关键点描述</li>
</ol>
<img src="/2024/05/02/13-20-59/QQ20240503-094908@2x.png" class>
<p>右图中的方向箭头，就是局部方向。</p>
<h4 id="计算局部方向"><a href="#计算局部方向" class="headerlink" title="计算局部方向"></a>计算局部方向</h4><img src="/2024/05/02/13-20-59/QQ20240503-095051@2x.png" class>
<p>图中是方向直方图</p>
<p>找到一个峰值，就是主导方向，我们需要使用的方向，以这个方向为北方。</p>
<h4 id="关键点描述符"><a href="#关键点描述符" class="headerlink" title="关键点描述符"></a>关键点描述符</h4><p>标准化：<br>旋转到新的北方朝上<br>放缩到相同尺寸</p>
<p>SIFT特征向量：<br><img src="/2024/05/02/13-20-59/QQ20240503-095512@2x.png" class><br>右图是一个：2X2的图，最好是使用4X4的直方图，一个里面有8个方向，叠加到一起，一个特征会有128个向量<br>将所有的向量，标准化到内积为1。</p>
<p>梯度：直方图加权梯度，方向最大的权最高</p>
<p>评估SIFT描述符:<br>通常一个直方图有8个方向，采用4X4的直方图，向量长128。</p>
<h3 id="匹配特征点"><a href="#匹配特征点" class="headerlink" title="匹配特征点"></a>匹配特征点</h3><p>如何在两个图的特征中，找到匹配的特征点？</p>
<h4 id="最邻近算法匹配"><a href="#最邻近算法匹配" class="headerlink" title="最邻近算法匹配"></a>最邻近算法匹配</h4><p>best-bin-first 算法基于k-d树</p>
<img src="/2024/05/02/13-20-59/QQ20240503-101702@2x.png" class>
<h4 id="基于小波的散列"><a href="#基于小波的散列" class="headerlink" title="基于小波的散列"></a>基于小波的散列</h4><img src="/2024/05/02/13-20-59/QQ20240503-101843@2x.png" class>
<p>小波：是一种接近滤波器的形式，从三个滤波器里输出三个数。</p>
<p>思想类似局部敏感哈希：<br><img src="/2024/05/02/13-20-59/QQ20240503-102231@2x.png" class><br>考虑了空间两点的距离</p>
<h4 id="例子-3D物体识别"><a href="#例子-3D物体识别" class="headerlink" title="例子 3D物体识别"></a>例子 3D物体识别</h4><img src="/2024/05/02/13-20-59/QQ20240503-102435@2x.png" class>
<p>从测试图中匹配：<br><img src="/2024/05/02/13-20-59/QQ20240503-102518@2x.png" class></p>
<h4 id="例子-遮挡下的识别"><a href="#例子-遮挡下的识别" class="headerlink" title="例子 遮挡下的识别"></a>例子 遮挡下的识别</h4><p>通过部分关键点，预测其他点去哪里了。</p>
<h2 id="鲁棒误差函数"><a href="#鲁棒误差函数" class="headerlink" title="鲁棒误差函数"></a>鲁棒误差函数</h2><p>SSD：差的平方和<br>NN: 最近邻居</p>
<h4 id="Lowe-a-better-way"><a href="#Lowe-a-better-way" class="headerlink" title="Lowe a better way"></a>Lowe a better way</h4><img src="/2024/05/02/13-20-59/QQ20240503-104558@2x.png" class>
<p>意义：<br>如果最佳匹配被遮挡了，第一和第二匹配相差不大，说明没有找到正确的匹配。</p>
<h3 id="模型拟合"><a href="#模型拟合" class="headerlink" title="模型拟合"></a>模型拟合</h3><h4 id="最小二乘线拟合-Typical-least-squares-line-fitting"><a href="#最小二乘线拟合-Typical-least-squares-line-fitting" class="headerlink" title="最小二乘线拟合 Typical least squares line fitting"></a>最小二乘线拟合 Typical least squares line fitting</h4><p>机器学习内容<br>只是$y$方向上的拟合。</p>
<h4 id="总体最小二乘法"><a href="#总体最小二乘法" class="headerlink" title="总体最小二乘法"></a>总体最小二乘法</h4><p>$ax+by=d$<br>$E=\sum_{i=1}^n(ax_i+by_i-d)^2$<br><img src="/2024/05/02/13-20-59/QQ20240503-105511@2x.png" class></p>
<p>推导公式：<br>\begin{split}<br>\frac{\partial E}{\partial d}=\sum_{i=1}^n-2\left(ax_i+by_i-d\right)=0\\<br>\Rightarrow d=\frac an\sum_{i=1}^nx_i+\frac bn\sum_{i=1}^nx_i=a\overline{x}+b\overline{y}<br>\end{split}</p>
<p>\begin{split}<br>&amp;E = \sum_{i=1}^{n}(a (x_{i}-\overline{x})+b (y_{i}-<br>\overline{y}))^{2}\\<br>&amp;=\left\|\begin{bmatrix}x_{i}-\overline{x}&amp;y_{i}-<br>\overline{y}\\<br>\vdots&amp;\vdots&amp;\vdots\\<br>\lfloor x_{i}-\overline{x}&amp;y_{i}-\overline{y}\rfloor<br>\end{bmatrix}\begin{bmatrix}a\\<br>b\end{bmatrix}\right\|^{2} \\<br>&amp;=\left(U \mathbf{h}\right)^{T}\left(U \mathbf{h}\right)\\<br>&amp;\frac{dE}{d\mathbf{h}}=2(U^TU)\mathbf{h}=0<br>\end{split}</p>
<img src="/2024/05/02/13-20-59/QQ20240503-110151@2x.png" class>
<ul>
<li>实际测量点$(x,y)$ 等于真实点$(u,v)$垂直于$a,b$方向上的偏动。</li>
<li>符合高斯噪声</li>
</ul>
<h3 id="鲁棒估计器"><a href="#鲁棒估计器" class="headerlink" title="鲁棒估计器"></a>鲁棒估计器</h3><p>求一个最小化：$\sum_i\rho\left(r_i\left(x_i,\theta_1\right);<br>\sigma_1\right)$</p>
<ul>
<li>$r_i\left(x_i,\theta_1\right)$：代表残差值,该点距离拟合曲线的距离</li>
<li>$\rho$：有尺度参数$\sigma$鲁棒函数</li>
</ul>
<p>一种鲁棒函数：<br><img src="/2024/05/02/13-20-59/QQ20240503-110931@2x.png" class></p>
<ul>
<li>当$u$很小的时候：</li>
<li>当$u$很大的时候：误差很大</li>
</ul>
<h2 id="RANSAC算法-RANdom-SAmple-Consensus"><a href="#RANSAC算法-RANdom-SAmple-Consensus" class="headerlink" title="RANSAC算法 RANdom SAmple Consensus"></a>RANSAC算法 RANdom SAmple Consensus</h2><h3 id="一般模型"><a href="#一般模型" class="headerlink" title="一般模型"></a>一般模型</h3><p>选取模型的最小集点数量：<br>距离阀值：<br>$f(d)=\frac{\sqrt{2}e^{-(\frac{d^2}{2\sigma^2})} }<br>{\sqrt{\pi}\sigma},d\geq0$</p>
<h4 id="计算N"><a href="#计算N" class="headerlink" title="计算N"></a>计算N</h4><img src="/2024/05/02/13-20-59/QQ20240503-112533@2x.png" class>
<p>模型所需的样本点数：<br>$N &gt; \log(1 - p ) / \log(1 - (1 - e )^S )$</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>两个匹配的特征：<br><img src="/2024/05/02/13-20-59/QQ20240503-113228@2x.png" class></p>
<ul>
<li>有5个特征匹配，2个不匹配，认为这两张图是相同的，错误</li>
<li>因为匹配特征中有噪音</li>
</ul>
<p>RANSAC算法<br><img src="/2024/05/02/13-20-59/QQ20240503-113151@2x.png" class></p>
<ul>
<li>去除高斯噪音，选择平均值</li>
</ul>
<h4 id="RANSAC算法-循环"><a href="#RANSAC算法-循环" class="headerlink" title="RANSAC算法 循环"></a>RANSAC算法 循环</h4><img src="/2024/05/02/13-20-59/QQ20240503-113501@2x.png" class>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉概论 相机和图像</title>
    <url>/2024/04/29/16-24-21/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.29：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/27/08-42-03/" title="Ud810 Intro-to-cv 笔记汇总">计算机视觉概率-笔记汇总</a>
</li>
</ul>
<h2 id="透视成像"><a href="#透视成像" class="headerlink" title="透视成像"></a>透视成像</h2><h4 id="建模投影"><a href="#建模投影" class="headerlink" title="建模投影"></a>建模投影</h4><p>坐标系：</p>
<img src="/2024/04/29/16-24-21/QQ20240501-091850@2x.png" class>
<ul>
<li>Center OF Projection:光学中心，在原点</li>
<li>不用担心翻转，图像放在坐标系前</li>
<li>坐标系符合右手定则，$z$轴指向相机，而不是指向世界</li>
</ul>
<p>坐标转换：<br>$(X,Y,Z)\to(-d\frac XZ,-d\frac YZ,-d)$</p>
<h4 id="齐次坐标-Homogeneous-coordinates"><a href="#齐次坐标-Homogeneous-coordinates" class="headerlink" title="齐次坐标 Homogeneous coordinates"></a>齐次坐标 Homogeneous coordinates</h4><p>二维：<br>$(x,y)\Rightarrow\left[\begin{array}{c}x\\y\\\mathbf{1}\end{array}\right]$</p>
<p>三维：<br>$(x,y,z)\Rightarrow\left[\begin{array}{c}x\\y\\z\\\mathbf{1}\end{array}\right]$</p>
<p>齐次转常规：<br>$\left[\begin{array}{c}x\\y\\z\\w\end{array}\right]\Rightarrow\left(x/w,y/w,z/w\right)$</p>
<p>例子：</p>
<p>\begin{split}<br>\begin{bmatrix}1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\0&amp;0&amp;1/f&amp;0\end{bmatrix}\begin{bmatrix}x\\|&amp;|\\y\\z\\1\end{bmatrix}&amp;=\begin{bmatrix}x\\y\\z/f\end{bmatrix}&amp;\Rightarrow\left(f\frac xz,f\frac yz\right)\\&amp;\Rightarrow\left(u,\nu\right)<br>\end{split}</p>
<p>通过焦距为$f$的投影，投影到世界上某个点$(x,y,z)$的图像中的坐标。<br>焦距：从中心投影到图像的距离</p>
<h4 id="投影的几何性质"><a href="#投影的几何性质" class="headerlink" title="投影的几何性质"></a>投影的几何性质</h4><img src="/2024/04/29/16-24-21/QQ20240501-093454@2x.png" class>
<ul>
<li>$O$，是投影中心</li>
<li>点，线，投影到点，线</li>
<li>多边形投影会改变</li>
</ul>
<p>平行线会改变：<br><strong>数学解释：</strong><br>三维平行线：<br>$x\left(t\right)=x_0+at\\y\left(t\right)=y_0+bt\\z\left(t\right)=z_0+ct$</p>
<p>投影坐标的投影方程：<br>$x’(t)=\frac{fx}z=\frac{f\left(x_0+at\right)}{z_0+ct}\\y’(t)=\frac{fy}z=\frac{f\left(y_0+bt\right)}{z_0+ct}$</p>
<p>$\begin{split}&amp;\text{In the limit as }t\to\pm\infty\quad x’(t)\to\frac{fa}c,\quad y’(t)\to\frac{fb}c\\&amp;\text{we have (for }c\neq0){:}\end{split}$</p>
<ul>
<li>公式中没有$x_0,y_0,z_0$，直线的起点不重要，直线沿着直线一直向前</li>
<li>不同的直线会汇聚到同一个点，(点不一定相同，取决于视角)</li>
<li>$c$不能为0，如果c为0，意味着平行线与$z$轴垂直，与投影面平行，永远不会相交</li>
</ul>
<p>平行线会在无限远的地方汇聚消失：消失点</p>
<p>三点透视</p>
<img src="/2024/04/29/16-24-21/QQ20240501-094912@2x.png" class>
<p>人类视觉：Müller-LyerIllusion</p>
<img src="/2024/04/29/16-24-21/QQ20240501-095137@2x.png" class>
<p>红线哪个更长，透视错觉</p>
<h4 id="其他投影模型"><a href="#其他投影模型" class="headerlink" title="其他投影模型"></a>其他投影模型</h4><p>正交投影： Orthographic<br>特殊的平行投影，假设光源无限远，光是平行的。<br><img src="/2024/04/29/16-24-21/QQ20240501-095431@2x.png" class><br>投影矩阵：<br>$\left[\begin{array}{cccc}1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\0&amp;0&amp;0&amp;1\end{array}\right]\left[\begin{array}{c}x\\y\\z\\1\end{array}\right]\boldsymbol{=}\left[\begin{array}{c}x\\y\\1\end{array}\right]\boldsymbol{\Rightarrow}(x,y)$</p>
<p>弱视角投影： Weak perspective<br>缩放投影<br>$\begin{bmatrix}1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\\\0&amp;0&amp;0&amp;1/s\end{bmatrix}\begin{bmatrix}x\\y\\z\\1\end{bmatrix}=\begin{bmatrix}x\\y\\1/s\end{bmatrix}\Rightarrow(sx,sy)$</p>
<ul>
<li>$s$:缩放因子</li>
</ul>
<h2 id="立体几何"><a href="#立体几何" class="headerlink" title="立体几何"></a>立体几何</h2><h4 id="多视角图像"><a href="#多视角图像" class="headerlink" title="多视角图像"></a>多视角图像</h4><img src="/2024/04/29/16-24-21/QQ20240501-100044@2x.png" class>
<p>结构和深度是模糊的，所以出现了多视角，本质来自投影</p>
<p>人类视觉具有感知深度的能力,主要元素：</p>
<ol>
<li>阴影 shading</li>
<li>纹理 texture</li>
<li>焦点/深度焦点 focus/defocus</li>
<li>移动 motion</li>
</ol>
<h4 id="立体视觉"><a href="#立体视觉" class="headerlink" title="立体视觉"></a>立体视觉</h4><p>两个眼睛看的图像不同，从两个视觉中恢复立体</p>
<p>双视觉图像，中心元素，前后的元素都在像它移动</p>
<p>随机点立体图：</p>
<p>人类可以直接融合双视角图像</p>
<img src="/2024/04/29/16-24-21/QQ20240501-101547@2x.png" class>
<p>相机是由光学中心(optical center)定义的</p>
<ul>
<li>校准，相机姿态</li>
<li>图像对应点</li>
</ul>
<h4 id="简单的立体系统"><a href="#简单的立体系统" class="headerlink" title="简单的立体系统"></a>简单的立体系统</h4><p>相机的俯视图：</p>
<img src="/2024/04/29/16-24-21/QQ20240501-102205@2x.png" class>
<p>计算深度的公式：<br>$\begin{split}\frac{B-x_l+x_r}{Z-f}&amp;=\frac BZ\\\\Z = f \frac B{x_l - x_r}\end{split}$</p>
<p>视角差为0，深度为无穷，月亮为什么一直会跟着，因为深度无限远，视角没有变化。</p>
<p>视角差的例子：</p>
<img src="/2024/04/29/16-24-21/QQ20240501-102755@2x.png" class>
<p>找出两张图，哪个在左边，哪个在右边，从上面的烟囱可以看出，右边图在右方拍摄。<br>红点是一个图像相同位置的点，右图向左移动一直，那个点才能到左图中的窗口点，这就是视角差。</p>
<p>从视差到深度：<br><img src="/2024/04/29/16-24-21/QQ20240501-103010@2x.png" class></p>
<p>通过视差图像，亮的地方视差大，暗的地方视差小。<br>根据视差与深度的反比关系，我们可以得到深度。</p>
<p>从深度到视差：<br>因为$y没有变化，视差来源于$x$，所以$D(x,y)$也是x的变化。</p>
<p>$(x’,y’)=(x+D(x,y), y)$</p>
<h2 id="对极几何"><a href="#对极几何" class="headerlink" title="对极几何"></a>对极几何</h2><h4 id="立体对应约束"><a href="#立体对应约束" class="headerlink" title="立体对应约束"></a>立体对应约束</h4><img src="/2024/04/29/16-24-21/QQ20240501-122809@2x.png" class>
<p>左侧的图像点p，可以点在一条直线上的任意一点，所以它对应在另一个图像中的点，是一条直线上任何一点。这条线成为极线。<br>这就叫做：极线约束 Epipolar constraint</p>
<ul>
<li>基线：相机点和位置点形成的</li>
</ul>
<p>为什么极线约束有用？<br>在极线中找对应另一个图像中的点。</p>
<p>例子：<br><img src="/2024/04/29/16-24-21/QQ20240501-123218@2x.png" class><br>极点是一个数学概念，所有的极线会在屏幕外面聚合。</p>
<p>平行图像的例子：<br><img src="/2024/04/29/16-24-21/QQ20240501-123433@2x.png" class></p>
<h2 id="立体对应-Stereo-correspondence"><a href="#立体对应-Stereo-correspondence" class="headerlink" title="立体对应 Stereo correspondence"></a>立体对应 Stereo correspondence</h2><p>软约束：对应点之间的关系</p>
<ol>
<li>相似性</li>
<li>唯一性</li>
<li>有序</li>
<li>视差梯度有限-深度变化不会太大</li>
</ol>
<h4 id="相似性"><a href="#相似性" class="headerlink" title="相似性"></a>相似性</h4><img src="/2024/04/29/16-24-21/QQ20240501-135443@2x.png" class>
<p>强度分布图：匹配位置具有差异</p>
<img src="/2024/04/29/16-24-21/QQ20240501-135637@2x.png" class>
<p>视差图的相似性:匹配的地方峰值最高</p>
<img src="/2024/04/29/16-24-21/QQ20240501-135743@2x.png" class>
<p>没有纹理的地方做匹配，没有匹配的地方？<br>原因：窗口太小，没有获得足够的纹理信息。</p>
<p>窗口大小的影响：<br><img src="/2024/04/29/16-24-21/QQ20240501-135937@2x.png" class><br>显示的是视差图</p>
<ul>
<li>小窗口，获得了树干信息</li>
<li>大窗口，树干和背景融合到了一起</li>
</ul>
<h4 id="有序性"><a href="#有序性" class="headerlink" title="有序性"></a>有序性</h4><p>单一固体中：<br>左图中的点 $a,b,c$，在右图中有相同的顺序$a,b,c$。</p>
<p>例外：在一个透明的表面<br><img src="/2024/04/29/16-24-21/QQ20240501-140403@2x.png" class><br>顺序会改变</p>
<p>例外：狭窄的遮挡<br><img src="/2024/04/29/16-24-21/QQ20240501-140508@2x.png" class></p>
<p>立体的最新技术:<br>优化算法的两种方法：</p>
<ol>
<li>一次扫描一条线</li>
<li>在二维图像中，多扫描一条垂直的线</li>
</ol>
<h4 id="动态规划公式"><a href="#动态规划公式" class="headerlink" title="动态规划公式"></a>动态规划公式</h4><img src="/2024/04/29/16-24-21/QQ20240501-141156@2x.png" class>
<p>假设已知左上角像素在右图中对应的位置。</p>
<ul>
<li>一对一：两个图中像素点都存在</li>
<li>左遮挡：左图可见，而右图不可见<br>左边的像素被映射到右边同一像素位置，所以看不见</li>
<li>右遮挡：相似</li>
</ul>
<h4 id="立体相似度"><a href="#立体相似度" class="headerlink" title="立体相似度"></a>立体相似度</h4><p>什么是一个好的立体相似：</p>
<ol>
<li>数据质量</li>
<li>平滑度，相邻像素差异最小<img src="/2024/04/29/16-24-21/QQ20240501-141943@2x.png" class>
Data term:  $E_\text{ data }=\sum_i\left(W_1(i)-W_2(i+D(i))\right)^2$<br>Smoothness term: $E_\text{ smooth }=\sum_{\text{ neighbors }i,j}\rho\left(D(i)-D(j)\right)$</li>
</ol>
<p>Total energy：$E = \alpha E_{_{\mathrm{data}}}(I_{_1},I_{_2},D)+ \beta E_{_{\mathrm{smooth}}}(D)$</p>
<p>找到一个使总体能量最小的参数。</p>
<p>更好的算法：<br>将图分割算法应用到立体相似处理。</p>
<h2 id="外置相机校准"><a href="#外置相机校准" class="headerlink" title="外置相机校准"></a>外置相机校准</h2><h4 id="几何相机标准"><a href="#几何相机标准" class="headerlink" title="几何相机标准"></a>几何相机标准</h4><p>两个方面：</p>
<ol>
<li>任何的世界坐标系</li>
<li>从3D到2D的相机坐标系<img src="/2024/04/29/16-24-21/QQ20240501-142833@2x.png" class>
从世界坐标系转换到相机坐标系</li>
</ol>
<h4 id="刚体变换"><a href="#刚体变换" class="headerlink" title="刚体变换"></a>刚体变换</h4><p>刚体有6个自由度：<br><img src="/2024/04/29/16-24-21/QQ20240501-143138@2x.png" class></p>
<ol>
<li>一个点可以定位刚体：$(x,y,z)$，三个自由度</li>
<li>加一个点，在矢量方向，经纬度，加两个自由度</li>
<li>可以旋转，加一个自由度</li>
</ol>
<h4 id="符号-F-amp-P"><a href="#符号-F-amp-P" class="headerlink" title="符号 F&amp;P"></a>符号 F&amp;P</h4><img src="/2024/04/29/16-24-21/QQ20240501-143635@2x.png" class>
<p>$^AP=\left(\begin{array}{c}^Ax\\^Ay\\^Az\\\end{array}\right)\Leftrightarrow\overline{OP}=\left(\begin{array}{c}^Ax\cdot\overline{i}\\\end{array}\right)+\left(\begin{array}{c}^Ay\cdot\overline{j}\\\end{array}\right)+\left(\begin{array}{c}^Az\cdot\overline{k}\\\end{array}\right)$</p>
<ul>
<li>上标代表所在坐标系</li>
</ul>
<img src="/2024/04/29/16-24-21/QQ20240501-143832@2x.png" class>
<p>坐标转换：已知了两个坐标原点之间的向量<br>$^BP=^AP+^B\left(O_A\right)$<br>或<br>$^BP=2^B\left(O_A\right)+2^AP$<br>就是向量加法</p>
<p>齐次坐标法：<br>\begin{split}\left[\begin{array}{c}^BP\\1\end{array}\right]=\left[\begin{array}{cc}K^BO_A\\0^T&amp;1\end{array}\right]\left[\begin{array}{c}^AP\\1\end{array}\right]\end{split}</p>
<p>变换可逆</p>
<h4 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h4><img src="/2024/04/29/16-24-21/QQ20240501-144200@2x.png" class>
<p>坐标原点重合<br>$\overrightarrow{OP}=\begin{pmatrix}i_A&amp;&amp;j_A&amp;&amp;k_A\end{pmatrix}\left(\begin{array}{c}A\\X\\\\A\\y\\\\A\\z\end{array}\right)=\begin{pmatrix}i_B&amp;&amp;j_B&amp;&amp;k_B\end{pmatrix}\left(\begin{array}{c}B\\X\\\\B\\z\end{array}\right)$</p>
<p>转换公式：基变换</p>
<p>$^BP=_A^BR^AP$<br>$_A^BR$: 在B的坐标系中描述A</p>
<p>旋转不可交换</p>
<h4 id="刚体变换-1"><a href="#刚体变换-1" class="headerlink" title="刚体变换"></a>刚体变换</h4><img src="/2024/04/29/16-24-21/QQ20240501-144753@2x.png" class>
<p>$^BP=\frac BAR^AP+^BO_A$<br>将坐标点旋转到B坐标系，加上加上A坐标系在B坐标系中的偏移量。</p>
<p>从世界坐标到相机坐标的转换：<br><img src="/2024/04/29/16-24-21/QQ20240501-145316@2x.png" class></p>
<h2 id="相机内部校准"><a href="#相机内部校准" class="headerlink" title="相机内部校准"></a>相机内部校准</h2><p>从3D相机坐标到2D图像坐标</p>
<h4 id="实际的参数"><a href="#实际的参数" class="headerlink" title="实际的参数"></a>实际的参数</h4><img src="/2024/04/29/16-24-21/QQ20240501-145813@2x.png" class>
<p>\begin{split}&amp;u = \alpha \frac{x}{z}-\alpha \cot(\theta ) \frac{y}{z}+u_{_0}\\&amp;\nu=\frac\beta{\sin(\theta)}\quad\frac{y}z+\nu_0\end{split}</p>
<ul>
<li>光学中心不在图像中间</li>
<li>坐标系不垂直</li>
<li>长宽像素缩放比不相同</li>
</ul>
<p>改善转换方程：<br>齐次坐标法：</p>
<p>\begin{split}<br>\begin{pmatrix}z^*u\\\\z^*\\\\nu\\\\z\end{pmatrix}=\begin{pmatrix}\alpha&amp;-\alpha\cot(\theta)&amp;u_0&amp;0\\\\0&amp;\frac{\beta}{\sin(\theta)}&amp;\nu_0&amp;0\\\\0&amp;0&amp;1&amp;0\end{pmatrix}\begin{pmatrix}x\\\\y\\\\z\\\\1\end{pmatrix}\\\vec{p^{\prime}}=K^c\vec{p}<br>\end{split}</p>
<p>5个自由度：<br><img src="/2024/04/29/16-24-21/QQ20240501-150253@2x.png" class></p>
<h4 id="结合内部参数和外部参数"><a href="#结合内部参数和外部参数" class="headerlink" title="结合内部参数和外部参数"></a>结合内部参数和外部参数</h4><img src="/2024/04/29/16-24-21/QQ20240501-150413@2x.png" class>
<h4 id="相机的全部参数"><a href="#相机的全部参数" class="headerlink" title="相机的全部参数"></a>相机的全部参数</h4><p>11个自由度：<br><img src="/2024/04/29/16-24-21/QQ20240501-150520@2x.png" class></p>
<h2 id="使用光谱校准相机"><a href="#使用光谱校准相机" class="headerlink" title="使用光谱校准相机"></a>使用光谱校准相机</h2><h4 id="校准方法"><a href="#校准方法" class="headerlink" title="校准方法"></a>校准方法</h4><p>利用已知点进行校准</p>
<p>切割法校准：<br>获取一些已知点，建立世界坐标系，测量设置点的坐标在世界中的相对位置，然后回恢复校准坐标。</p>
<h4 id="齐次校准"><a href="#齐次校准" class="headerlink" title="齐次校准"></a>齐次校准</h4><img src="/2024/04/29/16-24-21/QQ20240501-151052@2x.png" class>
<p>坐标系下的直线校准：<br><img src="/2024/04/29/16-24-21/QQ20240502-121531@2x.png" class></p>
<p><strong>SVD分解</strong>:奇异值分解</p>
<img src="/2024/04/29/16-24-21/QQ20240502-121634@2x.png" class>
<img src="/2024/04/29/16-24-21/QQ20240502-121827@2x.png" class>
<p>非齐次方法</p>
<h2 id="误差函数"><a href="#误差函数" class="headerlink" title="误差函数"></a>误差函数</h2><p>$\text{minimize }E=\sum_id(x_i^\prime,\hat{x}_i^\prime)$</p>
<p>如果有复杂的映射：<br>$\min_\mathbf{M}\sum_id(x_i^{\prime},\mathbf{MX}_i)$<br>用参数$M$修正</p>
<h4 id="黄金标准算法："><a href="#黄金标准算法：" class="headerlink" title="黄金标准算法："></a>黄金标准算法：</h4><p>标准归一化：<br>$\tilde{\mathbf{X}}_i=\mathbf{U}\mathbf{X}_i\tilde{\mathbf{x}}_i=\mathbf{T}\mathbf{x}_i$</p>
<p>$\min_\mathbf{M}\sum_id\left(\tilde{\mathbf{x}}_i,\tilde{\mathbf{M}}\tilde{\mathbf{X}}_i\right)$</p>
<p>$\mathbf{M}=\mathbf{T}^{-1}\tilde{\mathbf{M}}\mathbf{U}$</p>
<p>类似特征向量，转换为标准正交基上</p>
<h4 id="从M中找3D相机中心"><a href="#从M中找3D相机中心" class="headerlink" title="从M中找3D相机中心"></a>从M中找3D相机中心</h4><p>直接的方法：<br>如果能找到一个点$C$， 使得$\textbf{M C = 0}$，就是相机中心。<br>原理：<br>$\mathbf{X}=\lambda\mathbf{P}+(1-\lambda)\mathbf{C}$<br>$\mathbf{x}=\mathbf{M}\mathbf{X}=\lambda\mathbf{M}\mathbf{P}+(1-\lambda)\mathbf{M}\mathbf{C}$</p>
<p>简单的方法：<br>$\mathbf{C}=\begin{pmatrix}-\mathbf{Q}^{-1}\mathbf{b}\\\\1\end{pmatrix}$</p>
<h4 id="多平面校准"><a href="#多平面校准" class="headerlink" title="多平面校准"></a>多平面校准</h4><p>目前最常使用的：</p>
<p>优点：</p>
<ol>
<li>只需要一张纸</li>
<li>不需要知道相机内部参数</li>
<li>代码是公开的</li>
</ol>
<h2 id="多视角"><a href="#多视角" class="headerlink" title="多视角"></a>多视角</h2><h4 id="图像到图像投影"><a href="#图像到图像投影" class="headerlink" title="图像到图像投影"></a>图像到图像投影</h4><p>2D变换：<br><img src="/2024/04/29/16-24-21/QQ20240502-123620@2x.png" class></p>
<p>特殊投影变换：<br>变换：<br><img src="/2024/04/29/16-24-21/QQ20240502-123832@2x.png" class></p>
<p>刚体变换：<br><img src="/2024/04/29/16-24-21/QQ20240502-123903@2x.png" class></p>
<p>相似变换：<br><img src="/2024/04/29/16-24-21/QQ20240502-123927@2x.png" class></p>
<p>仿射变换：Affine transform<br><img src="/2024/04/29/16-24-21/QQ20240502-123942@2x.png" class></p>
<p>一般投影变换：<br><img src="/2024/04/29/16-24-21/QQ20240502-124130@2x.png" class></p>
<h2 id="马赛克"><a href="#马赛克" class="headerlink" title="马赛克"></a>马赛克</h2><p>平面图：<br><img src="/2024/04/29/16-24-21/QQ20240502-124454@2x.png" class><br>射线上任一点，投影到平面上，都是交点重合。</p>
<h4 id="图像重影"><a href="#图像重影" class="headerlink" title="图像重影"></a>图像重影</h4><p>相机中心相同，拍摄了两张照片：<br><img src="/2024/04/29/16-24-21/QQ20240502-124653@2x.png" class></p>
<p>全景图像：<br>保存相机中心不变，移动水平角度，拍摄照片，重合起来就是全景图像。</p>
<p>自然几何：<br><img src="/2024/04/29/16-24-21/QQ20240502-125002@2x.png" class><br>将图像投影到大平面上，组成全景图像，重叠部分混合。</p>
<p>相同位置的不同坐标：<br><img src="/2024/04/29/16-24-21/QQ20240502-125348@2x.png" class></p>
<p>关键是求解：<br><img src="/2024/04/29/16-24-21/QQ20240502-125433@2x.png" class></p>
<p>转换图像(图像扭曲)：是一种插值<br><img src="/2024/04/29/16-24-21/QQ20240502-125553@2x.png" class></p>
<p>全景图像方法：可以用于去除图像中元素<br><img src="/2024/04/29/16-24-21/QQ20240502-125804@2x.png" class></p>
<h4 id="3D图像"><a href="#3D图像" class="headerlink" title="3D图像"></a>3D图像</h4><img src="/2024/04/29/16-24-21/QQ20240502-125917@2x.png" class>
<h4 id="两种图像扭曲方法"><a href="#两种图像扭曲方法" class="headerlink" title="两种图像扭曲方法"></a>两种图像扭曲方法</h4><p>向前扭曲：错误方式<br><img src="/2024/04/29/16-24-21/QQ20240502-130407@2x.png" class></p>
<p>像素离散化时，某一点的像素位置移动了，需要分散给其他像素。<br><img src="/2024/04/29/16-24-21/QQ20240502-130543@2x.png" class></p>
<p>反向扭曲：<br><img src="/2024/04/29/16-24-21/QQ20240502-130652@2x.png" class></p>
<p>中间像素的插值法：<br><img src="/2024/04/29/16-24-21/QQ20240502-130719@2x.png" class></p>
<h2 id="投影几何"><a href="#投影几何" class="headerlink" title="投影几何"></a>投影几何</h2><p>基本平面知识</p>
<h2 id="基本矩阵"><a href="#基本矩阵" class="headerlink" title="基本矩阵"></a>基本矩阵</h2><img src="/2024/04/29/16-24-21/QQ20240502-131351@2x.png" class>
<img src="/2024/04/29/16-24-21/QQ20240502-131523@2x.png" class>
<img src="/2024/04/29/16-24-21/QQ20240502-131532@2x.png" class>
<h2 id="基础矩阵"><a href="#基础矩阵" class="headerlink" title="基础矩阵"></a>基础矩阵</h2><h4 id="弱校准"><a href="#弱校准" class="headerlink" title="弱校准"></a>弱校准</h4><img src="/2024/04/29/16-24-21/QQ20240502-131844@2x.png" class>
<img src="/2024/04/29/16-24-21/QQ20240502-131901@2x.png" class>
<img src="/2024/04/29/16-24-21/QQ20240502-131918@2x.png" class>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉概论 计算机视觉的图像处理</title>
    <url>/2024/04/27/09-07-10/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.27：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/27/08-42-03/" title="Ud810 Intro-to-cv 笔记汇总">计算机视觉概率-笔记汇总</a>
</li>
</ul>
<h2 id="图像作为函数"><a href="#图像作为函数" class="headerlink" title="图像作为函数"></a>图像作为函数</h2><h4 id="理解图像函数"><a href="#理解图像函数" class="headerlink" title="理解图像函数"></a>理解图像函数</h4><img src="/2024/04/27/09-07-10/QQ20240427-092957@2x.png" class>
<p>通常我们认为图像就是看到的东西，实际上，图像是一个函数$I(x, y)$，值为像素值。</p>
<img src="/2024/04/27/09-07-10/QQ20240427-093048@2x.png" class>
<p>上面两种形式是一个相同的函数，只是展示方式不同。</p>
<p>对图像做平滑处理，函数变为：<br><img src="/2024/04/27/09-07-10/QQ20240427-093326@2x.png" class></p>
<p>对图像做模糊处理，函数变为：<br><img src="/2024/04/27/09-07-10/QQ20240427-093357@2x.png" class></p>
<h4 id="图像函数"><a href="#图像函数" class="headerlink" title="图像函数"></a>图像函数</h4><p>$f(x, y)$：是图像在点$(x, y)$处的光强度或值<br>限制$x$,$y$的范围，以及强度的范围：$f:[a, b] x[c, d] \rightarrow[\min , \max ]$</p>
<blockquote>
<p>图像定义：$x$为列，$y$为行, 原点在图像左上角</p>
</blockquote>
<p>图像范围不是0-255，而是0-1，0为黑，1为白，最小值为黑色，最大值为白色，甚至可以存在负值图像。</p>
<blockquote>
<p>黑白定义来源光的强度，值越大，强度越高，为白色</p>
</blockquote>
<p>彩色图像：</p>
<p>\begin{split}<br>f(x, y)=\left[\begin{array}{l}<br>r(x, y) \\<br>g(x, y) \\<br>b(x, y)<br>\end{array}\right]<br>\end{split}</p>
<p>每个像素为一个向量，RGB：值分别为，红，绿，蓝的颜色强度。</p>
<h4 id="真实的菲利斯"><a href="#真实的菲利斯" class="headerlink" title="真实的菲利斯"></a>真实的菲利斯</h4><img src="/2024/04/27/09-07-10/QQ20240427-095451@2x.png" class>
<p>这与眼睛看到的图像是相同的函数。</p>
<h4 id="数字图像"><a href="#数字图像" class="headerlink" title="数字图像"></a>数字图像</h4><p>Sample：采样，对2D图像进行采样, 即像素<br>Quantize：量化，用一个有限数量的位数来表示它</p>
<blockquote>
<p>量化使用浮点图像？</p>
</blockquote>
<img src="/2024/04/27/09-07-10/QQ20240427-095943@2x.png" class>
<p>图像由$(x,y)$确定，而计算中的$(i,j)$，为行，列值，与$x$,$y$，相反。</p>
<p>有时图像也可用一维信号表示。</p>
<h4 id="图像是矩阵存储"><a href="#图像是矩阵存储" class="headerlink" title="图像是矩阵存储"></a>图像是矩阵存储</h4><blockquote>
<p>课程使用的MATLAB，目前多使用OpenCV，所以不记笔记。</p>
</blockquote>
<h4 id="图像噪声"><a href="#图像噪声" class="headerlink" title="图像噪声"></a>图像噪声</h4><p>$\vec{I}^{\prime}(x, y)=\vec{I}(x, y)+\vec{\eta}(x, y)$</p>
<img src="/2024/04/27/09-07-10/QQ20240427-100924@2x.png" class>
<p>椒盐噪声: 产生黑白像素点</p>
<img src="/2024/04/27/09-07-10/QQ20240427-101112@2x.png" class>
<p>脉冲噪声: 产生白色像素点</p>
<img src="/2024/04/27/09-07-10/QQ20240427-101206@2x.png" class>
<p>高斯噪声或正态分布噪声: 产生某些正态分布或某些高斯分布独立同分布的值</p>
<h4 id="Sigma-对高斯噪声的影响"><a href="#Sigma-对高斯噪声的影响" class="headerlink" title="Sigma 对高斯噪声的影响"></a>Sigma 对高斯噪声的影响</h4><img src="/2024/04/27/09-07-10/QQ20240427-101547@2x.png" class>
<p>不同sigma的图像函数。</p>
<ul>
<li>噪声有什么意思？<br>“零”<br>对于负值图像，错误的想法是0是黑色，255是白色，黑色白色，只是值范围内的最小值与最大值，0代表的是两者之间，灰色。非常小的sigma，是一个恒灰函数。</li>
<li>不同的sigma对应不同的图像值范围，(0-255), sigma5是合理的。</li>
</ul>
<h2 id="过滤和噪声"><a href="#过滤和噪声" class="headerlink" title="过滤和噪声"></a>过滤和噪声</h2><h4 id="如何过滤高斯噪声"><a href="#如何过滤高斯噪声" class="headerlink" title="如何过滤高斯噪声"></a>如何过滤高斯噪声</h4><p>采用平滑图像，在一维像素中，周围像素平均值设置为当前像素值，可以消除噪声。</p>
<h4 id="平均值的假设"><a href="#平均值的假设" class="headerlink" title="平均值的假设"></a>平均值的假设</h4><ul>
<li>真实像素值可能与周围像素值相近</li>
<li>每个像素的噪声相互独立<br>意味着如果取噪声的平均值，噪声将为0</li>
</ul>
<h4 id="加权移动平均线"><a href="#加权移动平均线" class="headerlink" title="加权移动平均线"></a>加权移动平均线</h4><img src="/2024/04/27/09-07-10/QQ20240427-103425@2x.png" class>
<p>非均匀权重：距离像素越近，权重越大，可以使图像更平滑。</p>
<h4 id="2D-移动平均线"><a href="#2D-移动平均线" class="headerlink" title="2D 移动平均线"></a>2D 移动平均线</h4><img src="/2024/04/27/09-07-10/1.png" class>
<p>简单的平均权重</p>
<img src="/2024/04/27/09-07-10/QQ20240427-103933@2x.png" class>
<p>移动平均之后的结果</p>
<h4 id="相关过滤"><a href="#相关过滤" class="headerlink" title="相关过滤"></a>相关过滤</h4><p>上述过滤为统一权重过滤，公式为：<br>核的size： 2 * k + 1；</p>
<p>$G[i, j]=\frac{1}{(2 k+1)^2} \sum_{u=-k}^k \sum_{v=-k}^k F[i+u, j+v]$</p>
<p>非均值权重的公式为：<br>$G[i, j]=\sum_{u=-k}^k \sum_{v=-k}^k H[u, v] F[i+u, j+v]$</p>
<h4 id="平均滤波器"><a href="#平均滤波器" class="headerlink" title="平均滤波器"></a>平均滤波器</h4><img src="/2024/04/27/09-07-10/QQ20240427-104618@2x.png" class>
<p>均值平滑后的结果：<br><img src="/2024/04/27/09-07-10/QQ20240427-104655@2x.png" class></p>
<p>效果不好的原因：<br>  如果一个两点失去了焦点，看起来就像是：<br><img src="/2024/04/27/09-07-10/QQ20240427-104815@2x.png" class></p>
<h4 id="高斯过滤"><a href="#高斯过滤" class="headerlink" title="高斯过滤"></a>高斯过滤</h4><p>高斯函数：<br><img src="/2024/04/27/09-07-10/QQ20240427-105107@2x.png" class></p>
<p>高斯过滤后的图像：更加平滑<br><img src="/2024/04/27/09-07-10/QQ20240427-105212@2x.png" class><br>不会出现尖锐的部分</p>
<h4 id="方差与标准差"><a href="#方差与标准差" class="headerlink" title="方差与标准差"></a>方差与标准差</h4><p>sigma越大，模糊程度越高, 是高斯的高度，平方是高斯的方差。<br>关注的有两点，矩阵大小和sigma的值。<br>高斯过滤:<br>不同的sigma：<br><img src="/2024/04/27/09-07-10/QQ20240427-105612@2x.png" class><br>sigma越大，过滤的内容越多。</p>
<p>不同的内核大小：<br><img src="/2024/04/27/09-07-10/QQ20240427-105651@2x.png" class></p>
<p>内核更大，平滑效果越好，</p>
<h4 id="两种高斯直线"><a href="#两种高斯直线" class="headerlink" title="两种高斯直线"></a>两种高斯直线</h4><img src="/2024/04/27/09-07-10/QQ20240427-110026@2x.png" class>
<ul>
<li>sigma越大，噪音越大</li>
<li>高斯过滤，相同平滑量的噪声量较小的会更加平滑</li>
</ul>
<blockquote>
<p>两个sigma对应不同阶段的参数</p>
</blockquote>
<h2 id="线性算子和卷积"><a href="#线性算子和卷积" class="headerlink" title="线性算子和卷积"></a>线性算子和卷积</h2><p>线性变换特点：可加性, 倍乘性</p>
<p>线性算子如何影响整个图像？</p>
<p>脉冲函数：<br><img src="/2024/04/27/09-07-10/QQ20240427-131003@2x.png" class><br>脉冲是一个很小的信号，体积为1。</p>
<p>输入一个脉冲，在黑盒中输出一个响应。<br><img src="/2024/04/27/09-07-10/QQ20240427-131216@2x.png" class></p>
<p>如果知道黑盒如何影响单个脉冲，就可以解释影响整个图像。</p>
<h4 id="过滤脉冲信号"><a href="#过滤脉冲信号" class="headerlink" title="过滤脉冲信号"></a>过滤脉冲信号</h4><img src="/2024/04/27/09-07-10/QQ20240427-131451@2x.png" class>
<p>展示了脉冲过滤的翻转情况，假设中心元素为脉冲元素e。</p>
<h4 id="互相关和卷积"><a href="#互相关和卷积" class="headerlink" title="互相关和卷积"></a>互相关和卷积</h4><p><strong>互相关</strong><br>直接应用过滤器：<br>$G[i, j]=\sum_{u=-k}^k \sum_{v=-k}^k H[u, v] F[i+u, j+v]$</p>
<p>$G=H \otimes F$</p>
<p><strong>卷积</strong><br>翻转上下，左右：</p>
<blockquote>
<p>相当于先把核进行上下左右翻转，然后在应用过滤器：</p>
</blockquote>
<p>$G[i, j]=\sum_{u=-k}^k \sum_{v=-k}^k H[u, v] F[i-u, j-v]$</p>
<p>$G=H \star F$</p>
<p>翻转过程为：(*为了方便看到翻转效果)<br><img src="/2024/04/27/09-07-10/QQ20240427-132224@2x.png" class></p>
<img src="/2024/04/27/09-07-10/QQ20240427-132133@2x.png" class>
<p>卷积实际是一种物理学</p>
<ul>
<li>在对称滤波器中，两种操作都是结果都是相同的</li>
<li>只有在非对称滤波器中，才能看到区别</li>
</ul>
<p><strong>卷积的性质</strong></p>
<ul>
<li>线性运算</li>
<li>结合律</li>
<li>交换律</li>
<li>单位脉冲和整个脉冲操作相同</li>
<li>微分性质<br>卷积的导数等于第一个元素的导数与第二个元素的卷积<br>$\frac\partial{\partial x}(f*g)=\frac{\partial f}{\partial x}*g$<br>边缘检测和梯度查找会用到</li>
</ul>
<h4 id="计算的复杂性和可分离性"><a href="#计算的复杂性和可分离性" class="headerlink" title="计算的复杂性和可分离性"></a>计算的复杂性和可分离性</h4><img src="/2024/04/27/09-07-10/QQ20240427-134946@2x.png" class>
<p>乘法的操作次数需要：$N<em>N</em>W*W$</p>
<p>线性可分离核：<br><img src="/2024/04/27/09-07-10/QQ20240427-135116@2x.png" class><br>假设其他位置元素为0，结果为一列向量乘行向量。</p>
<p>可分离卷积操作：<br>$G=H*F=(C*R)*F=C*(R*F)$<br>乘法操作次数为：$2{\cdot}W{\cdot}N^2&lt;&lt;W^2{\cdot}N^2$</p>
<h4 id="边界问题"><a href="#边界问题" class="headerlink" title="边界问题"></a>边界问题</h4><p>过滤器会不会从边界掉下去？<br>一共有三种边界处理方式：<br><img src="/2024/04/27/09-07-10/QQ20240427-140836@2x.png" class></p>
<p>具体的方法：</p>
<ul>
<li><p>剪辑-clip filter<br>假设边缘是黑的，结果会让边缘变暗，因为黑色渗入了</p>
<img src="/2024/04/27/09-07-10/QQ20240427-141058@2x.png" class>
</li>
<li><p>环绕法-wrap around<br>与傅里叶分析有关<br>假设图片一周围绕的像素与图像边缘有关：右边填充为左边的图像，上边填充为下边的图像</p>
<img src="/2024/04/27/09-07-10/QQ20240427-141413@2x.png" class>
<p>过滤效果：</p>
<img src="/2024/04/27/09-07-10/QQ20240427-141603@2x.png" class>
<p>图像的上边缘会泛红，来源于下边</p>
<blockquote>
<p>类似一种周期性信号，但是在图像过滤中效果不好</p>
</blockquote>
</li>
<li><p>边缘复制法-copy edge<br>填充复制图像边缘</p>
<img src="/2024/04/27/09-07-10/QQ20240427-141754@2x.png" class>
<p>效果: 图像基本保持不变，是一个合理的结果</p>
<img src="/2024/04/27/09-07-10/QQ20240427-141927@2x.png" class> 
</li>
<li><p>反射法-reflect across edge<br>填充的是边缘的反射</p>
<img src="/2024/04/27/09-07-10/QQ20240427-142124@2x.png" class> 
<p>效果也是很好：</p>
<img src="/2024/04/27/09-07-10/QQ20240427-142207@2x.png" class> 
</li>
</ul>
<h4 id="联联系线性滤波器"><a href="#联联系线性滤波器" class="headerlink" title="联联系线性滤波器"></a>联联系线性滤波器</h4><p>用脉冲对图像过滤，得到的还是原图像：<br><img src="/2024/04/27/09-07-10/QQ20240427-142606@2x.png" class></p>
<p>使用向右移动了一位的脉冲，过滤结果：<strong>图中进行的是相关操作</strong><br><img src="/2024/04/27/09-07-10/QQ20240427-142802@2x.png" class></p>
<blockquote>
<p>取向左还是向右，取决于做相关还是卷积操作：</p>
</blockquote>
<p>模糊的平滑滤镜：<br><img src="/2024/04/27/09-07-10/QQ20240427-143056@2x.png" class></p>
<p>特殊的过滤器：(脉冲的两倍，减去模糊)<br><img src="/2024/04/27/09-07-10/QQ20240427-143220@2x.png" class></p>
<blockquote>
<p>锐化图像，强调差异<br>  应用：<br>  <img src="/2024/04/27/09-07-10/QQ20240427-143339@2x.png" class> </p>
</blockquote>
<p>非锐化滤镜:(unsharp mask)<br>类似冲洗胶片，将白光照射，得到底片的负片。</p>
<p>对应上图的锐化：底片的负片减去模糊的部分，得到的是更清晰的图片。</p>
<p>模糊的部分是不清晰的蒙版，加入图像后得到了更清晰的图像。</p>
<h4 id="其他类型的噪音与对应的非线性过滤器"><a href="#其他类型的噪音与对应的非线性过滤器" class="headerlink" title="其他类型的噪音与对应的非线性过滤器"></a>其他类型的噪音与对应的非线性过滤器</h4><p>高斯噪音与椒盐噪音<br><img src="/2024/04/27/09-07-10/QQ20240427-144017@2x.png" class><br>过滤的本质是从周围的像素点找一个局部平均值替换。</p>
<p>当噪音趋于0时，可以很好的过滤噪音，如果把完全随机的值加入图像，就需要其他过滤方法，比如中值过滤器。</p>
<p>中值过滤器：(median filter)<br>将中间的白色像素90，替换为中值，非线性的，不可以复原.<br><img src="/2024/04/27/09-07-10/QQ20240427-145325@2x.png" class> </p>
<p>对椒盐噪音效果很好：<br><img src="/2024/04/27/09-07-10/QQ20240427-145558@2x.png" class> </p>
<p>非线性的过滤：<br><img src="/2024/04/27/09-07-10/QQ20240427-145725@2x.png" class><br>有时也称为边缘保留</p>
<blockquote>
<p>相对于均值过滤，它保留了图像像素边缘非平滑。</p>
</blockquote>
<h2 id="过滤用作模式匹配"><a href="#过滤用作模式匹配" class="headerlink" title="过滤用作模式匹配"></a>过滤用作模式匹配</h2><p>归一化相关性<br>两件事：1. 对过滤器进行标准化。2.用过滤器将box内的像素变为标准差为1。</p>
<h4 id="1D-信号"><a href="#1D-信号" class="headerlink" title="1D 信号"></a>1D 信号</h4><img src="/2024/04/27/09-07-10/QQ20240427-153302@2x.png" class> 
<p>用所示的滤波器，对信号进行归一化相关：<br>滤波器来源于信号的某一段，在此处，归一化相关后得到峰值。</p>
<blockquote>
<p>原理，在归一化处理后的正负图像中，乘一个自己相同的滤波器，得到的值最大，负负为正。峰值处：正值与正值对齐，负值与负值对齐。</p>
</blockquote>
<p>对应于图像：<br><img src="/2024/04/27/09-07-10/QQ20240427-154142@2x.png" class> </p>
<p>相关后的图像函数图，在相似位置的值为峰值<br><img src="/2024/04/27/09-07-10/QQ20240427-154201@2x.png" class> </p>
<blockquote>
<p>解释了过滤器如何用作模式匹配。</p>
</blockquote>
<h4 id="模式识别"><a href="#模式识别" class="headerlink" title="模式识别"></a>模式识别</h4><p>简单的例子：<br><img src="/2024/04/27/09-07-10/QQ20240428-104609@2x.png" class> </p>
<p>使用相关性进行检测：<br><img src="/2024/04/27/09-07-10/QQ20240428-104723@2x.png" class> </p>
<p>最亮的点就是检测到的位置。</p>
<p>沃尔多在哪里？<br><img src="/2024/04/27/09-07-10/QQ20240428-104903@2x.png" class> </p>
<img src="/2024/04/27/09-07-10/QQ20240428-104923@2x.png" class>
<p>相同的结果：找到最亮的点。</p>
<p>图像不同的情况：<br><img src="/2024/04/27/09-07-10/QQ20240428-105048@2x.png" class> </p>
<img src="/2024/04/27/09-07-10/QQ20240428-104923@2x.png" class>
<blockquote>
<p>匹配错误。</p>
</blockquote>
<h2 id="边缘检测：梯度"><a href="#边缘检测：梯度" class="headerlink" title="边缘检测：梯度"></a>边缘检测：梯度</h2><p>如果事先不知道要找的图像模板，如何在图像中找到有用的信息，图像的特征。</p>
<p>简化图像：</p>
<p>图中的所有信息大多来源于边缘。</p>
<img src="/2024/04/27/09-07-10/QQ20240428-105632@2x.png" class>
<p>边缘：<br><img src="/2024/04/27/09-07-10/QQ20240428-105718@2x.png" class></p>
<p>上图中分别有深度边缘，阴影边缘，颜色的边缘，形状的边缘。</p>
<img src="/2024/04/27/09-07-10/QQ20240428-105810@2x.png" class>
<p>图中包含了纹理边缘。</p>
<p>边缘检测：<br>找出图像函数的边缘像素：<br><img src="/2024/04/27/09-07-10/QQ20240428-110004@2x.png" class></p>
<p>从高度函数中可以看到，急剧变化的位置就是边缘像素。</p>
<p>边缘检测的问题：</p>
<ol>
<li>边缘有多大</li>
<li>变化的范围是多少</li>
</ol>
<p><strong>导数和边缘</strong></p>
<img src="/2024/04/27/09-07-10/QQ20240428-110311@2x.png" class>
<p>右图中的一阶导数函数图，极值点对应的就是图像边缘像素。</p>
<p><strong>什么是梯度</strong></p>
<img src="/2024/04/27/09-07-10/QQ20240428-111127@2x.png" class>
<p>梯度是：图像变化最快的方向。大小是单位长度上变化的强度。</p>
<p>图像的梯度：$\nabla f = [\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]$</p>
<p>梯度的方向：$\theta^2=\tan^{-1}(\frac{\partial f}{\partial y}/\frac{\partial f}{\partial x})$</p>
<p>边缘变化的强度：$\left\|\nabla f\right\|=\sqrt{(\frac{\partial f}{\partial x})^2+(\frac{\partial f}{\partial y})^2}$</p>
<p>幅度检测：</p>
<p>离散梯度：（在离散数中，极限无法靠近，只能使用有限差分）<br>定义为：<br>\begin{split}<br>\frac{\partial f\left(x,y\right)}{\partial x}&amp;\approx\frac{f\left(x+1,y\right)-f\left(x,y\right)}1\\&amp;\approx f\left(x+1,y\right)-f\left(x,y\right)<br>\end{split}</p>
<p>梯度是对$x$，还是$y$做偏导？<br><img src="/2024/04/27/09-07-10/QQ20240428-111919@2x.png" class></p>
<p>上图中可以看出，在垂直方向对x的梯度可以求出边缘像素，但是在水平方向的边缘像素效果不好。<strong>上图是一个对x的有限差分</strong><br>上图也是一个正负图，负数为黑色，正数为白色，零是灰色。</p>
<p><strong>图像的偏导数</strong><br><img src="/2024/04/27/09-07-10/QQ20240428-112419@2x.png" class><br>很容易可以看出：左边是对x的偏导图，右边是对y的偏导图。</p>
<p>使用的相关过滤器：</p>
<ul>
<li>对x：将右边的像素减去左边的像素，增强对比。</li>
<li>对y：有两种，取决于想让y上升还是下降</li>
</ul>
<p>离散梯度：使用运算符$H$实现<br>由公式可以得出，梯度为左边的像素减去自己的像素。<br><img src="/2024/04/27/09-07-10/QQ20240428-113714@2x.png" class></p>
<ul>
<li>第一种运算符：没有中间元素，且只输出了右边的梯度</li>
<li>第二种运算符：输出左边和右边的梯度平均值<br>公式推导：</li>
</ul>
<p>\begin{split}<br>\frac{\partial f\left(x,y\right)}{\partial x} \\<br>&amp;\approx\frac{1}{2}\{(f\left(x+1,y\right)-f\left(x,y\right) + f\left(x,y\right)-f\left(x - 1,y\right)\}\\<br>&amp;\approx\frac{1}{2}\{f\left(x+1,y\right)-f\left(x - 1,y\right)\}<br>\end{split}</p>
<p>经典的边缘算法：Sobel算子<br><img src="/2024/04/27/09-07-10/QQ20240428-123328@2x.png" class></p>
<ul>
<li>$S_x$：左右和左上右上，左下右下的方向导数。</li>
<li>$S_y$：同理</li>
</ul>
<p>梯度：$\nabla\mathbf{I}=[\mathbf{g}_\mathbf{X}\quad\mathbf{g}_\mathbf{y}]^\mathbf{T}$<br>强度：$\mathrm{g=(g_X^2+g_y^2)^{1/2}}$<br>方向：$\theta=\operatorname{atan2}(\mathfrak{g}_\mathrm{y},\mathfrak{g}_\mathrm{x})$</p>
<p>Sobel古老的例子：X windows</p>
<img src="/2024/04/27/09-07-10/QQ20240428-123839@2x.png" class>
<blockquote>
<p>右边的图像是做了阀值化thresholded</p>
</blockquote>
<p>著名的边缘算子：<br><img src="/2024/04/27/09-07-10/QQ20240428-123955@2x.png" class></p>
<h4 id="现实世界"><a href="#现实世界" class="headerlink" title="现实世界"></a>现实世界</h4><p>上述算子在现实中不会起作用，噪声太多。<br><img src="/2024/04/27/09-07-10/QQ20240428-124215@2x.png" class><br>所以必须先处理噪音，在边缘化。</p>
<p>解决方案：平滑<br><img src="/2024/04/27/09-07-10/QQ20240428-124357@2x.png" class><br>山峰就是边缘像素。</p>
<p><strong>算子的线性结合性</strong><br>利用卷积的求导公式: $\frac\partial{\partial x}(f*g)=\frac{\partial f}{\partial x}*g$</p>
<img src="/2024/04/27/09-07-10/QQ20240428-124636@2x.png" class>
<p><strong>二阶导数</strong></p>
<img src="/2024/04/27/09-07-10/QQ20240428-124732@2x.png" class>
<blockquote>
<p>零强度的位置就是边缘像素，不需要再找峰值了</p>
</blockquote>
<h2 id="2D边缘检测"><a href="#2D边缘检测" class="headerlink" title="2D边缘检测"></a>2D边缘检测</h2><h4 id="二维高斯滤波器的导数"><a href="#二维高斯滤波器的导数" class="headerlink" title="二维高斯滤波器的导数"></a>二维高斯滤波器的导数</h4><img src="/2024/04/27/09-07-10/QQ20240428-125006@2x.png" class>
<ul>
<li>公式：$(I\otimes g)\otimes h=I\otimes(g\otimes h)$<br>根据结合性</li>
<li>$h$: 只对x求导</li>
<li>$g$: 高斯平滑</li>
</ul>
<p>高斯滤波器的导数：<br><img src="/2024/04/27/09-07-10/QQ20240428-125509@2x.png" class></p>
<ul>
<li>左图：可以看出是一个相关操作,因为往右增为正方向。</li>
<li>右图：</li>
</ul>
<p>sigma的大小：<br><img src="/2024/04/27/09-07-10/QQ20240428-125956@2x.png" class> </p>
<p>不同的sigma对于图像边缘的影响：较小的值，有精细的特征,而较大的值仅仅检测到较大尺度的边缘。</p>
<h4 id="如何找到边缘？"><a href="#如何找到边缘？" class="headerlink" title="如何找到边缘？"></a>如何找到边缘？</h4><p>Canny算子:</p>
<ol>
<li>对高斯图像进行滤波</li>
<li>找幅度最大的方向，进行非极大值抑制，即细化操作</li>
<li>连接操作，将边缘连接起来<br>定义了两个阀值：极大阀值和极小阀值</li>
</ol>
<p>原图：<br><img src="/2024/04/27/09-07-10/QQ20240428-130730@2x.png" class> </p>
<p>梯度图：<br><img src="/2024/04/27/09-07-10/QQ20240428-130742@2x.png" class> </p>
<p>阀值处理图：消除一部分梯度够高的像素<br><img src="/2024/04/27/09-07-10/QQ20240428-130804@2x.png" class> </p>
<p>细化处理：(非极大值抑制)<br><img src="/2024/04/27/09-07-10/QQ20240428-130903@2x.png" class> </p>
<p>做法：某个局部有很多梯度点，只保留极值最高的点，抑制非极大值</p>
<p>为什么要进行细化操作：<br><img src="/2024/04/27/09-07-10/QQ20240428-131216@2x.png" class> </p>
<p>超过阀值的粗部分,只保留极大值，细化为一条直线。</p>
<img src="/2024/04/27/09-07-10/QQ20240428-131343@2x.png" class>
<p>在梯度方向找到极大值</p>
<p>另一个问题：下巴的边缘没有检测出来？<br><img src="/2024/04/27/09-07-10/QQ20240428-131520@2x.png" class><br>阀值太高，下巴的梯度没有通过阀值。</p>
<p>Canny阀值滞后:</p>
<ol>
<li>用高阀值检测边缘,找出强边缘像素</li>
<li>连接成强边缘</li>
<li>应用低阀值找出弱边缘像素</li>
<li>把强边缘延长到弱像素上<blockquote>
<p>如果一条边缘上只有弱边缘像素，就不是一个需要的边缘<br>强边缘可能会穿过一些弱边缘像素</p>
</blockquote>
</li>
</ol>
<p>Canny 算子的结果：<br><img src="/2024/04/27/09-07-10/QQ20240428-132327@2x.png" class></p>
<p>什么样的边缘图是好的？取决于你想用什么样的边缘图。</p>
<h4 id="简单的二维边缘检测滤波器"><a href="#简单的二维边缘检测滤波器" class="headerlink" title="简单的二维边缘检测滤波器"></a>简单的二维边缘检测滤波器</h4><img src="/2024/04/27/09-07-10/QQ20240428-132553@2x.png" class>
<p>偏导有多种顺序</p>
<p><strong>拉普拉斯算子</strong>用于求二阶偏导</p>
<img src="/2024/04/27/09-07-10/QQ20240428-132734@2x.png" class>
<p>公式：$\nabla^2h^2=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2f}{\partial y^2}$</p>
<ul>
<li>$\nabla^2$: 拉普拉斯算子</li>
<li>0为边缘像素</li>
</ul>
<h2 id="霍夫变换（Hough-transform）-直线"><a href="#霍夫变换（Hough-transform）-直线" class="headerlink" title="霍夫变换（Hough transform）:直线"></a>霍夫变换（Hough transform）:直线</h2><p>如何找到任意形状？</p>
<h4 id="参数模型（Parametric-model）"><a href="#参数模型（Parametric-model）" class="headerlink" title="参数模型（Parametric model）"></a>参数模型（Parametric model）</h4><p>参数模型是一个类</p>
<p>直线匹配：<br><img src="/2024/04/27/09-07-10/QQ20240428-133955@2x.png" class></p>
<p>边缘图：<br><img src="/2024/04/27/09-07-10/QQ20240428-133947@2x.png" class><br>困难：</p>
<ul>
<li>有很其他形状,多种模型</li>
<li>线不连续</li>
<li>噪音</li>
</ul>
<p>投票：</p>
<ol>
<li>每个像素点对有用模型投票</li>
<li>找出最高的模型</li>
</ol>
<blockquote>
<p>原因：有很多无效的像素点，但只要有真正的像素点投票是有效的，选出匹配的模型，就可以淘汰掉噪音。</p>
</blockquote>
<p>拟合直线的几个问题？</p>
<ol>
<li>给定一些点，哪些是一条线</li>
<li>有多少条线</li>
<li>有哪些点是属于线的</li>
</ol>
<p>霍夫变换：一种投票技术，可以解决上面的问题</p>
<ul>
<li>每个边缘点都会投票给兼容的线</li>
<li>找票数最多线</li>
<li>追踪票，可以找到属于线的点有哪些</li>
</ul>
<h4 id="霍夫空间Hough-parameter-space"><a href="#霍夫空间Hough-parameter-space" class="headerlink" title="霍夫空间Hough (parameter) space"></a>霍夫空间Hough (parameter) space</h4><img src="/2024/04/27/09-07-10/QQ20240428-135111@2x.png" class>
<p>对于一个点$(x,y)$，穿过这个点的直线为:$y_0=mx_0+b$，$m$,$b$为任意值。<br>所以对应霍夫空间的函数为：$b^2=-x_0m^2+y_0$</p>
<blockquote>
<p>将空间中的一点，对应到霍夫空间中的一条直线</p>
</blockquote>
<img src="/2024/04/27/09-07-10/QQ20240428-135421@2x.png" class>
<ul>
<li>霍夫空间中的交点，对应的参数，是过两点的直线</li>
<li>这就是从点中找线的方法</li>
</ul>
<p><strong>霍夫算法</strong></p>
<img src="/2024/04/27/09-07-10/QQ20240428-135710@2x.png" class>
<p>点对应于霍夫空间中的线：</p>
<ul>
<li>每个点都对自己经过的分区投票</li>
<li>票数最多的就是线<blockquote>
<p>具有偏差</p>
</blockquote>
</li>
</ul>
<p>为了防止出现垂直直线的表示，采用极坐标法表示图像</p>
<img src="/2024/04/27/09-07-10/QQ20240428-140007@2x.png" class>
<ul>
<li>$d$:原点到点距离</li>
<li>$\theta$: 表示到$x$轴的角度</li>
<li>$\quad x\cos\theta+y\sin\theta=d$</li>
</ul>
<blockquote>
<p>可以表示任何的直线<br>图像空间中的点是霍夫空间中的正弦曲线</p>
</blockquote>
<h4 id="基本霍夫变换算法"><a href="#基本霍夫变换算法" class="headerlink" title="基本霍夫变换算法"></a>基本霍夫变换算法</h4><p>霍夫累加器数组（收集投票的数组）：<br><img src="/2024/04/27/09-07-10/QQ20240428-140657@2x.png" class></p>
<p>霍夫算法：<br><img src="/2024/04/27/09-07-10/QQ20240428-140911@2x.png" class></p>
<p>霍夫变化的复杂性：<br>空间复杂性和时间复杂性都很大</p>
<p>直线的霍夫空间：<br>无噪音的直线<br><img src="/2024/04/27/09-07-10/QQ20240428-141342@2x.png" class><br>最亮的点：是像素最多的直线</p>
<p>正方形的霍夫空间：<br><img src="/2024/04/27/09-07-10/QQ20240428-141150@2x.png" class></p>
<h4 id="噪音对霍夫变化的影响"><a href="#噪音对霍夫变化的影响" class="headerlink" title="噪音对霍夫变化的影响"></a>噪音对霍夫变化的影响</h4><img src="/2024/04/27/09-07-10/QQ20240428-141725@2x.png" class>
<p>峰值不明确了</p>
<p>更多的噪音：<br><img src="/2024/04/27/09-07-10/QQ20240428-141839@2x.png" class></p>
<h4 id="霍夫变化的扩展"><a href="#霍夫变化的扩展" class="headerlink" title="霍夫变化的扩展"></a>霍夫变化的扩展</h4><p>使用<strong>梯度</strong>优化霍夫变换算法<br><img src="/2024/04/27/09-07-10/QQ20240428-142259@2x.png" class></p>
<blockquote>
<p>优化的霍夫变换的地方？</p>
</blockquote>
<p>扩展二：<br>改变阀值<br>扩展三：<br>改变投票箱的大小<br>扩展四：<br>相同的操作可以用的其他形状</p>
<blockquote>
<p>课上的算法对现实是不起作用的<br>论文中的算法也是不起作用，要把故事讲得好听<br>关键是理解原理</p>
</blockquote>
<h2 id="霍夫变换（Hough-transform）-圆"><a href="#霍夫变换（Hough-transform）-圆" class="headerlink" title="霍夫变换（Hough transform）:圆"></a>霍夫变换（Hough transform）:圆</h2><p>圆的方程：$\left(x_i-a\right)^2+\left(y_i-b\right)^2=r^2$</p>
<p>圆的霍夫空间：<br><img src="/2024/04/27/09-07-10/QQ20240429-091353@2x.png" class></p>
<p>使用更大的投票箱：<br><img src="/2024/04/27/09-07-10/QQ20240429-091756@2x.png" class></p>
<h4 id="圆的霍夫变换"><a href="#圆的霍夫变换" class="headerlink" title="圆的霍夫变换"></a>圆的霍夫变换</h4><p>投票箱在3D空间中：<br><img src="/2024/04/27/09-07-10/QQ20240429-091605@2x.png" class></p>
<p>梯度优化：<br><img src="/2024/04/27/09-07-10/QQ20240429-092109@2x.png" class></p>
<ul>
<li>过这点的圆心只可能在：点和圆心的连线，即梯度垂线上</li>
<li>这样就过滤掉其他位置的圆心投票</li>
</ul>
<p>算法：<br><img src="/2024/04/27/09-07-10/QQ20240429-091847@2x.png" class></p>
<h4 id="投票的实用技巧"><a href="#投票的实用技巧" class="headerlink" title="投票的实用技巧"></a>投票的实用技巧</h4><ul>
<li>剪枝，不要投没用得票</li>
<li>选一个合理的投票网格<ul>
<li>太大，投的错误太多</li>
<li>太小，噪音会影响</li>
</ul>
</li>
<li>投票给临近的投票箱，类似累加器平滑</li>
<li>使用梯度优化投票</li>
</ul>
<h4 id="优点缺点"><a href="#优点缺点" class="headerlink" title="优点缺点"></a>优点缺点</h4><p>优点：</p>
<ul>
<li>每个像素投票都是独立的，不受遮挡影响</li>
<li>噪音也不影响</li>
<li>可以在单图像找多形状</li>
</ul>
<p>缺点：</p>
<ul>
<li>参数的复杂性</li>
<li>非常规形状，投票很复杂</li>
</ul>
<h2 id="广义霍夫变换"><a href="#广义霍夫变换" class="headerlink" title="广义霍夫变换"></a>广义霍夫变换</h2><h4 id="非分析模型"><a href="#非分析模型" class="headerlink" title="非分析模型"></a>非分析模型</h4><p>霍夫表：<br>用于给非规则形状投票</p>
<p>建表：<br><img src="/2024/04/27/09-07-10/QQ20240429-093857@2x.png" class></p>
<ul>
<li>$c$：是一个定位点</li>
<li>$r$：位移矢量</li>
<li>$\theta_1$： 梯度</li>
<li>把$r$放入由$\theta$索引的表</li>
</ul>
<p>识别：</p>
<img src="/2024/04/27/09-07-10/QQ20240429-093933@2x.png" class>
<ul>
<li>计算边界点的梯度方向$\theta$</li>
<li>在表格中找出所有该方向的位移向量</li>
<li>给位移向量的终点投票？</li>
</ul>
<p>样例：</p>
<img src="/2024/04/27/09-07-10/QQ20240429-101040@2x.png" class>
<p>底部的边有相同的$\theta$，但是不一样的位移，对所有的位移投票。</p>
<img src="/2024/04/27/09-07-10/QQ20240429-101252@2x.png" class>
<p>当下这个像素点的位移图，来源于它相同$\theta$的投票，做了平移，他们公用一个索引点，最终底部的所有像素点投票得出了一条直线。</p>
<img src="/2024/04/27/09-07-10/QQ20240429-101605@2x.png" class>
<p>这条直线中，中心的投票数是最多的，通过对另一条边做相同操作，交点为中心点。</p>
<p>算法：默认不知道方向</p>
<img src="/2024/04/27/09-07-10/QQ20240429-102147@2x.png" class>
<p>不知道比例：<br><img src="/2024/04/27/09-07-10/QQ20240429-102238@2x.png" class></p>
<p><strong>识别中的应用</strong><br>基于视觉代码的索引：<br><img src="/2024/04/27/09-07-10/QQ20240429-102508@2x.png" class></p>
<p>生成代码块：<br><img src="/2024/04/27/09-07-10/QQ20240429-102615@2x.png" class><br>像素按蔟分类</p>
<p>生成特征：<br><img src="/2024/04/27/09-07-10/QQ20240429-102705@2x.png" class></p>
<p>生成位移投票：<br><img src="/2024/04/27/09-07-10/QQ20240429-102750@2x.png" class><br>在投票箱中，相同特征的投票数量就是特征的个数。<br>看轮胎投了几票</p>
<h2 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h2><p>计算视觉将图像视为数据，而不是信号，但频率分析的思想很有用。</p>
<p>basis sets：标准基(线代的定义)</p>
<p>应用到图像的标准基：<br>把图像视为非常大的空间中的点，例如一个N*N的图像，可以认为是一个一维的向量。<br>另一种做法：<br><img src="/2024/04/27/09-07-10/QQ20240429-140527@2x.png" class></p>
<ul>
<li>同一位置的图像：类似正弦余弦</li>
</ul>
<p>上面就是<strong>傅里叶基</strong>，递增的是频率</p>
<h4 id="正弦和"><a href="#正弦和" class="headerlink" title="正弦和"></a>正弦和</h4><p>公式：$A\sin(\omega x^2+\varphi^2)$<br>共有三个自由度：</p>
<ul>
<li>$A$：幅度</li>
<li>$\omega$：频率</li>
<li>$\varphi$：相位</li>
</ul>
<img src="/2024/04/27/09-07-10/QQ20240429-141002@2x.png" class>
<p>频率是最重要的，频率越高，摆动速度越快</p>
<h4 id="时间和频率"><a href="#时间和频率" class="headerlink" title="时间和频率"></a>时间和频率</h4><p>$g(t)=\sin(2\pi ft)+\frac13\sin(2\pi(3f)t)$<br><img src="/2024/04/27/09-07-10/QQ20240429-141307@2x.png" class></p>
<p>一种绘制光谱的方式</p>
<img src="/2024/04/27/09-07-10/QQ20240429-141409@2x.png" class>
<p>f越大，越接近方波</p>
<p>方波的正弦公式：$A\sum\limits_{k=1}^{\infty}\frac{1}{k}\sin\left(2\pi kt\right)$<br>图像：<br><img src="/2024/04/27/09-07-10/QQ20240429-141517@2x.png" class><br>不考虑相位</p>
<h4 id="傅里叶变换-1"><a href="#傅里叶变换-1" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h4><p>$f(x)\longrightarrow\boxed{\begin{array}{c}\text{Fourier}\\\text{Transform}\end{array}}\longrightarrow F(\omega)$</p>
<p>复数：$\begin{aligned}F\left(\omega\right)&amp;=R\left(\omega\right)+iI\left(\omega\right)\end{aligned}$<br>性质：</p>
<ul>
<li>$A = \pm\sqrt{R \left( \omega \right)^2 + I \left( \omega \right)^2}$</li>
<li>$\varphi^2=\tan^{-1}\frac{I\left(\omega\right)}{R\left(\omega\right)}$</li>
</ul>
<p>虚部是奇函数-正弦，实部是偶函数-余弦。</p>
<h4 id="计算傅里叶变换"><a href="#计算傅里叶变换" class="headerlink" title="计算傅里叶变换"></a>计算傅里叶变换</h4><p>基本性质：<br>$\int_{-\infty}^\infty\sin(ax+\phi)\sin(bx+\varphi)dx=0,\mathrm{~if~}a\neq b$</p>
<p>$\int_{-\infty}^\infty\sin(\alpha x+\phi)\sin(\alpha x+\varphi)dx=\pm\infty$</p>
<p>例子：<br>假设一个展开为余弦的函数：<br>$f\left(x\right)=\cos\left(2\pi\omega x\right)$</p>
<p>$C\left(u\right)=\int_{-\infty}^{\infty}f\left(x\right)\cos\left(2\pi ux\right)dx$</p>
<ul>
<li>如果$u == \omega$：积分为无穷</li>
<li>如果$u \neq \omega$：积分为0 </li>
</ul>
<p>得到脉冲图：<br><img src="/2024/04/27/09-07-10/QQ20240429-143042@2x.png" class></p>
<p>被称作与余弦对应的脉冲</p>
<p>如果是正弦，左边将会是负的脉冲：$\sin(-x) = -\sin(x)$</p>
<h4 id="傅里叶变换通用定义"><a href="#傅里叶变换通用定义" class="headerlink" title="傅里叶变换通用定义"></a>傅里叶变换通用定义</h4><p>定义：$F\left(u^2\right)=\int_{-\infty}^{\infty}f\left(x^2\right)e^{-i2\pi ux}dx$<br>其中：$e^{ik}=\text{ cos }k+i\sin k\quad i=\sqrt{-1}$</p>
<p>作用：从空间域转换到频域</p>
<p>傅里叶逆变换：$f\left(x\right)=\int_{-\infty}^\infty F\left(u\right)e^{i2\pi ux}du$<br>可以恢复信号</p>
<h4 id="频谱"><a href="#频谱" class="headerlink" title="频谱"></a>频谱</h4><p>常用频谱：<br><img src="/2024/04/27/09-07-10/QQ20240429-144050@2x.png" class><br>最后一个是幂频谱</p>
<p>频谱图：<br><img src="/2024/04/27/09-07-10/QQ20240429-144135@2x.png" class></p>
<p>局限性：<br>傅里叶级数积分有界</p>
<h4 id="傅里叶变换到傅里叶级数"><a href="#傅里叶变换到傅里叶级数" class="headerlink" title="傅里叶变换到傅里叶级数"></a>傅里叶变换到傅里叶级数</h4><p>级数：$F\left(k\right)=\frac1N\sum_{x=0}^{x=N-1}f\left(x\right)e^{-i\frac{2\pi kx}N}$</p>
<p>积分的另一种写法：离散化</p>
<h4 id="二维的傅里叶"><a href="#二维的傅里叶" class="headerlink" title="二维的傅里叶"></a>二维的傅里叶</h4><p>$F\left(u,\nu\right)=\int_{-\infty}^\infty\int_{-\infty}^\infty f\left(x,y\right)e^{-i2\pi\left(ux+\nu y\right)}dxdy\frac12$</p>
<p>离散：$F\left(k_x,k_y\right)=\frac1N\sum_{x=0}^{x=N-1}\sum_{y=0}^{y=N-1}f\left(x,y\right)e^{-i\frac{2\pi\left(k_xx+k_yy\right)}N}$</p>
<p>例子：<br>一个正弦曲线，处于特定的频率， 仅仅有垂直线组成：<br><img src="/2024/04/27/09-07-10/QQ20240429-144821@2x.png" class></p>
<p>右图是它的傅里叶频谱和功能谱。<br>亮点就是频率的尖峰</p>
<p>一个余弦曲线，频率更快：<br><img src="/2024/04/27/09-07-10/QQ20240429-145050@2x.png" class></p>
<blockquote>
<p>频率分量更高，峰值靠外</p>
</blockquote>
<p>线性：傅里叶变换是一个线性变换<br><img src="/2024/04/27/09-07-10/QQ20240429-145222@2x.png" class></p>
<h4 id="真实图像的光谱"><a href="#真实图像的光谱" class="headerlink" title="真实图像的光谱"></a>真实图像的光谱</h4><img src="/2024/04/27/09-07-10/QQ20240429-145348@2x.png" class>
<blockquote>
<p>自然图像有相同的光谱<br>重建图像才需要相位<br>用部分光谱重建图像，图像的变化如右图<br>高频率：告诉边缘在哪里，高频越亮，越清晰<br>明亮线垂直于轮廓线</p>
</blockquote>
<img src="/2024/04/27/09-07-10/QQ20240429-145542@2x.png" class>
<p>人造场景：</p>
<img src="/2024/04/27/09-07-10/QQ20240429-145613@2x.png" class>
<p>思考：</p>
<ul>
<li>如果不是周期函数，光谱会有倾斜</li>
<li></li>
</ul>
<h2 id="频率分析中的卷积"><a href="#频率分析中的卷积" class="headerlink" title="频率分析中的卷积"></a>频率分析中的卷积</h2><h4 id="傅里叶变换与卷积"><a href="#傅里叶变换与卷积" class="headerlink" title="傅里叶变换与卷积"></a>傅里叶变换与卷积</h4><p>卷积：$g = f * h$<br>傅里叶变换：<br>\begin{split}<br>G\left(u\right)&amp;=\int_{-\infty}^{\infty}g\left(x\right)e^{-i2\pi ux}dx \\<br>&amp;=\int_{-\infty}^\infty\int_{-\infty}^\infty f\left(\tau\right)h\left(x-\tau\right)e^{-i2\pi ux}d\tau dx \\<br>&amp;=\int_{-\infty}^\infty\int_{-\infty}^\infty\left[f\left(\tau\right)e^{-i2\pi u\tau}d\tau\right]\left[h\left(x-\tau\right)e^{-i2\pi u\left(x-\tau\right)}dx\right] \\<br>&amp;=\int_{-\infty}^{\infty}\left[f\left(\tau\right)e^{-i2\pi u\tau}d\tau\right]\int_{-\infty}^{\infty}\left[h\left(x^{\prime}\right)e^{-i2\pi ux^{\prime}}dx^{\prime}\right] \\<br>&amp;=F\left(\begin{array}{c}u\\\end{array}\right)H\left(\begin{array}{c}u\\\end{array}\right)<br>\end{split}</p>
<img src="/2024/04/27/09-07-10/QQ20240429-152239@2x.png" class>
<p><strong>空间中乘积$\Leftrightarrow$频率空间中卷积</strong></p>
<blockquote>
<p>大掩码的空间域卷积很复杂，用快速傅里叶变换可以转换为频率域乘法，避免卷积。</p>
</blockquote>
<h4 id="平滑和模糊中的应用"><a href="#平滑和模糊中的应用" class="headerlink" title="平滑和模糊中的应用"></a>平滑和模糊中的应用</h4><img src="/2024/04/27/09-07-10/QQ20240429-152734@2x.png" class>
<img src="/2024/04/27/09-07-10/QQ20240429-152812@2x.png" class>
<p><strong>瘦高斯的傅里叶是胖的</strong><br>原因：<br>如果高斯很瘦，就希望保留所有的频率，傅里叶就很胖。<br>高斯很胖，模糊了一切，就只保留一点点低频率，几乎没有高频率。<br>作用：<br>保存低频率，降低高频率。</p>
<blockquote>
<p>空间采样频率越低，傅里叶频率越高.<br>傅里叶的放缩原理</p>
</blockquote>
<p>效果图；<br><img src="/2024/04/27/09-07-10/QQ20240429-153249@2x.png" class></p>
<h4 id="傅里叶变换的性质"><a href="#傅里叶变换的性质" class="headerlink" title="傅里叶变换的性质"></a>傅里叶变换的性质</h4><img src="/2024/04/27/09-07-10/QQ20240429-153642@2x.png" class>
<p>傅里叶对：<br><img src="/2024/04/27/09-07-10/QQ20240429-153804@2x.png" class></p>
<h2 id="混叠-Aliasing"><a href="#混叠-Aliasing" class="headerlink" title="混叠 Aliasing"></a>混叠 Aliasing</h2><p>傅里叶基是如何混叠的？</p>
<h4 id="脉冲串的概念"><a href="#脉冲串的概念" class="headerlink" title="脉冲串的概念"></a>脉冲串的概念</h4><p>脉冲串的傅里叶变换是另一个脉冲串<br><img src="/2024/04/27/09-07-10/QQ20240429-154529@2x.png" class></p>
<blockquote>
<p>空间中脉冲距离越远，频率中脉冲越接近</p>
</blockquote>
<h4 id="采样和重构"><a href="#采样和重构" class="headerlink" title="采样和重构"></a>采样和重构</h4><img src="/2024/04/27/09-07-10/QQ20240429-154812@2x.png" class>
<p>采样是计算机如何存储连续信号提出的,如何重建原始信号？<br><img src="/2024/04/27/09-07-10/QQ20240429-154933@2x.png" class></p>
<p>在离散位置如何将信号连续起来。</p>
<p>样例：<br><img src="/2024/04/27/09-07-10/QQ20240429-155039@2x.png" class></p>
<p>采样密度不够，无法恢复图像。</p>
<p>如何防止锯齿出现？<br>过滤高频率，降低采样量。</p>
<h4 id="脉冲"><a href="#脉冲" class="headerlink" title="脉冲"></a>脉冲</h4><p>一维脉冲函数：$comb_M[x]=\sum_{k=-\infty}^{\infty}\delta[x-kM]$<br>图像：<br><img src="/2024/04/27/09-07-10/QQ20240429-155942@2x.png" class></p>
<p>二维脉冲函数：$comb_{M,N}(x,y)\equiv\sum_{k=-\infty}^\infty\sum_{l=-\infty}^\infty\delta\left(x-kM,y-lN\right)$<br>傅里叶变换：<br>$\sum_{k=-\infty}^n\sum_{l=-\infty}^n\delta\left(x-kM,y-lN\right)\Leftrightarrow\frac1{MN}\sum_{k=-\infty}^\infty\sum_{l=-\infty}^\infty\delta\left(u-\frac kM,\nu-\frac lN\right)$</p>
<h4 id="采样低频信号"><a href="#采样低频信号" class="headerlink" title="采样低频信号"></a>采样低频信号</h4><img src="/2024/04/27/09-07-10/QQ20240429-160402@2x.png" class>
<p>采样：连续信号乘离散梳函数</p>
<p>如果功能频率$W$满足：$W&lt;\frac1{2M}$，就可以恢复。</p>
<blockquote>
<p>这里有一些采样定理的内容</p>
</blockquote>
<h4 id="采样高频信号"><a href="#采样高频信号" class="headerlink" title="采样高频信号"></a>采样高频信号</h4><img src="/2024/04/27/09-07-10/QQ20240429-160831@2x.png" class>
<p>高频信号采样重叠了，采样之前，必须去处高频，以防重叠。</p>
<p>如何去处高频？<br><strong>梳状滤波器</strong><br><img src="/2024/04/27/09-07-10/QQ20240429-161033@2x.png" class></p>
<h4 id="图像中的重叠"><a href="#图像中的重叠" class="headerlink" title="图像中的重叠"></a>图像中的重叠</h4><p>图像缩小，如果直接丢掉行列，图像会变小。放大图像后，图像不像是缩小的图像，很模糊。</p>
<p>正确的做法是：用高斯滤波。<br><img src="/2024/04/27/09-07-10/QQ20240429-161449@2x.png" class><br>高斯滤波做了抗锯齿，所以效果更好。</p>
<h4 id="对比度敏感"><a href="#对比度敏感" class="headerlink" title="对比度敏感"></a>对比度敏感</h4><img src="/2024/04/27/09-07-10/QQ20240429-161728@2x.png" class>
<p>人类的视觉对高频不敏感。</p>
<h4 id="JPEG"><a href="#JPEG" class="headerlink" title="JPEG"></a>JPEG</h4><p>DCT：离散余弦变换<br>不保留高频信息，降低图像质量时，只保存低频率信息。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title>Ud810 Intro-to-cv 笔记汇总</title>
    <url>/2024/04/27/08-42-03/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.27：初稿</li>
<li>24.05.06：暂停</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/27/08-38-57/" title="计算机视觉概论 什么是计算机视觉">计算机视觉概论笔记-第一章 什么是计算机视觉</a></li>
<li><a href="/2024/04/27/09-07-10/" title="计算机视觉概论 计算机视觉的图像处理">计算机视觉概论笔记-第二章 计算机视觉的图像处理</a></li>
<li><a href="/2024/04/29/16-24-21/" title="计算机视觉概论 相机和图像">计算机视觉概论笔记-第三章 相机和图像</a></li>
<li><a href="/2024/05/02/13-20-59/" title="计算机视觉概论 图像特征">计算机视觉概论笔记-第四章 图像特征</a></li>
<li><a href="/2024/05/03/12-11-15/" title="计算机视觉概论 光反射成像，亮度，阴影">计算机视觉概论笔记-第五章 图像</a></li>
<li></li>
</ul>
<h2 id="课程简介"><a href="#课程简介" class="headerlink" title="课程简介"></a>课程简介</h2><ol>
<li>介绍</li>
<li>计算机视觉的图像处理</li>
<li>相机和相机模型的几何结构以及多视角</li>
<li>图像特征与匹配</li>
<li>图像如何形成，光与纹理，传感器如何形成图像</li>
<li>图像运动序列</li>
<li>图像运动跟踪</li>
<li>分类和识别</li>
<li>计算机视觉简单的模式识别 </li>
<li>人类视觉系统-生物方向</li>
</ol>
<h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><p><a href="https://docs.google.com/spreadsheets/d/1ecUGIyhYOfQPi3HPXb-7NndrLgpX_zgkwsqzfqHPaus/pubhtml">课程PPT，作业</a><br><a href="https://learn.udacity.com/courses/ud810">课程视频</a></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉概论 什么是计算机视觉</title>
    <url>/2024/04/27/08-38-57/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.27：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/27/08-42-03/" title="Ud810 Intro-to-cv 笔记汇总">计算机视觉概率-笔记汇总</a>
</li>
</ul>
<h2 id="计算机视觉不是图像处理"><a href="#计算机视觉不是图像处理" class="headerlink" title="计算机视觉不是图像处理"></a>计算机视觉不是图像处理</h2><h3 id="大脑进行构造图像"><a href="#大脑进行构造图像" class="headerlink" title="大脑进行构造图像"></a>大脑进行构造图像</h3><p><strong>球的运动</strong></p>
<img src="/2024/04/27/08-38-57/QQ20240427-084559@2x.png" class>
<p>小球做了相同的运动，只有阴影移动不同，我们认为小球飞起来了</p>
<img src="/2024/04/27/08-38-57/QQ20240427-084850@2x.png" class>
<p>大脑在描述看到的视觉</p>
<h3 id="计算机视觉是什么"><a href="#计算机视觉是什么" class="headerlink" title="计算机视觉是什么"></a>计算机视觉是什么</h3><img src="/2024/04/27/08-38-57/QQ20240427-085557@2x.png" class>
<p>计算机视觉: 计算模型，算法，真实图像的三种思考方式三角形</p>
<h3 id="课程简介"><a href="#课程简介" class="headerlink" title="课程简介"></a>课程简介</h3><ol>
<li>介绍</li>
<li>计算机视觉的图像处理</li>
<li>相机和相机模型的几何结构以及多视角</li>
<li>图像特征与匹配</li>
<li>图像如何形成，光与纹理，传感器如何形成图像</li>
<li>图像运动序列</li>
<li>图像运动跟踪</li>
<li>分类和识别</li>
<li>计算机视觉简单的模式识别 </li>
<li>人类视觉系统-生物方向</li>
</ol>
<h3 id="课程作业"><a href="#课程作业" class="headerlink" title="课程作业"></a>课程作业</h3><p>一共8次作业，PS1是简单的图像处理，PS2-8是课程设计作业。</p>
<p>使用Python-OpenCV，不需要matlab</p>
<ul>
<li>代码必须自己写，要理解相关代码</li>
<li>为什么要写书中存在的代码？<br>“当代码不起作用的时候，才会知道为什么”<br>书中的代码通常不起作用或效果不佳。</li>
</ul>
<h3 id="软件相关"><a href="#软件相关" class="headerlink" title="软件相关"></a>软件相关</h3><p>计算机视觉相关图像处理<br>使用Python和OpenCV，完成课程作业。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉, 图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title>人体姿态估计算法演变历史</title>
    <url>/2024/04/26/13-54-29/</url>
    <content><![CDATA[<h1 id="人体姿态估计算法演变过程"><a href="#人体姿态估计算法演变过程" class="headerlink" title="人体姿态估计算法演变过程"></a>人体姿态估计算法演变过程</h1><h2 id="传统视觉算法"><a href="#传统视觉算法" class="headerlink" title="传统视觉算法"></a>传统视觉算法</h2><p>基于两种，一种采用电磁标记坐标，另一种采用基于视觉的无标记分析<sup><a href="#fn_1" id="reffn_1">1</a></sup>。<br>研究可以分为两大类：基于模型（或生成）和无模型（或判别）方法。基于模型的方法采用先验的人体。姿态估计过程包括建模和估计。建模是似然函数的构造，考虑了相机模型、图像描述符、人体模型和匹配函数以及（物理）约束。</p>
<h3 id="基于生成模型方法"><a href="#基于生成模型方法" class="headerlink" title="基于生成模型方法"></a>基于生成模型方法</h3><p><strong>建模</strong><br>建模阶段的目标是在给定一组参数的情况下构建给出图像可能性的函数,参数包括，身体配置，体型和外观参数以及相机视角。</p>
<p>基于模型的方法使用人体模型，其中包括运动学结构和身体尺寸。此外，还使用一个函数描述了给定模型参数的人体如何在图像域中出现。图像通常不是使用原始的视觉输入，而是用边缘、颜色区域或轮廓来描述。需要视觉输入和人体模型生成外观之间的匹配函数来评估模型实例化对视觉输入的解释程度。</p>
<p><strong>运动学模型</strong></p>
<ul>
<li>运动学模型有关节组成，关节包含多个自由度(DOF),指示关节可以在多少个方向上移动。</li>
</ul>
<h3 id="基于无模型判别方法"><a href="#基于无模型判别方法" class="headerlink" title="基于无模型判别方法"></a>基于无模型判别方法</h3><h1 id="人体姿态估计算法"><a href="#人体姿态估计算法" class="headerlink" title="人体姿态估计算法"></a>人体姿态估计算法</h1><h2 id="传统计算机视觉方法-1990-2000"><a href="#传统计算机视觉方法-1990-2000" class="headerlink" title="传统计算机视觉方法 1990-2000"></a>传统计算机视觉方法 1990-2000</h2><h3 id="直接线性变换算法DLT-Direct-Linear-Transform"><a href="#直接线性变换算法DLT-Direct-Linear-Transform" class="headerlink" title="直接线性变换算法DLT(Direct Linear Transform)"></a>直接线性变换算法DLT(Direct Linear Transform)</h3><p>&gt;</p>
<blockquote>
<p>d</p>
</blockquote>
<h3 id="透视n点-PnP-算法-Perspective-n-Point"><a href="#透视n点-PnP-算法-Perspective-n-Point" class="headerlink" title="透视n点(PnP) 算法(Perspective-n-Point)"></a>透视n点(PnP) 算法(Perspective-n-Point)</h3><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1].Pope, R. 《Vision-based human motion analysis: An overview》. Computer Vision and Image Understanding 108, 期 1–2 (2007年): 4–18. <a href="https://doi.org/10.1016/J.CVIU.2006.10.016">https://doi.org/10.1016/J.CVIU.2006.10.016</a>.</p>
]]></content>
      <categories>
        <category>人体姿态估计</category>
      </categories>
      <tags>
        <tag>人体姿态估计, 论文</tag>
      </tags>
  </entry>
  <entry>
    <title>人体姿态估计相关链接</title>
    <url>/2024/04/26/12-29-38/</url>
    <content><![CDATA[<h1 id="人体姿态估计"><a href="#人体姿态估计" class="headerlink" title="人体姿态估计"></a>人体姿态估计</h1><h2 id="github仓库"><a href="#github仓库" class="headerlink" title="github仓库"></a>github仓库</h2><h2 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h2><ul>
<li><a href="https://2d3d.ai">2030AI-Peter</a><ul>
<li><a href="https://2d3d.ai/index.php/2020/06/14/human-pose-estimation-hrnet/">HPE沙漏模型 HRNet + HigherHRNet</a></li>
</ul>
</li>
</ul>
<h2 id="知乎"><a href="#知乎" class="headerlink" title="知乎"></a>知乎</h2><h2 id="网页"><a href="#网页" class="headerlink" title="网页"></a>网页</h2><ul>
<li><a href="https://blog.roboflow.com/pose-estimation-algorithms-history/">姿态估计算法：历史与演变</a><blockquote>
<p>从传统算法到深度学习算法</p>
</blockquote>
</li>
</ul>
<h2 id="youtube"><a href="#youtube" class="headerlink" title="youtube"></a>youtube</h2><ul>
<li><a href="https://www.youtube.com/playlist?list=PLoEMreTa9CNmPGaVQYDWydc2ZmEt6zdrV">Pose Estimation | Applied Deep Learning</a><blockquote>
<p>解读深度学习算法的相关论文</p>
</blockquote>
</li>
</ul>
<h2 id="课题组"><a href="#课题组" class="headerlink" title="课题组"></a>课题组</h2><h3 id="北京大学王亦洲"><a href="#北京大学王亦洲" class="headerlink" title="北京大学王亦洲"></a>北京大学王亦洲</h3><blockquote>
<p>三维人体姿态估计</p>
</blockquote>
<h3 id="北京大学刘宏"><a href="#北京大学刘宏" class="headerlink" title="北京大学刘宏"></a>北京大学刘宏</h3><ul>
<li>课题组 <a href="https://robotics.pkusz.edu.cn/">网页</a></li>
</ul>
<h4 id="李文豪-博士生"><a href="#李文豪-博士生" class="headerlink" title="李文豪-博士生"></a>李文豪-博士生</h4><ul>
<li>个人简介<a href="https://vegetebird.github.io/">主页</a><blockquote>
<p>3D人体姿态估计</p>
</blockquote>
</li>
</ul>
<h3 id="哈工大张盛平"><a href="#哈工大张盛平" class="headerlink" title="哈工大张盛平"></a>哈工大张盛平</h3><ul>
<li>课题组 <a href="https://homepage.hit.edu.cn/zhangshengping">网页</a><blockquote>
<p>3D人体姿态估计</p>
</blockquote>
</li>
</ul>
<h3 id="厦门大学曾鸣"><a href="#厦门大学曾鸣" class="headerlink" title="厦门大学曾鸣"></a>厦门大学曾鸣</h3><ul>
<li>课题组<a href="https://vcg.xmu.edu.cn/">网页</a></li>
</ul>
<h4 id="郑英林-博士生MSRA"><a href="#郑英林-博士生MSRA" class="headerlink" title="郑英林-博士生MSRA"></a>郑英林-博士生MSRA</h4><ul>
<li>个人简介<a href="https://yinglinzheng.netlify.app/">主页</a><blockquote>
<p>多人人体姿态估计，人脸识别</p>
</blockquote>
</li>
</ul>
<h1 id="相关工具"><a href="#相关工具" class="headerlink" title="相关工具"></a>相关工具</h1><h2 id="论文可视化探索"><a href="#论文可视化探索" class="headerlink" title="论文可视化探索"></a>论文可视化探索</h2><p><a href="https://www.connectedpapers.com/">connectedpapers</a></p>
]]></content>
      <categories>
        <category>人体姿态估计</category>
      </categories>
      <tags>
        <tag>人体姿态估计, HPE, 工具, 论文</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习笔记汇总</title>
    <url>/2024/04/24/12-18-34/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a></li>
<li><a href="/2024/04/23/11-01-00/" title="00-深度学习预告">00-预告</a></li>
<li><a href="/2024/04/23/11-01-01/" title="01-深度学习课程安排">01-课程安排</a></li>
<li><a href="/2024/04/23/11-01-02/" title="02-深度学习介绍">02-深度学习介绍</a></li>
<li><a href="/2024/04/23/11-01-03/" title="03-深度学习安装">03-安装</a></li>
<li><a href="/2024/04/23/11-01-04/" title="04-数据读取和操作">04-数据读取和操作</a></li>
<li><a href="/2024/04/23/11-01-05/" title="05-线性代数">05-线性代数</a></li>
<li><a href="/2024/04/23/11-01-06/" title="06-矩阵计算">06-矩阵计算</a></li>
<li><a href="/2024/04/23/11-01-07/" title="07-链式法则与自动求导">07-链式法则与自动求导</a></li>
<li><a href="/2024/04/23/11-01-08/" title="08-线性回归+基础优化算法">08-线性回归+基础优化算法</a></li>
<li><a href="/2024/04/23/11-01-09/" title="09-softmax回归">09-softmax回归</a></li>
<li><a href="/2024/04/23/11-01-10/" title="10-多层感知机">10-多层感知机</a></li>
<li><a href="/2024/04/23/11-01-11/" title="11-模型选择+过拟合和欠拟合">11-模型选择+过拟合和欠拟合</a></li>
<li><a href="/2024/04/23/11-01-12/" title="12 权重衰退 Weight Decay">12-权重衰退</a></li>
<li><a href="/2024/04/23/11-01-13/" title="13-丢弃法">13-丢弃法</a></li>
<li><a href="/2024/04/23/11-01-14/" title="14-数值稳定性+模型初始化和激活函数">14-数值稳定性</a></li>
<li><a href="/2024/04/23/11-01-15/" title="15-实战Kaggle比赛：预测房价">15-实战Kaggle比赛：预测房价</a>
</li>
</ul>
<blockquote>
<p>暂停，开始读论文</p>
</blockquote>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><strong>d2l官网</strong>：<a href="https://zh-v2.d2l.ai/index.html">动手学深度学习</a><br><strong>笔记来源</strong>：<a href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/tree/main">DeepLearning-MuLi-Notes</a><br><strong>小土堆笔记</strong>：<a href="https://github.com/AccumulateMore/CV/tree/main">含课程代码注释</a></p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ul>
<li><code>!pip install git+https://github.com/d2l-ai/d2l-zh@release  # installing d2l</code>下载较慢，使用<code>!pip install d2l==1.0.3</code>。</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记五 神经网络的学习</title>
    <url>/2024/04/23/12-43-40/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.23：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2024/04/18/15-40-33/" title="机器笔记汇总">吴恩达机器学习 - 笔记汇总</a>
</li>
</ul>
<h1 id="9-神经网络-学习（Neural-Networks-Learning）"><a href="#9-神经网络-学习（Neural-Networks-Learning）" class="headerlink" title="9 神经网络: 学习（Neural Networks: Learning）"></a>9 神经网络: 学习（Neural Networks: Learning）</h1><h2 id="9-1-代价函数（Cost-Function）"><a href="#9-1-代价函数（Cost-Function）" class="headerlink" title="9.1 代价函数（Cost Function）"></a>9.1 代价函数（Cost Function）</h2><p>神经网络的分类问题有两种：</p>
<ul>
<li><p>二元分类问题（0/1分类）</p>
<p>只有一个输出单元（$K=1$）</p>
</li>
<li><p>多元（$K$）分类问题</p>
<p>输出单元不止一个（$K\gt1$）</p>
</li>
</ul>
<p>神经网络的代价函数公式：</p>
<p>$h_\Theta(x) = a^{(L)} = g(\Theta^{(L-1)}a^{(L-1)}) = g(z^{(L)})$</p>
<p>$$ \begin{split} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1} } ( \Theta_{j,i}^{(l)})^2\end{split}$$</p>
<blockquote>
<p>$L$: 神经网络的总层数</p>
<p>$s_l$: 第 $l$ 层激活单元的数量（不包含偏置单元）</p>
<p>$h_\Theta(x)_k$: 分为第 $k$ 个分类($k^{th}$)的概率 $P(y=k | x ; \Theta) $</p>
<p>$K$: 输出层的输出单元数量，即类数 - 1</p>
<p>$y_k^{(i)}$: 第 $i$ 个训练样本的第 $k$ 个分量值</p>
<p>$y$: $K$ 维向量</p>
</blockquote>
<p>对照下逻辑回归中的代价函数：</p>
<p>$$<br>J(\theta) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2<br>$$</p>
<p>在神经网络的代价函数中，</p>
<ul>
<li>左边的变化实际上是为了求解 $K$ 分类问题，即公式会对每个样本特征都运行 $K$ 次，并依次给出分为第 $k$ 类的概率，$h_\Theta(x)\in \mathbb{R}^{K}, y \in \mathbb{R}^{K}$。</li>
<li>右边的正则化项比较容易理解，每一层有多维矩阵 $\Theta^{(l)}\in \mathbb{R}^{(s_l + 1)\times s_{l+1} }$，从左到右看这个三次求和式 $\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l+1} }$ ，就是对每一层间的多维矩权重 $\Theta^{(l)}$ ，依次平方后求取其除了偏置权重部分的和值，并循环累加即得结果。</li>
</ul>
<blockquote>
<p>$\mathbb{R}^{m}$: 即 $m$ 维向量</p>
<p>$\mathbb{R}^{m\times n}$: 即 $m \times n$ 维矩阵</p>
</blockquote>
<p>再次可见，神经网络背后的思想是和逻辑回归一样的，但由于计算复杂，实际上神经网络的代价函数 $J(\Theta)$ 是一个非凸（non-convex）函数。</p>
<h2 id="9-2-反向传播算法（Backpropagation-Algorithm）"><a href="#9-2-反向传播算法（Backpropagation-Algorithm）" class="headerlink" title="9.2 反向传播算法（Backpropagation Algorithm）"></a>9.2 反向传播算法（Backpropagation Algorithm）</h2><p>类似于回归模型中的梯度下降算法，为了求解神经网络最优化问题，我们也要计算 $\frac{\partial}{\partial\Theta}J(\Theta)$，以此 $\underset{\Theta}{\text{minimize} }J(\Theta)$ 。</p>
<p>在神经网络中，代价函数看上去虽然不复杂，但要注意到其中 $h_\Theta(x)$ 的求取实际上是由前向传播算法求得，即需从输入层开始，根据每层间的权重矩阵 $\Theta$ 依次计算激活单元的值 $a$。 在最优化代价函数时，我们必然也需要最优化每一层的权重矩阵，再次强调一下，<strong>算法最优化的是权重，而不是输入</strong>。</p>

<p><strong>反向传播算法</strong>用于计算每一层权重矩阵的偏导 $\frac{\partial}{\partial\Theta}J(\Theta)$，算法实际上是对代价函数求导的拆解。</p>
<ol>
<li><p>对于给定训练集 $\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$ ，初始化每层间的误差和矩阵 $\Delta$，即令所有的 $\Delta^{(l)}_{i,j}=0$，使得每个 $\Delta^{(l)}$ 为一个全零矩阵。</p>
</li>
<li><p>接下来遍历所有样本实例，对于每一个样本实例，有下列步骤：</p>
<ol>
<li><p>运行前向传播算法，得到初始预测 $a^{(L)}=h_\Theta(x)$ 。</p>
</li>
<li><p>运行反向传播算法，从输出层开始计算每一层预测的<strong>误差（error）</strong>，以此来求取偏导。</p>

<p>输出层的误差即为预测与训练集结果的之间的差值：$\delta^{(L)} = a^{(L)} - y$，</p>
<p>对于隐藏层中每一层的误差，都通过上一层的误差来计算：</p>
<p>$\delta^{(l)} = (\Theta^{(l)})^T\delta^{(l+1)} .*\ \frac{\partial a^{(l)} }{\partial z^{(l)} }\; \; \; \; \;  \text{for }l := L-1, L-2,\dots,2.$</p>
<p>隐藏层中，$a^{(l)}$ 即为增加偏置单元后的 $g(z^{(l)})$，$a^{(l)}$ 与 $\Theta^{(l)}$ 维度匹配，得以完成矩阵运算。</p>
<p>即对于隐藏层，有 $a^{(l)} = (g(z^{(l)})$ 添加偏置单元 $a^{(l)}_0 = 1)$</p>
<p>解得 $\frac{\partial}{\partial z^{(l)} }g(z^{(l)})=g’(z^{(l)})=g(z^{(l)}) .* \ (1-g(z^{(l)}))$，</p>
<p>则有 $\delta^{(l)} = (\Theta^{(l)})^T\delta^{(l+1)} .<em>\ a^{(l)} .</em>\ (1-a^{(l)}), \ \ a^{(l)}_0 = 1$。</p>
<blockquote>
<p>$\delta^{(l)}$ 求导前的公式不同于视频内容，经核实为视频内容错误。推导请阅下节。</p>
</blockquote>
<p>根据以上公式计算依次每一层的误差 $\delta^{(L)}, \delta^{(L-1)},\dots,\delta^{(2)}$。</p>
</li>
<li><p>依次求解并累加误差 $\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$，向量化实现即 $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$</p>
</li>
</ol>
</li>
<li><p>遍历全部样本实例，求解完 $\Delta$ 后，最后则求得偏导 $\frac \partial {\partial \Theta_{i,j}^{(l)} } J(\Theta)=D_{i,j}^{(l)}$</p>
<ul>
<li>$D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)$, if $j\neq0$,</li>
<li>$D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}$, if $j=0$.（对应于偏置单元）</li>
</ul>
</li>
</ol>
<blockquote>
<p>$\delta^{(l)}$: 第 $l$ 层的误差向量</p>
<p>$\delta^{(l)}_i$: 第 $l$ 层的第 $i$ 个激活单元的误差</p>
<p>$\Delta^{(l)}_{i,j}$: 从第 $l$ 层的第 $j$ 个单元映射到第 $l+1$ 层的第 $i$ 个单元的权重代价的偏导（所有样本实例之和）</p>
<p>$D^{(l)}_{i,j}$: $\Delta^{(l)}_{i,j}$ 的样本均值与正则化项之和</p>
<p>注：无需计算 $\delta^{(1)}$，因为输入没有误差。</p>
</blockquote>
<p>这就是反向传播算法，即从输出层开始不断<strong>向前迭代</strong>，根据<strong>上一层</strong>的误差依次计算当前层的误差，以求得代价函数的偏导。</p>
<blockquote>
<p>应用反向传播（BP）算法的神经网络被称为 BP 网络，也称前馈网络（向前反馈）。</p>
</blockquote>
<p>《机器学习》一书中提到的 BP 网络强大之处：</p>
<blockquote>
<p>任何布尔函数都可由两层神经网络准确表达，但所需的中间单元的数量随输入呈指数级增长;</p>
<p>任何连续函数都可由两层神经网络以任意精度逼近;</p>
<p>任何函数都可由三层神经网络以任意程度逼近。</p>
</blockquote>
<h2 id="9-3-直观理解反向传播（Backpropagation-Intuition）"><a href="#9-3-直观理解反向传播（Backpropagation-Intuition）" class="headerlink" title="9.3 直观理解反向传播（Backpropagation Intuition）"></a>9.3 直观理解反向传播（Backpropagation Intuition）</h2><p>这节给出了反向传播算法中误差的数学意义：</p>
<p>$cost(t) =y^{(t)} \ \log (h_\Theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\Theta(x^{(t)}))$</p>
<p>$\delta_j^{(l)} = \dfrac{\partial}{\partial z_j^{(l)} } cost(t)$</p>
<p>视频内容实际在上文都涉及到了，上节也做了解释：</p>
<blockquote>
<p>反向传播算法，即从输出层开始不断<strong>向前迭代</strong>，根据<strong>上一层</strong>的误差依次计算当前层的误差，以求得代价函数的偏导。</p>
</blockquote>
<p>前文提到输入层没有偏差，所以没有 $\delta^{(1)}$，同样的，偏置单元的值始终为 1，也没有误差，故一般会选择<strong>忽略偏置单元项的误差</strong>。</p>
<p><strong>神经网络中代价函数求导的推导过程</strong>：</p>
<p>代价函数无正则化项时：</p>
<p>$\begin{split} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \left[y^{(i)} \log ((h_\Theta (x^{(i)}))) + (1 - y^{(i)})\log (1 - (h_\Theta(x^{(i)})))\right] \end{split}$</p>
<p>再次的，为了方便起见，这里假设样本只有一个，则有：</p>
<p>$\begin{split} J(\Theta) = -\left[y \log ((h_\Theta (x))) + (1 - y)\log (1 - (h_\Theta(x)))\right] \end{split}$</p>
<p>忆及 $h_\Theta(x) = a^{(L)} = g(z^{(L)})$，$g(z) = \frac{1}{1+e^{(-z)} }$，代入后整理后可得：</p>
<p>$J(\Theta) ={y}\log \left( 1+{ {e}^{-z^{(L)} }} \right)+\left( 1-{y} \right)\log \left( 1+{ {e}^{z^{(L)} }} \right)$</p>

<p>再次为了便于计算，我们用到如上图这个三层（输入层一般不计数）神经网络。</p>
<p>忆及 $z^{(l)} = \Theta^{(l-1)}a^{(l-1)}$，我们有 $h_\Theta(x)=a^{(4)}= g(z^{(4)})=g(\Theta^{(3)}a^{(3)})$</p>
<p>观察考虑各变量与 $\Theta^{(3)}$ 之间的关系，有 $J(\Theta) \rightarrow  a^{(4)}\rightarrow z^{(4)}\rightarrow \Theta^{(3)}$</p>
<p>要计算 $J(\Theta)$ 的偏导，就要按照关系不断往前看，每一次回头看，就称为一次反向传播。</p>
<p>把回头看的关系说的“微积分一点”，那就是 $\Theta^{(3)}$ 的微小改变会引起 $z^{(4)}$ 的改变， $z^{(4)}$ 的微小改变会引起 $a^{(4)}$ 的改变，$a^{(4)}$ 的微小改变又会引起 $ J(\Theta)$ 的改变，关系方向也可以反过来写：$\Theta^{(3)} \rightarrow z^{(4)} \rightarrow a^{(4)} \rightarrow J(\Theta) $。</p>
<p>令 $\delta^{(l)} = \frac{\partial}{\partial z^{(l)} } J(\Theta)$，则有 $J(\Theta)$ 关于 $\Theta^{(3)}$ 的偏导：</p>
<p>$\frac{\partial}{\partial\Theta^{(3)} } J(\Theta) = \frac{\partial J(\Theta)}{\partial z^{(4)} }   \frac{\partial z^{(4)} }{\partial\Theta^{(3)} } = \delta^{(4)}\frac{\partial z^{(4)} }{\partial\Theta^{(3)} }$</p>
<p>再次忆及 $z^{(l)} = \Theta^{(l-1)}a^{(l-1)}$，则 $\frac{\partial z^{(4)} }{\partial\Theta^{(3)} } = a^{(3)}$</p>
<p>则对于输出层，我们证得 $\frac{\partial}{\partial\Theta^{(3)} } J(\Theta) =  a^{(3)}\delta^{(4)}$。</p>
<p>再次忆及 $g(z) = \frac{1}{1+e^{-z} }$，$a^{(L)}=g(z^{(L)})$</p>
<p>$\delta^{(4)}=\frac{\partial}{\partial z^{(4)} }J(\Theta)={ {y} }\frac{-e^{-z^{(4)} }}{1+e^{-z^{(4)} }}+\left( 1-{ {y} } \right)\frac{ {e^{z^{(4)} }} }{1+e^{z^{(4)} }} = g(z^{(4)}) - y = a^{(4)}-y$</p>
<p>即证得 $\delta^{(4)} = a^{(4)}-y$</p>
<p>对于任意的输出层 $L$ 及 $\Theta^{(L-1)}$，有 $J(\Theta) \rightarrow  a^{(L)}\rightarrow z^{(L)}\rightarrow \Theta^{(L-1)}$ 关系不变，故证得：<br>$$<br>\frac{\partial}{\partial\Theta^{(L-1)} } J(\Theta) =  a^{(L-1)}\delta^{(L)}, \ \ \delta^{(L)} = a^{(L)}-y<br>$$<br>好了，接下来来看一下 $J(\Theta)$ 关于 $\Theta^{(2)}$ 的偏导</p>
<p>仍然观察考虑各变量与 $\Theta^{(2)}$ 之间的关系，有 $J(\Theta)\rightarrow a^{(4)} \rightarrow z^{(4)} \rightarrow    a^{(3)} \rightarrow z^{(3)} \rightarrow\Theta^{(2)}$ </p>
<p>$\frac{\partial}{\partial \Theta^{(2)} }J(\Theta) = \frac{\partial J(\Theta)}{\partial z^{(3)} } \frac{\partial z^{(3)} }{\partial \Theta^{(2)} }=\delta^{(3)} \frac{\partial z^{(3)} }{\partial \Theta^{(2)} }=  a^{(2)}\delta^{(3)}$</p>
<p>$\delta^{(3)} = \frac{\partial}{\partial z^{(3)} }J(\Theta) =\frac{\partial J(\Theta)}{\partial z^{(4)} } \frac{\partial z^{(4)} }{\partial a^{(3)} }\frac{\partial a^{(3)} }{\partial z^{(3)} } = \delta^{(4)}\frac{\partial z^{(4)} }{\partial a^{(3)} }\frac{\partial a^{(3)} }{\partial z^{(3)} }$</p>
<p>易求得 $\frac{\partial z^{(4)} }{\partial a^{(3)} }=\Theta^{(3)}$</p>
<p>$g’(z) =\frac{e^{-z} }{(1+e^{-z})^2}=\frac{(1+e^{-z})-1}{(1+e^{-z})^2}=\frac{1}{1+e^{-z} }-\frac{1}{(1+e^{-z})^2}=g(z)(1-g(z))$</p>
<p>即 $g’(z^{(l)})=g(z^{(l)}) .* \ (1-g(z^{(l)}))$</p>
<p>有 $a^{(l)} = (g(z^{(l)})$ 添加偏置单元 $a^{(l)}_0 = 1)$，则 $\frac{\partial a^{(3)} }{\partial z^{(3)} }=a^{(3)} .*\ (1-a^{(3)})$，</p>
<blockquote>
<p>证明时为先求导后添加偏置单元，与前向传播算法顺序一致，实际实现时，求导和添加偏置单元的顺序可作调换，由于一般选择忽略偏置单元的误差，所以并不影响结果。</p>
</blockquote>
<p>即证得 $\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}.*(a^{(3)})’=(\Theta^{(3)})^T\delta^{(4)}.*\ a^{(3)} .*\ (1-a^{(3)})$</p>
<p>对于任意的隐藏层 $l + 1$ 及权重矩阵 $\Theta^{(l)}$，有 $J(\Theta)\rightarrow a^{(L)} \rightarrow z^{(L)} \rightarrow \dots \rightarrow a^{(l+1)} \rightarrow z^{(l+1)} \rightarrow\Theta^{(l)}$ 关系不变，故证得：</p>
<p>$$<br>\frac{\partial}{\partial\Theta^{(l)} } J(\Theta) =  a^{(l)}\delta^{(l+1)}, \ \ \delta^{(l)} = (\Theta^{(l)})^T\delta^{(l+1)}.*\ a^{(l)} .*\ (1-a^{(l)})\; \; \; \; \;  \text{for }l := L-1, L-2,\dots,2.<br>$$</p>
<p>再添回为了计算方便去掉的 $\frac{1}{m}$ 和正则化项（时刻记住偏置单元不正则化）等，即可得上节中 $J(\Theta)$ 的偏导。</p>
<h2 id="9-4-实现注意点-参数展开（Implementation-Note-Unrolling-Parameters）"><a href="#9-4-实现注意点-参数展开（Implementation-Note-Unrolling-Parameters）" class="headerlink" title="9.4 实现注意点: 参数展开（Implementation Note: Unrolling Parameters）"></a>9.4 实现注意点: 参数展开（Implementation Note: Unrolling Parameters）</h2><p>在 Octave/Matlab 中，如果要使用类似于 <code>fminunc</code> 等高级最优化函数，其函数参数、函数返回值等都为且只为向量，而由于神经网络中的权重是多维矩阵，所以需要用到参数展开这个技巧。</p>
<p>说白了，这个技巧就是把多个矩阵转换为一个长长的向量，便于传入函数，之后再根据矩阵维度，转回矩阵即可。</p>
<p>Octave 代码：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">% 多个矩阵展开为一个向量</span></span><br><span class="line">Theta1 = <span class="built_in">ones</span>(<span class="number">11</span>, <span class="number">10</span>);    <span class="comment">% 创建维度为 11 * 10 的矩阵</span></span><br><span class="line">Theta2 = <span class="built_in">ones</span>(<span class="number">2</span>, <span class="number">4</span>) * <span class="number">2</span>;  <span class="comment">% 创建维度为 2 * 4 的矩阵</span></span><br><span class="line">ThetaVec = [Theta1(:); Theta2(:)]; <span class="comment">% 将上面两个矩阵展开为向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 从一个向量重构还原回多个矩阵</span></span><br><span class="line">Theta1 = <span class="built_in">reshape</span>(ThetaVec(<span class="number">1</span>:<span class="number">110</span>), <span class="number">11</span>, <span class="number">10</span>)</span><br><span class="line">Theta2 = <span class="built_in">reshape</span>(ThetaVec(<span class="number">111</span>:<span class="number">118</span>), <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">% Theta2 = reshape(ThetaVec(111:(111 + 2 * 4) - 1), 2, 4)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>reshape(A,m,n)</code>: 将向量 A 重构为 m * n 维矩阵。</p>
</blockquote>
<h2 id="9-5-梯度检验（Gradient-Checking）"><a href="#9-5-梯度检验（Gradient-Checking）" class="headerlink" title="9.5 梯度检验（Gradient Checking）"></a>9.5 梯度检验（Gradient Checking）</h2><p>由于神经网络模型中的反向传播算法较为复杂，在小细节非常容易出错，从而无法得到最优解，故引入梯度检验。</p>
<p>梯度检验采用数值估算（Numerical estimation）梯度的方法，被用于验证反向传播算法的正确性。</p>

<p>把视 $\Theta$ 为一个实数，数值估算梯度的原理如上图所示，即有 $\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}$</p>
<p>其中，$\epsilon$ 为极小值，由于太小时容易出现数值运算问题，一般取 $10^{-4}$。</p>
<p>对于矩阵 $\Theta$，有 $\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$</p>
<p>Octave 代码：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">epsilon = <span class="number">1e-4</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n,</span><br><span class="line">  thetaPlus = theta;</span><br><span class="line">  thetaPlus(<span class="built_in">i</span>) += epsilon;</span><br><span class="line">  thetaMinus = theta;</span><br><span class="line">  thetaMinus(<span class="built_in">i</span>) -= epsilon;</span><br><span class="line">  gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus))/(<span class="number">2</span>*epsilon);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>在得出 gradApprox 梯度向量后，将其同之前计算的偏导 $D$ 比较，如果相等或很接近，即说明算法没有问题。</p>
<p>在确认算法<strong>没有问题后</strong>（一般只需运行一次），由于数值估计的梯度检验效率很低，所以一定要<strong>禁用它</strong>。</p>
<h2 id="9-6-随机初始化（Random-Initialization）"><a href="#9-6-随机初始化（Random-Initialization）" class="headerlink" title="9.6 随机初始化（Random Initialization）"></a>9.6 随机初始化（Random Initialization）</h2><p>逻辑回归中，初始参数向量全为 0 没什么问题，在神经网络中，情况就不一样了。</p>
<p>初始权重如果全为 0，忆及 $z^{(l)} = \Theta^{(l-1)}a^{(l-1)}$，则隐藏层除了偏置单元，都为 0，而每个单元求导的值也都一样，这就相当于是在不断<strong>重复计算同一结果</strong>，也就是算着算着，一堆特征在每一层都变成只有一个特征（虽然有很多单元，但值都相等），这样，神经网络的性能和效果都会大打折扣，故需要随机初始化初始权重。</p>
<p>随机初始化权重矩阵也为实现细节之一，用于打破对称性（Symmetry Breaking），使得 $\Theta^{(l)}_{ij} \in [-\epsilon,\epsilon]$ 。</p>
<p>Octave 代码：</p>
<p>当然，初始权重的波动也不能太大，一般限定在极小值 $\epsilon$ 范围内，即 $\Theta^{(l)}_{i,j} \in [-\epsilon, \epsilon]$。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">If the dimensions of Theta1 is <span class="number">10</span>x11, Theta2 is <span class="number">10</span>x11 and Theta3 is <span class="number">1</span>x11.</span><br><span class="line"></span><br><span class="line">Theta1 = <span class="built_in">rand</span>(<span class="number">10</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta2 = <span class="built_in">rand</span>(<span class="number">10</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta3 = <span class="built_in">rand</span>(<span class="number">1</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>rand(m,n)</code>: 返回一个在区间 $(0,1)$ 内均匀分布的随机矩阵。</p>
<p>$\epsilon$: 和梯度下降中的 $\epsilon$ 没有联系，这里只是一个任意实数，给定了权重矩阵初始化值的范围。</p>
</blockquote>
<h2 id="9-7-综合起来（Putting-It-Together）"><a href="#9-7-综合起来（Putting-It-Together）" class="headerlink" title="9.7 综合起来（Putting It Together）"></a>9.7 综合起来（Putting It Together）</h2><p>一般来说，应用神经网络有如下步骤：</p>
<ol>
<li><p>神经网络的建模（后续补充）</p>
<ul>
<li>选取特征，确定特征向量 $x$ 的维度，即输入单元的数量。</li>
<li>鉴别分类，确定预测向量 $h_\Theta(x)$ 的维度，即输出单元的数量。</li>
<li>确定隐藏层有几层以及每层隐藏层有多少个隐藏单元。</li>
</ul>
<blockquote>
<p>默认情况下，隐藏层至少要有一层，也可以有多层，层数越多一般意味着效果越好，计算量越大。</p>
</blockquote>
</li>
<li><p>训练神经网络</p>
<ol>
<li><p>随机初始化初始权重矩阵</p>
</li>
<li><p>应用前向传播算法计算初始预测</p>
</li>
<li><p>计算代价函数 $J(\Theta)$ 的值</p>
</li>
<li><p>应用后向传播宣发计算 $J(\Theta)$ 的偏导数</p>
</li>
<li><p>使用梯度检验检查算法的正确性，别忘了用完就禁用它</p>
</li>
<li><p>丢给最优化函数最小化代价函数</p>
<blockquote>
<p>由于神经网络的代价函数非凸，最优化时不一定会收敛在全局最小值处，高级最优化函数能确保收敛在某个<strong>局部</strong>最小值处。</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h2 id="9-8-自主驾驶（Autonomous-Driving）"><a href="#9-8-自主驾驶（Autonomous-Driving）" class="headerlink" title="9.8 自主驾驶（Autonomous Driving）"></a>9.8 自主驾驶（Autonomous Driving）</h2>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习, 深度学习, 吴恩达</tag>
      </tags>
  </entry>
  <entry>
    <title>72-优化算法</title>
    <url>/2024/04/23/11-06-12/</url>
    <content><![CDATA[<h2 id="72-优化算法"><a href="#72-优化算法" class="headerlink" title="72.优化算法"></a>72.优化算法</h2><h3 id="1-优化问题"><a href="#1-优化问题" class="headerlink" title="1.优化问题"></a>1.优化问题</h3><img src="/2024/04/23/11-06-12/72-01.png" class>
<h3 id="2-局部最小-vs-全局最小"><a href="#2-局部最小-vs-全局最小" class="headerlink" title="2.局部最小 vs 全局最小"></a>2.局部最小 vs 全局最小</h3><img src="/2024/04/23/11-06-12/72-02.png" class>
<h3 id="3-凸集和凸函数"><a href="#3-凸集和凸函数" class="headerlink" title="3.凸集和凸函数"></a>3.凸集和凸函数</h3><ul>
<li>凸集：形象化来说，就是这个集合上任意两个点连一条线，这个线在集合里面</li>
<li>凸函数：形象上来说函数上任取两个点连线，函数都在该线下面</li>
<li>凸优化问题：局部最小一定是全局最小。严格凸优化问题有唯一的全局最小。<ul>
<li>凸：线性回归，softmax回归</li>
<li>非凸：其他（MLP,CNN,RNN,attention）</li>
</ul>
</li>
</ul>
<h3 id="4-梯度下降"><a href="#4-梯度下降" class="headerlink" title="4.梯度下降"></a>4.梯度下降</h3><ul>
<li>梯度下降——最简单的迭代求解算法</li>
<li>随机梯度下降<ul>
<li>求导数需要求所有样本导数，样本多的情况下代价太大</li>
<li>理论依据：所用样本，和随机选取一个样本得到的数学期望是一样的。</li>
</ul>
</li>
<li>小批量随机梯度下降（实际应用的）<ul>
<li>计算原因：计算单样本的梯度难以完全利用硬件资源</li>
<li>采集一个随机子集</li>
<li>理论依据：无偏近，但降低了方差</li>
</ul>
</li>
</ul>
<h3 id="5-冲量法"><a href="#5-冲量法" class="headerlink" title="5.冲量法"></a>5.冲量法</h3><ul>
<li>使用平滑过的梯度对权重更新，不容易震荡</li>
<li>momentum</li>
</ul>
<img src="/2024/04/23/11-06-12/72-03.png" class>
<h3 id="6-Adam"><a href="#6-Adam" class="headerlink" title="6.Adam"></a>6.Adam</h3><ul>
<li>非常平滑，对于学习率不敏感</li>
<li>对于t比较小的时候，由于$v_0=0$,所以会导致一开始值比较小，做了一个修正。</li>
</ul>
<img src="/2024/04/23/11-06-12/72-04.png" class>
<ul>
<li>为什么除以$\sqrt{\widehat{s}_t}+\epsilon$？<ul>
<li>在nlp里面常用，起到正则化的作用，控制每个维度的值在合适的大小。</li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-06-12/72-05.png" class>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>深度学习模型大部分是非凸的</li>
<li>小批量随机梯度下降是最常见的优化算法</li>
<li>冲量是对梯度做平滑</li>
<li>Adam是对梯度做平滑，且对梯度各个维度值做重新调整，对于学习率不敏感</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>70-BERT微调</title>
    <url>/2024/04/23/11-06-10/</url>
    <content><![CDATA[<h2 id="70-BERT微调"><a href="#70-BERT微调" class="headerlink" title="70 BERT微调"></a>70 BERT微调</h2><h3 id="1-intro"><a href="#1-intro" class="headerlink" title="1.intro"></a>1.intro</h3><p>与图片分类不同，BERT预训练时使用的两个任务没有什么实际应用场景，所以使用BERT时多需要进行微调。</p>
<p>BERT对每一个token都返回一个特定长度的特征向量（课堂演示为128，bert-base是768，bert-large是1024），这些特征向量抽取了上下文信息。不同的任务使用不同的特征。</p>
<h3 id="2-具体应用"><a href="#2-具体应用" class="headerlink" title="2.具体应用"></a>2.具体应用</h3><h4 id="2-1句子分类"><a href="#2-1句子分类" class="headerlink" title="2.1句子分类"></a>2.1句子分类</h4><p>将句首的\<CLS\>token对应的向量输入到全连接层分类。对于一对句子也是同理，句子中间用\<SEP\>分开但仍只用第一个\<CLS\>对应的向量。</CLS\></SEP\></CLS\></p>
<p>关于为什么要使用\<CLS\>是因为预训练中判断句子是否连续任务中使用的是\<CLS\>，因此模型会“知道”\<CLS\>是句子级别分类用的向量，表示的信息应与句子整体有关。当然我们也可以不使用\<CLS\>选定自己想要的token，之后在微调中更新bert的权重即可。</CLS\></CLS\></CLS\></CLS\></p>
<h4 id="2-2命名实体识别"><a href="#2-2命名实体识别" class="headerlink" title="2.2命名实体识别"></a>2.2命名实体识别</h4><p>命名实体识别即识别一个词元是不是命名实体，例如人名、机构、位置。其方法是将每一个非特殊词元的向量放进全连接层分类（二分类多分类均可）。</p>
<h4 id="2-3问题回答"><a href="#2-3问题回答" class="headerlink" title="2.3问题回答"></a>2.3问题回答</h4><p>给定一个问题和描述文字，找出一个判断作为回答，微调方法为对片段中的每个词元预测它是不是回答的开头或结束。</p>
<p>总体而言，无论是句子级别还是词级别的分类任务，都只需要在bert的基础上加全连接层，bert中的权重是可以直接从预训练模型得到的，真正需要自己从头训练的只有全连接层的权重。</p>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h3><ul>
<li>即使下游任务各有不同，使用BERT微调时均只需要增加输出层</li>
<li>但根据任务的不同，输入的表示，和使用的BERT特征也会不一样</li>
</ul>
<p>在BERT的基础上微调使得多数任务都变的简单许多且效果相比从零开始训练要好很多，至此自然语言处理也向计算机视觉一样转向微调路线。</p>
<h3 id="4-QA"><a href="#4-QA" class="headerlink" title="4.QA"></a>4.QA</h3><p>Q1: BERT微调的时候固定预训练模型的参数吗？</p>
<blockquote>
<p>一般不固定，所有权重都进行训练。也可以固定住底部一些层来加速训练，但通常来说不固定效果更好。可以自行尝试固定哪几层在训练速度更快的前提下效果更好</p>
</blockquote>
<p>Q2: 为什么没讲YOLO？</p>
<blockquote>
<p>YOLO的较新版本已经跟之前版本很不一样了，且里面的技术细节很杂多。纯Python实现的yolo效果不见得比其他算法好，之所以表现出众是因为加入了大量的技术细节。只给大家实现一个本身大家可能不会太感兴趣。</p>
</blockquote>
<p>Q3: BERT在实际应用中怎样部署？用C++写代码吗？</p>
<blockquote>
<p>一般不需要用C++，可以将模型编译到C++（框架通常有支持）。但搬到C++也不能解决速度问题，BERT本身比resnet之类慢很多。</p>
</blockquote>
<p>Q4: 如果设备性能不高是不是不建议用BERT？</p>
<blockquote>
<p>可以用简化版本的BERT，如蒸馏版的BERT大约只有原模型十分之一大小。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>69-BERT预训练</title>
    <url>/2024/04/23/11-06-09/</url>
    <content><![CDATA[<h2 id="BERT预训练"><a href="#BERT预训练" class="headerlink" title="BERT预训练"></a>BERT预训练</h2><h3 id="2-BERT"><a href="#2-BERT" class="headerlink" title="2.BERT:"></a>2.BERT:</h3><h4 id="2-1-NLP里的迁移学习"><a href="#2-1-NLP里的迁移学习" class="headerlink" title="2.1 NLP里的迁移学习"></a>2.1 NLP里的迁移学习</h4><ul>
<li><p>使用预训练好的模型来抽取词，句子的特征</p>
<ul>
<li>例如word2vec或语言模型</li>
</ul>
</li>
<li><p>不更新预训练好的模型</p>
</li>
<li>需要构建新的网络来抓取任务需要的信息<ul>
<li>Word2vec忽略了时序信息</li>
<li>语言模型只看了一个方向</li>
</ul>
</li>
</ul>
<h4 id="2-2-BERT的动机"><a href="#2-2-BERT的动机" class="headerlink" title="2.2 BERT的动机"></a>2.2 BERT的动机</h4><ul>
<li>基于微调的NLP模型</li>
<li>预训练的模型抽取了足够多的信息</li>
<li>新的任务只需要增加一个简单地输出层</li>
</ul>
<img src="/2024/04/23/11-06-09/69-1.png" class>
<h4 id="2-3-BERT架构"><a href="#2-3-BERT架构" class="headerlink" title="2.3 BERT架构"></a>2.3 BERT架构</h4><ul>
<li>只有编码器的Transformer</li>
<li><p>两个版本：</p>
<ul>
<li>Base:#blocks=12,hidden size=768,#heads=12,#parameters=110M</li>
<li>Large:#blocks=24,hidden size=1024,#heads=16,#paramerter=340M</li>
</ul>
</li>
<li><p>在大规模数据上训练&gt;3B词</p>
</li>
</ul>
<h4 id="2-4-对输入的修改"><a href="#2-4-对输入的修改" class="headerlink" title="2.4 对输入的修改"></a>2.4 对输入的修改</h4><ul>
<li>每个样本是一个句子对</li>
<li>加入额外的片段嵌入</li>
<li>位置编码可学习</li>
</ul>
<img src="/2024/04/23/11-06-09/69-2.png" class>
<h4 id="2-5-预训练任务"><a href="#2-5-预训练任务" class="headerlink" title="2.5 预训练任务"></a>2.5 预训练任务</h4><h5 id="2-5-1-带掩码的语言模型"><a href="#2-5-1-带掩码的语言模型" class="headerlink" title="2.5.1 带掩码的语言模型"></a>2.5.1 带掩码的语言模型</h5><ul>
<li>Transformer的编码器是双向的，标准语言模型要求单向</li>
<li>带掩码的语言模型每次随机（15%概率）将一些词元换成<mask>
</mask></li>
</ul>
<h5 id="2-5-2-下一个句子预测"><a href="#2-5-2-下一个句子预测" class="headerlink" title="2.5.2 下一个句子预测"></a>2.5.2 下一个句子预测</h5><ul>
<li>预测一个句子对中两个句子是不是相邻</li>
<li><p>训练样本中：</p>
<ul>
<li>50%概率选择相邻句子对：<cls>this movie is great <sep> i like it <sep></sep></sep></cls></li>
<li>50%概率选择随机句子对：<cls>this movie is great<sep> hello world<sep></sep></sep></cls></li>
</ul>
</li>
<li><p>将<cls>对应的输出放到一个全连接层来预测</cls></p>
</li>
</ul>
<h4 id="2-6-总结"><a href="#2-6-总结" class="headerlink" title="2.6 总结"></a>2.6 总结</h4><ul>
<li>BERT针对微调设计</li>
<li>基于Transformer的编码器做了如下修改<ul>
<li>模型更大，训练数据更多</li>
<li>输入句子对，片段嵌入，可学习的位置编码</li>
<li>训练时使用两个任务：<ul>
<li>带掩码的语言模型</li>
<li>下一个句子预测</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3.代码实现"></a>3.代码实现</h3><h4 id="3-1-获取输入："><a href="#3-1-获取输入：" class="headerlink" title="3.1 获取输入："></a>3.1 获取输入：</h4><p>在自然语言处理中，有些任务（如情感分析）以单个文本作为输入，而有些任务（如自然语言推断）以一对文本序列作为输入。BERT输入序列明确地表示单个文本和文本对。当输入为单个文本时，BERT输入序列是特殊类别词元“<cls>”、文本序列的标记、以及特殊分隔词元“<sep>”的连结。当输入为文本对时，BERT输入序列是“<cls>”、第一个文本序列的标记、“<sep>”、第二个文本序列标记、以及“<sep>”的连结。我们将始终如一地将术语“BERT输入序列”与其他类型的“序列”区分开来。例如，一个<em>BERT输入序列</em>可以包括一个<em>文本序列</em>或两个<em>文本序列</em>。</sep></sep></cls></sep></cls></p>
<p>为了区分文本对，根据输入序列学到的片段嵌入eA和eB分别被添加到第一序列和第二序列的词元嵌入中。对于单文本输入，仅使用eA。</p>
<p>下面的<code>get_tokens_and_segments</code>将一个句子或两个句子作为输入，然后返回BERT输入序列的标记及其相应的片段索引。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_tokens_and_segments</span>(<span class="params">tokens_a, tokens_b=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取输入序列的词元及其片段索引&quot;&quot;&quot;</span></span><br><span class="line">    tokens = [<span class="string">&#x27;&lt;cls&gt;&#x27;</span>] + tokens_a + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">    <span class="comment"># 0和1分别标记片段A和B</span></span><br><span class="line">    segments = [<span class="number">0</span>] * (<span class="built_in">len</span>(tokens_a) + <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> tokens_b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        tokens += tokens_b + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">        segments += [<span class="number">1</span>] * (<span class="built_in">len</span>(tokens_b) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tokens, segments</span><br></pre></td></tr></table></figure>
<p>BERT选择Transformer编码器作为其双向架构。在Transformer编码器中常见是，位置嵌入被加入到输入序列的每个位置。然而，与原始的Transformer编码器不同，BERT使用<em>可学习的</em>位置嵌入。总之， 下图表明BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。</p>
<img src="/2024/04/23/11-06-09/69-3.png" class>
<h4 id="3-2-BERT实现"><a href="#3-2-BERT实现" class="headerlink" title="3.2 BERT实现"></a>3.2 BERT实现</h4><p>下面的<code>BERTEncoder</code>类类似于 <a href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html#sec-transformer">10.7节</a>中实现的<code>TransformerEncoder</code>类。与<code>TransformerEncoder</code>不同，<code>BERTEncoder</code>使用片段嵌入和可学习的位置嵌入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTEncoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span></span><br><span class="line"><span class="params">                 ffn_num_hiddens, num_heads, num_layers, dropout,</span></span><br><span class="line"><span class="params">                 max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.segment_embedding = nn.Embedding(<span class="number">2</span>, num_hiddens)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">f&quot;<span class="subst">&#123;i&#125;</span>&quot;</span>, d2l.EncoderBlock(</span><br><span class="line">                key_size, query_size, value_size, num_hiddens, norm_shape,</span><br><span class="line">                ffn_num_input, ffn_num_hiddens, num_heads, dropout, <span class="literal">True</span>))</span><br><span class="line">        <span class="comment"># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, max_len,</span><br><span class="line">                                                      num_hiddens))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens</span>):</span><br><span class="line">        <span class="comment"># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span></span><br><span class="line">        X = self.token_embedding(tokens) + self.segment_embedding(segments)</span><br><span class="line">        X = X + self.pos_embedding.data[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>假设词表大小为10000，为了演示<code>BERTEncoder</code>的前向推断，让我们创建一个实例并初始化它的参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocab_size, num_hiddens, ffn_num_hiddens, num_heads = <span class="number">10000</span>, <span class="number">768</span>, <span class="number">1024</span>, <span class="number">4</span></span><br><span class="line">norm_shape, ffn_num_input, num_layers, dropout = [<span class="number">768</span>], <span class="number">768</span>, <span class="number">2</span>, <span class="number">0.2</span></span><br><span class="line">encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,</span><br><span class="line">                      ffn_num_hiddens, num_heads, num_layers, dropout)</span><br></pre></td></tr></table></figure>
<p>我们将<code>tokens</code>定义为长度为8的2个输入序列，其中每个词元是词表的索引。使用输入<code>tokens</code>的<code>BERTEncoder</code>的前向推断返回编码结果，其中每个词元由向量表示，其长度由超参数<code>num_hiddens</code>定义。此超参数通常称为Transformer编码器的<em>隐藏大小</em>（隐藏单元数）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokens = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">2</span>, <span class="number">8</span>))</span><br><span class="line">segments = torch.tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">encoded_X = encoder(tokens, segments, <span class="literal">None</span>)</span><br><span class="line">encoded_X.shape</span><br></pre></td></tr></table></figure>
<h4 id="3-3-预训练任务"><a href="#3-3-预训练任务" class="headerlink" title="3.3 预训练任务"></a>3.3 预训练任务</h4><h5 id="3-3-1-遮掩语言模型"><a href="#3-3-1-遮掩语言模型" class="headerlink" title="3.3.1 遮掩语言模型"></a>3.3.1 遮掩语言模型</h5><p>我们实现了下面的<code>MaskLM</code>类来预测BERT预训练的掩蔽语言模型任务中的掩蔽标记。预测使用单隐藏层的多层感知机（<code>self.mlp</code>）。在前向推断中，它需要两个输入：<code>BERTEncoder</code>的编码结果和用于预测的词元位置。输出是这些位置的预测结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaskLM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的掩蔽语言模型任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, num_inputs=<span class="number">768</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MaskLM, self).__init__(**kwargs)</span><br><span class="line">        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.LayerNorm(num_hiddens),</span><br><span class="line">                                 nn.Linear(num_hiddens, vocab_size))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, pred_positions</span>):</span><br><span class="line">        num_pred_positions = pred_positions.shape[<span class="number">1</span>]</span><br><span class="line">        pred_positions = pred_positions.reshape(-<span class="number">1</span>)</span><br><span class="line">        batch_size = X.shape[<span class="number">0</span>]</span><br><span class="line">        batch_idx = torch.arange(<span class="number">0</span>, batch_size)</span><br><span class="line">        <span class="comment"># 假设batch_size=2，num_pred_positions=3</span></span><br><span class="line">        <span class="comment"># 那么batch_idx是np.array（[0,0,0,1,1]）</span></span><br><span class="line">        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)</span><br><span class="line">        masked_X = X[batch_idx, pred_positions]</span><br><span class="line">        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span class="number">1</span>))</span><br><span class="line">        mlm_Y_hat = self.mlp(masked_X)</span><br><span class="line">        <span class="keyword">return</span> mlm_Y_hat</span><br></pre></td></tr></table></figure>
<p>为了演示<code>MaskLM</code>的前向推断，我们创建了其实例<code>mlm</code>并对其进行了初始化。回想一下，来自<code>BERTEncoder</code>的正向推断<code>encoded_X</code>表示2个BERT输入序列。我们将<code>mlm_positions</code>定义为在<code>encoded_X</code>的任一输入序列中预测的3个指示。<code>mlm</code>的前向推断返回<code>encoded_X</code>的所有掩蔽位置<code>mlm_positions</code>处的预测结果<code>mlm_Y_hat</code>。对于每个预测，结果的大小等于词表的大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mlm = MaskLM(vocab_size, num_hiddens)</span><br><span class="line">mlm_positions = torch.tensor([[<span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>], [<span class="number">6</span>, <span class="number">1</span>, <span class="number">5</span>]])</span><br><span class="line">mlm_Y_hat = mlm(encoded_X, mlm_positions)</span><br><span class="line">mlm_Y_hat.shape</span><br></pre></td></tr></table></figure>
<p>通过掩码下的预测词元<code>mlm_Y</code>的真实标签<code>mlm_Y_hat</code>，我们可以计算在BERT预训练中的遮蔽语言模型任务的交叉熵损失。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mlm_Y = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]])</span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">mlm_l = loss(mlm_Y_hat.reshape((-<span class="number">1</span>, vocab_size)), mlm_Y.reshape(-<span class="number">1</span>))</span><br><span class="line">mlm_l.shape</span><br></pre></td></tr></table></figure>
<h5 id="3-3-2-下一句预测"><a href="#3-3-2-下一句预测" class="headerlink" title="3.3.2 下一句预测"></a>3.3.2 下一句预测</h5><p>下面的<code>NextSentencePred</code>类使用单隐藏层的多层感知机来预测第二个句子是否是BERT输入序列中第一个句子的下一个句子。由于Transformer编码器中的自注意力，特殊词元“<cls>”的BERT表示已经对输入的两个句子进行了编码。因此，多层感知机分类器的输出层（<code>self.output</code>）以<code>X</code>作为输入，其中<code>X</code>是多层感知机隐藏层的输出，而MLP隐藏层的输入是编码后的“<cls>”词元。</cls></cls></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NextSentencePred</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的下一句预测任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(NextSentencePred, self).__init__(**kwargs)</span><br><span class="line">        self.output = nn.Linear(num_inputs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># X的形状：(batchsize,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> self.output(X)</span><br></pre></td></tr></table></figure>
<p>我们可以看到，<code>NextSentencePred</code>实例的前向推断返回每个BERT输入序列的二分类预测。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoded_X = torch.flatten(encoded_X, start_dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># NSP的输入形状:(batchsize，num_hiddens)</span></span><br><span class="line">nsp = NextSentencePred(encoded_X.shape[-<span class="number">1</span>])</span><br><span class="line">nsp_Y_hat = nsp(encoded_X)</span><br><span class="line">nsp_Y_hat.shape</span><br></pre></td></tr></table></figure>
<h4 id="3-4-整合代码"><a href="#3-4-整合代码" class="headerlink" title="3.4 整合代码"></a>3.4 整合代码</h4><p>在预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。现在我们可以通过实例化三个类<code>BERTEncoder</code>、<code>MaskLM</code>和<code>NextSentencePred</code>来定义<code>BERTModel</code>类。前向推断返回编码后的BERT表示<code>encoded_X</code>、掩蔽语言模型预测<code>mlm_Y_hat</code>和下一句预测<code>nsp_Y_hat</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span></span><br><span class="line"><span class="params">                 ffn_num_hiddens, num_heads, num_layers, dropout,</span></span><br><span class="line"><span class="params">                 max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 hid_in_features=<span class="number">768</span>, mlm_in_features=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 nsp_in_features=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTModel, self).__init__()</span><br><span class="line">        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,</span><br><span class="line">                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,</span><br><span class="line">                    dropout, max_len=max_len, key_size=key_size,</span><br><span class="line">                    query_size=query_size, value_size=value_size)</span><br><span class="line">        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),</span><br><span class="line">                                    nn.Tanh())</span><br><span class="line">        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)</span><br><span class="line">        self.nsp = NextSentencePred(nsp_in_features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                pred_positions=<span class="literal">None</span></span>):</span><br><span class="line">        encoded_X = self.encoder(tokens, segments, valid_lens)</span><br><span class="line">        <span class="keyword">if</span> pred_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mlm_Y_hat = self.mlm(encoded_X, pred_positions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mlm_Y_hat = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 用于下一句预测的多层感知机分类器的隐藏层，0是“&lt;cls&gt;”标记的索引</span></span><br><span class="line">        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, <span class="number">0</span>, :]))</span><br><span class="line">        <span class="keyword">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat</span><br></pre></td></tr></table></figure>
<h4 id="3-5-小结"><a href="#3-5-小结" class="headerlink" title="3.5 小结"></a>3.5 小结</h4><ul>
<li>word2vec和GloVe等词嵌入模型与上下文无关。它们将相同的预训练向量赋给同一个词，而不考虑词的上下文（如果有的话）。它们很难处理好自然语言中的一词多义或复杂语义。</li>
<li>对于上下文敏感的词表示，如ELMo和GPT，词的表示依赖于它们的上下文。</li>
<li>ELMo对上下文进行双向编码，但使用特定于任务的架构（然而，为每个自然语言处理任务设计一个特定的体系架构实际上并不容易）；而GPT是任务无关的，但是从左到右编码上下文。</li>
<li>BERT结合了这两个方面的优点：它对上下文进行双向编码，并且需要对大量自然语言处理任务进行最小的架构更改。</li>
<li>BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。</li>
<li>预训练包括两个任务：掩蔽语言模型和下一句预测。前者能够编码双向上下文来表示单词，而后者则显式地建模文本对之间的逻辑关系。</li>
</ul>
<h3 id="Q-amp-A："><a href="#Q-amp-A：" class="headerlink" title="Q&amp;A："></a>Q&amp;A：</h3><h5 id="Q1-BERT是不是很少用在CV上？"><a href="#Q1-BERT是不是很少用在CV上？" class="headerlink" title="Q1:BERT是不是很少用在CV上？"></a>Q1:BERT是不是很少用在CV上？</h5><blockquote>
<p>transformer架构这几年在大量的用于CV上</p>
</blockquote>
<h5 id="Q2-展示一下10W-batch-训练结果？"><a href="#Q2-展示一下10W-batch-训练结果？" class="headerlink" title="Q2:展示一下10W batch 训练结果？"></a>Q2:展示一下10W batch 训练结果？</h5><blockquote>
<p>微调时会用到</p>
</blockquote>
<h5 id="Q3：使用BERT-large时显存不足，有什么方法吗？"><a href="#Q3：使用BERT-large时显存不足，有什么方法吗？" class="headerlink" title="Q3：使用BERT large时显存不足，有什么方法吗？"></a>Q3：使用BERT large时显存不足，有什么方法吗？</h5><blockquote>
<p>单机多卡，模型并行，或改用小模型</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>68-transformer架构</title>
    <url>/2024/04/23/11-06-08/</url>
    <content><![CDATA[<h3 id="1-transformer架构"><a href="#1-transformer架构" class="headerlink" title="1.transformer架构"></a>1.transformer架构</h3><ul>
<li>基于encoder-decoder架构来处理序列对</li>
<li>跟使用注意力的seq2seq不同，transformer是纯基于注意力</li>
</ul>
<p>&lt;</p>
<h3 id="2-多头注意力"><a href="#2-多头注意力" class="headerlink" title="2.多头注意力"></a>2.多头注意力</h3><ul>
<li><p>对同一key，value，query，希望抽取不同的信息</p>
<ul>
<li>例如短距离关系和长距离关系</li>
</ul>
</li>
<li><p>多头注意力使用h个独立的注意力池化</p>
<ul>
<li><p>合并各个头（head）输出得到最终输出</p>
</li>
<li><img src="/2024/04/23/11-06-08/68-02.png" class>
</li>
</ul>
</li>
<li><p>数学表达式</p>
<img src="/2024/04/23/11-06-08/68-02.png" class>
</li>
</ul>
<h3 id="3-有掩码的多头注意力"><a href="#3-有掩码的多头注意力" class="headerlink" title="3.有掩码的多头注意力"></a>3.有掩码的多头注意力</h3><ul>
<li>解码器对序列中一个元素输出的时候，不应该考虑该元素之后的元素</li>
<li>可以用掩码来实现<ul>
<li>也就是计算$x_i$输出的时候，假装当前序列长度为i</li>
</ul>
</li>
</ul>
<h3 id="4-基于位置的前馈网络"><a href="#4-基于位置的前馈网络" class="headerlink" title="4.基于位置的前馈网络"></a>4.基于位置的前馈网络</h3><ul>
<li>将输入形状变化（b,n,d）变换成（bn，d）；输出形状由（bn，d）变成（b，n，d）</li>
<li>作用两个全连接层</li>
<li>等价于两层核窗口为1的一维卷积层（全连接）</li>
</ul>
<h3 id="5-层归一化"><a href="#5-层归一化" class="headerlink" title="5.层归一化"></a>5.层归一化</h3><ul>
<li>批量归一化对每个特征/通道里元素进行归一化<ul>
<li>不适合序列长度会变的nlp应用</li>
</ul>
</li>
<li>层归一化对每个样本里面的元素进行归一化（ layer norm ）</li>
</ul>
<h3 id="6-信息传递"><a href="#6-信息传递" class="headerlink" title="6.信息传递"></a>6.信息传递</h3><ul>
<li>将编码器输出作为解码中第i个transformer块中多头注意力的key和value<ul>
<li>query来自目标序列</li>
</ul>
</li>
<li>意味着编码器和解码器中块的个数，输出维度都是一样的</li>
</ul>
<h3 id="7-预测"><a href="#7-预测" class="headerlink" title="7.预测"></a>7.预测</h3><ul>
<li>预测第t+1个输出时</li>
<li>解码器中输入前t个预测值（顺序）<ul>
<li>在自注意力中，前t个预测值作为key和value，第t个预测值还作为query</li>
</ul>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>transformer是一个纯使用注意力的encoder-decoder</li>
<li>编码器和解码器都有n个transformer块</li>
<li>每个块里面使用多头注意力，基于位置的前馈网络，层归一化</li>
</ul>
<h3 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h3><ul>
<li>多头注意力，concat和相加取平均怎么选择？<ul>
<li>老师认为concat保留的信息更全面，更好</li>
</ul>
</li>
<li>为什么在获取词向量之后，需要对词向量进行缩放（乘以embedding size的开方之后再加上PE）<ul>
<li>embedding之后，向量长度变长，元素值变小，乘以之后可以保证在-1，1之间，和position大小差不多</li>
</ul>
</li>
<li>num of head是什么？<ul>
<li>类似卷积的多通道，多个attention关注的是不同的特征</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>65-注意力分数</title>
    <url>/2024/04/23/11-06-05/</url>
    <content><![CDATA[<h3 id="65-注意力分数"><a href="#65-注意力分数" class="headerlink" title="65 注意力分数"></a>65 注意力分数</h3><ul>
<li>在上一节中，我们使用高斯核来对查询和键之间的关系建模。我们可以将上一节中的高斯核函数部分视为注意力评分函数，简称评分函数，然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，我们将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h4 id="掩蔽softmax操作"><a href="#掩蔽softmax操作" class="headerlink" title="掩蔽softmax操作"></a>掩蔽softmax操作</h4><ul>
<li>正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。 例如，为了在 <a href="https://zh-v2.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#sec-machine-translation">9.5节</a>中高效处理小批量数据集， 某些文本序列被填充了没有意义的特殊词元。 为了仅将有意义的词元作为值来获取注意力汇聚， 我们可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。 通过这种方式，我们可以在下面的<code>masked_softmax</code>函数中 实现这样的<em>掩蔽softmax操作</em>（masked softmax operation）， 其中任何超出有效长度的位置都被掩蔽并置为0。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="加性注意力"><a href="#加性注意力" class="headerlink" title="加性注意力"></a>加性注意力</h4><ul>
<li>一般来说，当查询和键是不同长度的矢量时， 我们可以使用加性注意力作为评分函数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdditiveAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        <span class="comment"># 在维度扩展后，</span></span><br><span class="line">        <span class="comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span></span><br><span class="line">        <span class="comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># 使用广播方式进行求和</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>
<h4 id="缩放点积注意力"><a href="#缩放点积注意力" class="headerlink" title="缩放点积注意力"></a>缩放点积注意力</h4><ul>
<li>使用点积可以得到计算效率更高的评分函数， 但是点积操作要求查询和键具有相同的长度<em>d</em>。 假设查询和键的所有元素都是独立的随机变量， 并且都满足零均值和单位方差， 那么两个向量的点积的均值为0，方差为<em>d</em>。 为确保无论向量长度如何， 点积的方差在不考虑向量长度的情况下仍然是1， 我们将点积除以√d,在下面的缩放点积注意力的实现中，我们使用了暂退法进行模型正则化。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># queries的形状：(batch_size，查询的个数，d)</span></span><br><span class="line">    <span class="comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span></span><br><span class="line">    <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">    <span class="comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 设置transpose_b=True为了交换keys的最后两个维度</span></span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ul>
<li>将注意力汇聚的输出计算可以作为值的加权平均，选择不同的注意力评分函数会带来不同的注意力汇聚操作。</li>
<li>当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的“点－积”注意力评分函数的计算效率更高。</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>63-束搜索</title>
    <url>/2024/04/23/11-06-03/</url>
    <content><![CDATA[<h2 id="束搜索"><a href="#束搜索" class="headerlink" title="束搜索"></a>束搜索</h2><p>在序列生成问题中，常用的方法是一个个词元地进行生成，但是先前步生成的词元会影响之后词元的概率分布，为此，我们需要使用搜索算法来得到一个较好的序列</p>
<h3 id="贪心搜索"><a href="#贪心搜索" class="headerlink" title="贪心搜索"></a>贪心搜索</h3><p>贪心搜索即每个时间步都选择具有最高条件概率的词元。</p>
<script type="math/tex; mode=display">
y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y_{t'-1}, \mathbf{c})</script><p>我们的目标是找到一个最有序列，他的联合概率，也就是每步之间的条件概率的乘积，最大。</p>
<script type="math/tex; mode=display">
\prod_{t'=1}^{T'} P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})</script><p>然而，贪心搜索很可能搜索到的不是最优解，例如：<img src="/2024/04/23/11-06-03/Greedy_or_not.png" class></p>
<p>左侧的搜索方式为贪心搜索，每次找到当前条件概率最大的选项进行预测，但是这样可能会导致之后的条件概率较小，从而导致最终的联合概率较小，生成的序列不优。</p>
<p>而右侧的选择方式虽然在第二步选择了较小的选项，但之后在第三步时有了条件概率为0.6选项，最终结果反而更好。</p>
<h3 id="穷举搜索"><a href="#穷举搜索" class="headerlink" title="穷举搜索"></a>穷举搜索</h3><p>穷举搜索枚举所有可能的输出序列及其概率，然后选择概率最大的作为最终的输出，枚举搜索可以保证得到最优解，但是计算复杂度很高，难以实现</p>
<h3 id="束搜索（beam-search）"><a href="#束搜索（beam-search）" class="headerlink" title="束搜索（beam search）"></a>束搜索（beam search）</h3><p>束搜索综合了贪心搜索和穷举搜索，在能接受的计算成本下得到比贪心搜索更好的结果。</p>
<p>束搜索有一个超参数，名为<strong>束宽（beam size）</strong>$k$，束搜索的具体流程如下：</p>
<ul>
<li>1：在第一时间步选择条件概率最高的k个选项</li>
<li><p>2：对随后的每个时间步，基于上一时间步的k个候选输出序列预测这一时间步的所有可能选项的条件概率，从中取k个最大的</p>
</li>
<li><p>3：最后基于每步得到的序列，删去截止符和其后元素，获得最终候选序列集合，取出加权条件概率最大的</p>
</li>
</ul>
<p>加权条件概率公式如下：</p>
<script type="math/tex; mode=display">
\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c}),</script><p>式中$\frac{1}{L^\alpha}$用于调整长序列的评估值使得长短序列间的比较公平</p>
<p>束宽k的选择：</p>
<ul>
<li>k=1时实际为贪心搜索</li>
<li>k越小搜索速度越快，但结果越差，k越大则搜索速度越慢，但结果越好</li>
</ul>
<p>束搜索只在测试时使用</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>62-序列到序列学习</title>
    <url>/2024/04/23/11-06-02/</url>
    <content><![CDATA[<h2 id="62-序列到序列学习"><a href="#62-序列到序列学习" class="headerlink" title="62-序列到序列学习"></a>62-序列到序列学习</h2><h3 id="1-应用举例：机器翻译"><a href="#1-应用举例：机器翻译" class="headerlink" title="1. 应用举例：机器翻译"></a>1. 应用举例：机器翻译</h3><ul>
<li>给定一个源语言的句子，自动翻译成目标语言</li>
<li>这两个句子可以有不同的长度</li>
</ul>
<h3 id="2-模型架构：Seq2seq"><a href="#2-模型架构：Seq2seq" class="headerlink" title="2. 模型架构：Seq2seq"></a>2. 模型架构：Seq2seq</h3><img src="/2024/04/23/11-06-02/62-01.png" class>
<ul>
<li><p>序列到序列模型由<strong>编码器-解码器</strong>构成。</p>
</li>
<li><p><strong>编码器</strong>RNN可以是<strong>双向</strong>，由于输入的句子是完整地，可以正着看，也可以反着看；而<strong>解码器</strong>只能是<strong>单向</strong>，由于预测时，只能正着去预测。</p>
</li>
<li>编码器，解码器采用<strong>不同的RNN</strong>，此RNN也可以是GRU，LSTM等。</li>
</ul>
<h3 id="3-编码器-解码器细节"><a href="#3-编码器-解码器细节" class="headerlink" title="3. 编码器-解码器细节"></a>3. 编码器-解码器细节</h3><img src="/2024/04/23/11-06-02/62-02.png" class>
<ul>
<li><p>编码器的RNN<strong>没有</strong>连接<strong>输出层</strong></p>
</li>
<li><p><strong>编码器</strong>的<strong>最后时间步的隐状态</strong>用作<strong>解码器</strong>的<strong>初始隐状态</strong>（图中箭头的传递）</p>
</li>
</ul>
<h3 id="4-训练和推理"><a href="#4-训练和推理" class="headerlink" title="4. 训练和推理"></a>4. 训练和推理</h3><img src="/2024/04/23/11-06-02/62-03.png" class>
<ul>
<li>第3节中提到编码器没有输出层，只有解码器有，于是损失函数的计算只关注解码器的输出层。</li>
<li>训练和预测（推理）有区别的，训练时解码器使用目标句子（真值）作为输入，以指导模型训练；而推理时无法提前得知真值，需要一步一步进行预测。</li>
</ul>
<h3 id="5-衡量生成序列的好坏：BLEU"><a href="#5-衡量生成序列的好坏：BLEU" class="headerlink" title="5. 衡量生成序列的好坏：BLEU"></a>5. 衡量生成序列的好坏：BLEU</h3><h4 id="5-1-BLUE值定义："><a href="#5-1-BLUE值定义：" class="headerlink" title="5.1 BLUE值定义："></a>5.1 BLUE值定义：</h4><img src="/2024/04/23/11-06-02/62-04.png" class>
<p>宗成庆老师《统计自然语言处理》（第二版）一书中关于BLEU的定义：</p>
<img src="/2024/04/23/11-06-02/62-05.png" class>
<p>同时，吴恩达深度学习课程中也是使用这一方式定义。但观察两种方式，BP惩罚因子的计算是一致的，pn也是使用了几何平均的方式，只是对于wn这一加权值的选择有所不同。</p>
<h4 id="5-2-定义式解析"><a href="#5-2-定义式解析" class="headerlink" title="5.2 定义式解析"></a>5.2 定义式解析</h4><img src="/2024/04/23/11-06-02/62-06.png" class>
<p>BLEU值衡量的是精确率，而且对不同n-gram进行集成打分。</p>
<ul>
<li><p>BP惩罚因子：为了惩罚过短的句子，由于过短的句子基数小，精确率容易提升，所以加上一个BP乘子，当预测句子长度&lt;参考句子长度，则BP&lt;1。</p>
</li>
<li><p>wn的选择：李沐老师课程中是采用了$\frac{1}{2^n}$​作为加权因子，n越大，加权因子越小，但由于pn&lt;1，赋予的权重越大，即长匹配具有更高的权重。而宗老师的书中所述：在BLEU的基线系统中取N＝4，wn＝1/N，也可以参考。</p>
</li>
</ul>
<h3 id="6-QA"><a href="#6-QA" class="headerlink" title="6. QA"></a>6. QA</h3><p>问题：LSTM、GRU、Seq2Seq的区别是什么？</p>
<blockquote>
<p>Seq2Seq是一种由编码器和解码器组成的框架，而LSTM、GRU是组成编码器和解码器的一种单元。</p>
</blockquote>
<p>问题：encoder的输出和decoder的输入，拼接和按位相加起来有什么区别么？</p>
<blockquote>
<p>不能够按位加，由于encoder的输出最后维度是hidden_size，而decoder的输入最后维度是embedding_size，可能不一样，所以用拼接。</p>
</blockquote>
<p>问题：embedding层是做word2vec吗？</p>
<blockquote>
<p>这里不是，这里是从头开始训练。现在用的比较多得都是预训练，BERT等。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>61-编码器-解码器架构</title>
    <url>/2024/04/23/11-06-01/</url>
    <content><![CDATA[<h2 id="编码器-解码器架构"><a href="#编码器-解码器架构" class="headerlink" title="编码器-解码器架构"></a>编码器-解码器架构</h2><h3 id="CNN中的解释"><a href="#CNN中的解释" class="headerlink" title="CNN中的解释"></a>CNN中的解释</h3><p>考虑一个CNN模型：</p>
<img src="/2024/04/23/11-06-01/CNN.png" class>
<p>整个CNN实际上可以看作一个编码器，解码器两部分。</p>
<ul>
<li>底层的神经网络，也就是编码器将输入编码成能被模型识别的中间表达形式，也就是特征</li>
<li>解码器将中间结果解码为输出</li>
</ul>
<h3 id="RNN中的解释"><a href="#RNN中的解释" class="headerlink" title="RNN中的解释"></a>RNN中的解释</h3><p>对于RNN而言，同样有着类似的划分</p>
<img src="/2024/04/23/11-06-01/RNN.png" class>
<ul>
<li>编码器将输入文本表示为向量</li>
<li>解码器将向量表示为输出</li>
</ul>
<h3 id="抽象的编码器-解码器架构"><a href="#抽象的编码器-解码器架构" class="headerlink" title="抽象的编码器-解码器架构"></a>抽象的编码器-解码器架构</h3><p>指一个模型被分为两块：</p>
<ul>
<li>一块是编码器，也叫encoder，用于将输入处理为一个中间状态</li>
<li>一块是解码器，也叫decoder，用于将中间状态表示为输出</li>
<li>解码器也可以有额外的输入提供信息</li>
</ul>
<img src="/2024/04/23/11-06-01/encoder-decoder.png" class>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>58-深层循环神经网络</title>
    <url>/2024/04/23/11-01-58/</url>
    <content><![CDATA[<h2 id="58-深层循环神经网络"><a href="#58-深层循环神经网络" class="headerlink" title="58 深层循环神经网络"></a>58 深层循环神经网络</h2><p>x</p>
<h3 id="1-深层循环神经网络"><a href="#1-深层循环神经网络" class="headerlink" title="1.深层循环神经网络"></a>1.深层循环神经网络</h3><p>之前讲的RNN都只有一个隐藏层（序列变长不算是深度），而一个隐藏层的RNN一旦做的很宽就容易出现过拟合。因此我们考虑将网络做的更深而非更宽，每层都只做一点非线性，靠层数叠加得到更加非线性的模型。</p>
<p>浅RNN：输入-隐层-输出</p>
<p>深RNN：输入-隐层-隐层-…-输出</p>
<img src="/2024/04/23/11-01-58/58-01.png" class>
<p>（课程视频中的图片有错误，最后输出层后一时间步是不受前一步影响的，即没有箭头）</p>
<h3 id="2-公式"><a href="#2-公式" class="headerlink" title="2.公式"></a>2.公式</h3><div align="center">

![](http://latex.codecogs.com/svg.latex?\mathbf{H}_t^1=f_1(\mathbf{H_{t-1}^1},\mathbf{X_t}))

</div>

<p><em>第一层的第t步状态是关于第一层第t-1步状态和第t步输入的函数</em></p>
<div align="center">

<p><img src="http://latex.codecogs.com/svg.latex?\mathbf{H}_t^j=f_j(\mathbf{H_{t-1}^j},\mathbf{H_{t}^{j-1}" alt>})</p>
<p>&lt;/div&gt;<br><em>第j层的第t步状态是关于当前层上一步步状态和上一层当前步的函数</em></p>
<div align="center">

<p><img src="http://latex.codecogs.com/svg.latex?\mathbf{O}_t=g(\mathbf{H}_t^L" alt>)</p>
<p>&lt;/div&gt;<br><em>由最后一个隐藏层得到输出</em></p>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h3><ul>
<li>深度循环神经网络使用多个隐藏层来获得更多的非线性性</li>
</ul>
<p>将RNN/GRU/LSTM做深都是一个道理，三者只是使用的函数f不同。</p>
<h3 id="4-QA"><a href="#4-QA" class="headerlink" title="4.QA"></a>4.QA</h3><p>Q1: NLP那个方向好找工作？文本翻译是不是现在只在学术研究中才需要自己实现？（2021-7-27）</p>
<blockquote>
<p>文本翻译已经是一个很成熟的领域，NLP挺好找工作，人产生的文本远多于图片。</p>
</blockquote>
<p>Q2: 关于BPTT</p>
<blockquote>
<p>课上不讲，书上有讲原理</p>
</blockquote>
<p>Q3: 深层RNN是不是每层都需要一个初始hidenstate?</p>
<blockquote>
<p>是的</p>
</blockquote>
<p>Q4: 可不可以手动实现hidden_size不一样的多层RNN？</p>
<blockquote>
<p>应该没问题，但通常大家不会去调hidden_size，因为网络不会做的很深，最后还有全连接层。</p>
</blockquote>
<p>Q5: 关于课上提到的classifier</p>
<blockquote>
<p>分类的任务在最后的全连接层完成</p>
</blockquote>
</div></div>]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>57-长短期记忆网络(LSTM)</title>
    <url>/2024/04/23/11-01-57/</url>
    <content><![CDATA[<h1 id="长短期记忆网络"><a href="#长短期记忆网络" class="headerlink" title="长短期记忆网络"></a>长短期记忆网络</h1><h3 id="2-长短期记忆网络："><a href="#2-长短期记忆网络：" class="headerlink" title="2.长短期记忆网络："></a>2.长短期记忆网络：</h3><ul>
<li>忘记门：将值朝0减少</li>
<li>输入门：决定是不是忽略掉输入数据</li>
<li>输出门：决定是不是使用隐状态</li>
</ul>
<p>可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。 长短期记忆网络引入了<em>记忆元</em>（memory cell），或简称为<em>单元</em>（cell）。 有些文献认为记忆元是隐状态的一种特殊类型， 它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。 为了控制记忆元，我们需要许多门。 其中一个门用来从单元中输出条目，我们将其称为<em>输出门</em>（output gate）。 另外一个门用来决定何时将数据读入单元，我们将其称为<em>输入门</em>（input gate）。 我们还需要一种机制来重置单元的内容，由<em>遗忘门</em>（forget gate）来管理， 这种设计的动机与门控循环单元相同， 能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。 让我们看看这在实践中是如何运作的。</p>
<h4 id="2-1-门："><a href="#2-1-门：" class="headerlink" title="2.1 门："></a>2.1 门：</h4><p>输入门：<img src="https://latex.codecogs.com/svg.image?I_{t}=\sigma&space;(X_{t}W_{xi}&plus;H_{t-1}W_{hi}&plus;b_{i})" title="I_{t}=\sigma (X_{t}W_{xi}+H_{t-1}W_{hi}+b_{i})"></p>
<p>忘记门：<img src="https://latex.codecogs.com/svg.image?F_{t}=\sigma&space;(X_{t}W_{xf}&plus;H_{t-1}W_{hf}&plus;b_{f})" title="F_{t}=\sigma (X_{t}W_{xf}+H_{t-1}W_{hf}+b_{f})"></p>
<p>输出门：<img src="https://latex.codecogs.com/svg.image?O_{t}=\sigma&space;(X_{t}W_{xo}&plus;H_{t-1}W_{ho}&plus;b_{o})" title="O_{t}=\sigma (X_{t}W_{xo}+H_{t-1}W_{ho}+b_{o})"></p>
<p>这三个门的算式和普通RNN计算Ht算式相同。</p>
<img src="/2024/04/23/11-01-57/57-1.png" class>
<h4 id="2-2候选记忆单元"><a href="#2-2候选记忆单元" class="headerlink" title="2.2候选记忆单元"></a>2.2候选记忆单元</h4><p><img src="https://latex.codecogs.com/svg.image?\widetilde{C_{t}}=tanh(X_{t}W_{xc}&plus;H_{t-1}W_{hc}&plus;b_{c})" title="\widetilde{C_{t}}=tanh(X_{t}W_{xc}+H_{t-1}W_{hc}+b_{c})"></p>
<p>相当于在ht-1到ht的预测中又加了一层隐藏单元</p>
<img src="/2024/04/23/11-01-57/57-2.png" class>
<h4 id="2-2记忆单元"><a href="#2-2记忆单元" class="headerlink" title="2.2记忆单元"></a>2.2记忆单元</h4><p><img src="https://latex.codecogs.com/svg.image?C_{t}=F_{t}\odot&space;C_{t-1}&plus;I_{t}\odot&space;\widetilde{C_{t}}" title="C_{t}=F_{t}\odot C_{t-1}+I_{t}\odot \widetilde{C_{t}}"></p>
<p>如果遗忘门始终为(1)且输入门始终为(0)， 则过去的记忆元 将随时间被保存并传递到当前时间步。 引入这种设计是为了缓解梯度消失问题， 并更好地捕获序列中的长距离依赖关系。</p>
<img src="/2024/04/23/11-01-57/57-3.png" class>
<h4 id="2-3隐状态"><a href="#2-3隐状态" class="headerlink" title="2.3隐状态"></a>2.3隐状态</h4><p><img src="https://latex.codecogs.com/svg.image?H_{t}=O_{t}\odot&space;tanh(C_{t})" title="H_{t}=O_{t}\odot tanh(C_{t})"></p>
<p>最后，我们需要定义如何计算隐状态， 这就是输出门发挥作用的地方。 在长短期记忆网络中，它仅仅是记忆元的的门控版本。 这就确保了Ht的值始终在区间((-1, 1))内.</p>
<p>只要输出门接近1，我们就能够有效地将所有记忆信息传递给预测部分， 而对于输出门接近(0)，我们只保留记忆元内的所有信息，而不需要更新隐状态。</p>
<img src="/2024/04/23/11-01-57/57-4.png" class>
<h4 id="2-4总结"><a href="#2-4总结" class="headerlink" title="2.4总结"></a>2.4总结</h4><p>LSTM的计算流程：</p>
<p><img src="https://latex.codecogs.com/svg.image?I_{t}=\sigma&space;(X_{t}W_{xi}&plus;H_{t-1}W_{hi}&plus;b_{i})" title="I_{t}=\sigma (X_{t}W_{xi}+H_{t-1}W_{hi}+b_{i})"></p>
<p><img src="https://latex.codecogs.com/svg.image?F_{t}=\sigma&space;(X_{t}W_{xf}&plus;H_{t-1}W_{hf}&plus;b_{f})" title="F_{t}=\sigma (X_{t}W_{xf}+H_{t-1}W_{hf}+b_{f})"></p>
<p><img src="https://latex.codecogs.com/svg.image?O_{t}=\sigma&space;(X_{t}W_{xo}&plus;H_{t-1}W_{ho}&plus;b_{o})" title="O_{t}=\sigma (X_{t}W_{xo}+H_{t-1}W_{ho}+b_{o})"></p>
<p><img src="https://latex.codecogs.com/svg.image?\widetilde{C_{t}}=tanh(X_{t}W_{xc}&plus;H_{t-1}W_{hc}&plus;b_{c})" title="\widetilde{C_{t}}=tanh(X_{t}W_{xc}+H_{t-1}W_{hc}+b_{c})"></p>
<p><img src="https://latex.codecogs.com/svg.image?C_{t}=F_{t}\odot&space;C_{t-1}&plus;I_{t}\odot&space;\widetilde{C_{t}}" title="C_{t}=F_{t}\odot C_{t-1}+I_{t}\odot \widetilde{C_{t}}"></p>
<p><img src="https://latex.codecogs.com/svg.image?H_{t}=O_{t}\odot&space;tanh(C_{t})" title="H_{t}=O_{t}\odot tanh(C_{t})"></p>
<h3 id="3-从零实现"><a href="#3-从零实现" class="headerlink" title="3.从零实现"></a>3.从零实现</h3><p>加载时光机器数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>
<h4 id="3-1初始化模型参数"><a href="#3-1初始化模型参数" class="headerlink" title="3.1初始化模型参数"></a>3.1初始化模型参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_lstm_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = three()  <span class="comment"># 候选记忆元参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,</span><br><span class="line">              b_c, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>
<h4 id="3-2定义模型"><a href="#3-2定义模型" class="headerlink" title="3.2定义模型"></a>3.2定义模型</h4><p>在初始化函数中， 长短期记忆网络的隐状态需要返回一个<em>额外</em>的记忆元， 单元的值为0，形状为（批量大小，隐藏单元数）。 因此，我们得到以下的状态初始化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_lstm_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>
<p>实际模型的定义与我们前面讨论的一样： 提供三个门和一个额外的记忆元。 请注意，只有隐状态才会传递到输出层， 而记忆元(\mathbf{C}_t)不直接参与输出计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lstm</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></table></figure>
<h4 id="3-3训练和预测"><a href="#3-3训练和预测" class="headerlink" title="3.3训练和预测"></a>3.3训练和预测</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_lstm_params,</span><br><span class="line">                            init_lstm_state, lstm)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-57/57-5.png" class>
<h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><h5 id="Q1：请问LSTM如果不要C-把公式里的换成，好像可以实现隐藏状态往下传递？"><a href="#Q1：请问LSTM如果不要C-把公式里的换成，好像可以实现隐藏状态往下传递？" class="headerlink" title="Q1：请问LSTM如果不要C,把公式里的换成，好像可以实现隐藏状态往下传递？"></a>Q1：请问LSTM如果不要C,把公式里的<img src="https://latex.codecogs.com/svg.image?&space;C_{t-1}" title=" C_{t-1}">换成<img src="https://latex.codecogs.com/svg.image?&space;H_{t-1}" title=" H_{t-1}">，好像可以实现隐藏状态往下传递？</h5><blockquote>
<p><img src="https://latex.codecogs.com/svg.image?&space;C_{t-1}" title=" C_{t-1}">的可以约束<img src="https://latex.codecogs.com/svg.image?&space;H_{t-1}" title=" H_{t-1}">的大小在0-1之间，避免梯度爆炸，而且使算式更加自然，c换成h复杂度降低。</p>
</blockquote>
<h5 id="Q2：I-F-O-C-tilda的初始化为零？"><a href="#Q2：I-F-O-C-tilda的初始化为零？" class="headerlink" title="Q2：I,F,O,C_tilda的初始化为零？"></a>Q2：I,F,O,C_tilda的初始化为零？</h5><blockquote>
<p>这些是计算的中间变量，不需要初始化</p>
</blockquote>
<h5 id="Q3：如何计算模型占用显存，batch占用的显存？"><a href="#Q3：如何计算模型占用显存，batch占用的显存？" class="headerlink" title="Q3：如何计算模型占用显存，batch占用的显存？"></a>Q3：如何计算模型占用显存，batch占用的显存？</h5><blockquote>
<p>取决于框架和库，没法具体算</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>56-门控循环单元(GRU)</title>
    <url>/2024/04/23/11-01-56/</url>
    <content><![CDATA[<h2 id="56-门控循环单元-GRU"><a href="#56-门控循环单元-GRU" class="headerlink" title="56-门控循环单元(GRU)"></a>56-门控循环单元(GRU)</h2><h3 id="1-动机：如何关注一个序列"><a href="#1-动机：如何关注一个序列" class="headerlink" title="1. 动机：如何关注一个序列"></a>1. 动机：如何关注一个序列</h3><ul>
<li>不是每个观察值都是同等重要</li>
</ul>
<img src="/2024/04/23/11-01-56/56-01.png" class>
<p>比如上图中的序列，若干个猫中出现了一个鼠，那么我们应该重点关注这个鼠，而中间重复出现的猫则减少关注。文本序列同理，通常长文本我们需要关注的是几个关键词，关键句。</p>
<ul>
<li>想只记住相关的观察需要：<ul>
<li>能<strong>关注</strong>的机制（<strong>更新门</strong>）：顾名思义，是否需要根据我的输入，更新隐藏状态</li>
<li>能<strong>遗忘</strong>的机制（<strong>重置门</strong>）：更新候选项时，是否要考虑前一隐藏状态。</li>
</ul>
</li>
</ul>
<h3 id="2-门的概念"><a href="#2-门的概念" class="headerlink" title="2. 门的概念"></a>2. 门的概念</h3><ul>
<li><p>更新门Zt，重置门Rt的公式大体相同，唯一不同的是学习到的参数。</p>
</li>
<li><p>需要注意的是，计算门的方式和原来RNN的实现中计算新的隐状态相似，只是激活函数改成了sigmoid。</p>
</li>
<li>门本来是电路中的一个概念，0,1代表不同的电平，可以用于控制电路的通断。此处sigmoid将门的数值归一化到0到1之间，是一种”软更新”方式。而从后面的公式上可以看出，本讲课程采用的是低电平有效（越靠近0，门的作用越明显）的方式控制。</li>
</ul>
<img src="/2024/04/23/11-01-56/56-02.png" class>
<h3 id="3-候选隐状态"><a href="#3-候选隐状态" class="headerlink" title="3. 候选隐状态"></a>3. 候选隐状态</h3><img src="/2024/04/23/11-01-56/56-03.png" class>
<ul>
<li><p>候选隐状态，如果抛开公式中的$R_{t}$遗忘门来说，这个和之前RNN中计算当前步的隐状态没有差别。</p>
</li>
<li><p>但是这里引入了遗忘门，如果$R_{t}$无限接近于0，那么此时候选隐状态将不再考虑前一隐状态的影响，也就是和MLP没有区别，起到“遗忘”的作用；</p>
</li>
<li>反之，如果$R_{t}$无限接近于1，那么与RNN计算隐状态的过程没有差别，不进行遗忘。</li>
<li>公式中的⊙表示逐元素乘积。</li>
</ul>
<blockquote>
<p>为什么叫候选隐状态？</p>
<p>在RNN中，这个所谓的候选隐状态就是当前步的隐状态（$R_{t}$无限接近1时）。但是由于引入了更新门，我们需要考虑是直接沿用上一步的隐藏状态，还是像RNN一样使用当前步计算的隐状态。所以这个结合了当前输入计算的隐状态，不能立马变成当前的$H_{t}$，而是需要用更新门和前一隐状态$H_{t-1}$做一个加权，所以它是一个候选项。</p>
</blockquote>
<h3 id="4-隐状态"><a href="#4-隐状态" class="headerlink" title="4. 隐状态"></a>4. 隐状态</h3><img src="/2024/04/23/11-01-56/56-04.png" class>
<p>用更新门对<strong>候选隐状态</strong>和<strong>前一隐状态</strong>做加权，得到当前步<strong>隐状态</strong>的值。</p>
<p>如果$Z_{t}$无限接近于0，更新起作用，候选隐状态“转正”，变为当前隐状态。</p>
<p>如果$Z_{t}$无限接近于1，更新不起作用，当前隐状态还是沿用前一隐状态。</p>
<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><img src="/2024/04/23/11-01-56/56-05.png" class>
<p>上图四行公式概括了GRU模型。在RNN的基础上，最重要的是引入了<strong>更新门和重置门</strong>，来决定前一隐状态对当前隐状态的影响。以最开始的猫鼠序列的例子来说，如果我的模型一直看到猫，模型可以学习到隐状态不怎么去更新，于是隐状态一直保留了猫的信息，而看到老鼠，隐状态才进行更新。</p>
<ul>
<li>对于一个更具体的例子而言(语言模型)：</li>
</ul>
<p>“The cat, which already ate ……, __(is/ was) full.”，假设我的句子很长，预测完前面的词后需要预测下一个词is还是was，如果引入这种更新/重置的机制，那我们的模型可以在was这个词之前尽可能去保持隐状态的信息，从而即使阅读了一个很长的定语从句，但我们还是保留了cat这个词的单数信息，从而模型预测下一个词为’was’。</p>
<ul>
<li>一个与RNN的联动在于：</li>
</ul>
<p>如果更新门完全发挥作用（无限接近于0），重置门不起作用（无限接近于1），此时GRU模型退化为RNN模型。</p>
<h3 id="6-QA"><a href="#6-QA" class="headerlink" title="6. QA"></a>6. QA</h3><p>问题：GRU为什么需要两个门？</p>
<blockquote>
<p>重置门和更新门各司其职。重置门单方面控制自某个节点开始，之前的记忆（隐状态）不在乎了，直接清空影响，同时也需要更新门帮助它实现记忆的更新。更新门更多是用于处理梯度消失问题，可以选择一定程度地保留记忆，防止梯度消失。</p>
<p>重置门影响的是当前步新的候选隐状态的计算，更新门影响的是当前步隐状态的更新程度。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>54-循环神经网络RNN</title>
    <url>/2024/04/23/11-01-54/</url>
    <content><![CDATA[<h3 id="使用潜变量"><a href="#使用潜变量" class="headerlink" title="使用潜变量"></a>使用潜变量</h3><ul>
<li>RNN使用了隐藏层来记录过去发生的所有事件的信息，从而引入时许的特性，并且避免常规序列模型每次都要重新计算前面所有已发生的事件而带来的巨大计算量。<img src="https://github.com/kinza99/DeepLearning-MuLi-Notes/blob/main/imgs/54/54-01.png" alt="截屏2022-02-12 下午2.17.32"></li>
</ul>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><ul>
<li>流程如下，首先有一个输入序列，对于时刻t，我们用t-1时刻的输入x~t-1~和潜变量h~t-1~来计算新的潜变量h~t~。同时，对于t时刻的输出o~t~，则直接使用h~t~来计算得到。注意，计算第一个潜变量只需要输入即可（因为前面并不存在以往的潜变量）。<img src="https://github.com/kinza99/DeepLearning-MuLi-Notes/blob/main/imgs/54/54-02.png" alt="截屏2022-02-12 下午2.34.14"> </li>
<li>值得注意的是，RNN本质也是一种MLP，尤其是将h~t-1~这一项去掉时就完全退化成了MLP。RNN的核心其实也就是h~t-1~这一项，它使得模型可以和前面的信息联系起来，将时序信息储存起来，可以把RNN理解为是包含时序信息的MLP。</li>
</ul>
<h3 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h3><ul>
<li>为了衡量一个语言模型的好坏，例如分类模型，可以使用平均交叉熵来衡量，就是将预测概率的负对数值求和之后再去平均，即常用的交叉熵损失。但是由于某些历史原因，NLP往往不是用这种方式，而是在这种方式的基础上最后再取指数，即exp，这样得到的结果如果是1，说明完美；如果是无穷大，说明结果很差。</li>
</ul>
<h3 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h3><ul>
<li>在T个时间步中进行反向传播，会由于产生O(T)长度的梯度乘法链，导致导数数值不稳定，这里使用一个限制θ，通常为5到10，来控制梯度乘法链的长度。使用如下的公式<img src="https://github.com/kinza99/DeepLearning-MuLi-Notes/blob/main/imgs/54/54-03.png" alt="截屏2022-02-12 下午3.10.38">在这个公式中，如果梯度长度大于θ，梯度g会变为<script type="math/tex; mode=display">
\frac{\theta}{\Vert g \Vert} g</script>这样再对g求2范式就变成了θ，所以可以把梯度限制在θ以下。</li>
</ul>
<h3 id="更多的应用RNNs"><a href="#更多的应用RNNs" class="headerlink" title="更多的应用RNNs"></a>更多的应用RNNs</h3><ul>
<li>基本的应用如下图<img src="https://github.com/kinza99/DeepLearning-MuLi-Notes/blob/main/imgs/54/54-04.png" alt="截屏2022-02-12 下午3.36.29"></li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>53-语言模型</title>
    <url>/2024/04/23/11-01-53/</url>
    <content><![CDATA[<h3 id="53-语言模型"><a href="#53-语言模型" class="headerlink" title="53 语言模型"></a>53 语言模型</h3><ul>
<li><p>语言模型的目标：</p>
<p>假设长度为<em>T</em>的文本序列中的词元依次为<em>x</em>~1~,<em>x</em>~2~,…,<em>x~T~</em>。 于是，<em>x~t~</em>（1≤<em>t</em>≤<em>T</em>） 可以被认为是文本序列在时间步<em>t</em>处的观测或标签。 在给定这样的文本序列时目标是估计序列的联合概率<em>P</em>(<em>x</em>~1~,<em>x~2~</em>,…,<em>x~T~</em>)</p>
</li>
</ul>
<h4 id="学习语言模型"><a href="#学习语言模型" class="headerlink" title="学习语言模型"></a>学习语言模型</h4><ul>
<li><p>基本想法：</p>
<p><em>P</em>(<em>x</em>~1~,<em>x~2~</em>,…,<em>x~T~</em>) = <em>P</em>(<em>x~t~</em>∣<em>x</em>~1~,…,<em>x~t−1~</em>). (1 &lt;= t &lt;= T) 共T个结果相乘</p>
<p>例如，包含了四个单词的一个文本序列的概率是：</p>
<p><em>P</em>(deep,learning,is,fun)=<em>P</em>(deep)<em>P</em>(learning∣deep)<em>P</em>(is∣deep,learning)<em>P</em>(fun∣deep,learning,is) </p>
<p>为了训练语言模型，我们需要计算单词的概率， 以及给定前面几个单词后出现某个单词的条件概率。 这些概率本质上就是语言模型的参数。训练数据集中词的概率可以根据给定词的相对词频来计算。 例如，可以将估计值<em>P</em>^(deep) 计算为任何以单词“deep”开头的句子的概率。 一种（稍稍不太精确的）方法是统计单词“deep”在数据集中的出现次数， 然后将其除以整个语料库中的单词总数。 这种方法效果不错，特别是对于频繁出现的单词。</p>
</li>
<li><p>基本想法的问题：</p>
<p>由于连续单词对“deep learning”的出现频率要低得多， 所以估计这类单词正确的概率要困难得多。 特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计可能都不容易。 而对于三个或者更多的单词组合，情况会变得更糟。 许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。 除非我们提供某种解决方案，来将这些单词组合指定为非零计数， 否则将无法在语言模型中使用它们。 如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。</p>
</li>
</ul>
<h4 id="马尔可夫模型与n元语法"><a href="#马尔可夫模型与n元语法" class="headerlink" title="马尔可夫模型与n元语法"></a>马尔可夫模型与n元语法</h4><ul>
<li>如果<em>P</em>(x~t+1~∣<em>x~t~</em>,…,<em>x</em>~1~)=<em>P</em>(x~t+1~∣<em>x~t~</em>)， 则序列上的分布满足一阶马尔可夫性质。 阶数越高，对应的依赖关系就越长。 这种性质推导出了许多可以应用于序列建模的近似公式：<ul>
<li>P(x~1~,x~2~,x~3~,x~4~) = P(x~1~)P(x~2~)P(x~3~)P(x~4~)</li>
<li>P(x~1~,x~2~,x~3~,x~4~) = P(x~1~)P(x~2~ |x~1~)P(x~3~|x~2~)P(x~4~|x~3~)</li>
<li>P(x~1~,x~2~,x~3~,x~4~) = P(x~1~)P(x~2~ |x~1~)P(x~3~|x~1~,x~2~)P(x~4~|x~2~,x~3~)</li>
</ul>
</li>
</ul>
<h4 id="自然语言统计"><a href="#自然语言统计" class="headerlink" title="自然语言统计"></a>自然语言统计</h4><ul>
<li>在真实数据上如果进行自然语言统计：</li>
</ul>
<p>根据前几节介绍的时光机器数据集构建词表，并打印前10个最常用的单词</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">tokens = d2l.tokenize(d2l.read_time_machine())</span><br><span class="line"><span class="comment"># 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起</span></span><br><span class="line">corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">vocab = d2l.Vocab(corpus)</span><br><span class="line">vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[(<span class="string">&#x27;the&#x27;</span>, <span class="number">2261</span>),</span><br><span class="line"> (<span class="string">&#x27;i&#x27;</span>, <span class="number">1267</span>),</span><br><span class="line"> (<span class="string">&#x27;and&#x27;</span>, <span class="number">1245</span>),</span><br><span class="line"> (<span class="string">&#x27;of&#x27;</span>, <span class="number">1155</span>),</span><br><span class="line"> (<span class="string">&#x27;a&#x27;</span>, <span class="number">816</span>),</span><br><span class="line"> (<span class="string">&#x27;to&#x27;</span>, <span class="number">695</span>),</span><br><span class="line"> (<span class="string">&#x27;was&#x27;</span>, <span class="number">552</span>),</span><br><span class="line"> (<span class="string">&#x27;in&#x27;</span>, <span class="number">541</span>),</span><br><span class="line"> (<span class="string">&#x27;that&#x27;</span>, <span class="number">443</span>),</span><br><span class="line"> (<span class="string">&#x27;my&#x27;</span>, <span class="number">440</span>)]</span><br></pre></td></tr></table></figure>
<ul>
<li>正如我们所看到的，最流行的词看起来很无聊， 这些词通常被称为<em>停用词</em>（stop words），因此可以被过滤掉。 尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。 此外，还有个明显的问题是词频衰减的速度相当地快。 例如，最常用单词的词频对比，第10个还不到第1个的1/5。 为了更好地理解，我们可以画出的词频图：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs]</span><br><span class="line">d2l.plot(freqs, xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>,</span><br><span class="line">         xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>我们可以发现：词频以一种明确的方式迅速衰减。 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。</p>
<ul>
<li>我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bigram_tokens = [pair <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">1</span>], corpus[<span class="number">1</span>:])]</span><br><span class="line">bigram_vocab = d2l.Vocab(bigram_tokens)</span><br><span class="line">bigram_vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[((<span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">309</span>),</span><br><span class="line"> ((<span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">169</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;had&#x27;</span>), <span class="number">130</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;was&#x27;</span>), <span class="number">112</span>),</span><br><span class="line"> ((<span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">109</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>), <span class="number">102</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;was&#x27;</span>), <span class="number">99</span>),</span><br><span class="line"> ((<span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">85</span>),</span><br><span class="line"> ((<span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;i&#x27;</span>), <span class="number">78</span>),</span><br><span class="line"> ((<span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), <span class="number">73</span>)]</span><br></pre></td></tr></table></figure>
<p>这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的， 只有一个与“the time”有关。 我们再进一步看看三元语法的频率是否表现出相同的行为方式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trigram_tokens = [triple <span class="keyword">for</span> triple <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">    corpus[:-<span class="number">2</span>], corpus[<span class="number">1</span>:-<span class="number">1</span>], corpus[<span class="number">2</span>:])]</span><br><span class="line">trigram_vocab = d2l.Vocab(trigram_tokens)</span><br><span class="line">trigram_vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;traveller&#x27;</span>), <span class="number">59</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>), <span class="number">30</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;medical&#x27;</span>, <span class="string">&#x27;man&#x27;</span>), <span class="number">24</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;seemed&#x27;</span>, <span class="string">&#x27;to&#x27;</span>), <span class="number">16</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), <span class="number">15</span>),</span><br><span class="line"> ((<span class="string">&#x27;here&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;there&#x27;</span>), <span class="number">15</span>),</span><br><span class="line"> ((<span class="string">&#x27;seemed&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;me&#x27;</span>), <span class="number">14</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;did&#x27;</span>, <span class="string">&#x27;not&#x27;</span>), <span class="number">14</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;saw&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">13</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;began&#x27;</span>, <span class="string">&#x27;to&#x27;</span>), <span class="number">13</span>)]</span><br></pre></td></tr></table></figure>
<ul>
<li>最后，我们直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> bigram_vocab.token_freqs]</span><br><span class="line">trigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> trigram_vocab.token_freqs]</span><br><span class="line">d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel=<span class="string">&#x27;token: x&#x27;</span>,</span><br><span class="line">         ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">&#x27;unigram&#x27;</span>, <span class="string">&#x27;bigram&#x27;</span>, <span class="string">&#x27;trigram&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://zh-v2.d2l.ai/_images/output_language-models-and-dataset_789d14_54_0.svg" alt="../_images/output_language-models-and-dataset_789d14_54_0.svg"></p>
<h4 id="读取长序列数据"><a href="#读取长序列数据" class="headerlink" title="读取长序列数据"></a>读取长序列数据</h4><ul>
<li><p>由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。在前几节中我们以一种相当特别的方式做到了这一点： 当序列变得太长而不能被模型一次性全部处理时， 我们可能希望拆分这样的序列方便模型读取。</p>
</li>
<li><p>在介绍该模型之前，我们看一下总体策略。 假设我们将使用神经网络来训练语言模型， 模型中的网络一次处理具有预定义长度 （例如<em>n</em>个时间步）的一个小批量序列。 现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。首先，由于文本序列可以是任意长的， 例如整本《时光机器》（<em>The Time Machine</em>）， 于是任意长的序列可以被我们划分为具有相同时间步数的子序列。 当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。 假设网络一次只处理具有<em>n</em>个时间步的子序列。下画出了从原始文本序列获得子序列的所有不同的方式， 其中<em>n</em>=5，并且每个时间步的词元对应于一个字符。 请注意，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。</p>
<p><img src="https://zh-v2.d2l.ai/_images/timemachine-5gram.svg" alt="../_images/timemachine-5gram.svg"></p>
</li>
</ul>
<p>因此，我们应该从中选择哪一个呢？ 事实上，他们都一样的好。 然而，如果我们只选择一个偏移量， 那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。 因此，我们可以从随机偏移量开始划分序列， 以同时获得<em>覆盖性</em>（coverage）和<em>随机性</em>（randomness）。 下面，我们将描述如何实现<em>随机采样</em>（random sampling）和 <em>顺序分区</em>（sequential partitioning）策略。</p>
<h4 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h4><ul>
<li>在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列。下面的代码每次可以从数据中随机生成一个小批量。 在这里，参数<code>batch_size</code>指定了每个小批量中子序列样本的数目， 参数<code>num_steps</code>是每个子序列中预定义的时间步数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用随机抽样生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    <span class="comment"># 减去1，是因为我们需要考虑标签</span></span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># 长度为num_steps的子序列的起始索引</span></span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class="line">    <span class="comment"># 在随机抽样的迭代过程中，</span></span><br><span class="line">    <span class="comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span></span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="comment"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_subseqs // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        <span class="comment"># 在这里，initial_indices包含子序列的随机起始索引</span></span><br><span class="line">        initial_indices_per_batch = initial_indices[i: i + batch_size]</span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure>
<ul>
<li>下面我们生成一个从0到34的序列。 假设批量大小为2，时间步数为5，这意味着可以生成 ⌊(35−1)/5⌋=6个“特征－标签”子序列对。 如果设置小批量大小为2，我们只能得到3个小批量。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br></pre></td></tr></table></figure>
<h4 id="顺序分区"><a href="#顺序分区" class="headerlink" title="顺序分区"></a>顺序分区</h4><ul>
<li>在迭代过程中，除了对原始序列可以随机抽样外， 我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始划分序列</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)</span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure>
<ul>
<li>基于相同的设置，通过顺序分区读取每个小批量的子序列的特征<code>X</code>和标签<code>Y</code>。 通过将它们打印出来可以发现： 迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br></pre></td></tr></table></figure>
<ul>
<li>现在，我们将上面的两个采样函数包装到一个类中， 以便稍后可以将其用作数据迭代器。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SeqDataLoader</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> use_random_iter:</span><br><span class="line">            self.data_iter_fn = d2l.seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class="line">        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class="line">        self.batch_size, self.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)</span><br></pre></td></tr></table></figure>
<ul>
<li>最后，我们定义了一个函数<code>load_data_time_machine</code>， 它同时返回数据迭代器和词表， 因此可以与其他带有<code>load_data</code>前缀的函数类似地使用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_time_machine</span>(<span class="params">batch_size, num_steps,  <span class="comment">#@save</span></span></span><br><span class="line"><span class="params">                           use_random_iter=<span class="literal">False</span>, max_tokens=<span class="number">10000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    data_iter = SeqDataLoader(</span><br><span class="line">        batch_size, num_steps, use_random_iter, max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>51-序列模型</title>
    <url>/2024/04/23/11-01-51/</url>
    <content><![CDATA[<h1 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h1><h3 id="2-序列数据"><a href="#2-序列数据" class="headerlink" title="2.序列数据"></a>2.序列数据</h3><ul>
<li>实际中很多数据是有时序的</li>
<li>电影的评价随时间变化而变化<ul>
<li>拿了奖后评分上升，直到奖项被遗忘</li>
<li>看了很多好电影后，人们的期望变高</li>
<li>季节性：贺岁片，暑期档</li>
<li>导演、演员的负面报道导致评分变低</li>
</ul>
</li>
</ul>
<h4 id="2-1-更多例子"><a href="#2-1-更多例子" class="headerlink" title="2.1 更多例子"></a>2.1 更多例子</h4><ul>
<li><p>音乐、文本、语言和视频都是连续的</p>
<ul>
<li>标题“狗咬人”远没有“人咬狗”那么令人惊讶</li>
</ul>
</li>
<li><p>大地震发生后，很有可能会有几次较小的余震</p>
</li>
<li>人的互动是连续的，从网上吵架可以看出</li>
<li>预测明天的股价要比填补昨天遗失的股价更困难</li>
</ul>
<h3 id="3-统计工具"><a href="#3-统计工具" class="headerlink" title="3.统计工具"></a>3.统计工具</h3><ul>
<li><p>在时间t观察到<img src="https://latex.codecogs.com/svg.image?X_{t}" title="X_{t}">，那么得到T个不独立的随机变量<img src="https://latex.codecogs.com/svg.image?(X_{1},...X_{t})\sim&space;p(X)" title="(X_{1},...X_{t})\sim p(X)"></p>
</li>
<li><p>使用条件概率展开</p>
<p><img src="https://latex.codecogs.com/svg.image?p(a,b)=p(a)p(b|a)=p(b)p(a|b)" title="p(a,b)=p(a)p(b|a)=p(b)p(a|b)"></p>
</li>
</ul>
<img src="/2024/04/23/11-01-51/51-01.png" class>
<h3 id="4-序列模型"><a href="#4-序列模型" class="headerlink" title="4.序列模型"></a>4.序列模型</h3><img src="/2024/04/23/11-01-51/51-02.png" class>
<ul>
<li>对条件概率建模</li>
</ul>
<img src="/2024/04/23/11-01-51/51-03.png" class>
<h4 id="4-1-方案A-马尔科夫假设"><a href="#4-1-方案A-马尔科夫假设" class="headerlink" title="4.1 方案A:马尔科夫假设"></a>4.1 方案A:马尔科夫假设</h4><img src="/2024/04/23/11-01-51/51-04.png" class>
<ul>
<li>假设当前数据只跟τ个过去数据点相关</li>
</ul>
<img src="/2024/04/23/11-01-51/51-05.png" class>
<h4 id="4-2-方案B-潜变量模型"><a href="#4-2-方案B-潜变量模型" class="headerlink" title="4.2 方案B:潜变量模型"></a>4.2 方案B:潜变量模型</h4><img src="/2024/04/23/11-01-51/51-06.png" class>
<ul>
<li>引入潜变量<img src="https://latex.codecogs.com/svg.image?h_{t}" title="h_{t}">来表示过去信息<img src="https://latex.codecogs.com/svg.image?h_{t}=f(x_{1},...x_{t-1})" title="h_{t}=f(x_{1},...x_{t-1})"><ul>
<li>这样<img src="https://latex.codecogs.com/svg.image?x_{t}=p(x_{t}|h_{t})" title="x_{t}=p(x_{t}|h_{t})"></li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-01-51/51-07.png" class>
<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h3><ul>
<li>时序模型中，当前数据跟之前观察到的数据相关</li>
<li>自回归模型使用自身过去数据来预测未来</li>
<li>马尔科夫模型假设当前只跟最近少数数据相关，从而简化模型</li>
<li>潜变量模型使用潜变量来概括历史信息</li>
</ul>
<h3 id="6-Q-amp-A："><a href="#6-Q-amp-A：" class="headerlink" title="6.Q&amp;A："></a>6.Q&amp;A：</h3><h5 id="Q1-在常规范围内tau是不是越大越好。刚才例子tau-5是不是比4好？"><a href="#Q1-在常规范围内tau是不是越大越好。刚才例子tau-5是不是比4好？" class="headerlink" title="Q1:在常规范围内tau是不是越大越好。刚才例子tau=5是不是比4好？"></a>Q1:在常规范围内tau是不是越大越好。刚才例子tau=5是不是比4好？</h5><blockquote>
<p>当然比4好，也有局限性，tau特别大，训练样本变小，模型变复杂</p>
</blockquote>
<h5 id="Q2：潜变量模型和隐马尔科夫模型有什么区别？"><a href="#Q2：潜变量模型和隐马尔科夫模型有什么区别？" class="headerlink" title="Q2：潜变量模型和隐马尔科夫模型有什么区别？"></a>Q2：潜变量模型和隐马尔科夫模型有什么区别？</h5><blockquote>
<p>没有太多联系，两个不同的观点，但是潜变量模型可以使用隐马尔科夫假设。潜变量-怎么建模，隐马尔科夫-这个数据和之前多少个数据有关。</p>
</blockquote>
<h5 id="Q3：若预测一个月，tau-30-预测7天，tau-7，是否有这样的关系？"><a href="#Q3：若预测一个月，tau-30-预测7天，tau-7，是否有这样的关系？" class="headerlink" title="Q3：若预测一个月，tau=30,预测7天，tau=7，是否有这样的关系？"></a>Q3：若预测一个月，tau=30,预测7天，tau=7，是否有这样的关系？</h5><blockquote>
<p>tau取决于对数据的理解，没有固定的规则</p>
</blockquote>
<h5 id="Q4：在预测未来方面，现在的sota模型能做到多好？"><a href="#Q4：在预测未来方面，现在的sota模型能做到多好？" class="headerlink" title="Q4：在预测未来方面，现在的sota模型能做到多好？"></a>Q4：在预测未来方面，现在的sota模型能做到多好？</h5><blockquote>
<p>具体问题具体分析，在有些领域做得好比如写作，写代码，在一些领域做的不好，比如预测股票。</p>
</blockquote>
<h5 id="Q5-tau能够随着xt的变化而变化吗？这样感觉更符合实际情况"><a href="#Q5-tau能够随着xt的变化而变化吗？这样感觉更符合实际情况" class="headerlink" title="Q5:tau能够随着xt的变化而变化吗？这样感觉更符合实际情况"></a>Q5:tau能够随着xt的变化而变化吗？这样感觉更符合实际情况</h5><blockquote>
<p>当然可以，有计算量的增加，也不一定更好</p>
</blockquote>
<h5 id="Q6-预测电池之类很多参数的未来变化趋势时怎么长步预测？"><a href="#Q6-预测电池之类很多参数的未来变化趋势时怎么长步预测？" class="headerlink" title="Q6:预测电池之类很多参数的未来变化趋势时怎么长步预测？"></a>Q6:预测电池之类很多参数的未来变化趋势时怎么长步预测？</h5><blockquote>
<p>与数据关系比较大，负类样本较少，所以比较难训练</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>50-课程竞赛：牛仔行头检测</title>
    <url>/2024/04/23/11-01-50/</url>
    <content><![CDATA[<h2 id="课程竞赛：牛仔行头检测"><a href="#课程竞赛：牛仔行头检测" class="headerlink" title="课程竞赛：牛仔行头检测"></a>课程竞赛：牛仔行头检测</h2><h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><p>图像中的目标检测，检测牛仔的装备，主要包括：夹克，墨镜，靴子，牛仔帽，腰带</p>
<p>有6937张训练图片，12660个标注框，数据集使用MS-COCO格式，可以调用pycocotools库，评测使用mAP（评测对每个类预测的框的好坏）</p>
<p>挑战：五个类别出现次数不同，墨镜、夹克次数多，牛仔帽、靴子其次，腰带很少</p>
<img src="/2024/04/23/11-01-50/%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1.png" class>
<h3 id="安排"><a href="#安排" class="headerlink" title="安排"></a>安排</h3><p>Kaggle不支持mAP，提交结果csv文件时网址不同</p>
<p>公私榜分配和之前不同，之前kaggle从所有数据中选出一部分作为私榜，本次限定了时间公榜结束后12小时内拿到私榜数据并提交结果，至多提交三次</p>
<p>提供了一个示例程序</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>49-样式迁移</title>
    <url>/2024/04/23/11-01-49/</url>
    <content><![CDATA[<h2 id="样式迁移"><a href="#样式迁移" class="headerlink" title="样式迁移"></a>样式迁移</h2><p>样式迁移类似于手机相机中的滤镜，指的是给定内容图片和风格图片，合成一张新的图片，使得他的内容与内容图片相似，而风格却是风格图片的样子，如下例：</p>
<img src="/2024/04/23/11-01-49/%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB%E7%A4%BA%E4%BE%8B.png" class>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>如下图所示，可以用一个预训练好的神经网络来实现样式迁移：</p>
<ul>
<li>1：复制内容图片来初始化一张图片，之后将这张图片中的像素作为参数进行更新，最终得到合成的图片</li>
<li>2：将内容图片，当前的合成图片，样式图片分别输入一个相同的预训练好的神经网络</li>
<li>3：假设该神经网络的不同层分别提取与内容和风格相关的信息，可以据此得到一个损失：<ul>
<li>将合成图片与<strong>内容</strong>相关的层的输出与<strong>内容</strong>图片对应层的输出进行处理，得到<strong>内容损失</strong></li>
<li>将合成图片与<strong>风格</strong>相关的层的输出与<strong>风格</strong>图片对应层的输出进行处理，得到<strong>风格损失</strong></li>
<li>对合成图片本身，统计图片中的高频噪点（即过明或过暗像素点），得到<strong>全变分损失</strong></li>
<li>将三部分损失加起来得到总的样式迁移的<strong>损失函数</strong></li>
</ul>
</li>
<li>4：利用3定义的损失函数，以合成图片的每个像素点为参数进行梯度下降，得到最终合成的图片</li>
</ul>
<img src="/2024/04/23/11-01-49/CNN%E5%AE%9E%E7%8E%B0.png" class>
<h3 id="内容损失"><a href="#内容损失" class="headerlink" title="内容损失"></a>内容损失</h3><p>神经网络内容相关层的输出的相似度可以直接反应两张图片在内容上的相似度，因此内容损失可以直接将对应层的输出视为内容直接求平方差损失。</p>
<h3 id="风格损失：格拉姆矩阵"><a href="#风格损失：格拉姆矩阵" class="headerlink" title="风格损失：格拉姆矩阵"></a>风格损失：格拉姆矩阵</h3><p>对于内容层，可以直接将对应层的输出求平方差损失，但是对于风格则略有不同</p>
<p>一般认为，风格层对应的输出的多个通道分别对应着不同类型的信息，如果将输出的形状从$[batch_size=1,channels,h,w]$转化为$[channels,h*w]$就能得到通道数个向量，以$x_1,x_2…x_c$表示，代表不同通道所提取出的不同信息，而风格可以视作这些信息之间的关联，即相似度。</p>
<p>定义格拉姆矩阵$\bold X * \bold X^T \in R^{c \cdot c}$，矩阵的第i行第j列即向量$x_i$与$x_j$的内积，这个矩阵就代表了一张图片的风格。</p>
<p>对于生成图片和风格图片的格拉姆矩阵求平方差损失就能得到所需的风格损失</p>
<p>此外，为了让风格损失不受格拉姆矩阵及向量的大小影响，实际将格拉姆矩阵除以这些大小$channnels<em>h</em>w$。</p>
<h3 id="全变分损失"><a href="#全变分损失" class="headerlink" title="全变分损失"></a>全变分损失</h3><p>用于去除高频噪点（过明过暗像素点）</p>
<script type="math/tex; mode=display">
\sum_{i, j} \left|x_{i, j} - x_{i+1, j}\right| + \left|x_{i, j} - x_{i, j+1}\right|</script>]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>48-全连接卷积神经网络（FCN）</title>
    <url>/2024/04/23/11-01-48/</url>
    <content><![CDATA[<h2 id="48-全连接卷积神经网络（FCN）"><a href="#48-全连接卷积神经网络（FCN）" class="headerlink" title="48.全连接卷积神经网络（FCN）"></a>48.全连接卷积神经网络（FCN）</h2><img src="/2024/04/23/11-01-48/48-01.png" class>
<ul>
<li><p>代码</p>
<ul>
<li>见code</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>47-转置卷积</title>
    <url>/2024/04/23/11-01-47/</url>
    <content><![CDATA[<h3 id="47-转置卷积"><a href="#47-转置卷积" class="headerlink" title="47-转置卷积"></a>47-转置卷积</h3><h3 id="1-转置卷积"><a href="#1-转置卷积" class="headerlink" title="1.转置卷积"></a>1.转置卷积</h3><ul>
<li>转置卷积和卷积的区别：<ul>
<li>卷积不会增大输入的高宽，通常要么不变、要么减半</li>
<li>转置卷积则可以用来增大输入高宽</li>
</ul>
</li>
<li>转置卷积的具体实现：</li>
</ul>
<img src="/2024/04/23/11-01-47/47-01.png" class>
<p>如图所示，input里的每个元素和kernel相乘，最后把对应位置相加，相当于卷积的逆变换</p>
<ul>
<li>为什么称之为“转置：<ul>
<li>对于卷积Y=X*W<ul>
<li>可以对W构造一个V，使得卷积等价于矩阵乘法Y’=VX’</li>
<li>这里Y’,X’是Y,X对应的向量版本</li>
</ul>
</li>
<li>转置卷积等价于Y’=VTX’</li>
<li>如果卷积将输入从（h，w）变成了（h‘，w’）<ul>
<li>同样超参数的转置卷积则从（h‘，w’）变成为（h，w）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-转置卷积是一种卷积"><a href="#2-转置卷积是一种卷积" class="headerlink" title="2.转置卷积是一种卷积"></a>2.转置卷积是一种卷积</h3><ul>
<li><p>重新排列输入和核</p>
<ul>
<li>当填充为0步幅为1时：<ul>
<li>将输入填充k-1（k时核窗口）</li>
<li>将核矩阵上下、左右翻转</li>
<li>然后做正常卷积（填充0、步幅1）</li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-01-47/47-02.png" class>
<ul>
<li>当填充为p步幅为1时：<ul>
<li>将输如填充k-p-1（k是核窗口）</li>
<li>将核矩阵上下、左右翻转</li>
<li>然后做正常卷积（填充0、步幅1）</li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-01-47/47-03.png" class>
<ul>
<li><p>当填充为p步幅为s时：</p>
<ul>
<li><p>在行和列之间插入s-1行或列</p>
</li>
<li><p>将输如填充k-p-1（k是核窗口）</p>
</li>
<li>将核矩阵上下、左右翻转</li>
<li>然后做正常卷积（填充0、步幅1）</li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-01-47/47-04.png" class>
</li>
<li><p>形状换算</p>
<ul>
<li><p>输入高（宽）为n，核k，填充p，步幅s：</p>
<ul>
<li><p>转置卷积：n‘=sn+k-2p-s</p>
</li>
<li><p>卷积：n’=[(n-k-2p+s)/s]向下取整</p>
</li>
</ul>
</li>
<li><p>如果让高宽成倍增加，那么k=2p+s</p>
</li>
</ul>
</li>
<li><p>同反卷积的关系</p>
<ul>
<li>数学上的反卷积是指卷积的逆运算<ul>
<li>若Y=conv（X,K），那么X=deconv（Y,K）</li>
</ul>
</li>
<li>反卷积很少用在深度学习中<ul>
<li>我们说的反卷积神经网络指用了转置卷积的神经网络</li>
</ul>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>转置卷积是一种变化了输入和核的卷积，来得到上采用的目的</li>
<li>不等同于数学上的反卷积操作</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>46.语义分割</title>
    <url>/2024/04/23/11-01-46/</url>
    <content><![CDATA[<h2 id="46-语义分割"><a href="#46-语义分割" class="headerlink" title="46 语义分割"></a>46 语义分割</h2><h3 id="1-语义分割"><a href="#1-语义分割" class="headerlink" title="1.语义分割"></a>1.语义分割</h3><p>有时只能实现框选的目标检测还是太粗糙了，无法得到更精细的信息。语义分割将图片中的每个像素分类到对应的类别。</p>
<p>分割这一概念在计算机视觉中由来已久。最早的图片分割对给定图片使用聚类等方法把语义上比较像的像素放在一起，但通常不会告诉我们这些像素到底是什么。而语义分割可以告诉我们每个像素对应的label是什么。</p>
<p>这也意味着我们需要对图片的每一个像素都做label，使得语义分割成为了一个比较精细且大的任务。语义分割的数据集成本也较高，往往规模小像素高。常用的数据集之一是Pascal VOC2012。</p>
<img src="/2024/04/23/11-01-46/46-01.jpg" class>
<h3 id="2-应用"><a href="#2-应用" class="headerlink" title="2.应用"></a>2.应用</h3><p>背景虚化：传统的背景替换往往采用绿幕。在没有绿幕的情况下传统相机可以通过光圈来实现背景虚化，对于手机等设备而言背景虚化通常使用的都是语义分割或结合图像景深信息。</p>
<p>路面分割：如无人驾驶时用于实时识别周围物体，实现找路的功能。</p>
<h3 id="3-实例分割"><a href="#3-实例分割" class="headerlink" title="3.实例分割"></a>3.实例分割</h3><p>语义分割只关心像素属于哪一类，而实例分割则更进一步，如图片里有两只狗，则需要得出哪个像素属于哪一只狗。可以将其理解为目标检测的进化版本。</p>
<img src="/2024/04/23/11-01-46/46-02.png" class>
<h3 id="4-QA"><a href="#4-QA" class="headerlink" title="4.QA"></a>4.QA</h3><p>Q1: 能否做更细的语义分割如狗的头/身/腿？</p>
<blockquote>
<p>可以，有标注数据即可，不过可能会出现不同标注者对身体部位分界不同之类的问题。针对狗头这一例子可以考虑使用姿态识别得到关节点。</p>
</blockquote>
<p>Q2: 目标检测里做图像增广，目标框做对应变换后不再是矩形怎么办？</p>
<blockquote>
<p>如果很关心角度信息可以给框加入一个表示旋转角度的feature，也可以考虑在旋转后的原框外面画一个大框把它圈起来，这个大框是可以计算出来的。</p>
</blockquote>
<p>Q3: 把人像语义分割做到slides中的效果大概需要多少训练集？</p>
<blockquote>
<p>人像这块技术相对比较成熟，应该能找到很好的预训练模型在其基础上调整即可。人像的形状是比较容易做的，难点主要在于光照不同（可能使背景与人像/衣服模糊）</p>
</blockquote>
<p>Q4: 三维语义分割标注怎么做？</p>
<blockquote>
<p>（这里把三维理解成有景深的图片）一个简单的做法是把图片压缩成2维，也可以使用三维卷积。三维的分割实际上是好做的，因为一个物体的像素景深往往是连续的且与背景差距较大。</p>
</blockquote>
<p>Q5: 自动驾驶用语义分割，实例分割还是目标检测更合适？</p>
<blockquote>
<p>自动驾驶需要用到大量不同的模型，语义分割主要用于路面分割，目标检测用于检测前车/行人及其距离/速度。</p>
</blockquote>
<p>Q6: 语义分割有什么标注工具？</p>
<blockquote>
<p>国内外的数据标注公司会有这方面的平台，可以自己找找，老师认为这个比较简单，工具大同小异。</p>
<p>（弹幕提及较多：labelme）</p>
</blockquote>
<p>Q7: 摄像头怕过曝，逆光相关</p>
<blockquote>
<p>过曝不常见，但欠曝在光线不足时很常见，一种方法是做大量数据增广。逆光更难做一点，不过photoshop可以模拟出逆光效果用于数据增强，也可以在采集数据时采集一些逆光照片，检查低置信度的照片确认是否为逆光，之后加以标注对模型重新训练。这样的方法可能会涉及到数据隐私问题。</p>
</blockquote>
<p>Q8: 自动驾驶用纯视觉方案能不能做到很可靠？</p>
<blockquote>
<p>tesla做的就是纯视觉，国内/Google用激光雷达（贵但精准），此外大家都会用摄像头和雷达。老师的广电是使用纯摄像头方案一方面是因为技术团队在这方面有积累，另一个可能的原因是摄像头便宜，第三个原因是tesla的算力很大能很好处理大量摄像头信息，最后是Tesla有大量的数据积累，大量的数据可以弥补传感器方面的劣势。</p>
<p>理论上纯视觉自动驾驶是可行的，但目前只有Tesla做的还算可靠。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>44.物体检测算法：R-CNN,SSD,YOLO</title>
    <url>/2024/04/23/11-01-44/</url>
    <content><![CDATA[<h2 id="44-物体检测算法：R-CNN-SSD-YOLO"><a href="#44-物体检测算法：R-CNN-SSD-YOLO" class="headerlink" title="44.物体检测算法：R-CNN,SSD,YOLO"></a>44.物体检测算法：R-CNN,SSD,YOLO</h2><h3 id="1-区域卷积神经网络"><a href="#1-区域卷积神经网络" class="headerlink" title="1.区域卷积神经网络"></a>1.区域卷积神经网络</h3><h4 id="1-1-R-CNN"><a href="#1-1-R-CNN" class="headerlink" title="1.1.R-CNN"></a>1.1.R-CNN</h4><ul>
<li>使用启发式搜索算法来选择锚框</li>
<li>使用预训练模型来对每个锚框抽取特征（每个锚框当作一个图片，用CNN）</li>
<li>训练一个SVM来类别分类（神经网络之前，category prediction）</li>
<li>训练一个线性回归模型来预测边缘框偏移（bounding box prediction）</li>
<li>兴趣区域（Rol）池化层<ul>
<li>给定一个锚框，均匀分割（如果没法均匀分割，取整）成 n x m 块，输出每块的最大值（max pooling）</li>
<li>不管锚框多大，总是输出nm个值</li>
<li>目的：每个锚框都可以变成想要的形状</li>
</ul>
</li>
</ul>
<h4 id="1-2-Fast-RCNN"><a href="#1-2-Fast-RCNN" class="headerlink" title="1.2 Fast RCNN"></a>1.2 Fast RCNN</h4><ul>
<li><p>RCNN需要对每个锚框进行CNN运算，这些特征抽取计算有重复，并且锚框数量大，特征抽取的计算量也大。Fast RCNN改进了这种计算量大的问题</p>
</li>
<li><p>使用CNN对整张图片抽取特征（快的关键）</p>
</li>
<li><p>使用Rol池化层对每个锚框（将在原图片中搜索到的锚框，映射到CNN得到的结果上），生成固定长度的特征</p>
</li>
</ul>
<img src="/2024/04/23/11-01-44/44-01.png" class>
<h4 id="1-3-Faster-RCNN"><a href="#1-3-Faster-RCNN" class="headerlink" title="1.3 Faster RCNN"></a>1.3 Faster RCNN</h4><ul>
<li>在Fast RCNN基础上变得更快</li>
<li>使用一个 <strong>区域提议网络来替代启发式搜索获得更好的锚框</strong></li>
<li>如下图所示，将CNN结果输入到卷积层，然后用锚框去圈区域，这些锚框很多有好有坏，然后进行预测，binary 预测是预测这个锚框的好坏，即有没有有效的圈住物体，bounding box prediction预测是对锚框进行一些改进，最后用NMS（非极大值抑制）对锚框进行合并。</li>
<li>具体来说，区域提议网络的计算步骤如下：<ol>
<li>使用填充为1的3×3的卷积层变换卷积神经网络的输出，并将输出通道数记为c。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为c的新特征。</li>
<li>以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们。</li>
<li>使用锚框中心单元长度为c的特征，分别预测该锚框的二元类别（含目标还是背景）和边界框。</li>
<li>使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域。</li>
</ol>
</li>
</ul>
<img src="/2024/04/23/11-01-44/44-02.png" class>
<h4 id="1-4-Mask-RCNN"><a href="#1-4-Mask-RCNN" class="headerlink" title="1.4 Mask RCNN"></a>1.4 Mask RCNN</h4><ul>
<li>如果有<strong>像素级别的标号</strong>，使用FCN（fully convolutional network）利用这些信息。可以提升CNN的性能</li>
<li><strong>Rol align</strong>。之前的Rol进行池化的时候，如果没法整除，可以直接取整。但是像素级别的标号预测的时候，会造成偏差的累积，导致边界预测不准确。未来避免这种情况，使用Rol align，也就是当没法整除，对每个像素值进行按比例分配。</li>
<li>具体来说，Mask R-CNN将兴趣区域汇聚层替换为了<em>兴趣区域对齐</em>层，使用<em>双线性插值</em>（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。 兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图。 它们不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置。</li>
</ul>
<img src="/2024/04/23/11-01-44/44-03.png" class>
<h4 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5 总结"></a>1.5 总结</h4><ul>
<li>R-CNN是最早也是最有名的一类基于锚框和CNN的目标检测算法</li>
<li>Fast/Faster RCNN 持续提升性能</li>
<li>Faster RCNN和Mask RCNN是在要求高精度场景下常用的算法（但是速度是最慢的）</li>
</ul>
<h3 id="2-单发多框检测（SSD-single-shot-detection）"><a href="#2-单发多框检测（SSD-single-shot-detection）" class="headerlink" title="2. 单发多框检测（SSD single shot detection）"></a>2. 单发多框检测（SSD single shot detection）</h3><ul>
<li>生成锚框<ul>
<li>对每个像素，生成多个以它为中心的锚框</li>
<li>给定 n 个大小$s_1,…,s_n$和m个高宽比，生成n+m-1个锚框，其大小和高宽比分别为：$(s_1,r_1),(s_2,r_1)…,(s_n,r_1),(s_1,r_2),…,(s_1,r_m)$</li>
</ul>
</li>
<li>SSD模型<ul>
<li>对多个分辨率下的卷积特征，生成锚框，预测</li>
<li>一个基础网络，抽取特征，然后用多个卷积层来减半高宽</li>
<li>在每段都生成锚框<ul>
<li>底部段拟合小物体</li>
<li>顶部段拟合大物体</li>
</ul>
</li>
<li>对每个锚框预测类别和边缘框</li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-01-44/44-04.png" class>
<ul>
<li>总结<ul>
<li>速度快，精度很低。这么多年，作者没有持续的提升，但是启发了后面的一系列工作，实现上相对比较简单。</li>
<li>SSD通过单神经网络来检测模型（single shot）</li>
<li>以像素为中心的产生多个锚框</li>
<li>在多个段的输出上进行多尺度的检测</li>
</ul>
</li>
</ul>
<h3 id="3-YOLO（you-only-look-once）"><a href="#3-YOLO（you-only-look-once）" class="headerlink" title="3. YOLO（you only look once）"></a>3. YOLO（you only look once）</h3><ul>
<li>SSD中锚框大量重复，因此浪费了很多计算资源</li>
<li>YOLO将图片均分为 S X S 个锚框</li>
<li>每个锚框预测 B 个边缘框（防止多个物体出现在一个锚框里面）</li>
<li>后续版本 v2 v3 v4 有持续改进</li>
<li>非锚框算法</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>43-树叶分类竞赛技术总结</title>
    <url>/2024/04/23/11-01-43/</url>
    <content><![CDATA[<h2 id="43-树叶分类竞赛技术总结"><a href="#43-树叶分类竞赛技术总结" class="headerlink" title="43-树叶分类竞赛技术总结"></a>43-树叶分类竞赛技术总结</h2><h3 id="1-比赛结果"><a href="#1-比赛结果" class="headerlink" title="1. 比赛结果"></a>1. 比赛结果</h3><ul>
<li>176类，18353训练样本</li>
</ul>
<img src="/2024/04/23/11-01-43/43-01.png" class>
<ul>
<li>165只队伍参加<ul>
<li>41只队伍精度 &gt; 98% (非常好)</li>
<li>83只队伍精度 &gt; 95% (够用)</li>
</ul>
</li>
</ul>
<h3 id="2-结果分析"><a href="#2-结果分析" class="headerlink" title="2. 结果分析"></a>2. 结果分析</h3><ul>
<li><p>16只队伍提供了代码：</p>
<ul>
<li><a href="https://www.kaggle.com/c/classify-leaves/code">Classify Leaves | Kaggle</a></li>
</ul>
</li>
<li><p>额外加上Neko Kiku</p>
<ul>
<li>很多人参考了此代码 <a href="https://www.kaggle.com/nekokiku/simple-resnet-baseline">simple resnet baseline | Kaggle</a></li>
</ul>
</li>
</ul>
<h3 id="3-技术分析"><a href="#3-技术分析" class="headerlink" title="3. 技术分析"></a>3. 技术分析</h3><p>相比于课程介绍的代码，同学们主要做了下面这些加强：</p>
<ul>
<li><p><strong>数据增强</strong>，在测试时多次使用稍弱的增强然后取平均</p>
</li>
<li><p>使用<strong>多个模型</strong>预测，最后结果加权平均</p>
<ul>
<li>有使用10种模型的，也有使用单一模型的</li>
</ul>
</li>
<li><strong>训练算法</strong>和<strong>学习率</strong></li>
<li><strong>清理数据</strong></li>
</ul>
<h3 id="4-模型方面"><a href="#4-模型方面" class="headerlink" title="4. 模型方面"></a>4. 模型方面</h3><ul>
<li><p>模型多为ResNet变种</p>
<ul>
<li>DenseNet，ResNeXt，ResNeSt,  …</li>
<li>EfficientNet</li>
</ul>
</li>
<li><p>优化算法多为Adam或其变种</p>
</li>
<li>学习率一般是Cosine或者训练不动时往下调</li>
</ul>
<h3 id="5-AutoGluon"><a href="#5-AutoGluon" class="headerlink" title="5. AutoGluon"></a>5. AutoGluon</h3><ul>
<li>15行代码， 安装加训练耗时100分钟<ul>
<li><a href="https://www.kaggle.com/zhreshold/autogluon-vision-0-96-with-15-lines">AutoGluon.vision: 0.96+ with 15 lines | Kaggle</a></li>
</ul>
</li>
<li>精度96%<ul>
<li>可以通过定制化提升精度</li>
<li>下一个版本将搜索更多的模型超参数</li>
<li>AG目前主要仍是关注工业界应用上，而非比赛</li>
</ul>
</li>
</ul>
<h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h3><ul>
<li>提升精度思路：根据数据挑选增强，使用新模型、新优化算法，多个模型融合，测试时使用增强</li>
<li>数据相对简单，排名有相对随机性</li>
<li>在工业界应用中：<ul>
<li>少使用模型融合和测试时增强，计算代价过高</li>
<li>通常固定模型超参数，而将精力主要花在提升数据质量</li>
</ul>
</li>
</ul>
<p>比赛/学术界：固定数据，调模型</p>
<p>工业界：固定模型，调数据</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>41-物体检测和数据集</title>
    <url>/2024/04/23/11-01-41/</url>
    <content><![CDATA[<h3 id="41-物体检测和数据集"><a href="#41-物体检测和数据集" class="headerlink" title="41 物体检测和数据集"></a>41 物体检测和数据集</h3><h4 id="物体检测"><a href="#物体检测" class="headerlink" title="物体检测"></a>物体检测</h4><ol>
<li>图片分类和目标检测在任务上的区别：图片分类已知有一个确定目标，任务是识别该目标属于何种分类，而目标检测不仅需要检测出图片中所有感兴趣的目标类别，并确定其位置，所以目标检测要比图片分类更复杂应用场景更广。</li>
<li>图片分类和目标检测在数据集上的区别：由于目标检测中每一张图片可能存在多个目标，每个目标我们不仅需要分类，还需要确定边缘框以给出目标位置信息，因此目标检测数据集的标注成本要显著高于图片分类，也就导致了目标检测数据集较小。</li>
<li>边缘框：用一个尽量小矩形框将目标物体大体框起来，边框的位置信息就可以表示目标位置在图片中的位置信息，常见的边缘框有两种表示方法：</li>
</ol>
<ul>
<li>（左上x，左上y，右下x，右下y）</li>
<li>（左上x，左上y，宽，高）</li>
</ul>
<ol>
<li>目标检测数据集的常见表示：每一行表示一个物体，对于每一个物体而言，用“图片文件名，物体类别，边缘框”表示，由于边缘框用4个数值表示，因此对于每一行的那一个物体而言，需要用6个数值表示。</li>
<li>目标检测领域常用数据集：COCO（80类物体，330K图片，所有图片共标注1.5M物体）</li>
</ol>
<h4 id="边缘框实现"><a href="#边缘框实现" class="headerlink" title="边缘框实现"></a>边缘框实现</h4><ol>
<li>目标的位置</li>
</ol>
<p>在图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别。 然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。 在计算机视觉里，我们将这类任务称为<em>目标检测</em>（object detection）或<em>目标识别</em>（object recognition）。目标检测在多个领域中被广泛使用。 例如，在无人驾驶里，我们需要通过识别拍摄到的视频图像里的车辆、行人、道路和障碍物的位置来规划行进线路。 机器人也常通过该任务来检测感兴趣的目标。安防领域则需要检测异常目标，如歹徒或者炸弹。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面加载本节将使用的示例图像。可以看到图像左边是一只狗，右边是一只猫。 它们是这张图像里的两个主要目标。</span></span><br><span class="line"></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">img = d2l.plt.imread(<span class="string">&#x27;../img/catdog.jpg&#x27;</span>)</span><br><span class="line">d2l.plt.imshow(img);</span><br></pre></td></tr></table></figure>
<ol>
<li>边界框</li>
</ol>
<ul>
<li><p>在目标检测中，我们通常使用<em>边界框</em>（bounding box）来描述对象的空间位置。 边界框是矩形的，由矩形左上角的以及右下角的<em>x</em>和<em>y</em>坐标决定。 另一种常用的边界框表示方法是边界框中心的(<em>x</em>,<em>y</em>)轴坐标以及框的宽度和高度。</p>
</li>
<li><p>在这里，我们定义在这两种表示法之间进行转换的函数：<code>box_corner_to_center</code>从两角表示法转换为中心宽度表示法，而<code>box_center_to_corner</code>反之亦然。 输入参数<code>boxes</code>可以是长度为4的张量，也可以是形状为（<em>n</em>，4）的二维张量，其中<em>n</em>是边界框的数量。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_corner_to_center</span>(<span class="params">boxes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从（左上，右下）转换到（中间，宽度，高度）&quot;&quot;&quot;</span></span><br><span class="line">    x1, y1, x2, y2 = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    cx = (x1 + x2) / <span class="number">2</span></span><br><span class="line">    cy = (y1 + y2) / <span class="number">2</span></span><br><span class="line">    w = x2 - x1</span><br><span class="line">    h = y2 - y1</span><br><span class="line">    boxes = torch.stack((cx, cy, w, h), axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_center_to_corner</span>(<span class="params">boxes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从（中间，宽度，高度）转换到（左上，右下）&quot;&quot;&quot;</span></span><br><span class="line">    cx, cy, w, h = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    x1 = cx - <span class="number">0.5</span> * w</span><br><span class="line">    y1 = cy - <span class="number">0.5</span> * h</span><br><span class="line">    x2 = cx + <span class="number">0.5</span> * w</span><br><span class="line">    y2 = cy + <span class="number">0.5</span> * h</span><br><span class="line">    boxes = torch.stack((x1, y1, x2, y2), axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br></pre></td></tr></table></figure>
<ul>
<li>我们将根据坐标信息定义图像中狗和猫的边界框。 图像中坐标的原点是图像的左上角，向右的方向为<em>x</em>轴的正方向，向下的方向为<em>y</em>轴的正方向。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># bbox是边界框的英文缩写</span></span><br><span class="line">dog_bbox, cat_bbox = [<span class="number">60.0</span>, <span class="number">45.0</span>, <span class="number">378.0</span>, <span class="number">516.0</span>], [<span class="number">400.0</span>, <span class="number">112.0</span>, <span class="number">655.0</span>, <span class="number">493.0</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>我们可以将边界框在图中画出，以检查其是否准确。 画之前，我们定义一个辅助函数<code>bbox_to_rect</code>。 它将边界框表示成<code>matplotlib</code>的边界框格式。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_to_rect</span>(<span class="params">bbox, color</span>):</span><br><span class="line">    <span class="comment"># 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：</span></span><br><span class="line">    <span class="comment"># ((左上x,左上y),宽,高)</span></span><br><span class="line">    <span class="keyword">return</span> d2l.plt.Rectangle(</span><br><span class="line">        xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>在图像上添加边界框之后，我们可以看到两个物体的主要轮廓基本上在两个框内。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(dog_bbox, <span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(cat_bbox, <span class="string">&#x27;red&#x27;</span>));</span><br></pre></td></tr></table></figure>
<ol>
<li>小结</li>
</ol>
<ul>
<li>目标检测不仅可以识别图像中所有感兴趣的物体，还能识别它们的位置，该位置通常由矩形边界框表示。</li>
<li>我们可以在两种常用的边界框表示（中间，宽度，高度）和（左上，右下）坐标之间进行转换。</li>
</ul>
<h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>目标检测领域没有像MNIST和Fashion-MNIST那样的小数据集。 为了快速测试目标检测模型，我们收集并标记了一个小型数据集。 首先，我们拍摄了一组香蕉的照片，并生成了1000张不同角度和大小的香蕉图像。 然后，我们在一些背景图片的随机位置上放一张香蕉的图像。 最后，我们在图片上为这些香蕉标记了边界框。</p>
<ol>
<li>下载数据集</li>
</ol>
<ul>
<li>包含所有图像和CSV标签文件的香蕉检测数据集可以直接从互联网下载。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon, image, np, npx</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> mxnet <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">npx.set_np()</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;banana-detection&#x27;</span>] = (</span><br><span class="line">    d2l.DATA_URL + <span class="string">&#x27;banana-detection.zip&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;5de26c8fce5ccdea9f91267273464dc968d20d72&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>读取数据集</li>
</ol>
<ul>
<li>通过<code>read_data_bananas</code>函数，我们读取香蕉检测数据集。 该数据集包括一个的CSV文件，内含目标类别标签和位于左上角和右下角的真实边界框坐标。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data_bananas</span>(<span class="params">is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取香蕉检测数据集中的图像和标签&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;banana-detection&#x27;</span>)</span><br><span class="line">    csv_fname = os.path.join(data_dir, <span class="string">&#x27;bananas_train&#x27;</span> <span class="keyword">if</span> is_train</span><br><span class="line">                             <span class="keyword">else</span> <span class="string">&#x27;bananas_val&#x27;</span>, <span class="string">&#x27;label.csv&#x27;</span>)</span><br><span class="line">    csv_data = pd.read_csv(csv_fname)</span><br><span class="line">    csv_data = csv_data.set_index(<span class="string">&#x27;img_name&#x27;</span>)</span><br><span class="line">    images, targets = [], []</span><br><span class="line">    <span class="keyword">for</span> img_name, target <span class="keyword">in</span> csv_data.iterrows():</span><br><span class="line">        images.append(torchvision.io.read_image(</span><br><span class="line">            os.path.join(data_dir, <span class="string">&#x27;bananas_train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span></span><br><span class="line">                         <span class="string">&#x27;bananas_val&#x27;</span>, <span class="string">&#x27;images&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;img_name&#125;</span>&#x27;</span>)))</span><br><span class="line">        <span class="comment"># 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），</span></span><br><span class="line">        <span class="comment"># 其中所有图像都具有相同的香蕉类（索引为0）</span></span><br><span class="line">        targets.append(<span class="built_in">list</span>(target))</span><br><span class="line">    <span class="keyword">return</span> images, torch.tensor(targets).unsqueeze(<span class="number">1</span>) / <span class="number">256</span></span><br></pre></td></tr></table></figure>
<ul>
<li>通过使用<code>read_data_bananas</code>函数读取图像和标签，以下<code>BananasDataset</code>类别将允许我们创建一个自定义<code>Dataset</code>实例来加载香蕉检测数据集。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BananasDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于加载香蕉检测数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, is_train</span>):</span><br><span class="line">        self.features, self.labels = read_data_bananas(is_train)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features)) + (<span class="string">f&#x27; training examples&#x27;</span> <span class="keyword">if</span></span><br><span class="line">              is_train <span class="keyword">else</span> <span class="string">f&#x27; validation examples&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.features[idx].<span class="built_in">float</span>(), self.labels[idx])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br></pre></td></tr></table></figure>
<ul>
<li>最后，我们定义<code>load_data_bananas</code>函数，来为训练集和测试集返回两个数据加载器实例。对于测试集，无须按随机顺序读取它。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_bananas</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载香蕉检测数据集&quot;&quot;&quot;</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=<span class="literal">True</span>),</span><br><span class="line">                                             batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=<span class="literal">False</span>),</span><br><span class="line">                                           batch_size)</span><br><span class="line">    <span class="keyword">return</span> train_iter, val_iter</span><br></pre></td></tr></table></figure>
<ul>
<li>让我们读取一个小批量，并打印其中的图像和标签的形状。 图像的小批量的形状为（批量大小、通道数、高度、宽度），看起来很眼熟：它与我们之前图像分类任务中的相同。 标签的小批量的形状为（批量大小，<em>m</em>，5），其中<em>m</em>是数据集的任何图像中边界框可能出现的最大数量。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size, edge_size = <span class="number">32</span>, <span class="number">256</span></span><br><span class="line">train_iter, _ = load_data_bananas(batch_size)</span><br><span class="line">batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_iter))</span><br><span class="line">batch[<span class="number">0</span>].shape, batch[<span class="number">1</span>].shape</span><br></pre></td></tr></table></figure>
<ol>
<li>小结</li>
</ol>
<ul>
<li>我们收集的香蕉检测数据集可用于演示目标检测模型。</li>
<li>用于目标检测的数据加载与图像分类的数据加载类似。但是，在目标检测中，标签还包含真实边界框的信息，它不出现在图像分类中。</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>39-实战kaggle竞赛：CIFAR-10</title>
    <url>/2024/04/23/11-01-39/</url>
    <content><![CDATA[<h1 id="CIFAR-10"><a href="#CIFAR-10" class="headerlink" title="CIFAR-10"></a>CIFAR-10</h1><p>首先，导入竞赛所需要的包和模块：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h4 id="2-1-下载数据集："><a href="#2-1-下载数据集：" class="headerlink" title="2.1 下载数据集："></a>2.1 下载数据集：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;cifar10_tiny&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;kaggle_cifar10_tiny.zip&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;2068874e4b9a9f0fb07ebe0ad2b29754449ccacd&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你使用完整的Kaggle竞赛的数据集，设置demo为False</span></span><br><span class="line">demo = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> demo:</span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;cifar10_tiny&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data_dir = <span class="string">&#x27;../data/cifar-10/&#x27;</span></span><br></pre></td></tr></table></figure>
<p>为了便于入门，我们提供包含前1000个训练图像和5个随机测试图像的数据集的小规模样本，如果要获取完整数据集，你需要将一下demo变量设置为False</p>
<h4 id="2-2-整理数据集"><a href="#2-2-整理数据集" class="headerlink" title="2.2 整理数据集"></a>2.2 整理数据集</h4><p>首先我们用以下函数读取CSV文件中的标签，它返回一个字典，该字典将文件名中不带拓展名德部分映射到其标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_csv_labels</span>(<span class="params">fname</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取fname来给标签字典返回一个文件名&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 跳过文件头行(列名)</span></span><br><span class="line">        lines = f.readlines()[<span class="number">1</span>:]</span><br><span class="line">    tokens = [</span><br><span class="line">        <span class="comment"># 训练样本 : 1000l.rstrip().split(&#x27;,&#x27;) for l in lines]</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(((name, label) <span class="keyword">for</span> name, label <span class="keyword">in</span> tokens))</span><br><span class="line"></span><br><span class="line">labels = read_csv_labels(os.path.join(data_dir, <span class="string">&#x27;trainLabels.csv&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;# 训练样本 :&#x27;</span>, <span class="built_in">len</span>(labels))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;# 类别 :&#x27;</span>, <span class="built_in">len</span>(<span class="built_in">set</span>(labels.values())))</span><br><span class="line"><span class="comment"># 训练样本 : 1000</span></span><br><span class="line"><span class="comment"># 类别 : 10</span></span><br></pre></td></tr></table></figure>
<p>接下来，我们定义reorg_train_valid函数来将验证集从原始的训练集中拆分出来。此函数中的参数valid_ratio是验证集中的样本数与原始训练集中的样本数之比。更具体的说，令n等于样本最少的类别中的图像数量，而r是比率。验证集将为每个类别拆分出max([nr],1)张图像。让我们以valid_ratio=0.1为例，由于原始的训练集有50000张图像，因此trian_valid_test/train路径中将有45000张图像用于训练，而剩下5000张图像将作为路径train_valid_test/valid中的验证集。组织数据集后，同类别的图像将被放置在同一文件夹下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">copyfile</span>(<span class="params">filename, target_dir</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文件复制到目标目录&quot;&quot;&quot;</span></span><br><span class="line">    os.makedirs(target_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    shutil.copy(filename, target_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_train_valid</span>(<span class="params">data_dir, labels, valid_ratio</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将验证集从原始的训练集中拆分出来&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 训练数据集中样本最少的类别中的样本数</span></span><br><span class="line">    n = collections.Counter(labels.values()).most_common()[-<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 验证集中每个类别的样本数</span></span><br><span class="line">    n_valid_per_label = <span class="built_in">max</span>(<span class="number">1</span>, math.floor(n * valid_ratio))</span><br><span class="line">    label_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> train_file <span class="keyword">in</span> os.listdir(os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>)):</span><br><span class="line">        label = labels[train_file.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]]</span><br><span class="line">        fname = os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>, train_file)</span><br><span class="line">        copyfile(fname, os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>,</span><br><span class="line">                                     <span class="string">&#x27;train_valid&#x27;</span>, label))</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label_count <span class="keyword">or</span> label_count[label] &lt; n_valid_per_label:</span><br><span class="line">            copyfile(fname, os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>,</span><br><span class="line">                                         <span class="string">&#x27;valid&#x27;</span>, label))</span><br><span class="line">            label_count[label] = label_count.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            copyfile(fname, os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>,</span><br><span class="line">                                         <span class="string">&#x27;train&#x27;</span>, label))</span><br><span class="line">    <span class="keyword">return</span> n_valid_per_label</span><br></pre></td></tr></table></figure>
<p>其中os.listdir显示指定路径下的文件和文件夹列表</p>
<p>下面的reorg_test函数用来预测期间整理测试集，以方便读取。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_test</span>(<span class="params">data_dir</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在预测期间整理测试集，以方便读取&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> test_file <span class="keyword">in</span> os.listdir(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>)):</span><br><span class="line">        copyfile(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>, test_file),</span><br><span class="line">                 os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">                              <span class="string">&#x27;unknown&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>最后我们使用一个函数来调用前面定义的函数read_csv_labels，reorg_train_valid和reorg_test。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_cifar10_data</span>(<span class="params">data_dir, valid_ratio</span>):</span><br><span class="line">    labels = read_csv_labels(os.path.join(data_dir, <span class="string">&#x27;trainLabels.csv&#x27;</span>))</span><br><span class="line">    reorg_train_valid(data_dir, labels, valid_ratio)</span><br><span class="line">    reorg_test(data_dir)</span><br></pre></td></tr></table></figure>
<p>在这里，我们只将样本数据集的批量大小设置为32.在实际训练和测试中，应该使用Kaggle竞赛的完整数据集，并将batch_size设置为更大的整数，例如128.我们将10%的训练样本作为调整超参数的验证集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">32</span> <span class="keyword">if</span> demo <span class="keyword">else</span> <span class="number">128</span></span><br><span class="line">valid_ratio = <span class="number">0.1</span></span><br><span class="line">reorg_cifar10_data(data_dir, valid_ratio)</span><br></pre></td></tr></table></figure>
<h3 id="3-图像增广"><a href="#3-图像增广" class="headerlink" title="3.图像增广"></a>3.图像增广</h3><p>使用图像增广来解决过拟合问题。在训练中，我们可以随机水平翻转图像。我们可以对彩色图像的三个RGB通道执行标准化。下面为一些可以调整的操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform_train = torchvision.transforms.Compose([</span><br><span class="line">    <span class="comment"># 在高度和宽度上将图像放大到40像素的正方形</span></span><br><span class="line">    torchvision.transforms.Resize(<span class="number">40</span>),</span><br><span class="line">    <span class="comment"># 随机裁剪出一个高度和宽度均为40像素的正方形图像，</span></span><br><span class="line">    <span class="comment"># 生成一个面积为原始图像面积0.64到1倍的小正方形，</span></span><br><span class="line">    <span class="comment"># 然后将其缩放为高度和宽度均为32像素的正方形</span></span><br><span class="line">    torchvision.transforms.RandomResizedCrop(<span class="number">32</span>, scale=(<span class="number">0.64</span>, <span class="number">1.0</span>),</span><br><span class="line">                                                   ratio=(<span class="number">1.0</span>, <span class="number">1.0</span>)),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 标准化图像的每个通道</span></span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>],</span><br><span class="line">                                     [<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>])])</span><br></pre></td></tr></table></figure>
<p>在测试期间，我们只对图像执行标准化，以消除评估结果中的随机性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform_test = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>],</span><br><span class="line">                                     [<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>])])</span><br></pre></td></tr></table></figure>
<h3 id="4-读取数据集"><a href="#4-读取数据集" class="headerlink" title="4.读取数据集"></a>4.读取数据集</h3><p>读取由原始图像组成的数据集，每个样本都包括一张图片和一个标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(</span><br><span class="line">    os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, folder),</span><br><span class="line">    transform=transform_train) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;train_valid&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">valid_ds, test_ds = [torchvision.datasets.ImageFolder(</span><br><span class="line">    os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, folder),</span><br><span class="line">    transform=transform_test) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">&#x27;valid&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<p>当验证集在超参数调整过程中用于模型评估中，不应引入图像增广的随机性。在最终预测之前，我们根据训练集合验证集组合而成的训练模型进行训练，以充分利用所有标记的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iter, train_valid_iter = [torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> dataset <span class="keyword">in</span> (train_ds, train_valid_ds)]</span><br><span class="line"></span><br><span class="line">valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                         drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                        drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-定义模型"><a href="#5-定义模型" class="headerlink" title="5.定义模型"></a>5.定义模型</h3><p>直接使用Resnet-18模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    num_classes = <span class="number">10</span></span><br><span class="line">    net = d2l.resnet18(num_classes, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="6-定义训练函数"><a href="#6-定义训练函数" class="headerlink" title="6.定义训练函数"></a>6.定义训练函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,</span></span><br><span class="line"><span class="params">          lr_decay</span>):</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=<span class="number">0.9</span>,</span><br><span class="line">                              weight_decay=wd)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)</span><br><span class="line">    num_batches, timer = <span class="built_in">len</span>(train_iter), d2l.Timer()</span><br><span class="line">    legend = [<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        legend.append(<span class="string">&#x27;valid acc&#x27;</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=legend)</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        net.train()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            l, acc = d2l.train_batch_ch13(net, features, labels,</span><br><span class="line">                                          loss, trainer, devices)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>],</span><br><span class="line">                              <span class="literal">None</span>))</span><br><span class="line">        <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, valid_acc))</span><br><span class="line">        scheduler.step()</span><br><span class="line">    measures = (<span class="string">f&#x27;train loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">                <span class="string">f&#x27;train acc <span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        measures += <span class="string">f&#x27;, valid acc <span class="subst">&#123;valid_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(measures + <span class="string">f&#x27;\n<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span>&#x27;</span></span><br><span class="line">          <span class="string">f&#x27; examples/sec on <span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>使用随机梯度下降和学习率规划来训练模型，以更快达到收敛。</p>
<h3 id="7-训练和验证模型"><a href="#7-训练和验证模型" class="headerlink" title="7.训练和验证模型"></a>7.训练和验证模型</h3><p>以下所有超参数都可以调节</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">devices, num_epochs, lr, wd = d2l.try_all_gpus(), <span class="number">20</span>, <span class="number">2e-4</span>, <span class="number">5e-4</span></span><br><span class="line">lr_period, lr_decay, net = <span class="number">4</span>, <span class="number">0.9</span>, get_net()</span><br><span class="line">train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,</span><br><span class="line">      lr_decay)</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-39/39-01.png" class>
<h3 id="8-Q-amp-A"><a href="#8-Q-amp-A" class="headerlink" title="8.Q&amp;A"></a>8.Q&amp;A</h3><h5 id="Q1-深度学习的损失函数一般是非凸的吗？"><a href="#Q1-深度学习的损失函数一般是非凸的吗？" class="headerlink" title="Q1:深度学习的损失函数一般是非凸的吗？"></a>Q1:深度学习的损失函数一般是非凸的吗？</h5><blockquote>
<p>损失函数一般是凸的，但是神经网络是非凸的(非单层)。凸函数表示能力有限。</p>
</blockquote>
<h5 id="Q2-训练时的训练集交叉熵loss大于验证集，但是训练集acc也是大于验证集的？"><a href="#Q2-训练时的训练集交叉熵loss大于验证集，但是训练集acc也是大于验证集的？" class="headerlink" title="Q2:训练时的训练集交叉熵loss大于验证集，但是训练集acc也是大于验证集的？"></a>Q2:训练时的训练集交叉熵loss大于验证集，但是训练集acc也是大于验证集的？</h5><blockquote>
<p>应该是因为在训练集上加了数据增广</p>
</blockquote>
<h5 id="Q3-normalize参数怎么来的？"><a href="#Q3-normalize参数怎么来的？" class="headerlink" title="Q3:normalize参数怎么来的？"></a>Q3:normalize参数怎么来的？</h5><blockquote>
<p>由imagenet数据集上RGB的均值和方差</p>
</blockquote>
<h5 id="Q4：weight-decay和lr-decay的作用有什么区别吗？"><a href="#Q4：weight-decay和lr-decay的作用有什么区别吗？" class="headerlink" title="Q4：weight decay和lr decay的作用有什么区别吗？"></a>Q4：weight decay和lr decay的作用有什么区别吗？</h5><blockquote>
<p>weight decay是对权重更新的操作——正则化（统计），lr decay 是作用在学习率上——为了收敛（优化模型）</p>
</blockquote>
<h5 id="Q5-scheduler怎么设置是最好的最优的，怎么选择？"><a href="#Q5-scheduler怎么设置是最好的最优的，怎么选择？" class="headerlink" title="Q5:scheduler怎么设置是最好的最优的，怎么选择？"></a>Q5:scheduler怎么设置是最好的最优的，怎么选择？</h5><blockquote>
<p>现在一般选用cosine函数，参数设置较少   。最好在前期保证比较大的lr，后期lr可以变小一点。具体流行什么说不准</p>
</blockquote>
<h5 id="Q6-lr-decay和weight-decay的效果？"><a href="#Q6-lr-decay和weight-decay的效果？" class="headerlink" title="Q6:lr decay和weight decay的效果？"></a>Q6:lr decay和weight decay的效果？</h5><blockquote>
<p>效果类似，但是本质不同。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>38-第二次竞赛树叶分类结果</title>
    <url>/2024/04/23/11-01-38/</url>
    <content><![CDATA[<h2 id="第二次竞赛树叶分类结果"><a href="#第二次竞赛树叶分类结果" class="headerlink" title="第二次竞赛树叶分类结果"></a>第二次竞赛树叶分类结果</h2><h3 id="任务简介"><a href="#任务简介" class="headerlink" title="任务简介"></a>任务简介</h3><p>对176种叶子进行分类，每类叶子的数据大约有100张图片，数据较干净，没有复杂背景且颜色并不关键，模型只要能识别出形状即可</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>165个队伍参加，1800次提交，私榜排名第一的同学正确率有0.99272，李沐老师私下使用AutoML训练大约0.98，由于结果较好，6-20名的队伍如果分享代码也可获得签名书</p>
<h3 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h3><p>待参加比赛的同学分享代码后下周课程上讲解，但B站视频并未上传此部分。</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>37-微调</title>
    <url>/2024/04/23/11-01-37/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>很多时候，例如我们想对家具进行分类，但是往往在努力收集数据得到的数据集也比较小假如我们想识别图片中不同类型的椅子，然后向用户推荐购买链接。 一种可能的方法是首先识别100把普通椅子，为每把椅子拍摄1000张不同角度的图像，然后在收集的图像数据集上训练一个分类模型。 尽管这个椅子数据集可能大于Fashion-MNIST数据集，但实例数量仍然不到ImageNet中的十分之一。 适合ImageNet的复杂模型可能会在这个椅子数据集上过拟合。 此外，由于训练样本数量有限，训练模型的准确性可能无法满足实际要求。为了避免这种情况，我们可以有两种方法：<ul>
<li>显然的想法就是收集更多的数据，但是，收集和标记数据可能需要大量的时间和金钱。 例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究资金。 尽管目前的数据收集成本已大幅降低，但这一成本仍不能忽视。</li>
<li>我们可以考虑迁移学习将从<em>源数据集</em>学到的知识迁移到<em>目标数据集</em>。 例如，尽管ImageNet数据集中的大多数图像与椅子无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合。 这些类似的特征也可能有效地识别椅子。</li>
</ul>
</li>
</ul>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ul>
<li>如图所示，微调包括以下四个步骤：<ol>
<li>在源数据集（例如ImageNet数据集）上预训练神经网络模型，即<strong><em>源模型</em></strong>。</li>
<li>创建一个新的神经网络模型，即<strong><em>目标模型</em></strong>。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。</li>
<li>向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。</li>
</ol>
</li>
</ul>
<img src="/2024/04/23/11-01-37/37-01.png" class>
<ol>
<li>通常来讲，微调速度更快，并且具有较强的正则性，一般学习率比较小，也不需要很多轮的数据迭代，对于不同的任务往往只需要改变最后输出层，这一层随机初始化即可。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>迁移学习将从源数据集中学到的知识“迁移”到目标数据集，微调是迁移学习的常见技巧。</li>
<li>除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调。但是，目标模型的输出层需要从头开始训练。</li>
<li>通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率。</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>36-数据增广</title>
    <url>/2024/04/23/11-01-36/</url>
    <content><![CDATA[<h2 id="36-数据增广"><a href="#36-数据增广" class="headerlink" title="36 数据增广"></a>36 数据增广</h2><p>数据增广不仅用于处理图片，也可用于文本和语音，这里只涉及到图片。</p>
<h3 id="1-使用增强数据训练"><a href="#1-使用增强数据训练" class="headerlink" title="1. 使用增强数据训练"></a>1. 使用增强数据训练</h3><p>采集数据得到的训练场景与实际部署场景不同是常见的问题，这种变化有时会显著影响模型表现。在训练集中尽可能模拟部署时可能遇到的场景对模型的泛化性十分重要。</p>
<p>数据增强是指在一个已有数据集上操作使其有更多的多样性。对语音来说可以加入不同的背景噪音，对图片而言可以改变其颜色，形状等。</p>
<p>一般来说不会先将数据集做增广后存下来再用于训练；而是直接在线生成，从原始数据中读图片并随机做数据增强，再进入模型训练。通常只在训练时做数据增强而测试时不用。可以将数据增强理解为一个正则项。</p>
<h3 id="2-增强手段"><a href="#2-增强手段" class="headerlink" title="2. 增强手段"></a>2. 增强手段</h3><h4 id="2-1-翻转"><a href="#2-1-翻转" class="headerlink" title="2.1 翻转"></a>2.1 翻转</h4><p>一些例子：左右翻转，上下翻转</p>
<p>要注意不是所有增强策略都总是可行，如建筑图片上下翻转就不太合适，而之前的树叶分类竞赛中的树叶图片就没关系。</p>
<img src="/2024/04/23/11-01-36/36-01.png" class>
<h4 id="2-2-切割"><a href="#2-2-切割" class="headerlink" title="2.2 切割"></a>2.2 切割</h4><p>从图片中切割一块然后变形到固定形状。一般做法是随机取一个高宽比，随机取图片大小（切下部分占原图的百分数），随机取位置。</p>
<img src="/2024/04/23/11-01-36/36-02.png" class>
<h4 id="2-3-颜色"><a href="#2-3-颜色" class="headerlink" title="2.3 颜色"></a>2.3 颜色</h4><p>改变色调，饱和度，明亮度。</p>
<img src="/2024/04/23/11-01-36/36-03.png" class>
<h4 id="2-4-其他"><a href="#2-4-其他" class="headerlink" title="2.4 其他"></a>2.4 其他</h4><p>还可以有很多种不同的方法，如高斯模糊，部分像素变黑，图片变形，锐化等等。理论上讲Photoshop能做到的都可以用作图片数据增强，但效果好坏另当别论。如果测试集中有类似的效果那么相应的数据增广手段会更有效。</p>
<img src="/2024/04/23/11-01-36/36-04.png" class>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><ul>
<li>数据增广通过变形数据来获取多样性从而使得模型泛化性能更好</li>
<li>常见图片增广包括翻转，切割，变色</li>
</ul>
<h3 id="4-QA"><a href="#4-QA" class="headerlink" title="4. QA"></a>4. QA</h3><p>Q1: 理论上是不是原始样本足够多就不需要做增广？</p>
<blockquote>
<p>是的，但实际情况中很难有足够多样性的图片能覆盖测试的所有情况。</p>
<p>数据量大也不一定意味着足够多样，可能简单情况已经很充分，但对于很难的情况覆盖率不够。</p>
</blockquote>
<p>Q2: （代码实现中的）num_worker值是不是根据GPU性能而定？</p>
<blockquote>
<p>是。</p>
<p>这里老师还提到虽然深度学习主要用GPU，但CPU也不能太差，否则可能数据预处理跟不上，CPU的内存带宽和到显卡的带宽不够。具体决定num_worker可以自己定一个值然后跑一个epoch看看耗时。</p>
</blockquote>
<p>Q3: 金融风控领域经常面临极度偏斜数据（欺诈样本极少），是否可对正样本做数据增广？</p>
<blockquote>
<p>可以，类似地震预测等等正样本少的情况都可以尝试对正样本做增广，负样本可以不用。</p>
</blockquote>
<p>Q4: 测试一般做什么样的增广？如何理解对测试集增广能提高精度？</p>
<blockquote>
<p>一般不对测试集做增广。也可以对一张测试图像做增广，将每个增广出的图片都做一次预测最后取平均，会一定程度改善精度。但这样会使得对每张图片预测计算量成倍增长，所以使用较少。</p>
</blockquote>
<p>Q5: 课件里提到的对比实验固定了所有随机种子吗？昨晚增广后训练精度下降是不是意味着还可以继续训练减少gap？</p>
<blockquote>
<p>没有。</p>
<p>是的，课堂演示时往往跑的epoch较少，另外训练到后期gap一般不会减少。</p>
</blockquote>
<p>Q6: 图片增广后需要人工一张张确认效果吗？</p>
<blockquote>
<p>不用全看，大概看看效果即可。</p>
</blockquote>
<p>Q7: 图片增广后训练数据与测试数据分布可能不同，会对模型最终精度有影响吗？</p>
<blockquote>
<p>首先多数图片增广手段不改变数据分布，因为亮度变化等是随机的，数据的均值不变，翻转不影响分布，crop可能会有改变但影响不大。</p>
<p>后面还有问题提到对增广不改变数据分布的理解，可理解成增广不改变均值但稍微增大方差。很多时候讨论训练集和测试集分布是否相同不是看原始的像素分布而是看各label比例或图片色调等是否差不多。</p>
</blockquote>
<p>Q8: 关于图神经网络</p>
<blockquote>
<p>图神经网络很强大但不好训练，目前落地还太早了</p>
</blockquote>
<p>Q9: 关于mosaic和crop</p>
<blockquote>
<p>把多张图片拼起来训练。这里老师理解错了问题，提到了加马赛克和本节代码中一次展示八张图片只是一起显示而不是使用了crop方法。</p>
</blockquote>
<p>Q10: 用对一个事物的视频描述做数据集是不是会比增广更有效？</p>
<blockquote>
<p>可以这么认为，但拍视频是很贵的事情，获取视频往往划不来。</p>
</blockquote>
<p>Q11: 多张图片叠加是否也是有效的增广方式？</p>
<blockquote>
<p>是的，这种方法叫mix-up，非常有用。</p>
<p>后面有问到为什么mix-up有用，老师也不清楚。</p>
<p>lable的叠加是对两张图片label按特定分布随机取权重加权求和</p>
</blockquote>
<p>Q12: 做车辆位置识别如果实际应用场景摄像头高度角度清晰度都和训练集不一样，是不是只能针对场景单独采集数据重新打标训练？</p>
<blockquote>
<p>是，可以考虑将实际部署时识别错误的数据加入训练集使得训练集测试集分布趋同</p>
</blockquote>
<p>Q13: 是否会出现图像增广减小类间差异，混淆不同类别的情况？</p>
<blockquote>
<p>那倒不会。可以考虑不要crop太小的区域。</p>
</blockquote>
<p>Q14: 实际操作用torchvision还是albumentation?</p>
<blockquote>
<p>都差不多</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>35-分布式训练</title>
    <url>/2024/04/23/11-01-35/</url>
    <content><![CDATA[<h2 id="35-分布式训练"><a href="#35-分布式训练" class="headerlink" title="35-分布式训练"></a>35-分布式训练</h2><h3 id="1-分布式计算"><a href="#1-分布式计算" class="headerlink" title="1.分布式计算"></a>1.分布式计算</h3><ul>
<li>本质上来说和之前讲的单机多卡并行没有区别。二者之间的区别是分布式计算是通过网络把数据从一台机器搬到另一台机器</li>
</ul>
<img src="/2024/04/23/11-01-35/35-01.png" class>
<h3 id="2-GPU机器架构"><a href="#2-GPU机器架构" class="headerlink" title="2. GPU机器架构"></a>2. GPU机器架构</h3><ul>
<li>总的来说，gpu到gpu的通讯是很快的，gpu到cpu慢一点。机器到机器更慢。因而总体性能的关键就是尽量在本地做通讯而少在机器之间做通讯</li>
</ul>
<h5 id="2-1-样例：计算一个小批量"><a href="#2-1-样例：计算一个小批量" class="headerlink" title="2.1 样例：计算一个小批量"></a>2.1 样例：计算一个小批量</h5><ul>
<li>每个worker从参数服务器那里获取模型参数：首先把样本复制到机器的内存，然后把样本分到每个gpu上</li>
<li>复制参数到每个gpu上：同样，先把每一次的参数放到内存里，然后再复制到每个gpu上</li>
<li>每个gpu计算梯度</li>
<li>再主内存上把所有gpu上的梯度加起来</li>
<li>梯度从主内存传回服务器</li>
<li>每个服务器对梯度求和，并更新参数</li>
</ul>
<h5 id="2-2-总结"><a href="#2-2-总结" class="headerlink" title="2.2 总结"></a>2.2 总结</h5><ul>
<li>由于gpu到gpu和gpu到内存的通讯速度还不错，因此我们尽量再本地做聚合（如梯度相加），并减少再网络上的多次通讯</li>
</ul>
<h3 id="3-关于性能"><a href="#3-关于性能" class="headerlink" title="3.  关于性能"></a>3.  关于性能</h3><h5 id="3-1-对于同步SGD："><a href="#3-1-对于同步SGD：" class="headerlink" title="3.1 对于同步SGD："></a>3.1 对于<strong>同步SGD</strong>：</h5><ul>
<li>这里每个worker都是同步计算一个批量，称为同步SGD</li>
<li>假设有n个ggpu，每个gpu每次处理b个样本，那么同步SGD等价于再单gpu运行批量大小为nb的SGD</li>
<li>再理想情况下，n个gpu可以得到相对单gpu的n倍加速</li>
</ul>
<h5 id="3-2-性能："><a href="#3-2-性能：" class="headerlink" title="3.2 性能："></a>3.2 <strong>性能</strong>：</h5><ul>
<li>t1 = 在单gpu上计算b个样本梯度时间</li>
<li>假设有m个参数，一个worker每次发送和接受m个参数、梯度<ul>
<li>t2 = 发送和接受所用时间</li>
</ul>
</li>
<li>每个批量的计算时间为max（t1，t2）<ul>
<li>选取足够大的b使t1&gt;t2</li>
<li>增加b或n导致更大的批量 大小，当值需要更多计算来得到给定的模型精度</li>
</ul>
</li>
</ul>
<h5 id="3-3-性能的权衡"><a href="#3-3-性能的权衡" class="headerlink" title="3.3 性能的权衡"></a>3.3 性能的权衡</h5><img src="/2024/04/23/11-01-35/35-02.png" class>
<h3 id="4-实践时的建议"><a href="#4-实践时的建议" class="headerlink" title="4. 实践时的建议"></a>4. 实践时的建议</h3><ul>
<li>使用一个大数据集</li>
<li>需要好的gpu-gpu和机器-机器带宽</li>
<li>高效的数据读取和预处理</li>
<li>模型需要有好的计算和通讯比<ul>
<li>Inception&gt;ResNet&gt;AlexNet</li>
</ul>
</li>
<li>使用足够大的批量大小来得到更好的系统性能</li>
<li>使用高效的优化算法对应大批量大小</li>
</ul>
<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><ul>
<li>分布式同步数据并行是多gpu数据并行在多机器上的拓展</li>
<li>网络通讯通常是瓶颈</li>
<li>需要注意使用特别大的批量大小时的收敛效率</li>
<li>更复杂的分布式有异步、模型并行（这里没有介绍）</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>33-单机多卡并行</title>
    <url>/2024/04/23/11-01-34/</url>
    <content><![CDATA[<h2 id="单机多卡并行"><a href="#单机多卡并行" class="headerlink" title="单机多卡并行"></a>单机多卡并行</h2><p>一台机器可以安装多个GPU（一般为1-16个），在训练和预测时可以将一个小批量计算切分到多个GPU上来达到加速目的，常用的切分方案有数据并行，模型并行，通道并行。</p>
<h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><p>将小批量的数据分为n块，每个GPU拿到完整的参数，对这一块的数据进行前向传播与反向传播，计算梯度。</p>
<p>数据并行通常性能比模型并行更好，因为对数据进行划分使得各个GPU的计算内容更加均匀。</p>
<h4 id="数据并行的大致流程"><a href="#数据并行的大致流程" class="headerlink" title="数据并行的大致流程"></a>数据并行的大致流程</h4><img src="/2024/04/23/11-01-34/DataParallel.png" class>
<p>主要分为五部</p>
<ul>
<li>1：每个GPU读取一个数据块（灰色部分）</li>
<li>2：每个GPU读取当前模型的参数（橙色部分）</li>
<li>3：每个GPU计算自己拿到数据块的梯度（绿色部分）</li>
<li>4：GPU将计算得到的梯度传给内存（CPU）（绿色箭头）</li>
<li>5：利用梯度对模型参数进行更新（橙色箭头）</li>
</ul>
<p>数据并行并行性较好，主要因为当每个GPU拿到的数据量相同时计算量也相似，各个GPU的运算时间相近，幸能较好</p>
<h3 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h3><p>将整个模型分为n个部分，每个GPU拿到这个部分的参数和负责上一个部分的GPU的输出作为输入来进行计算，反向传播同理。</p>
<p>模型并行通常用于模型十分巨大，参数众多，即使在每个mini-batch只有一个样本的情况下单个GPU的显存仍然不够的情况，但并行性较差，可能有时会有GPU处于等待状态。</p>
<h3 id="通道并行"><a href="#通道并行" class="headerlink" title="通道并行"></a>通道并行</h3><p>通道并行是数据并行和模型并行同时进行</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>当一个模型能用单卡计算时，通常使用数据并行扩展到多卡</li>
<li>模型并行则用在超大模型上</li>
</ul>
<h3 id="Q-amp-A（部分有价值的）"><a href="#Q-amp-A（部分有价值的）" class="headerlink" title="Q&amp;A（部分有价值的）"></a>Q&amp;A（部分有价值的）</h3><ul>
<li>问1：若有4块GPU，两块显存大两块显存小怎么办？</li>
<li>答1：<br>若GPU运算性能相同，则训练取决于小显存的GPU的显存大小，更大的显存相当于浪费掉<br>若GPU运算性能不同，一般即为显存大的GPU性能更好，可以在分配数据时多分配一点</li>
<li></li>
<li>问2：数据拆分后，需存储的数据量会变大吗？会降低性能吗？</li>
<li>答2：每个GPU都单独存储了一份模型，这部分的数据量变大了，但如果只考虑运算时的中间变量，则中间变量的大小与数据量呈线性关系，每个GPU的数据小了，中间变量也会变小，所有GPU的中间变量加起来大小是不变的。<br>数据拆分后性能会变低，在下节课讲解（数据通讯的开销，每个GPU的batch-size变小可能无法跑满GPU，总batch-size变大则相同计算量下训练次数变少）</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>34-多GPU训练实现</title>
    <url>/2024/04/23/11-01-34/</url>
    <content><![CDATA[<h2 id="34-多GPU训练实现"><a href="#34-多GPU训练实现" class="headerlink" title="34 多GPU训练实现"></a>34 多GPU训练实现</h2><p>本讲内容为代码实现，这里整理QA，其余内容参考代码部分。</p>
<p>Q1: keras从tf分离，书籍会不会需要重新整理？</p>
<blockquote>
<p>暂时不会有影响</p>
</blockquote>
<p>Q2: 是否可以通过把resnet中的卷积层全替换成mlp来实现一个很深的网络？</p>
<blockquote>
<p>可以，有这样做的paper，但是通过一维卷积（等价于全连接层）做的，如果直接换成全连接层很可能会过拟合。</p>
</blockquote>
<p>Q3: 为什么batch norm是一种正则但只加快训练不提升精度？</p>
<blockquote>
<p>老师也不太清楚并认为这是很好的问题，可以去查阅论文。</p>
</blockquote>
<p>Q4: all_reduce, all_gather主要起什么作用？实际使用时发现pytorch的类似分布式op不能传导梯度，会破坏计算图不能自动求导，如何解决？</p>
<blockquote>
<p>all_reduce是把n个东西加在一起再把所有东西复制回去，all_gather则只是把来自不同地方东西合并但不相加。使用分布式的东西会破坏自动求导，跨GPU的自动求导并不好做，老师不确定pytorch能不能做到这一功能，如果不能就只能手写。</p>
</blockquote>
<p>Q5: 两个GPU训练时最后的梯度是把两个GPU上的梯度相加吗？</p>
<blockquote>
<p>是的。mini-batch的梯度就是每个样本的梯度求和，多GPU时同理，每个GPU向将自己算的那部分样本梯度求和，最后再将两个GPU的计算得的梯度求和。</p>
</blockquote>
<p>Q6: 为什么参数大的模型不一定慢？flop数多的模型性能更好是什么原理？</p>
<blockquote>
<p>性能取决于每算一个乘法需要访问多少个bit，计算量与内存访问的比值越高越好。通常CPU/GPU不会被卡在频率上而是访问数据/内存上，所以参数量小，算力高的模型性能较好（如卷积，矩阵乘法）。</p>
</blockquote>
<p>Q7: 为什么分布到多GPU上测试精度会比单GPU抖动大？</p>
<blockquote>
<p>抖动是因为学习率变大了，使用GPU数对测试精度没有影响，只会影响性能。但为了得到更好的速度需要把batchsize调大，使得收敛情况发生变化，把学习率上调就使得精度更抖。</p>
</blockquote>
<p>Q8: batchsize太大会导致loss nan吗？</p>
<blockquote>
<p>不会，batchsize中的loss是求均值的，理论上batchsize更大数值稳定性会更好，出现数值不稳定问题可能是学习率没有调好。</p>
</blockquote>
<p>Q9: GPU显存如何优化？</p>
<blockquote>
<p>显存手动优化很难，靠的是框架，pytorch的优化做的还不错。除非特别懂框架相关技术不然建议把batchsize调小或是把模型做简单一点。</p>
</blockquote>
<p>Q10: 对于精度来说batchsize=1是一种最好的情况吗？</p>
<blockquote>
<p>可能是。</p>
</blockquote>
<p>Q11: parameter server可以和pytorch结合吗，具体如何实现？</p>
<blockquote>
<p>pytorch没有实现parameter server，但mxnet和tensorflow有。但是有第三方实现如byteps支持pytorch。</p>
</blockquote>
<p>Q12: 用了nn.DataParallel()，是不是数据集也被自动分配到了多个GPU上？</p>
<blockquote>
<p>是的。在算net.forward()的时候会分开。</p>
</blockquote>
<p>Q13: 验证集准确率震荡大那个参数影响最大？</p>
<blockquote>
<p>学习率。</p>
</blockquote>
<p>Q14: 为了让网络前几层能够训练能否采用不同stage采用不同学习率的方法？</p>
<blockquote>
<p>可以，主要的问题是麻烦，不好确定各部分学习率相差多少。</p>
</blockquote>
<p>Q15: 在用torch的数据并行中将inputs和labels放到GPU0是否会导致性能问题，因为这些数据最终回被挪一次到其他GPU上。</p>
<blockquote>
<p>数据相比梯度来说很少，不会对性能有太大影响。但这个操作看上去的确很多余，老师认为不需要做，但不这样做会报错。</p>
</blockquote>
<p>Q16: 为什么batchsize较小精度会不怎么变化？</p>
<blockquote>
<p>学习率太大了，batchsize小学习率就不能太大。</p>
</blockquote>
<p>Q17: 使用两块不同型号GPU影响深度学习性能吗？</p>
<blockquote>
<p>需要算好两块GPU的性能差。如一块GPU的性能是另一块的2倍，那么在分配任务时也应该分得2倍的任务量。保证各GPU在同样时间内算完同一部分。</p>
</blockquote>
<p>Q18: 课内竞赛直接用教材的VGG11但不收敛，同样的dataloader用resnet可以收敛，如何解决这一问题？</p>
<blockquote>
<p>可能是学习率太大，也可考虑加入batch normalization。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>32-深度学习硬件</title>
    <url>/2024/04/23/11-01-32/</url>
    <content><![CDATA[<h2 id="32-深度学习硬件"><a href="#32-深度学习硬件" class="headerlink" title="32-深度学习硬件"></a>32-深度学习硬件</h2><h3 id="1-DSP-数字信号处理"><a href="#1-DSP-数字信号处理" class="headerlink" title="1.DSP:数字信号处理"></a>1.DSP:数字信号处理</h3><ul>
<li><p>为数字信号处理算法设计：点积、卷积、FFT</p>
</li>
<li><p>低功耗，高性能</p>
<ul>
<li>比移动GPU快5倍，功耗更低</li>
</ul>
</li>
<li>VLIW：very long instruction word<ul>
<li>频率低，核少，但是一条指令可以进行上百次的累加，便于重复</li>
</ul>
</li>
<li>缺点：编程和调试困难，编译器良莠不齐（做的人少，工具不是很好用）</li>
</ul>
<h3 id="2-可编程阵列（FPGA）"><a href="#2-可编程阵列（FPGA）" class="headerlink" title="2.可编程阵列（FPGA）"></a>2.可编程阵列（FPGA）</h3><ul>
<li>有大量的可以用来编程的逻辑单元和可配置链接</li>
<li>可以配置成计算复杂函数<ul>
<li>编程语言：VHDL Verilog</li>
</ul>
</li>
<li>通常比通用硬件更高效，但是体积更大不方便</li>
<li>缺点：工具链质量良莠不齐，一次编译需要数个小时（烧一次板子，物理上的改变）</li>
<li>用途：主要用来模拟，看看效果好不好，如果好可以进一步造芯片</li>
</ul>
<h3 id="3-AI-ASIC"><a href="#3-AI-ASIC" class="headerlink" title="3.AI ASIC"></a>3.AI ASIC</h3><ul>
<li>深度学习热门领域（针对特定领域）<ul>
<li>大公司都在造自己的芯片（Intel Qualcomm Google Amazon Facebook）</li>
</ul>
</li>
<li>Google TPU 是标志性芯片（听说在Google内部已经盛行 取代GPU了）<ul>
<li>能够媲美 Nvidia GPU性能</li>
<li>在Google 大量部署</li>
<li>核心是 systolic array（时间快 容易造）</li>
</ul>
</li>
<li>systolic array<ul>
<li>计算单元（PE）阵列</li>
<li>特别适合做矩阵乘法</li>
<li>设计和制造相对简单（核少）</li>
<li>矩阵乘法例子：见PPT<ul>
<li>对于一般的矩阵乘法：通过切开、填充来匹配SA大小</li>
<li>批量输入来降低延迟（避免空等，先出的硬件空闲）</li>
<li>通常有其他硬件单元来处理别的NN操作子，例如激活层</li>
</ul>
</li>
<li>缺点：只针对深度学习这方面有用，别的方面效果不大</li>
</ul>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>灵活性、易用性：Intel(CPU) &gt; GPU &gt; DSP &gt; FPGA &gt; ASIC</li>
<li>性能功耗：Intel(CPU) &lt; GPU &lt; DSP &lt; FPGA &lt; ASIC</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>31-CPU和GPU</title>
    <url>/2024/04/23/11-01-31/</url>
    <content><![CDATA[<h1 id="CPU和GPU"><a href="#CPU和GPU" class="headerlink" title="CPU和GPU"></a>CPU和GPU</h1><h3 id="1-CPU："><a href="#1-CPU：" class="headerlink" title="1.CPU："></a>1.CPU：</h3><h4 id="1-1-提升CPU利用率一："><a href="#1-1-提升CPU利用率一：" class="headerlink" title="1.1 提升CPU利用率一："></a>1.1 提升CPU利用率一：</h4><ul>
<li>在计算a+b之前，需要准备数据</li>
<li><p>主内存-&gt;L3-&gt;L2-&gt;L1-&gt;寄存器</p>
<ul>
<li>L1访问延时：0.5ms</li>
<li>L2访问延时：7ns（14XL1）</li>
<li>主内存访问延时：100ns(200XL1)</li>
</ul>
</li>
<li><p>提升空间和时间的内存本地性</p>
<ul>
<li>时间：重用数据使它们在缓存里</li>
<li>空间：按序读写数据是的可以预读取</li>
</ul>
</li>
</ul>
<h4 id="1-2-样例分析："><a href="#1-2-样例分析：" class="headerlink" title="1.2 样例分析："></a>1.2 样例分析：</h4><ul>
<li>如果一个矩阵是按行存储，访问一行比访问一列要快<ul>
<li>CPU一次读取64字节（缓存线）</li>
<li>CPU会“聪明的”提前读取下一个（缓存线）</li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-01-31/31-01.png" class>
<h4 id="1-3-提升CPU利用率二："><a href="#1-3-提升CPU利用率二：" class="headerlink" title="1.3 提升CPU利用率二："></a>1.3 提升CPU利用率二：</h4><ul>
<li><p>高端CPU有几十个核</p>
<ul>
<li>EC2 P3.16xlarge:2 Intel Xeon CPUs,32物理核</li>
</ul>
</li>
<li><p>并行来利用所用核</p>
<ul>
<li>超线程不一定提升性能，因为他们共享寄存器</li>
</ul>
</li>
</ul>
<h4 id="1-4-样例分析："><a href="#1-4-样例分析：" class="headerlink" title="1.4 样例分析："></a>1.4 样例分析：</h4><ul>
<li>左边比右边慢（python）</li>
</ul>
<img src="/2024/04/23/11-01-31/31-02.png" class>
<ul>
<li>左边调用n次函数，每次调用有开销</li>
<li>右边很容易被并行（例如下面的C++实现）</li>
</ul>
<img src="/2024/04/23/11-01-31/31-03.png" class>
<h3 id="2-CPU-vs-GPU"><a href="#2-CPU-vs-GPU" class="headerlink" title="2.CPU vs GPU:"></a>2.CPU vs GPU:</h3><img src="/2024/04/23/11-01-31/31-04.png" class>
<h4 id="2-1-提升GPU利用率"><a href="#2-1-提升GPU利用率" class="headerlink" title="2.1 提升GPU利用率"></a>2.1 提升GPU利用率</h4><ul>
<li><p>并行</p>
<ul>
<li>使用数千个线程</li>
</ul>
</li>
<li><p>内存本地性</p>
<ul>
<li>缓存更小，架构更简单</li>
</ul>
</li>
<li><p>少用控制语句</p>
<ul>
<li>支持有限</li>
<li>同步开销大</li>
</ul>
</li>
</ul>
<h4 id="2-2-CPU-GPU-带宽"><a href="#2-2-CPU-GPU-带宽" class="headerlink" title="2.2 CPU/GPU 带宽"></a>2.2 CPU/GPU 带宽</h4><img src="/2024/04/23/11-01-31/31-05.png" class>
<h4 id="2-3-更多的CPUs和GPUs"><a href="#2-3-更多的CPUs和GPUs" class="headerlink" title="2.3 更多的CPUs和GPUs"></a>2.3 更多的CPUs和GPUs</h4><ul>
<li>CPU:AMD,ARM</li>
<li>GPU:AMD,Intel,ARM,Qualcomm…</li>
</ul>
<h4 id="2-4-CPU-GPU高性能计算编程"><a href="#2-4-CPU-GPU高性能计算编程" class="headerlink" title="2.4 CPU/GPU高性能计算编程"></a>2.4 CPU/GPU高性能计算编程</h4><ul>
<li>CPU：C++或者任何高性能语言<ul>
<li>编译器成熟</li>
</ul>
</li>
<li>GPU<ul>
<li>Nvida上用CUDA<ul>
<li>编译器和驱动成熟</li>
</ul>
</li>
<li>其他用OpenCL<ul>
<li>质量取决于硬件厂商</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h3><ul>
<li>CPU:可以处理通用计算。性能优化考虑数据读写效率和多线程</li>
<li>GPU：使用更多的小核和更好的内存带宽，适合能大规模并行的计算任务</li>
</ul>
<h3 id="4-Q-amp-A"><a href="#4-Q-amp-A" class="headerlink" title="4.Q&amp;A"></a>4.Q&amp;A</h3><h5 id="Q1-如果要提高泛化性，就要增加数据？调参的意思是不是最大？"><a href="#Q1-如果要提高泛化性，就要增加数据？调参的意思是不是最大？" class="headerlink" title="Q1:  如果要提高泛化性，就要增加数据？调参的意思是不是最大？"></a>Q1:  如果要提高泛化性，就要增加数据？调参的意思是不是最大？</h5><blockquote>
<p>提高泛化性的有效手段是增加数据，但是数据的质量很重要，少量高质量数据和大量低质量数据可能有1:10或者1:100的换算关系。实际应用场景对调参要求不高，因为有不断增加的数据。</p>
</blockquote>
<h5 id="Q2-alexnet模型比resnet要大，为什么计算上resnet比alexnet运算量大？"><a href="#Q2-alexnet模型比resnet要大，为什么计算上resnet比alexnet运算量大？" class="headerlink" title="Q2:alexnet模型比resnet要大，为什么计算上resnet比alexnet运算量大？"></a>Q2:alexnet模型比resnet要大，为什么计算上resnet比alexnet运算量大？</h5><blockquote>
<p>alexnet后面用到的几个连续的全连接层使模型变大，但是resnet使用的卷积层在少量参数下更消耗计算资源。模型大小和计算复杂度不能直接换算。</p>
</blockquote>
<h5 id="Q3-训练时为什么使用w-lr-w-grad-而不写做w-w-lr-w-grad"><a href="#Q3-训练时为什么使用w-lr-w-grad-而不写做w-w-lr-w-grad" class="headerlink" title="Q3:训练时为什么使用w-=lr*w.grad,而不写做w=w-lr*w.grad?"></a>Q3:训练时为什么使用w-=lr*w.grad,而不写做w=w-lr*w.grad?</h5><blockquote>
<p>因为第二种写法定义了一个新的tensor，梯度参数会成为false</p>
</blockquote>
<h5 id="Q4-llc是显存还是缓存，是l1-l2-还是l3"><a href="#Q4-llc是显存还是缓存，是l1-l2-还是l3" class="headerlink" title="Q4:llc是显存还是缓存，是l1,l2,还是l3?"></a>Q4:llc是显存还是缓存，是l1,l2,还是l3?</h5><blockquote>
<p>llc是缓存，last level cash,是最后一层缓存，具体是ln取决于一共有几层缓存。</p>
</blockquote>
<h5 id="Q5-做计算时把for-lopps运算尽可能向量化？"><a href="#Q5-做计算时把for-lopps运算尽可能向量化？" class="headerlink" title="Q5:做计算时把for_lopps运算尽可能向量化？"></a>Q5:做计算时把for_lopps运算尽可能向量化？</h5><blockquote>
<p>是的，尽量不要用python写for-loop</p>
</blockquote>
<h5 id="Q6-可视化时，需要把数据在cpu和GPU之间切换，如何避免频繁传输？常见的错误操作有哪些？怎么看到和排查这种错误？"><a href="#Q6-可视化时，需要把数据在cpu和GPU之间切换，如何避免频繁传输？常见的错误操作有哪些？怎么看到和排查这种错误？" class="headerlink" title="Q6:可视化时，需要把数据在cpu和GPU之间切换，如何避免频繁传输？常见的错误操作有哪些？怎么看到和排查这种错误？"></a>Q6:可视化时，需要把数据在cpu和GPU之间切换，如何避免频繁传输？常见的错误操作有哪些？怎么看到和排查这种错误？</h5><blockquote>
<p>可视化操作不需要太担心，只要不是计算中来回传递就好。深度学习框架会有限制，只能在一个设备上做。框架没报错一般不会有太多问题</p>
</blockquote>
<h5 id="Q7-go怎么样？"><a href="#Q7-go怎么样？" class="headerlink" title="Q7:go怎么样？"></a>Q7:go怎么样？</h5><blockquote>
<p>go分布式系统做的很好，和深度学习的分布式不太一样</p>
</blockquote>
<h5 id="Q8-怎样复现论文？"><a href="#Q8-怎样复现论文？" class="headerlink" title="Q8:怎样复现论文？"></a>Q8:怎样复现论文？</h5><blockquote>
<p>80%的论文无法复现，要读懂每一句话，和明白作者实现的细节。</p>
</blockquote>
<h5 id="Q9：分布式和高性能的区别？"><a href="#Q9：分布式和高性能的区别？" class="headerlink" title="Q9：分布式和高性能的区别？"></a>Q9：分布式和高性能的区别？</h5><blockquote>
<p>没有本质区别，分布式更多考虑容错。高性能是分布式的一个应用</p>
</blockquote>
<h5 id="Q10-自动驾驶烧钱，短时间难以落地是不是和nas一样？"><a href="#Q10-自动驾驶烧钱，短时间难以落地是不是和nas一样？" class="headerlink" title="Q10:自动驾驶烧钱，短时间难以落地是不是和nas一样？"></a>Q10:自动驾驶烧钱，短时间难以落地是不是和nas一样？</h5><blockquote>
<p>不是，自动驾驶有很好的商业前景。nas没有太多意义。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>30 第二部分完结竞赛：图片分类</title>
    <url>/2024/04/23/11-01-30/</url>
    <content><![CDATA[<h2 id="30-第二部分完结竞赛：图片分类"><a href="#30-第二部分完结竞赛：图片分类" class="headerlink" title="30 第二部分完结竞赛：图片分类"></a>30 第二部分完结竞赛：图片分类</h2><p>竞赛地址：<a href="https://www.kaggle.com/c/classify-leaves">https://www.kaggle.com/c/classify-leaves</a></p>
<p>任务：给出叶子图片预测树种，共20000张图，176类，每类至少50张图。训练样本18353张，测试样本8800张。</p>
<p>这次公榜私榜是随机分的，正常来说两榜的名次差别不会太大。</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>29-残差网络（ResNet）</title>
    <url>/2024/04/23/11-01-29/</url>
    <content><![CDATA[<h3 id="残差网络（ResNet）"><a href="#残差网络（ResNet）" class="headerlink" title="残差网络（ResNet）"></a>残差网络（ResNet）</h3><p>随着我们设计越来越深的网络，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在ResNet这种网络中，添加层会使网络更具表现力</p>
<h4 id="函数类"><a href="#函数类" class="headerlink" title="函数类"></a>函数类</h4><ul>
<li>假设有一类特定的神经网络架构F，它包括学习速率和其他超参数设置。 对于所有<em>f</em>∈F，存在一些参数集（例如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。 现在假设<em>f</em>∗是我们真正想要找到的函数，如果是<em>f</em>∗∈F，那我们可以轻而易举的训练得到它，但通常我们不会那么幸运。 相反，我们将尝试找到一个函数<em>f</em>∗，这是我们在F中的最佳选择。</li>
<li>为了得到更近似真正<em>f</em>∗的函数我们需要设计一个更强大的架构F’，但是如果先前的框架F不包含于新框架F‘中就可能导致如下图中左侧的最优函数离实际预测函数误差反而随框架边强而增大，这不是我们期望的结果，所以我们选择使用下图中右侧的嵌套函数类以解决这个问题</li>
<li>引入方法：对于深度神经网络，如果我们能将新添加的层训练成<em>恒等映射</em>（identity function）<em>f</em>(<strong>x</strong>)=<strong>x</strong>，新模型和原模型将同样有效。 同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</li>
</ul>
<img src="/2024/04/23/11-01-29/29-01.png" class>
<img src="/2024/04/23/11-01-29/29-02.png" class>
<h4 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h4><ul>
<li><p>神经网络中的具体实现：假设我们的原始输入为<em>x</em>，而希望学出的理想映射为<em>f</em>(<strong>x</strong>)，左图虚线框中的部分需要直接拟合出该映射<em>f</em>(<strong>x</strong>)，而右图虚线框中的部分则需要拟合出残差映射<em>f</em>(<strong>x</strong>)−<strong>x</strong>。而右图正是ResNet的基础架构–<em>残差块</em>（residual block）</p>
</li>
<li><p>残差块的代码实现：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span><br><span class="line"><span class="params">                 use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 第一个卷积层</span></span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="comment"># 第二个卷积层</span></span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 如果使用1 x 1卷积以使得输入变换成需要的形状</span></span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 对应第一个卷积层的批量规范化层</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        <span class="comment"># 对应第二个卷积层的批量规范化层</span></span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 第一层：卷积 -&gt; 规范化 -&gt; relu激活</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        <span class="comment"># 第二层：卷积 -&gt; 规范化</span></span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="comment"># 如果要让输入变换成需要的形状</span></span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            <span class="comment"># 对X使用1 x 1卷积，以使输出成为需要的形状</span></span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="comment"># 嵌套模型的实现，即对上一次训练后的模型进行嵌套</span></span><br><span class="line">        Y += X</span><br><span class="line">        <span class="comment"># relu激活并输出</span></span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-29/29-03.png" class>
<h4 id="ResNet模型"><a href="#ResNet模型" class="headerlink" title="ResNet模型"></a>ResNet模型</h4><ul>
<li>ResNet的前两层跟之前介绍的GoogLeNet中的一样： 在输出通道数为64、步幅为2的7×7卷积层后，接步幅为2的3×3的最大汇聚层。 不同之处在于ResNet每个卷积层后增加了批量规范化层。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>GoogLeNet在后面接了4个由Inception块组成的模块。 ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span><br><span class="line"><span class="params">                 first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<ul>
<li>接着在ResNet加入所有残差块，这里每个模块使用2个残差块。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>最后，与GoogLeNet一样，在ResNet中加入全局平均汇聚层，以及全连接层输出。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class="line">                    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>每个模块有4个卷积层（不包括恒等映射的1×1卷积层）。 加上第一个7×7卷积层和最后一个全连接层，共有18层。 因此，这种模型通常被称为ResNet-18。 通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。 虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。</li>
</ul>
<img src="/2024/04/23/11-01-29/29-04.png" class>
<h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><ul>
<li>我们在Fashion-MNIST数据集上训练ResNet</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-29/29-05.png" class>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。</li>
<li>残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。</li>
<li>利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。</li>
<li>残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>28-批量归一化</title>
    <url>/2024/04/23/11-01-28/</url>
    <content><![CDATA[<h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><p>深层神经网络的训练，尤其是使网络在较短时间内收敛是十分困难的，<strong>批量归一化[batch normalization]</strong>是一种流行且有效的技术，能加速深层网络的收敛速度，目前仍被广泛使用。</p>
<h3 id="训练深层网络时的问题"><a href="#训练深层网络时的问题" class="headerlink" title="训练深层网络时的问题"></a>训练深层网络时的问题</h3><img src="/2024/04/23/11-01-28/deep_model.png" class>
<p>深度神经网络在训练时会遇到一些问题：</p>
<ul>
<li>收敛速度慢：<ul>
<li>由于训练时先正向传播后反向传播，且每层的梯度一般较小，若网络较深，则反向传播时会出现类似于梯度消失的现象，导致距离数据更近的层梯度较小，收敛慢，而距离输出更近的层梯度较大，收敛快。然而底部的层一般都用于提取较基础的特征信息，上方的层收敛后，由于底部提取基础特征的层仍在变化，上方的层一直在不停的重新训练，导致整个网络难以收敛，训练较慢。</li>
</ul>
</li>
<li>内部协变量转移：<ul>
<li>分布偏移：偏移在视频课程中并未出现，但在《动手学深度学习》这本书中有提到过，在<a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/environment.html">4.9. 环境和分布偏移</a>部分。偏移指的是训练数据可能和测试数据的分布不同，比如利用来自真实的猫和狗的照片的训练数据训练模型，然后让模型去预测动画中的猫和狗的图片。<img src="/2024/04/23/11-01-28/cat-dog-train.svg" alt="cat-dog-train"><img src="/2024/04/23/11-01-28/cat-dog-test.svg" alt="cat-dog-test">这显然会降低正确率也会对模型的进一步优化带来干扰。一般情况下对于分布偏移我们毫无办法，然而，在一些特定场景中，如果假定一些训练数据和测试数据分布的前提条件，就能对分布偏移进行处理，其中之一就是协变量偏移。</li>
<li>协变量偏移：协变量偏移假设输入的分布可能随时间变化，但标签函数（条件分布$P(y|\bold x)$）没有改变。统计学家称这为<em>协变量偏移</em>（covariate shift）并给出了一些解决方案</li>
<li><strong>内部协变量偏移(Internal Covariate Shift)</strong>：每一层的参数在更新过程中，会改变下一层输入的分布，导致网络参数变幻莫测，难以收敛，神经网络层数越多，表现得越明显。</li>
<li><font color="red">注意：</font>
* <font color="red">1：内部协变量偏移这个词与标准的协变量偏移所有区别。</font>
* <font color="red">2：能缓解内部协变量偏移仅仅是批量归一化的作者提出的假想，后续论文证实批量归一化实际对内部协变量偏移的缓解帮助不大</font>
* <font color="red">3：批量归一化一般只影响模型的收敛速度，不影响精度</font></li>
</ul>
</li>
<li>过拟合：<ul>
<li>由于网络深度加深，变得更为复杂，使得网络容易过拟合。</li>
</ul>
</li>
</ul>
<h3 id="批量归一化-1"><a href="#批量归一化-1" class="headerlink" title="批量归一化"></a>批量归一化</h3><p><strong>批量归一化(batch normalization)</strong>在 <a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#ioffe-szegedy-2015">[Ioffe &amp; Szegedy, 2015]</a>中被提出，用于解决上述训练深度网络时的这些问题，然而这只是人们的感性理解，关于批量归一化具体是怎样帮助训练这个问题目前仍待进一步研究。</p>
<p>批量归一化尝试将每个训练中的mini-batch小批量数据（即会导致参数更新的数据）在每一层的结果进行归一化，使其更稳定，归一化指的是对于当前小批量中的所有样本，求出期望和方差，然后将每个样本减去期望再除以标准差。</p>
<h3 id="形式化表达"><a href="#形式化表达" class="headerlink" title="形式化表达"></a>形式化表达</h3><p>下面的运算均为向量运算，向量中的每个维度代表一个特征，对于每个特征分别进行计算再拼接在一起即为向量运算。</p>
<p>设$ \bold x \in \mathcal{B}$为来自一个小批量$\mathcal{B}$的输入，批量规范化BN根据下式进行转换</p>
<script type="math/tex; mode=display">
\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}.</script><p>式中$\hat{\boldsymbol{\mu}}_\mathcal{B}$为小批量$\mathcal{B}$样本均值，$\hat{\boldsymbol{\sigma}}_\mathcal{B}$为样本标准差：</p>
<script type="math/tex; mode=display">
\begin{split}\begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\
\hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon\end{aligned}\end{split}</script><p>其中$\epsilon$用于防止分母为0，经过减期望与除以标准差后得到期望为1方差为0的小批量数据。然而，期望和方差为了使小批量有更自由的选择，再将其乘拉伸参数$\boldsymbol {\gamma}$，加偏移参数$\boldsymbol \beta$，这两个参数与$\bold x$同样大小，是模型中的可学习参数，与其他参数一同更新。</p>
<p>由于$\hat{\boldsymbol{\mu}}_\mathcal{B}$和$\hat{\boldsymbol{\sigma}}_\mathcal{B}$为由当前小批量计算的值，实际上是整个分布对应的期望与标准差的估计值，由于小批量的随机选择，$\hat{\boldsymbol{\mu}}_\mathcal{B}$和$\hat{\boldsymbol{\sigma}}_\mathcal{B}$会给模型带来一定的与输入数据有关的噪音，而这些噪音也能对模型进行正则化，防止过拟合。为何这种噪音能加快训练并带来正则化还有待研究，不过已有理论说明了为什么批量规范化最适应$50∼100$范围中的中等批量大小的问题。</p>
<p>训练时不能使用整个数据集，只能一步步的训练和更新；而预测时模型已然固定，可以根据整个数据集精确计算均值和方差。因此，批量归一化对于训练和预测时有两种不同模式。</p>
<h3 id="批量归一化层"><a href="#批量归一化层" class="headerlink" title="批量归一化层"></a>批量归一化层</h3><p>批量归一化不再单独的考虑单个样本，需要对整个mini-batch进行，因此需要考虑多种情况。</p>
<h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。如下：</p>
<script type="math/tex; mode=display">
\mathbf{h} = \phi(\mathrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}))</script><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>在卷积层中，我们将通道视作每个位置的特征，将每个样本中的每个位置视作一个样本进行计算。每个通道都有着自己的拉伸参数${\gamma}$和偏移参数$\beta$，所有通道加在一起组成了拉伸参数向量$\boldsymbol {\gamma}$和偏移参数向量$\boldsymbol \beta$，若样本数为m，卷积输出为p*q，计算时对m*p*q个向量进行批量归一化运算（即视作有m*p*q个样本）</p>
<h4 id="预测过程中的批量归一化"><a href="#预测过程中的批量归一化" class="headerlink" title="预测过程中的批量归一化"></a>预测过程中的批量归一化</h4><p>在训练过程中，我们需要不断地更新模型，方差和均值也就在不断地变化，就必须计算当前小批量数据对应的方差和均值，然而预测时我们的模型已经确定下来，可以用在整个训练数据集上得到的均值和方差来对预测时的结果进行归一化。</p>
<h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><ul>
<li><p>在实际实现时，一般使用指数加权平均来更新小批量的均值和方差，指数加权平均将旧值和当前计算结果不断进行加权平均，最终做到平滑的向更新值靠拢，公式如下：</p>
</li>
<li><script type="math/tex; mode=display">
S_t = 
\begin{cases} 
Y_1, &t = 1 \\\\ 
\beta S_{t-1} + (1-\beta)Y_t, &t > 1 
\end{cases}</script></li>
<li><p>批量归一化的参数可以通过动量梯度下降，RMSProp，Adam等多种优化方法进行训练。</p>
</li>
</ul>
<h3 id="吴恩达老师深度学习课程中的批量归一化"><a href="#吴恩达老师深度学习课程中的批量归一化" class="headerlink" title="吴恩达老师深度学习课程中的批量归一化"></a>吴恩达老师深度学习课程中的批量归一化</h3><p>吴恩达老师深度学习课程中的批量归一化中的部分内容与本课程有所出入，考虑到批量归一化这部分内容还没有精确的理论解释，目前的认识仅限于直觉，故将两课程中的区别即补充罗列在此作为参考：</p>
<ul>
<li>关于dropout：<ul>
<li>本课中提到批量归一化有正则化效果，无需再进行dropout</li>
<li>吴恩达老师课程中提到批量归一化正则化效果较差，不能作为正则化的手段，必要时需要dropout</li>
</ul>
</li>
<li>对于线性层（包括其他带有偏置项的层）后的批量归一化，由于归一化时减去了均值，偏置项被消掉，可以省略归一化层之前的偏置项</li>
<li>标准化的输入能使梯度下降加快，批归一化能使得每层的输入都被归一化，这也是训练更快的原因之一</li>
<li>批量归一化可以使得不同层之间互相的影响减少，从而应对数据偏移，增强鲁棒性。</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>26-网络中的网络（NiN）</title>
    <url>/2024/04/23/11-01-26/</url>
    <content><![CDATA[<h2 id="26-网络中的网络（NiN）"><a href="#26-网络中的网络（NiN）" class="headerlink" title="26-网络中的网络（NiN）"></a>26-网络中的网络（NiN）</h2><h3 id="1-动机"><a href="#1-动机" class="headerlink" title="1. 动机"></a>1. 动机</h3><p><strong>全连接层的问题</strong></p>
<ul>
<li><strong>卷积层</strong>需要的<strong>参数较少</strong></li>
<li>而卷积层后的第一个<strong>全连接层</strong>的<strong>参数较多</strong></li>
</ul>
<img src="/2024/04/23/11-01-26/26-01.png" class>
<p>以VGG为例(图示)，全连接层需要先Flatten，输入维度为512x7x7，输出维度为4096，则需要参数个数为512x7x7x4096=102M。</p>
<h3 id="2-NiN块"><a href="#2-NiN块" class="headerlink" title="2. NiN块"></a>2. NiN块</h3><ul>
<li>核心思想：一个卷积层后面跟两个1x1的卷积层，后两层起到全连接层的作用。</li>
</ul>
<img src="/2024/04/23/11-01-26/26-02.png" class>
<h3 id="3-NiN架构"><a href="#3-NiN架构" class="headerlink" title="3. NiN架构"></a>3. NiN架构</h3><ul>
<li>无全连接层</li>
<li>交替使用NiN块和步幅为2的最大池化层<ul>
<li>逐步减小高宽和增大通道数</li>
</ul>
</li>
<li>最后使用全局平均池化得到输出<ul>
<li>其输入通道是类别数</li>
</ul>
</li>
</ul>
<h3 id="4-NiN-Networks"><a href="#4-NiN-Networks" class="headerlink" title="4. NiN Networks"></a>4. NiN Networks</h3><img src="/2024/04/23/11-01-26/26-03.png" class>
<p>NiN架构如上图右边所示，若干个NiN块(图示中为4个块)+池化层；前3个块后接最大池化层，最后一块连接一个全局平均池化层。</p>
<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><ul>
<li>NiN块结构：使用卷积层加两个1x1卷积层<ul>
<li>后者对每个像素增加了非线性性</li>
</ul>
</li>
<li>NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层<ul>
<li>不容易过拟合，更少的参数个数</li>
</ul>
</li>
</ul>
<h3 id="6-代码"><a href="#6-代码" class="headerlink" title="6.代码"></a>6.代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果在Colab上跑, 或没有安装过d2l包, 需要最开始pip install d2l</span></span><br><span class="line">!pip install git+https://github.com/d2l-ai/d2l-zh@release  <span class="comment"># installing d2l</span></span><br></pre></td></tr></table></figure>
<p><strong>NiN块</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义NiN块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU())</span><br></pre></td></tr></table></figure>
<p><strong>NiN模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),          <span class="comment">#全局平均池化，高宽都变成1</span></span><br><span class="line">    nn.Flatten())             <span class="comment">#消掉最后两个维度, 变成(batch_size, 10)</span></span><br></pre></td></tr></table></figure>
<p><strong>demo测试，查看每个块的输出情况</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">Sequential output shape:   torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">MaxPool2d output shape:     torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">Sequential output shape:   torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">MaxPool2d output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Sequential output shape:   torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">MaxPool2d output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Dropout output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Sequential output shape:   torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">AdaptiveAvgPool2d output shape:   torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">Flatten output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p><strong>训练模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;Figure size 252x180 <span class="keyword">with</span> <span class="number">1</span> Axes&gt;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>25 使用块的网络 VGG</title>
    <url>/2024/04/23/11-01-25/</url>
    <content><![CDATA[<h2 id="25-使用块的网络-VGG"><a href="#25-使用块的网络-VGG" class="headerlink" title="25 使用块的网络 VGG"></a>25 使用块的网络 VGG</h2><p>Alexnet最大的问题在于长得不规则，结构不甚清晰，也不便于调整。想要把网络做的更深更大需要更好的设计思想和标准框架。</p>
<h3 id="1-VGG块"><a href="#1-VGG块" class="headerlink" title="1. VGG块"></a>1. VGG块</h3><p>直到现在更深更大的模型也是我们努力的方向，在当时AlexNet比LeNet更深更大得到了更好的精度，大家也希望把网络做的更深更大。选择之一是使用更多的全连接层，但全连接层的成本很高；第二个选择是使用更多的卷积层，但缺乏好的指导思想来说明在哪加，加多少。最终VGG采取了将卷积层组合成块，再把卷积块组合到一起的思路。</p>
<p>VGG块可以看作是AlexNet思路的拓展，AlexNet中将三个相同的卷积层放在一起再加上一个池化层，而VGG将其拓展成可以使用任意个3x3，不改变输入大小的的卷积层，最后加上一个2x2的最大池化层。</p>
<img src="/2024/04/23/11-01-25/25-01.PNG" class>
<p>为什么选择3x3卷积呢？在计算量相同的情况下选用更大的卷积核涉及对网络会越浅，VGG作者经过实验发现用3x3卷积的效果要比5x5好，也就是说神经网络库深且窄的效果会更好。</p>
<h3 id="2-VGG架构"><a href="#2-VGG架构" class="headerlink" title="2. VGG架构"></a>2. VGG架构</h3><p>多个VGG块后接全连接层，不同次数的重复块得到不同的架构，如VGG-16, VGG-19等，后面的数字取决于网络层数。</p>
<p>可以讲VGG看作是将AlexNet中连续卷积的部分取出加以推广和复制，并删去了AlexNet中不那么规整的前几层。</p>
<img src="/2024/04/23/11-01-25/25-02.PNG" class>
<p>VGG较AlexNet相比性能有很大的提升，而代价是处理样本速度的降低和内存占用的增加。</p>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><ul>
<li><p>VGG使用可重复使用的卷积块来构建深度卷积网络</p>
</li>
<li><p>不同卷积块个数和超参数可以得到不同复杂度的变种</p>
</li>
</ul>
<p>这些思想影响了后面神经网络的设计，在之后的模型中被广泛使用。</p>
<h3 id="4-QA"><a href="#4-QA" class="headerlink" title="4. QA"></a>4. QA</h3><p>Q1: 视觉领域人工特征的研究还有无进展？</p>
<blockquote>
<p>现在在计算机视觉做人工特征是一种“政治不正确”的事，可能会因被认为没有novelty而发不出paper ;-)</p>
<p>老师认为人工特征提取确实应该被取代掉，随着技术进步可以把这部分工作交给机器，人去做更高级的事。</p>
</blockquote>
<p>Q2: 需要学习特征值/特征向量/奇异值分解的知识吗？</p>
<blockquote>
<p>这门课中不一定会讲，但很多深度学习模型用到矩阵分解的思想，但是用的不多，想学可以学。</p>
</blockquote>
<p>Q3: Colab限时12小时与验证码的解决方法</p>
<blockquote>
<p>充钱</p>
</blockquote>
<p>Q4: 训练loss一直下降，测试loss一只不降的原因</p>
<blockquote>
<p>代码写错了/过拟合(训练集和测试集很不一样)</p>
</blockquote>
<p>Q5: 为什么VGG（1，1，224，224）输入高宽减半后通道数是64？</p>
<blockquote>
<p>第一个卷积层的输出通道选的是64。(通道数变化是自定的，和高宽变化没有关系)</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>24-AlexNet</title>
    <url>/2024/04/23/11-01-24/</url>
    <content><![CDATA[<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h3 id="1-历史"><a href="#1-历史" class="headerlink" title="1.历史"></a>1.历史</h3><h4 id="1-1-2000-流行的机器学习方法——SVM，核方法"><a href="#1-1-2000-流行的机器学习方法——SVM，核方法" class="headerlink" title="1.1 2000 流行的机器学习方法——SVM，核方法"></a>1.1 2000 流行的机器学习方法——SVM，核方法</h4><ul>
<li>核方法替代了之前的神经网络网络方法，SVM对于调参不敏感，现在也有一些应用</li>
<li>本质上是特征提取，具体的方法是选择核函数来计算，把特征映射到高纬空间，使得他们线性可分</li>
<li>经过核函数计算之后，原问题可以转化为凸优化问题，这是2006年左右的研究热点</li>
<li><p>核方法有很多漂亮的定理，有很好的数学解释性</p>
</li>
<li><p>2010年左右，深度学习才兴起</p>
</li>
</ul>
<h4 id="1-2-2000计算机视觉主要方法——几何学"><a href="#1-2-2000计算机视觉主要方法——几何学" class="headerlink" title="1.2 2000计算机视觉主要方法——几何学"></a>1.2 2000计算机视觉主要方法——几何学</h4><ul>
<li>首先还是对图片进行特征抽取</li>
<li>希望把计算机视觉问题描述成几何问题，建立（非）凸优化模型，可以得到很多漂亮的定理。</li>
<li>可以假设这是一个几何问题，假设这个假设被满足了，可以推出很好的效果</li>
</ul>
<h4 id="1-3-2010计算机视觉的热点问题——特征工程"><a href="#1-3-2010计算机视觉的热点问题——特征工程" class="headerlink" title="1.3 2010计算机视觉的热点问题——特征工程"></a>1.3 2010计算机视觉的热点问题——特征工程</h4><ul>
<li>特征工程就是怎么抽取一张图片的特征，因为直接输入一张图片效果非常的差</li>
<li>特征描述子：SIFT,SURF</li>
</ul>
<h4 id="1-4-硬件的发展奠定了深度学习的兴起"><a href="#1-4-硬件的发展奠定了深度学习的兴起" class="headerlink" title="1.4 硬件的发展奠定了深度学习的兴起"></a>1.4 硬件的发展奠定了深度学习的兴起</h4><ul>
<li>数据的增长，硬件的计算能力奠定了人们对于方法的选择</li>
</ul>
<img src="/2024/04/23/11-01-24/24-01.png" class>
<h4 id="1-5-ImageNet（2010）"><a href="#1-5-ImageNet（2010）" class="headerlink" title="1.5 ImageNet（2010）"></a>1.5 ImageNet（2010）</h4><img src="/2024/04/23/11-01-24/24-02.png" class>
<ul>
<li><p>AlexNet赢得了2012年ImageNet竞赛冠军</p>
</li>
<li><p>本质上是一个加强版的LeNet，更深更大</p>
</li>
<li><p>AlexNet主要改进措施：</p>
<ul>
<li>dropout（正则）</li>
<li>ReLu（梯度更大）</li>
<li>MaxPooling（取最大值，梯度相对增大）</li>
</ul>
</li>
<li><p>影响：计算机视觉方法论的改变，从人工提取特征过渡到CNN学习特征</p>
</li>
</ul>
<img src="/2024/04/23/11-01-24/24-03.png" class>
<h3 id="2-AlexNet架构"><a href="#2-AlexNet架构" class="headerlink" title="2.AlexNet架构"></a>2.AlexNet架构</h3><img src="/2024/04/23/11-01-24/24-04.png" class>
<img src="/2024/04/23/11-01-24/24-05.png" class>
<img src="/2024/04/23/11-01-24/24-06.png" class>
<ul>
<li>网络代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">这里，我们使用一个<span class="number">11</span>*<span class="number">11</span>的更大窗口来捕捉对象。</span><br><span class="line">    <span class="comment"># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class="line">    <span class="comment"># 另外，输出通道的数目远大于LeNet</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    <span class="comment"># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class="line">    <span class="comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class="line">    <span class="comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>更多细节<ul>
<li>激活函数从sigmoid变成Relu，减缓梯度消失</li>
<li>隐藏全连接层后加入了丢弃层（2个4096之后加入了dropout）</li>
<li>数据增强，将一张图片进行变化，选取多个位置、光照之类的。</li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-01-24/24-07.png" class>
<ul>
<li>复杂度对比<ul>
<li>参数个数增加，每次更新数据增加</li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-01-24/24-08.png" class>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h3><ul>
<li>AlexNet 是更大更深的LeNet，10x参数个数，260x计算复杂度</li>
<li>新加入了dropout，relu，maxpooling，数据增强</li>
<li>标志着新一轮神经网络热潮开始了</li>
</ul>
<h3 id="4-QA"><a href="#4-QA" class="headerlink" title="4.QA"></a>4.QA</h3><ul>
<li>问题大部分都在问如何炼丹，炼丹的理论，为啥炼丹的步骤要这样不哪有？<ul>
<li>老师说这个确实不好理解，只能从自己的角度去尝试解释</li>
</ul>
</li>
<li>数据增强了，但是效果还不如之前的，为啥？<ul>
<li>太正常了，属于超参数没调好</li>
</ul>
</li>
<li>为啥LeNet不属于深度卷积神经网络？<ul>
<li>为了包装现在的产品，更好卖（确实是这么回答的），这个我们研究者需要学习，好好宣传自己的产品</li>
</ul>
</li>
<li>网络要求输入的size是固定的，实际使用的时候图片不一定是要求的size，怎么处理？<ul>
<li>如果是大的图片，在保持长宽比的情况下，把短边压成输入的size，然后在新的图片中随机抠出来几张图片（要求和网络输入一致）进行预测。效果上不会有太大的影响</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>23-经典卷积神经网络LeNet</title>
    <url>/2024/04/23/11-01-23/</url>
    <content><![CDATA[<h3 id="1-LeNet卷积神经网络"><a href="#1-LeNet卷积神经网络" class="headerlink" title="1.LeNet卷积神经网络"></a>1.LeNet卷积神经网络</h3><h4 id="1-1-手写数字识别"><a href="#1-1-手写数字识别" class="headerlink" title="1.1 手写数字识别"></a>1.1 手写数字识别</h4><ul>
<li>LeNet网络最早是为了应用于手写数字的识别应用。</li>
<li>应用背景：<ul>
<li>邮政局希望可以自动读出信件上的邮政编码</li>
<li>人们希望可以用支票自动取钱</li>
</ul>
</li>
<li>该模型在80年代末的银行被真正的部署</li>
</ul>
<img src="/2024/04/23/11-01-23/23-01.png" class>
<h4 id="1-2-MNIST"><a href="#1-2-MNIST" class="headerlink" title="1.2 MNIST"></a>1.2 MNIST</h4><ul>
<li>LeNet所使用的数据集</li>
<li>50，000个训练数据</li>
<li>10，000个测试数据</li>
<li>图像大小为28*28</li>
<li>10类</li>
</ul>
<img src="/2024/04/23/11-01-23/23-02.png" class>
<h4 id="1-3-LeNet的具体模型"><a href="#1-3-LeNet的具体模型" class="headerlink" title="1.3 LeNet的具体模型"></a>1.3 LeNet的具体模型</h4><img src="/2024/04/23/11-01-23/23-03.png" class>
<h4 id="1-4-总结"><a href="#1-4-总结" class="headerlink" title="1.4 总结"></a>1.4 总结</h4><ul>
<li>LeNet是早期成功的神经网络</li>
<li>先使用卷积层来学习图片空间信息</li>
<li>然后使用全连接层来转换到类别空间</li>
</ul>
<h3 id="2-代码部分"><a href="#2-代码部分" class="headerlink" title="2.代码部分"></a>2.代码部分</h3><h4 id="2-1-定义网络结构和准备工作"><a href="#2-1-定义网络结构和准备工作" class="headerlink" title="2.1 定义网络结构和准备工作"></a>2.1 定义网络结构和准备工作</h4><ul>
<li>导入所需的库</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#导入所需的库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<ul>
<li>定义网络结构（具体可参考上文“具体模型”的图）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义网络结构</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>查看每一层数据的变化情况</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#把每一层数据的shape给打印出来</span></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)<span class="comment">#创建符合要求的张量</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)<span class="comment">#通过每一层</span></span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape: \t&#x27;</span>,X.shape)<span class="comment">#打印</span></span><br></pre></td></tr></table></figure>
<h4 id="2-2-模型训练"><a href="#2-2-模型训练" class="headerlink" title="2.2 模型训练"></a>2.2 模型训练</h4><ul>
<li>下载数据集</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span><span class="comment">#批量大小</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)<span class="comment">#下载或加载数据集，得到训练和测试集的迭代对象</span></span><br></pre></td></tr></table></figure>
<ul>
<li>使用GPU计算模型在数据集上的精度</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    <span class="comment"># 正确预测的数量，总预测的数量</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)<span class="comment">#创建一个累加器，包含2个要累加的元素</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">                <span class="comment"># BERT微调所需的（之后将介绍）</span></span><br><span class="line">                X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())<span class="comment">#把每一组数据预测结果正确的个数和长度累加</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>训练函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)<span class="comment">#对linear类型的层用xavier初始化</span></span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])<span class="comment">#动画需要</span></span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练损失之和，训练准确率之和，范例数</span></span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()<span class="comment">#梯度清零</span></span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)<span class="comment">#正向传播</span></span><br><span class="line">            l = loss(y_hat, y)<span class="comment">#计算损失</span></span><br><span class="line">            l.backward()<span class="comment">#反向传播</span></span><br><span class="line">            optimizer.step()<span class="comment">#梯度下降</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])<span class="comment">#训练损失之和，训练准确率之和，范例数</span></span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)<span class="comment">#评估测试集的精度</span></span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>运行</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-23/23-04.png" class>
<h4 id="2-3-总结"><a href="#2-3-总结" class="headerlink" title="2.3 总结"></a>2.3 总结</h4><ul>
<li>卷积神经网络（CNN）是一类使用卷积层的网络。</li>
<li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。</li>
<li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li>
<li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li>
<li>LeNet是最早发布的卷积神经网络之一（80年代）</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>22-池化层</title>
    <url>/2024/04/23/11-01-22/</url>
    <content><![CDATA[<h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><p>本节将介绍<em>池化</em>（pooling）层，它具有目的：类似于数据增强，降低卷积层对位置的敏感性；一定程度减少计算。</p>
<h2 id="最大池化层和平均池化层"><a href="#最大池化层和平均池化层" class="headerlink" title="最大池化层和平均池化层"></a>最大池化层和平均池化层</h2><p>与卷积层类似，池化层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口遍历的每个位置计算一个输出。<br>然而，不同于卷积层中的输入与卷积核之间的互相关计算，<strong>池化层不包含参数</strong>。<br>相反，池运算符是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为<em>最大池化层</em>（maximum pooling）和<em>平均池化层</em>（average pooling）。</p>
<p>在这两种情况下，与互相关运算符一样，池化窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。计算最大值或平均值是取决于使用了最大池化层还是平均池化层。</p>
<p><img src="http://d2l.ai/_images/pooling.svg" alt="池化窗口形状为 $2\times 2$ 的最大池化层。着色部分是第一个输出元素，以及用于计算这个输出的输入元素: $\max(0, 1, 3, 4)=4$."><br>上图中的输出张量的高度为$2$，宽度为$2$。这四个元素为每个池化窗口中的最大值：</p>
<script type="math/tex; mode=display">
\max(0, 1, 3, 4)=4,\\
\max(1, 2, 4, 5)=5,\\
\max(3, 4, 6, 7)=7,\\
\max(4, 5, 7, 8)=8.\\</script><p>池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，池化操作称为$p \times q$池化。</p>
<p>回到本节开头提到的对象边缘检测示例，现在我们将使用卷积层的输出作为$2\times 2$最大池化的输入。<br>设置卷积层输入为<code>X</code>，池化层输出为<code>Y</code>。<br>无论<code>X[i, j]</code>和<code>X[i, j + 1]</code>的值是否不同，或<code>X[i, j + 1]</code>和<code>X[i, j + 2]</code>的值是否不同，池化层始终输出<code>Y[i, j] = 1</code>。<br>也就是说，使用$2\times 2$最大池化层，即使在高度或宽度上移动一个元素，卷积层仍然可以识别到模式。</p>
<p>在下面的代码中的<code>pool2d</code>函数，我们(<strong>实现池化层的前向传播</strong>)。然而，这里我们没有卷积核，输出为输入中每个区域的最大值或平均值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):  <span class="comment"># 枚举输出的每个位置，[i,j]对应输入的位置[i至i+p_h,j至j+p_w]</span></span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:  <span class="comment"># 最大池化</span></span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].<span class="built_in">max</span>()  <span class="comment"># max函数返回最大值</span></span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:  <span class="comment"># 平均池化</span></span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()  <span class="comment"># mean函数返回平均值</span></span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<p>我们可以构建上图中的输入张量<code>X</code>，[<strong>验证二维最大池化层的输出</strong>]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[4., 5.],
        [7., 8.]])
</code></pre><p>此外，我们还可以(<strong>验证平均池化层</strong>)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[2., 3.],
        [5., 6.]])
</code></pre><h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>与卷积层一样，池化层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。<br>下面，我们用深度学习框架中内置的二维最大池化层，来演示池化层中填充和步幅的使用。<br>我们首先构造了一个输入张量<code>X</code>，它有四个维度，其中样本数和通道数都是1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape(</span><br><span class="line">    (<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))  <span class="comment"># 维度[batch_size，通道数，H，W]</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])
</code></pre><p>默认情况下，(<strong>深度学习框架中的步幅与池化窗口的大小相同</strong>)。<br>因此，如果我们使用形状为<code>(3, 3)</code>的池化窗口，那么默认情况下，我们得到的步幅形状为<code>(3, 3)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[10.]]]])
</code></pre><p>[<strong>填充和步幅可以手动设定</strong>]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[ 5.,  7.],
          [13., 15.]]]])
</code></pre><p>当然，我们可以(<strong>设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度</strong>)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[ 5.,  7.],
          [13., 15.]]]])
</code></pre><h2 id="多个通道"><a href="#多个通道" class="headerlink" title="多个通道"></a>多个通道</h2><p>在处理多通道输入数据时，[<strong>池化层在每个输入通道上单独运算</strong>]，而不是像卷积层一样在通道上对输入进行汇总。<br>这意味着池化层的输出通道数与输入通道数相同。<br>下面，我们将在通道维度上连结张量<code>X</code>和<code>X + 1</code>，以构建具有2个通道的输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)  <span class="comment"># 在第一个维度也就是通道维度拼接</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],

         [[ 1.,  2.,  3.,  4.],
          [ 5.,  6.,  7.,  8.],
          [ 9., 10., 11., 12.],
          [13., 14., 15., 16.]]]])
</code></pre><p>如下所示，池化后输出通道的数量仍然是2。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[[[ 5.,  7.],
          [13., 15.]],

         [[ 6.,  8.],
          [14., 16.]]]])
</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>对于给定输入元素，最大池化层会输出该窗口内的最大值，平均池化层会输出该窗口内的平均值。</li>
<li>池化层的主要优点之一是减轻卷积层对位置的过度敏感。</li>
<li>我们可以指定池化层的填充和步幅。</li>
<li>使用最大池化层以及大于1的步幅，可减少空间维度（如高度和宽度）。</li>
<li>池化层的输出通道数与输入通道数相同。</li>
</ul>
<h2 id="问题和练习"><a href="#问题和练习" class="headerlink" title="问题和练习"></a>问题和练习</h2><ol>
<li>你能将平均池化层作为卷积层的特殊情况实现吗？</li>
</ol>
<blockquote>
<p>设卷积层大小是$m\times n$，卷积层里面每个元素参数是$\dfrac{1} {m\times n}$，这样就是一个平均池化层作为卷积层的实现</p>
</blockquote>
<ol>
<li>假设池化层的输入大小为$c\times h\times w$，则汇聚窗口的形状为$p_h\times p_w$，填充为$(p_h, p_w)$，步幅为$(s_h, s_w)$。这个池化层的计算成本是多少？</li>
</ol>
<blockquote>
<p>$ c\times \left \lfloor \dfrac {h-p_h+s_h}{s_h}\right \rfloor \times \left \lfloor \dfrac {w-p_w+s_w}{s_w}\right \rfloor $</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>21-多个输入和输出通道</title>
    <url>/2024/04/23/11-01-21/</url>
    <content><![CDATA[<h1 id="21-多个输入和输出通道"><a href="#21-多个输入和输出通道" class="headerlink" title="21-多个输入和输出通道"></a>21-多个输入和输出通道</h1><h3 id="1-多个输入通道："><a href="#1-多个输入通道：" class="headerlink" title="1.多个输入通道："></a>1.多个输入通道：</h3><ul>
<li><p>彩色图像可能有RGB三个通道</p>
</li>
<li><p>转换为灰度会丢失信息</p>
</li>
</ul>
<img src="/2024/04/23/11-01-21/21-01.png" class>
<ul>
<li>每个通道都有一个卷积和，结果是所有通道卷积结果的和</li>
</ul>
<img src="/2024/04/23/11-01-21/21-02.png" class>
<ul>
<li>输入<strong>X</strong>:<img src="https://latex.codecogs.com/svg.image?c_{i}\times&space;n_{h}\times&space;n_{w}" title="c_{i}\times n_{h}\times n_{w}"></li>
<li>核<strong>W</strong>：<img src="https://latex.codecogs.com/svg.image?c_{i}\times&space;k_{h}\times&space;k_{w}" title="c_{i}\times k_{h}\times k_{w}"></li>
<li>输出<strong>Y</strong>:<img src="https://latex.codecogs.com/svg.image?m_{h}\times&space;m_{w}" title="m_{h}\times m_{w}"></li>
</ul>
<p><img src="https://latex.codecogs.com/svg.image?Y=\sum&space;_{i=0}^{c_{i}}X_{i,:,:}\bigstar&space;W_{i,:,:}" title="Y=\sum _{i=0}^{c_{i}}X_{i,:,:}\bigstar W_{i,:,:}"></p>
<p>多个输入通道：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br></pre></td></tr></table></figure>
<h3 id="2-多个输出通道"><a href="#2-多个输出通道" class="headerlink" title="2.多个输出通道"></a>2.多个输出通道</h3><ul>
<li>无论有多少输入通道，到目前位置我们植绒到单输出通道</li>
<li>我们可以有多个三维卷积核，每个核生成一个输出通道</li>
<li>输入<strong>X</strong>:<img src="https://latex.codecogs.com/svg.image?c_{i}\times&space;k_{h}\times&space;k_{w}" title="c_{i}\times k_{h}\times k_{w}"></li>
<li>核<strong>W</strong>：<img src="https://latex.codecogs.com/svg.image?c_{o}\times&space;c_{i}\times&space;k_{h}\times&space;k_{w}" title="c_{o}\times c_{i}\times k_{h}\times k_{w}"></li>
<li>输出<strong>Y</strong>：<img src="https://latex.codecogs.com/svg.image?c_{o}\times&space;m_{h}\times&space;m_{w}" title="c_{o}\times m_{h}\times m_{w}"></li>
</ul>
<p><img src="https://latex.codecogs.com/svg.image?Y_{i,:,:}=X\bigstar&space;W_{i,:,:}\qquad&space;for&space;\quad&space;i=1,...,c_{o}" title="Y_{i,:,:}=X\bigstar W_{i,:,:}\qquad for \quad i=1,...,c_{o}"></p>
<p>多个输出通道：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-多个输入和输出通道"><a href="#3-多个输入和输出通道" class="headerlink" title="3.多个输入和输出通道"></a>3.多个输入和输出通道</h3><ul>
<li>每个通道可以识别特定的模式</li>
</ul>
<img src="/2024/04/23/11-01-21/21-03.png" class>
<ul>
<li>输入通道核识别并组合输入中的模式</li>
</ul>
<h3 id="4-1X1卷积层"><a href="#4-1X1卷积层" class="headerlink" title="4.1X1卷积层"></a>4.1X1卷积层</h3><img src="/2024/04/23/11-01-21/21-04.png" class>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br></pre></td></tr></table></figure>
<h3 id="5-二维卷积层"><a href="#5-二维卷积层" class="headerlink" title="5.二维卷积层"></a>5.二维卷积层</h3><img src="/2024/04/23/11-01-21/21-05.png" class>
<h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h3><ul>
<li>输出通道数是卷积层的超参数</li>
<li>每个输入通道有独立的二维卷积和，所有通道结果相加得到一个输出通道结果</li>
<li>每个输出通道有独立的三维卷积核</li>
</ul>
<h3 id="7-Q-amp-A"><a href="#7-Q-amp-A" class="headerlink" title="7.Q&amp;A"></a>7.Q&amp;A</h3><h5 id="Q1-网络越深，Padding-0-越多，这里是否会影响性能？"><a href="#Q1-网络越深，Padding-0-越多，这里是否会影响性能？" class="headerlink" title="Q1:网络越深，Padding 0 越多，这里是否会影响性能？"></a>Q1:网络越深，Padding 0 越多，这里是否会影响性能？</h5><blockquote>
<p>这里性能分为计算性能和网络性能，Padding 0 不会影响网络精度，但会使计算复杂</p>
</blockquote>
<h5 id="Q2-计算卷积时，bias的有无对结果影响大吗？bias的作用怎么解释？"><a href="#Q2-计算卷积时，bias的有无对结果影响大吗？bias的作用怎么解释？" class="headerlink" title="Q2:计算卷积时，bias的有无对结果影响大吗？bias的作用怎么解释？"></a>Q2:计算卷积时，bias的有无对结果影响大吗？bias的作用怎么解释？</h5><blockquote>
<p>因为正则化的操作，bias对结果影响不大，但加入bias对计算性能基本无影响，故默认加入bias</p>
</blockquote>
<h5 id="Q3-如果是一个rgb图像，加上深度图，相当于是四个通道吗？"><a href="#Q3-如果是一个rgb图像，加上深度图，相当于是四个通道吗？" class="headerlink" title="Q3:如果是一个rgb图像，加上深度图，相当于是四个通道吗？"></a>Q3:如果是一个rgb图像，加上深度图，相当于是四个通道吗？</h5><blockquote>
<p>不是，输入输出通道单列，这里使用3d的卷积，输入变为4维，核是5维</p>
</blockquote>
<h5 id="Q4-怎么理解1x1卷积核不识别空间模式？"><a href="#Q4-怎么理解1x1卷积核不识别空间模式？" class="headerlink" title="Q4:怎么理解1x1卷积核不识别空间模式？"></a>Q4:怎么理解1x1卷积核不识别空间模式？</h5><blockquote>
<p>因为输出的一个像素只对应输入的一个像素，所以没有获取到空间信息</p>
</blockquote>
<h5 id="Q5-是不是可以3x3x3和1x1xN的卷积层叠加，来进行空间信息的检测和信息融合，以及输出通道的调整？"><a href="#Q5-是不是可以3x3x3和1x1xN的卷积层叠加，来进行空间信息的检测和信息融合，以及输出通道的调整？" class="headerlink" title="Q5:是不是可以3x3x3和1x1xN的卷积层叠加，来进行空间信息的检测和信息融合，以及输出通道的调整？"></a>Q5:是不是可以3x3x3和1x1xN的卷积层叠加，来进行空间信息的检测和信息融合，以及输出通道的调整？</h5><blockquote>
<p>是的，mobile net就是这种思想                                                                                                                                                                                                                                                                                 </p>
</blockquote>
<h5 id="Q6：3d卷积是处理视频问题的吧？也可以处理rgb加深度信息吗？"><a href="#Q6：3d卷积是处理视频问题的吧？也可以处理rgb加深度信息吗？" class="headerlink" title="Q6：3d卷积是处理视频问题的吧？也可以处理rgb加深度信息吗？"></a>Q6：3d卷积是处理视频问题的吧？也可以处理rgb加深度信息吗？</h5><blockquote>
<p>都可以，rgb加深度信息甚至可以用2d卷积处理。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>20-填充和步幅</title>
    <url>/2024/04/23/11-01-20/</url>
    <content><![CDATA[<h2 id="20-填充和步幅"><a href="#20-填充和步幅" class="headerlink" title="20-填充和步幅"></a>20-填充和步幅</h2><h3 id="1-填充"><a href="#1-填充" class="headerlink" title="1. 填充"></a>1. 填充</h3><p><strong>填充</strong>(Padding)指的是在输入周围添加额外的行/列</p>
<img src="/2024/04/23/11-01-20/20-01.png" class>
<p><strong>维度变化</strong>：</p>
<img src="/2024/04/23/11-01-20/20-02.png" class>
<p><strong>两种不同的卷积方式</strong>：<br>①Valid 卷积：不进行填充，卷积运算过后得到的矩阵形状为(n-f+1)×(n-f+1)。 </p>
<p>②Same 卷积：先对矩阵进行填充，然后再进行卷积运算，使得运算前后矩阵大小不变。</p>
<img src="/2024/04/23/11-01-20/20-03.png" class>
<h3 id="2-步幅"><a href="#2-步幅" class="headerlink" title="2. 步幅"></a>2. 步幅</h3><p><strong>想法来源：</strong>如果按照原来的操作(卷积步长为1)，那么给定输入大小为224x224，在使用5x5卷积核的情况下，需要<strong>55层</strong>才能将输出降低到4x4，也就是说，需要大量的计算才能得到维度较小的输出。</p>
<p><strong>步幅</strong>是指行/列的滑动步长</p>
<img src="/2024/04/23/11-01-20/20-04.png" class>
<p><strong>维度变化</strong>:</p>
<img src="/2024/04/23/11-01-20/20-05.png" class>
<p>注意：第三点可以当做结论来记(Same卷积或Valid卷积(且s≥k时))。一般来说，如果n是偶数，s取2，池化层做Valid卷积(不填充)且k=2，此时输出维度直接可以写成n/2 x n/2。如果怕搞混，直接记第一个公式每次现推也可。</p>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><ul>
<li><p>填充和步幅是卷积层的<strong>超参数</strong></p>
</li>
<li><p><strong>填充</strong>(padding)在输入周围添加额外的行/列，来控制输出形状的减少量</p>
</li>
<li><strong>步幅</strong>(stride)是每次滑动核窗口时的行/列的步长，可以成倍地减少输出形状</li>
</ul>
<h3 id="4-代码"><a href="#4-代码" class="headerlink" title="4. 代码"></a>4. 代码</h3><h4 id="4-1-填充和步幅"><a href="#4-1-填充和步幅" class="headerlink" title="4.1 填充和步幅"></a>4.1 填充和步幅</h4><p><strong>导入包，定义comp_conv2d函数  (进行卷积操作, 输出后两维，便于观察高宽的维度变化)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape) <span class="comment">#X的维度之前加入批量大小数(batch_size)和输入通道数(channel_in)</span></span><br><span class="line">    Y = conv2d(X)                    </span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])  <span class="comment">#去掉前面的两维后(原来四维) 进行输出</span></span><br></pre></td></tr></table></figure>
<h4 id="4-2-padding"><a href="#4-2-padding" class="headerlink" title="4.2 padding"></a>4.2 padding</h4><p><strong>在所有侧边填充1个像素(padding=1, 即(1,1))</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) <span class="comment">#输入输出通道数为1, 卷积核大小3x3, 填充为1(上下左右各填充一行)</span></span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))         </span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Size([<span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<p><strong>填充不同的高度和宽度(padding=(2,1))</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Size([<span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<h4 id="4-3-stride"><a href="#4-3-stride" class="headerlink" title="4.3 stride"></a>4.3 stride</h4><p><strong>将高度和宽度的步幅设置为2</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p><strong>一个稍微复杂的例子</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>19-卷积层</title>
    <url>/2024/04/23/11-01-19/</url>
    <content><![CDATA[<h2 id="19-卷积层"><a href="#19-卷积层" class="headerlink" title="19-卷积层"></a>19-卷积层</h2><h4 id="本讲文字介绍部分请参考沐神在线书籍-：https-zh-v2-d2l-ai-chapter-convolutional-neural-networks-why-conv-html"><a href="#本讲文字介绍部分请参考沐神在线书籍-：https-zh-v2-d2l-ai-chapter-convolutional-neural-networks-why-conv-html" class="headerlink" title="本讲文字介绍部分请参考沐神在线书籍~：https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/why-conv.html"></a>本讲文字介绍部分请参考沐神在线书籍~：<a href="https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/why-conv.html">https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/why-conv.html</a></h4><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X,K</span>):    <span class="comment">#X为输入，K为核矩阵</span></span><br><span class="line">    h,w=K.shape    <span class="comment">#h得到K的行数，w得到K的列数</span></span><br><span class="line">    Y=torch.zeros((X.shape[<span class="number">0</span>]-h+<span class="number">1</span>,X.shape[<span class="number">1</span>]-w+<span class="number">1</span>))  <span class="comment">#用0初始化输出矩阵Y</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):   <span class="comment">#卷积运算</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">          Y[i,j]=(X[i:i+h,j:j+w]*K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#样例点测试</span></span><br><span class="line">X=torch.tensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">K=torch.tensor([[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">corr2d(X,K)</span><br></pre></td></tr></table></figure>
<pre><code>&gt;&gt;&gt; tensor([[19., 25.],
            [37., 43.]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#实现二维卷积层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_</span>(<span class="params">self,kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>()._init_()</span><br><span class="line">        self.weight=nn.Parameter(torch.rand(kerner_size))</span><br><span class="line">        self.bias=nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x,self.weight)+self.bias </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X=torch.ones((<span class="number">6</span>,<span class="number">8</span>))</span><br><span class="line">X[:,<span class="number">2</span>:<span class="number">6</span>]=<span class="number">0</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>
<pre><code>&gt;&gt;&gt; tensor([[1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">K=torch.tensor([[-<span class="number">1</span>,<span class="number">1</span>]])  <span class="comment">#这个K只能检测垂直边缘</span></span><br><span class="line">Y=corr2d(X,K)</span><br><span class="line">Y</span><br></pre></td></tr></table></figure>
<pre><code>&gt;&gt;&gt; tensor([[ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corr2d(X.t(),K)</span><br></pre></td></tr></table></figure>
<pre><code>&gt;&gt;&gt; tensor([[0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat - Y)**<span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    conv2d.weight.data[:] -= <span class="number">3e-2</span> * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;batch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&gt;&gt;&gt; batch 2, loss 3.852
    batch 4, loss 1.126
    batch 6, loss 0.386
    batch 8, loss 0.145
    batch 10, loss 0.057
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<pre><code>&gt;&gt;&gt; tensor([[-1.0173,  0.9685]])
</code></pre>]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>18-预测房价竞赛总结</title>
    <url>/2024/04/23/11-01-18/</url>
    <content><![CDATA[<h2 id="18-预测房价竞赛总结"><a href="#18-预测房价竞赛总结" class="headerlink" title="18-预测房价竞赛总结"></a>18-预测房价竞赛总结</h2><h3 id="1-方法总结"><a href="#1-方法总结" class="headerlink" title="1.方法总结"></a>1.方法总结</h3><blockquote>
<p>下面提供了排行榜前几使用的方法介绍链接</p>
</blockquote>
<ul>
<li><p>第二和第七：autogluon</p>
<p><a href="https://www.bilibili.com/video/BV1rh411m7Hb/">https://www.bilibili.com/video/BV1rh411m7Hb/</a></p>
</li>
<li><p>第三：h2o</p>
<p><a href="https://www.kaggle.com/wuwawa/automl-using-h2o">https://www.kaggle.com/wuwawa/automl-using-h2o</a></p>
</li>
<li><p>第四：随机森林</p>
<p><a href="https://www.kaggle.com/jackzh/the-4th-place-approach-random-forest">https://www.kaggle.com/jackzh/the-4th-place-approach-random-forest</a></p>
</li>
</ul>
<h3 id="2-分析"><a href="#2-分析" class="headerlink" title="2.分析"></a>2.分析</h3><ul>
<li><p>已知的排名靠前的4个成绩均使用了集成学习</p>
</li>
<li><p>目前不知道是否有使用书中的mlp取得好成绩</p>
<blockquote>
<p>通过调参数，是能够取得很好的结果的</p>
</blockquote>
<p> 对于mlp来说，特征预处理和超参数的调节是取得好成绩的基础</p>
</li>
<li><p>数据的难点</p>
<ul>
<li><p>数值较大</p>
<blockquote>
<p>梯度相对较大，容易发生梯度爆炸</p>
</blockquote>
<p>一个解决方案是可以对数据取对数，再进行标准化</p>
</li>
<li><p>有文本特征（地址，介绍）</p>
<blockquote>
<p>这些文字可能含有较多的噪声，对模型产生影响</p>
</blockquote>
<p>解决办法日后会讲解，比如第二名用的transformer</p>
</li>
<li><p>训练数据是前6个月，公榜是后3个月，私榜是再往后3个月</p>
<blockquote>
<p>利用历史的数据进行训练，在实践中自然会有不同的影响（可能过拟合）</p>
<p>因此公榜与私榜的排名有一定差异</p>
</blockquote>
<pre><code>       这个问题称为Covariate Shift，没有特别好的解决方案  ，可以让模型尽可能稳定，不去仔细调参
</code></pre></li>
</ul>
</li>
</ul>
<h3 id="3-关于automl"><a href="#3-关于automl" class="headerlink" title="3.关于automl"></a>3.关于automl</h3><h4 id="3-1-课程内容"><a href="#3-1-课程内容" class="headerlink" title="3.1 课程内容"></a>3.1 课程内容</h4><blockquote>
<p>这一部分李沐老师要表达的主要是我们应该深入去了解本后的原理，不要因为有”自动化”深度学习而产生一种依赖心理或者变得没有深究深度学习的动力，学习deep learning仍然是有意义的</p>
</blockquote>
<ul>
<li><p>数据科学家80%时间在处理数据，20%调模型</p>
<blockquote>
<p>处理数据是automl不能做的，automl的作用主要在调模型这块，数据科学家仍然能大展身手</p>
</blockquote>
</li>
<li><p>Automl现在能处理一些基础的情况</p>
<blockquote>
<p>目前节省10%时间，未来节省20%时间</p>
</blockquote>
</li>
<li><p>为什么还要学习深度学习</p>
<p>正如买菜只需要用到四则运算甚至不用，我们仍然需要学习三角函数去进行更深入的科学研究等其他事情。当人人都会用Automl的时候，我们仍然需要懂得一些底层的原理，毕竟Automl也是有局限性的，需要我们不断改进，或者想出其他算法。另一方面，我们也要肯定Automl带来的便利。</p>
</li>
</ul>
<h4 id="3-2-补充内容"><a href="#3-2-补充内容" class="headerlink" title="3.2 补充内容"></a>3.2 补充内容</h4><h5 id="AutoGluon"><a href="#AutoGluon" class="headerlink" title="AutoGluon"></a>AutoGluon</h5><blockquote>
<p>与大部分automl框架是基于超参数搜索技术的不同，Autogluon会利用多个机器学习包来训练模型</p>
</blockquote>
<ul>
<li><p>房价预测竞赛中模型的改动</p>
<blockquote>
<p>1.对于数据中数值比较大且数据变化大的数值取log,CPU上训练2个小时，最终排第七</p>
<p>2.房子描述里包含大量文本，使用mutimodal选项来用transformer提取特征，并做多模型融合,用GPU才跑得动，排名第二</p>
</blockquote>
</li>
<li><p>AutoGluon背后的技术</p>
<blockquote>
<p>1.stacking</p>
<p>2.k-则交叉bagging</p>
<p>3.多层stacking</p>
</blockquote>
</li>
<li><p>总结</p>
<blockquote>
<p>1.autogluon在合理的计算开销下得到还不错的模型</p>
<p>2.虽然autogluon可以做自动特征抽取，但是当加入一些人工数据处理也是不错的方法</p>
<p>3.对于比较大的数据集计算开销仍然是瓶颈，需要使用GPU甚至多台机器做分布式训练，这仍是AutoML未来的研究方向</p>
<p>4.具体讲解可参考：<a href="https://www.bilibili.com/video/BV1F84y1F7Ps/?spm_id_from=333.788.recommend_more_video.1">https://www.bilibili.com/video/BV1F84y1F7Ps/?spm_id_from=333.788.recommend_more_video.1</a></p>
</blockquote>
</li>
</ul>
<h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h3><p>这节课本身就是一次对预测房价竞赛的总结，主要介绍了排名的分布情况以及一些队伍使用的方法。</p>
<h3 id="5-预测房价竞赛总结-Q-amp-A"><a href="#5-预测房价竞赛总结-Q-amp-A" class="headerlink" title="5.预测房价竞赛总结 Q&amp;A"></a>5.预测房价竞赛总结 Q&amp;A</h3><p><strong>Q1: 统计学专业本科生未来从事人工智能如何规划</strong></p>
<blockquote>
<p>注重动手能力的培养</p>
</blockquote>
<p><strong>Q2: 避免overfit是调参好还是不调参好？老师有何经验分享？</strong></p>
<blockquote>
<p>调参是需要的，首先最好有一个比较好的验证集；当你找到一个在验证集效果比较好的超参数值的时候，最好在这一值上调或下调一点看看是否敏感，如果比较敏感说明这点可能只是在这点凑巧效果好罢了，泛化性就不好；当然在实践中调参并没有像在竞赛中那么重要</p>
</blockquote>
<p><strong>Q3: 老师说的80%时间处理数据是指的找数据、清理数据这些？数据搭建pipeline不就好了， ？为什么改进模型等等不占主要时间？</strong></p>
<blockquote>
<p>处理数据并不是搭建pipeline就好了，你需要决定从哪里获取数据、怎样获取数据、如何处理噪音（清理数据）……这些都是很费时间的</p>
</blockquote>
<p><strong>Q4: AutoML与ML有严格的特征区别吗</strong></p>
<blockquote>
<p>AutoML可以看作是ML中的一类算法</p>
</blockquote>
<p><strong>Q5: 用mlp做竞赛时发现层数深的时候预测出来的房价全是一样的，层数浅一点还不会出现这个问题，为什么？</strong></p>
<blockquote>
<p>应该是梯度爆炸，或者梯度消失，也就是数值稳定性出现问题</p>
</blockquote>
<p><strong>Q6：MLP有值得精细调参的价值吗？</strong></p>
<blockquote>
<p>有。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>17-使用和购买GPU</title>
    <url>/2024/04/23/11-01-17/</url>
    <content><![CDATA[<h2 id="使用和购买GPU"><a href="#使用和购买GPU" class="headerlink" title="使用和购买GPU"></a>使用和购买GPU</h2><h3 id="使用GPU"><a href="#使用GPU" class="headerlink" title="使用GPU"></a>使用GPU</h3><p>（简而言之，自2000年以来，GPU性能每10年增长1000倍，本节主要介绍如何利用这种计算性能进行研究，首先是使用单个GPU，然后是如何使用多个GPU和多个服务器）</p>
<ul>
<li><p>准备：</p>
<ul>
<li><p>（首先确保至少安装了一个NVDIA GPU，然后下载<a href="https://developer.nvidia.com/cuda-downloads">NVIDIA驱动和CUDA</a>并按照提示设置适当的路径）</p>
</li>
<li><p>查看显卡信息：        </p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Fri Jan <span class="number">14</span> 03:<span class="number">23</span>:<span class="number">18</span> <span class="number">2022</span></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI <span class="number">418.67</span>       Driver Version: <span class="number">418.67</span>       CUDA Version: <span class="number">10.1</span>     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   <span class="number">0</span>  Tesla V100-SXM2...  Off  | <span class="number">00000000</span>:<span class="number">00</span>:1B<span class="number">.0</span> Off |                    <span class="number">0</span> |</span><br><span class="line">| N/A   43C    P0    74W / 300W |   1608MiB / 16130MiB |      <span class="number">0</span>%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   <span class="number">1</span>  Tesla V100-SXM2...  Off  | <span class="number">00000000</span>:<span class="number">00</span>:1C<span class="number">.0</span> Off |                    <span class="number">0</span> |</span><br><span class="line">| N/A   42C    P0    62W / 300W |   1706MiB / 16130MiB |      <span class="number">9</span>%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   <span class="number">2</span>  Tesla V100-SXM2...  Off  | <span class="number">00000000</span>:<span class="number">00</span>:1D<span class="number">.0</span> Off |                    <span class="number">0</span> |</span><br><span class="line">| N/A   64C    P0    68W / 300W |     11MiB / 16130MiB |      <span class="number">0</span>%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">|   <span class="number">3</span>  Tesla V100-SXM2...  Off  | <span class="number">00000000</span>:<span class="number">00</span>:1E<span class="number">.0</span> Off |                    <span class="number">0</span> |</span><br><span class="line">| N/A   57C    P0    45W / 300W |     11MiB / 16130MiB |      <span class="number">0</span>%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   <span class="type">Type</span>   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|    <span class="number">0</span>      <span class="number">5034</span>      C   ...conda3/envs/d2l-en-release-<span class="number">0</span>/<span class="built_in">bin</span>/python  1597MiB |</span><br><span class="line">|    <span class="number">1</span>      <span class="number">5034</span>      C   ...conda3/envs/d2l-en-release-<span class="number">0</span>/<span class="built_in">bin</span>/python  1695MiB |</span><br></pre></td></tr></table></figure>
<p>（可以看到这里显示有4块Tesla V100的GPU，Memory-Usage显示的是“当前使用空间 / 总空间”，GPU-Util显示的是模型训练时GPU的使用率，如果为50%以下说明模型可能不太好）</p>
<ul>
<li><p>准备（续）：</p>
<ul>
<li>在PyTorch中，每个数组都有一个设备（device）， 我们通常将其称为上下文（context）。 默认情况下，所有变量和相关的计算都分配给CPU。 有时上下文可能是GPU。 当我们跨多个服务器部署作业时，事情会变得更加棘手。 通过智能地将数组分配给上下文， 我们可以最大限度地减少在设备之间传输数据的时间。 例如，当在带有GPU的服务器上训练神经网络时， 我们通常希望模型的参数在GPU上。</li>
<li>要运行此部分中的程序，至少需要两个GPU。 注意，对于大多数桌面计算机来说，这可能是奢侈的，但在云中很容易获得。 例如，你可以使用AWS EC2的多GPU实例。 本书的其他章节大都不需要多个GPU， 而本节只是为了展示数据如何在不同的设备之间传递。</li>
</ul>
</li>
<li><p>计算设备：</p>
<ul>
<li><p>我们可以指定用于存储和计算的设备，如CPU和GPU。 默认情况下，张量是在内存中创建的，然后使用CPU计算它。所有的深度学习框架都是默认在CPU上做运算，如果要使用GPU则需要指定计算机更换运算位置 。</p>
</li>
<li><p>在PyTorch中，CPU和GPU可以用<code>torch.device(&#39;cpu&#39;)</code> 和<code>torch.device(&#39;cuda&#39;)</code>表示。</p>
</li>
<li><p><code>cpu</code>设备意味着所有物理CPU和内存， 这意味着PyTorch的计算将尝试使用所有CPU核心。 然而，<code>gpu</code>设备只代表一个卡和相应的显存。 </p>
</li>
<li><p>如果有多个GPU，我们使用<code>torch.device(f&#39;cuda:&#123;i&#125;&#39;)</code> 来表示第<em>i</em>块GPU（<em>i</em>从0开始）。 另外，<code>cuda:0</code>和<code>cuda</code>是等价的。</p>
</li>
<li><p>```python<br>import torch<br>from torch import nn</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  &quot;&quot;&quot;指定cpu, gpu设备&quot;&quot;&quot;</span><br><span class="line">  torch.device(&#x27;cpu&#x27;), torch.device(&#x27;cuda&#x27;), torch.device(&#x27;cuda:1&#x27;)</span><br><span class="line">  # cpu, gpu0, gpu1</span><br><span class="line">  </span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  输出:</span><br><span class="line">  (device(type=&#x27;cpu&#x27;), device(type=&#x27;cuda&#x27;), device(type=&#x27;cuda&#x27;, index=1))</span><br><span class="line">  &quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>```python<br>“””查询可用gpu数量”””<br>torch.cuda.device_count()</p>
<p>“””<br>输出:<br>2<br>“””</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  &quot;&quot;&quot;定义了两个方便的函数， 这两个函数允许我们在不存在所需所有GPU的情况下运行代码&quot;&quot;&quot;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  &quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;</span><br><span class="line">  def try_gpu(i=0):</span><br><span class="line">  # 不输入参数则默认i = 0 </span><br><span class="line">      if torch.cuda.device_count() &gt;= i + 1:</span><br><span class="line">          return torch.device(f&#x27;cuda:&#123;i&#125;&#x27;)</span><br><span class="line">       # 如果当前可用gpu的总数大于等于i+1，则返回第i个gpu（从0计数）    </span><br><span class="line">      return torch.device(&#x27;cpu&#x27;)</span><br><span class="line">    # 否则证明当前没有更多可用gpu，则返回cpu</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  &quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]&quot;&quot;&quot;</span><br><span class="line">  def try_all_gpus():</span><br><span class="line">      devices = [torch.device(f&#x27;cuda:&#123;i&#125;&#x27;)</span><br><span class="line">               for i in range(torch.cuda.device_count())]</span><br><span class="line">      # 所有可用gpu设备序号组成的列表devices</span><br><span class="line">      </span><br><span class="line">      return devices if devices else [torch.device(&#x27;cpu&#x27;)]</span><br><span class="line">    # 如果列表devices不为空则证明此时有可用的gpu，则返回可用gpu序号列表；否则证明没有可用gpu，则返回cpu</span><br><span class="line">  </span><br><span class="line">  try_gpu(), try_gpu(10), try_all_gpus()</span><br><span class="line">  # 测试函数功能</span><br><span class="line">  # try_gpu():检测是否有第i=0号gpu</span><br><span class="line">  # try_gpu(10):检测是否有第i=10号gpu</span><br><span class="line">  # try_all_gpus():返回所有可用gpu序号列表，如果没有gpu则返回cpu</span><br><span class="line">  </span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  输出：</span><br><span class="line">  (device(type=&#x27;cuda&#x27;, index=0),</span><br><span class="line">   device(type=&#x27;cpu&#x27;),</span><br><span class="line">   [device(type=&#x27;cuda&#x27;, index=0), device(type=&#x27;cuda&#x27;, index=1)])</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  </span><br><span class="line">  # device(type=&#x27;cuda&#x27;, index=0): 有第0号gpu</span><br><span class="line">  # device(type=&#x27;cpu&#x27;): 没有第10号gpu</span><br><span class="line">  # [device(type=&#x27;cuda&#x27;, index=0), device(type=&#x27;cuda&#x27;, index=1)]: 共有序号为0、1的两个gpu</span><br><span class="line">  </span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>张量与GPU</p>
<ul>
<li><p>```python<br>“””我们可以查询张量所在的设备。 默认情况下，张量是在CPU上创建的。”””<br>x = torch.tensor([1, 2, 3])<br>x.device</p>
<p>“””<br>device(type=’cpu’)<br>“””</p>
<h1 id="默认情况下，张量是在CPU上创建的"><a href="#默认情况下，张量是在CPU上创建的" class="headerlink" title="默认情况下，张量是在CPU上创建的"></a>默认情况下，张量是在CPU上创建的</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 注意：无论何时我们要对多个项进行操作， 它们都必须在同一个设备上。 例如，如果我们对两个张量求和， 我们需要确保两个张量都位于同一个设备上， 否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。</span><br><span class="line"></span><br><span class="line">- 存储在GPU上：</span><br><span class="line"></span><br><span class="line">  ```python</span><br><span class="line">  &quot;&quot;&quot;我们在第一个gpu上创建张量变量X&quot;&quot;&quot; </span><br><span class="line">  X = torch.ones(2, 3, device=try_gpu())</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  tensor([[1., 1., 1.],</span><br><span class="line">          [1., 1., 1.]], device=&#x27;cuda:0&#x27;)</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  </span><br><span class="line">  &quot;&quot;&quot;假设你至少有两个GPU，下面的代码将在第二个GPU上创建一个随机张量&quot;&quot;&quot;</span><br><span class="line">  Y = torch.rand(2, 3, device=try_gpu(1))</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  tensor([[0.3432, 0.4088, 0.7725],</span><br><span class="line">          [0.0571, 0.3341, 0.2544]], device=&#x27;cuda:1&#x27;)</span><br><span class="line">  &quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>复制：如果我们要计算<code>X + Y</code>，我们需要决定在哪里执行这个操作。 例如，如下图所示， 我们可以将<code>X</code>传输到第二个GPU并在那里执行操作。 <em>不要</em>简单地<code>X</code>加上<code>Y</code>，因为这会导致异常， 运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。 由于<code>Y</code>位于第二个GPU上，所以我们需要将<code>X</code>移到那里， 然后才能执行相加运算。</p>
<img src="/2024/04/23/11-01-17/17-01.png" class>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>```python<br>“””将gpu(0)中的X复制到gpu(1)中的Z”””<br>Z = X.cuda(1)<br>print(X)<br>print(Z)</p>
<p>“””<br>tensor([[1., 1., 1.],</p>
<pre><code>    [1., 1., 1.]], device=&#39;cuda:0&#39;)
</code></pre><p>tensor([[1., 1., 1.],</p>
<pre><code>    [1., 1., 1.]], device=&#39;cuda:1&#39;)
</code></pre><p>“””</p>
</li>
</ul>
<pre><code>&quot;&quot;&quot;现在数据在同一个GPU上（Z和Y都在），我们可以将它们相加。&quot;&quot;&quot;
Y + Z

&quot;&quot;&quot;
tensor([[1.3432, 1.4088, 1.7725],
        [1.0571, 1.3341, 1.2544]], device=&#39;cuda:1&#39;)
&quot;&quot;&quot;


&quot;&quot;&quot;如果变量Z已经存在于第i个GPU上，再调用Z.cuda(i)只会返回Z并不会复制并分配新内存&quot;&quot;&quot;
Z.cuda(1) is Z

&quot;&quot;&quot;
True
&quot;&quot;&quot;
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  - 旁注：人们使用GPU来进行机器学习，因为单个GPU相对运行速度快。 但是在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。 这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收）， 然后才能继续进行更多的操作。 这就是为什么拷贝操作要格外小心。根据经验，多个小操作比一个大操作糟糕得多。 此外，一次执行几个操作比代码中散布的许多单个操作要好得多（除非你确信自己在做什么）。 如果一个设备必须等待另一个设备才能执行其他操作， 那么这样的操作可能会阻塞。 这有点像排队订购咖啡，而不像通过电话预先订购： 当你到店的时候，咖啡已经准备好了。当我们打印张量或将张量转换为NumPy格式时， 如果数据不在内存中，框架会首先将其复制到内存中， 这会导致额外的传输开销。 更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。        </span><br><span class="line"></span><br><span class="line">- 神经网络与GPU</span><br><span class="line"></span><br><span class="line">  - 类似地，神经网络模型可以指定设备。 下面的代码将模型参数放在GPU上。</span><br><span class="line"></span><br><span class="line">    ```python</span><br><span class="line">    net = nn.Sequential(nn.Linear(3, 1))</span><br><span class="line">    net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure>
</code></pre><ul>
<li><p>当输入为GPU上的张量时，模型将在同一GPU上计算结果。总之，只要所有的数据和参数都在同一个设备上， 我们就可以有效地学习模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net(X)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0.5037],</span></span><br><span class="line"><span class="string">        [0.5037]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].weight.data.device</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">device(type=&#x27;cuda&#x27;, index=0)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="购买GPU"><a href="#购买GPU" class="headerlink" title="购买GPU"></a>购买GPU</h3><p>目前，AMD和NVIDIA是专用GPU的两大主要制造商。NVIDIA是第一个进入深度学习领域的公司，通过CUDA为深度学习框架提供更好的支持。因此，大多数买家选择NVIDIA GPU。</p>
<p>NVIDIA提供两种类型的GPU，针对个人用户（例如，通过GTX和RTX系列）和企业用户（通过其Tesla系列）。这两种类型的GPU提供了相当的计算能力。但是，企业用户GPU通常使用强制（被动）冷却、更多内存和ECC（纠错）内存。这些GPU更适用于数据中心，通常成本是消费者GPU的十倍。</p>
<p>如果你是一个拥有100个服务器的大公司，你应该考虑英伟达Tesla系列，或者在云中使用GPU服务器。对于实验室或10+服务器的中小型公司，英伟达RTX系列可能是最具成本效益的。你可以购买超微或华硕机箱的预配置服务器，这些服务器可以有效地容纳4-8个GPU。</p>
<p>GPU供应商通常每一到两年发布一代，例如2017年发布的GTX 1000（Pascal）系列和2019年发布的RTX 2000（Turing）系列。每个系列都提供几种不同的型号，提供不同的性能级别。GPU性能主要是以下三个参数的组合：</p>
<ol>
<li><strong>计算能力</strong>。通常我们追求32位浮点计算能力。16位浮点训练（FP16）也进入主流。如果你只对预测感兴趣，还可以使用8位整数。最新一代图灵GPU提供4-bit加速。不幸的是，目前训练低精度网络的算法还没有普及。</li>
<li><strong>内存大小</strong>。随着你的模型变大或训练期间使用的批量变大，你将需要更多的GPU内存。检查HBM2（高带宽内存）与GDDR6（图形DDR）内存。HBM2速度更快，但成本更高。</li>
<li><strong>内存带宽</strong>。只有当你有足够的内存带宽时，你才能最大限度地利用你的计算能力。如果使用GDDR6，请追求宽内存总线。</li>
</ol>
<p>对于大多数用户来说，只需看看计算能力就足够了。请注意，许多GPU提供不同类型的加速。例如，NVIDIA的Tensor Cores将操作符子集的速度提高了5×</p>
<p>。确保你的库支持这一点。GPU内存应不小于4GB（8GB更好）。尽量避免将GPU也用于显示GUI（改用内置显卡）。如果无法避免，请添加额外的2GB RAM以确保安全。</p>
<p>下图比较了各种GTX 900、GTX 1000和RTX 2000系列的（GFlops）和价格（Price）。价格是维基百科上的建议价格。</p>
<img src="/2024/04/23/11-01-17/17-02.png" class>
<p>我们可以看到很多事情：</p>
<ol>
<li>在每个系列中，价格和性能大致成比例。Titan因拥有大GPU内存而有相当的溢价。然而，通过比较980 Ti和1080 Ti可以看出，较新型号具有更好的成本效益。RTX 2000系列的价格似乎没有多大提高。然而，它们提供了更优秀的低精度性能（FP16、INT8和INT4）。</li>
<li>GTX 1000系列的性价比大约是900系列的两倍。</li>
<li>对于RTX 2000系列，浮点计算能力是价格的“仿射”函数。</li>
</ol>
<img src="/2024/04/23/11-01-17/17-03.png" class>
<p>上图显示了能耗与计算量基本成线性关系。其次，后一代更有效率。这似乎与对应于RTX 2000系列的图表相矛盾。然而，这是TensorCore不成比例的大能耗的结果。</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>16-Pytorch神经网络基础</title>
    <url>/2024/04/23/11-01-16/</url>
    <content><![CDATA[<h2 id="Pytorch神经网络基础"><a href="#Pytorch神经网络基础" class="headerlink" title="Pytorch神经网络基础"></a>Pytorch神经网络基础</h2><h3 id="层和块"><a href="#层和块" class="headerlink" title="层和块"></a>层和块</h3><p>在之前的内容中，我们认识了一些神经网络，比如：线性回归，Softmax回归，多层感知机；他们有的是整个模型，有的是一层神经网络，有的甚至只是一个单元，他们的功能以及复杂程度也各不相同，但他们都有着如下三个特征：</p>
<ul>
<li>接受一些输入</li>
<li>产生对应的输出</li>
<li>由一组可调整参数描述</li>
</ul>
<p>对于一些复杂的网络，研究讨论比层大但比整个模型小的部分很有意义，因为复杂的网络中经常有重复出现的部分，每个部分也常常有自己的功能。考虑到上面的三个特征，这就使得我们思考是否可以对这些部分进行一个抽象，这就得到了块的概念：块指单个层，多个层组成的部分，或者整个模型本身。使用块对整个模型进行描述就简便许多，这一过程是递归的，块的内部还可以划分为多个块，直至满足需要为止。</p>
<p>PyTorch帮我们实现了块的大部分所需功能，包括自动求导，我们只需从nn.Module继承并改写其中的一部分就能得到我们需要的块以及模型，具体做法和细节见代码中的注释</p>
<h3 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h3><p>在选择了架构并设置了超参数后，我们就进入了训练阶段。此时，我们的目标是找到使损失函数最小化的模型参数值。经过训练后，我们将需要使用这些参数来做出未来的预测。此外，有时我们希望提取参数，以便在其他环境中复用它们，将模型保存下来，以便它可以在其他软件中执行，或者为了获得科学的理解而进行检查。</p>
<p>此部分主要为代码实现，笔记见代码中的注释</p>
<h3 id="延后初始化"><a href="#延后初始化" class="headerlink" title="延后初始化"></a>延后初始化</h3><p>有时在建立网络时，我们不会指定网络的输入输出维度，也就不能确定网络的参数形状，深度学习框架支持延后初始化，即当第一次将数据传入模型时自动的得到所有的维度，然后初始化所有的参数。</p>
<p>PyTorch也支持这一点，比如nn.LazyLinear，但本门课程中并未介绍。</p>
<h3 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h3><p>深度学习成功背后的一个因素是神经网络的灵活性：我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。同样的，对于层而言，深度学习框架并不能满足我们所有的需求，然而，层本身也具有极大的灵活性，我们可以自定义想要的层。</p>
<p>此部分主要为代码实现，笔记见代码中的注释</p>
<h3 id="读写文件"><a href="#读写文件" class="headerlink" title="读写文件"></a>读写文件</h3><p>到目前为止，我们讨论了如何处理数据，以及如何构建、训练和测试深度学习模型。然而，有时我们希望保存训练的模型，以备将来在各种环境中使用（比如在部署中进行预测）。此外，当运行一个耗时较长的训练过程时，最佳的做法是定期保存中间结果，以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>15-实战Kaggle比赛：预测房价</title>
    <url>/2024/04/23/11-01-15/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h3 id="课程练习，kaggle比赛"><a href="#课程练习，kaggle比赛" class="headerlink" title="课程练习，kaggle比赛"></a>课程练习，kaggle比赛</h3><ul>
<li><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">kaggle</a></li>
<li>有一份解决方案公布在论坛里</li>
</ul>
<h3 id="实战Kaggle比赛：预测房价"><a href="#实战Kaggle比赛：预测房价" class="headerlink" title="实战Kaggle比赛：预测房价"></a>实战Kaggle比赛：预测房价</h3><ul>
<li><p>实际上这个时间竞赛已经结束了。</p>
</li>
<li><p>这节目的是提供大家一个实际操作的机会，使用的大多是前面学到的知识。课程提供了代码样本，酒体内容详见含注释的代码中。</p>
</li>
<li>简单介绍一下数据集的内容，数据集是加州2020年几乎全部的房子交易记录，这里选择前半年的数据作为训练集，后半年作为验证集，包括许多的信息，如：ID，洗手间个数，卧室个数，政府预测价格等等共41个特征，连结如下：<a href="https://www.kaggle.com/c/california-house-prices/data?select=train.csv">https://www.kaggle.com/c/california-house-prices/data?select=train.csv</a> 需要大家自行下载数据集（没有kaglle账户的要注册才可以下载）</li>
<li>在真实数据集上，已经提供的程序得到的效果会很差，需要我们使用各种方法来优化，例如dropout，weught decay等，期待大家取得好成绩！</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>14-数值稳定性+模型初始化和激活函数</title>
    <url>/2024/04/23/11-01-14/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="14-数值稳定性-模型初始化和激活函数"><a href="#14-数值稳定性-模型初始化和激活函数" class="headerlink" title="14-数值稳定性+模型初始化和激活函数"></a>14-数值稳定性+模型初始化和激活函数</h2><h3 id="1-数值稳定性"><a href="#1-数值稳定性" class="headerlink" title="1. 数值稳定性"></a>1. 数值稳定性</h3><p>数值稳定性是深度学习中比较重要的点，特别是当神经网络变得很深的时候，数值通常很容易变得不稳定。</p>
<h4 id="1-1-神经网络的梯度"><a href="#1-1-神经网络的梯度" class="headerlink" title="1.1 神经网络的梯度"></a>1.1 神经网络的梯度</h4><img src="/2024/04/23/11-01-14/14-01.png" class>
<p><strong>考虑d层神经网络</strong></p>
<ul>
<li><p>t表示层数，$h^{t-1}$表示第<em>t-1</em>层的输出，经过一个$f_{t}$函数后，得到第<em>t</em>层的输出。</p>
</li>
<li><p>最终输出y的表示：输入x经过若干层(<em>d</em>层)的函数作用，最后被损失函数作用得到输出y。</p>
</li>
</ul>
<p><strong>计算损失函数<em>L</em>关于第<em>t</em>层参数$W_{t}$的梯度</strong></p>
<ul>
<li><p>由链导法则得到上图中乘积公式</p>
</li>
<li><p>需要进行d-t次<strong>矩阵乘法</strong>（为什么是矩阵乘法？答：由于所有的<em>h</em>都是一些<strong>向量</strong>，导数中分子分母均为向量，所以求导得到的是矩阵，维数为[分子维度]x[分母维度]，可以参考第6节<a href="https://www.bilibili.com/video/BV1eZ4y1w7PY">视频</a>和<a href="./06-矩阵计算.md">笔记</a>）。这也是导致数值稳定性问题的<strong>主要因素</strong>，由于做了太多次的矩阵乘法。</p>
</li>
</ul>
<h4 id="1-2-数值稳定性的常见两个问题"><a href="#1-2-数值稳定性的常见两个问题" class="headerlink" title="1.2 数值稳定性的常见两个问题"></a>1.2 数值稳定性的常见两个问题</h4><p><strong>梯度爆炸</strong></p>
<p>假设梯度都是一些比1大的数比如1.5，做100次乘积之后得到$4\times 10^{17}$，这个数字很容易带来一些浮点数上限的问题（需了解更多请参考计算机系统-计算机中浮点数的存储方式）。</p>
<p><strong>梯度消失</strong></p>
<p>假设梯度都是一些比1小的数比如0.8，做100次乘积之后得到$2\times10^{-10}$，也可能会带来浮点数下溢的问题。</p>
<h4 id="1-3-例子：MLP"><a href="#1-3-例子：MLP" class="headerlink" title="1.3 例子：MLP"></a>1.3 例子：MLP</h4><p>此处我们着重探讨<a href="#11-神经网络的梯度">1.1节</a>中所述的求梯度时所做的d-t次矩阵乘法，并以一个实例MLP来探讨其结果的具体形式。</p>
<img src="/2024/04/23/11-01-14/14-02.png" class>
<ul>
<li><p>第一行公式，定义$h^{t}$和$h^{t-1}$(均为向量)的函数关系$f_{t}$，第t层的权重矩阵作用于t-1层的输出$h^{t-1}$后经过激活函数$\sigma$得到$h^{t}$，注意激活函数$\sigma$逐元素计算。</p>
</li>
<li><p>第二行公式：这里用到链导法则，激活函数$\sigma$先对内部向量逐元素求导，然后把求导后这个向量变成对角矩阵（可以理解为链导法则中内部向量$W_{t}h_{t-1}$对自身进行求导，变成一个nxn的对角矩阵，更多请参考<a href="https://nndl.github.io/nndl-book.pdf">邱锡鹏 《神经网络与深度学习》</a><sup><a href="#fn_ 图片1" id="reffn_ 图片1"> 图片1</a></sup>）</p>
</li>
</ul>
<img src="/2024/04/23/11-01-14/14-03.png" class>
<blockquote id="fn_图片1">
<sup>图片1</sup>. 引自<a href="https://nndl.github.io/nndl-book.pdf">邱锡鹏 《神经网络与深度学习》</a>附录：数学基础 <a href="#reffn_图片1" title="Jump back to footnote [图片1] in the text."> &#8617;</a>
</blockquote>
<ul>
<li>视频中<strong>勘误说明</strong>：链导法则中$\frac{\partial W^{t}h^{t-1}}{\partial h^{t-1}}= W^{t}$而不是$\left (W^{t} \right )^{T}$（这点由分子分母维度也容易推出），故最终求导结果包含$W^{t}$，而不是其转置。</li>
</ul>
<h4 id="1-3-梯度爆炸"><a href="#1-3-梯度爆炸" class="headerlink" title="1.3 梯度爆炸"></a>1.3 梯度爆炸</h4><h5 id="1-3-1-使用ReLU作为激活函数"><a href="#1-3-1-使用ReLU作为激活函数" class="headerlink" title="1.3.1 使用ReLU作为激活函数"></a>1.3.1 使用ReLU作为激活函数</h5><img src="/2024/04/23/11-01-14/14-04.png" class>
<p>由于激活函数Relu求导后或者是1或者是0，变为对角矩阵的斜对角线元素后，与$W^{i}$做乘积，斜对角线为1的部分会使得W中元素保留，最终该连乘式中有一些元素来自$\prod\left ( W^{i} \right )$ ，如果大部分$W^{i}$中 值都大于1，且层数比较大，那么连乘之后可能导致梯度爆炸的问题。</p>
<h5 id="1-3-2-梯度爆炸问题"><a href="#1-3-2-梯度爆炸问题" class="headerlink" title="1.3.2 梯度爆炸问题"></a>1.3.2 梯度爆炸问题</h5><ul>
<li><p>值超出值域（infinity）</p>
<ul>
<li>对于16位浮点数尤为严重（数值区间 [6e-5 , 6e4]），GPU用16位浮点数更快</li>
</ul>
</li>
<li><p>对学习率敏感</p>
<ul>
<li><p>如果学习率太大→大参数值→更大的梯度，如此循环几次，容易导致梯度爆炸</p>
</li>
<li><p>如果学习率太小→训练无进展</p>
</li>
<li><p>我们可能需要在训练过程中不断调整学习率</p>
</li>
</ul>
</li>
</ul>
<h4 id="1-4-梯度消失"><a href="#1-4-梯度消失" class="headerlink" title="1.4 梯度消失"></a>1.4 梯度消失</h4><h5 id="1-4-1-使用Sigmoid作为激活函数"><a href="#1-4-1-使用Sigmoid作为激活函数" class="headerlink" title="1.4.1 使用Sigmoid作为激活函数"></a>1.4.1 使用Sigmoid作为激活函数</h5><img src="/2024/04/23/11-01-14/14-05.png" class>
<ul>
<li>蓝色曲线为函数值</li>
<li>黄色曲线为梯度，注意到当输入x值取±6时，此时梯度已经变得很小，由图也可以看出，当输入值稍大或稍小都很容易引起小梯度。</li>
</ul>
<img src="/2024/04/23/11-01-14/14-06.png" class>
<p>所以最终连乘式中$\prod diag\left ( \sigma ^{‘}\left ( W^{i}h^{i-1} \right ) \right )$项乘出来会很小，导致整个梯度很小，产生梯度消失问题。</p>
<h5 id="1-4-2-梯度消失的问题"><a href="#1-4-2-梯度消失的问题" class="headerlink" title="1.4.2 梯度消失的问题"></a>1.4.2 梯度消失的问题</h5><ul>
<li><p>梯度值变为0</p>
<ul>
<li>对16位浮点数尤为严重</li>
</ul>
</li>
<li><p>训练没有进展</p>
<ul>
<li>不管如何选择学习率，由于梯度已经为0了，学习率x梯度=0</li>
</ul>
</li>
<li>对于底部层尤为严重<ul>
<li>仅仅顶部层训练得较好。第<em>t</em>层导数包含d-t个矩阵乘积，越往底层走，t越小，乘得越多，梯度消失越严重，所以底部层效果更差。</li>
<li>无法让神经网络更深。只能把顶部层训练得比较好，底部层跑不动，这和给一个浅的神经网络没有什么区别。</li>
</ul>
</li>
</ul>
<h3 id="2-模型初始化和激活函数"><a href="#2-模型初始化和激活函数" class="headerlink" title="2. 模型初始化和激活函数"></a>2. 模型初始化和激活函数</h3><h4 id="2-1-让训练更加稳定"><a href="#2-1-让训练更加稳定" class="headerlink" title="2.1 让训练更加稳定"></a>2.1 让训练更加稳定</h4><p>我们的一个核心目标是如何让训练更稳定，梯度值不要太大也不要太小</p>
<ul>
<li>目标：让梯度值在合理的范围内<ul>
<li>例如 [1e-6, 1e3]</li>
</ul>
</li>
<li><p>常用方法：</p>
<ul>
<li>将乘法变加法：<ul>
<li>ResNet（跳跃连接，如果很多层，加入加法进去）</li>
<li>LSTM（引入记忆细胞，更新门，遗忘门，通过门权重求和，控制下一步是否更新）</li>
</ul>
</li>
<li><p>归一化：</p>
<ul>
<li><p>梯度归一化（归一化均值，方差）</p>
</li>
<li><p>梯度裁剪(clipping)：比如大于/小于一个固定的阈值，就让梯度等于这个阈值，将梯度限制在一个范围中。（可以缓解梯度爆炸）</p>
</li>
</ul>
</li>
<li>合理的权重初始和激活函数：本节课讲述重点</li>
</ul>
</li>
</ul>
<p><strong>下面我们重点探讨最后一种方法：合理的权重初始和激活函数</strong></p>
<h4 id="2-2-基本假设：让每层的均值-方差是一个常数"><a href="#2-2-基本假设：让每层的均值-方差是一个常数" class="headerlink" title="2.2 基本假设：让每层的均值/方差是一个常数"></a>2.2 基本假设：让每层的均值/方差是一个常数</h4><ul>
<li><p><strong>将每层的输出和梯度都看做随机变量</strong></p>
<p>比如第i层有100维，就将输出和梯度分别看成100个随机变量</p>
</li>
<li><p><strong>让它们的均值和方差都保持一致</strong></p>
<p>我们的目标，这样不管神经网络多深，最后一层总与第一层差不多，从而不会梯度爆炸和消失</p>
</li>
</ul>
<p>根据我们的假设，可以列出如下方程式：</p>
<img src="/2024/04/23/11-01-14/14-07.png" class>
<h4 id="2-3-权重初始化"><a href="#2-3-权重初始化" class="headerlink" title="2.3 权重初始化"></a>2.3 权重初始化</h4><ul>
<li>在合理值区间里随机初始参数</li>
<li>训练<strong>开始</strong>的时候更容易有数值不稳定<ul>
<li>远离最优解的地方损失函数表面可能很复杂</li>
<li>最优解附近表面会比较平</li>
</ul>
</li>
<li>使用N(0, 0.01)分布来初始可能对小网络没问题，但不能保证深度神经网络</li>
</ul>
<h4 id="2-4-例子：MLP"><a href="#2-4-例子：MLP" class="headerlink" title="2.4 例子：MLP"></a>2.4 例子：MLP</h4><p>下面我们以MLP为例，考虑需要什么条件，才能满足<a href="#22-基本假设：让每层的均值/方差是一个常数">2.2节</a>的假设。</p>
<h5 id="2-4-1-模型假设"><a href="#2-4-1-模型假设" class="headerlink" title="2.4.1 模型假设"></a>2.4.1 模型假设</h5><ul>
<li>每一层<strong>权重</strong>中的变量均为<strong>独立同分布</strong>，并设出均值、方差。</li>
<li>每一层<strong>输入</strong>的变量<strong>独立于</strong>该层<strong>权重</strong>变量。同时<strong>输入变量</strong>之间<strong>独立同分布</strong>。</li>
<li>假设没有激活函数(先简化分析，之后会考虑有激活函数的情况)，可以求得该层输出的期望为0。</li>
</ul>
<img src="/2024/04/23/11-01-14/14-08.png" class>
<p>此处用到了一个重要性质：</p>
<img src="/2024/04/23/11-01-14/14-09.png" class>
<p>更多均值、方差运算可以参考<a href="https://blog.csdn.net/MissXy_/article/details/80705828">期望、方差、协方差及相关系数的基本运算</a></p>
<h5 id="2-4-2-正向方差"><a href="#2-4-2-正向方差" class="headerlink" title="2.4.2 正向方差"></a>2.4.2 正向方差</h5><img src="/2024/04/23/11-01-14/14-10.png" class>
<ul>
<li><p>第二行的计算中仍然用到了<a href="241模型假设">2.4.1节</a>的期望的重要性质：如果两个变量独立，它们乘积的均值=均值的乘积，再结合w的期望为0(注意w和h独立，w之间独立同分布)，即有第二行末项期望为0。</p>
</li>
<li><p>最后一行由于wi,j独立同分布，方差相同，加上做了hj独立同分布的假设，所以可以写成 <strong>[t-1层输出维度] x [t层权重方差] x [t-1层输出方差]</strong> 的形式</p>
</li>
<li><p>此时，我们回过头来看我们的终极目标<a href="#22-基本假设：让每层的均值/方差是一个常数">2.2节</a>的假设，每层输出期望为0我们已经可以满足(2.4.1节已经推导出)，而方差相同这一目标，通过上图的推导，我们发现需要&lt;$n_{t-1}\gamma _{t}=1$。</p>
</li>
</ul>
<h5 id="2-4-3-反向均值和方差"><a href="#2-4-3-反向均值和方差" class="headerlink" title="2.4.3 反向均值和方差"></a>2.4.3 反向均值和方差</h5><img src="/2024/04/23/11-01-14/14-11.png" class>
<p>反向的情况和正向的类似，不过此时我们需要满足的式子变为$n_{t}\gamma _{t}=1$。</p>
<h5 id="2-4-4-Xavier初始"><a href="#2-4-4-Xavier初始" class="headerlink" title="2.4.4 Xavier初始"></a>2.4.4 Xavier初始</h5><ul>
<li><p>上述推导带来的问题：难以同时满足$n_{t-1}\gamma _{t}=1$和$n_{t}\gamma _{t}=1$。（需要每层输出的维度都相同）</p>
</li>
<li><p>采用Xavier折中解决，不能同时满足上面两式，转而满足 [<strong>上面两式做加法后除以2</strong>] 得到的式子，用两种分布进行初始化（每层方差、均值满足推导式）。</p>
<img src="/2024/04/23/11-01-14/14-12.png" class>
</li>
<li><p>如果能确定每层输入、输出维度大小，则能确定该层权重的方差大小。</p>
</li>
<li>权重初始化方式：正态分布、均匀分布，均值/方差满足Xavier的假设。</li>
</ul>
<h5 id="2-4-5-假设线性的激活函数"><a href="#2-4-5-假设线性的激活函数" class="headerlink" title="2.4.5 假设线性的激活函数"></a>2.4.5 假设线性的激活函数</h5><p>真实情况下，我们并不会用线性的激活函数（这样相当于没有进行激活），这里为了简化问题，假设激活函数是线性的。</p>
<ul>
<li><strong>正向</strong></li>
</ul>
<img src="/2024/04/23/11-01-14/14-13.png" class>
<p>上述推导表明，为了使得前向传播的均值为0，方差固定的话，激活函数必须f(x)=x，这种恒等映射。</p>
<ul>
<li><strong>反向</strong></li>
</ul>
<img src="/2024/04/23/11-01-14/14-14.png" class>
<p>PPT上的推导似乎有点问题（上图中第二行方程），笔者重新进行了下述推导，读者也可自行推导验证：</p>
<img src="/2024/04/23/11-01-14/14-15.png" class>
<p><strong>通过正向和反向的推导，我们可以得出的【结论】是：当激活函数为f(x)=x，这种恒等映射更有利于维持神经网络的稳定性。</strong></p>
<h5 id="2-4-6-检查常用激活函数"><a href="#2-4-6-检查常用激活函数" class="headerlink" title="2.4.6 检查常用激活函数"></a>2.4.6 检查常用激活函数</h5><img src="/2024/04/23/11-01-14/14-16.png" class>
<p>对于常用激活函数：tanh，relu满足在零点附近有f(x)=x，而sigmoid函数在零点附近不满足要求，可以对sigmoid函数进行调整（根据Taylor展开式，调整其过原点）</p>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><ul>
<li>当数值过大或者过小时，会导致数值问题。</li>
<li><p>常发生在深度模型中，因为其会对n个数累乘。</p>
</li>
<li><p>合理的权重初始值(如Xavier)和激活函数的选取(如relu, tanh, 调整后的sigmoid)可以提升数值稳定性。</p>
</li>
</ul>
<h3 id="4-Q-amp-A"><a href="#4-Q-amp-A" class="headerlink" title="4.Q&amp;A"></a>4.Q&amp;A</h3><p><strong>问题：nan, inf是怎么产生的以及怎么解决的？</strong></p>
<blockquote>
<p>NaN和Inf怎么产生的：参考<a href="https://blog.csdn.net/qq_16334327/article/details/86526854">出现nan、inf原因</a></p>
<p>如何解决：参考<a href="https://blog.csdn.net/u011119817/article/details/103908065">深度学习中nan和inf的解决</a>以及<a href="https://zhuanlan.zhihu.com/p/89588946#:~:text=一般来说，出现NaN有以下几种情况： 1.,如果在迭代的100轮以内，出现NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。 可以不断降低学习率直至不出现NaN为止，一般来说低于现有学习率1-10倍即可。">训练网络loss出现Nan解决办法 </a></p>
</blockquote>
<p><strong>问题：训练过程中，如果网络层的输出的中间层特征元素的值突然变成nan了，是发生梯度爆炸了吗？</strong></p>
<blockquote>
<p>参考<a href="https://zhuanlan.zhihu.com/p/89588946#:~:text=一般来说，出现NaN有以下几种情况： 1.,如果在迭代的100轮以内，出现NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。 可以不断降低学习率直至不出现NaN为止，一般来说低于现有学习率1-10倍即可。">训练网络loss出现Nan解决办法 </a></p>
</blockquote>
<p><strong>问题：老师，让每层方差是一个常数的方法，您指的是batch normalization吗？想问一下bn层为什么要有伽马和贝塔？去掉可以吗</strong></p>
<blockquote>
<p>让每层方差是一个常数，和batch norm没有太多关系，(本节课介绍的方法是合理地初始化权重和设置激活函数)。batch norm可以让你的输出变成一个均值为0，方差差不多是一个固定值的东西，但它不一定能保证你的梯度。</p>
</blockquote>
<p>(此处节选几个重要的Q&amp;A，建议观看完整Q&amp;A，获得更深的理解)</p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>13-丢弃法</title>
    <url>/2024/04/23/11-01-13/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="13-丢弃法"><a href="#13-丢弃法" class="headerlink" title="13-丢弃法"></a>13-丢弃法</h2><h3 id="1-丢弃法动机、实现及原则"><a href="#1-丢弃法动机、实现及原则" class="headerlink" title="1.丢弃法动机、实现及原则"></a>1.丢弃法动机、实现及原则</h3><h4 id="1-1动机"><a href="#1-1动机" class="headerlink" title="1.1动机"></a>1.1动机</h4><ul>
<li>一个好的模型需要对输入数据的扰动鲁棒（健壮性）</li>
</ul>
<h4 id><a href="#" class="headerlink" title=" "></a> </h4><img src="/2024/04/23/11-01-13/13-02.jpg" class>       
<img src="/2024/04/23/11-01-13/13-03.jpg" class>
<h4 id="1-2如何实现模型的这一能力"><a href="#1-2如何实现模型的这一能力" class="headerlink" title="1.2如何实现模型的这一能力"></a>1.2如何实现模型的这一能力</h4><ul>
<li>使用有噪音的数据。</li>
<li>丢弃法：在层之间加入噪音。</li>
</ul>
<h4 id="1-3加入噪音的原则"><a href="#1-3加入噪音的原则" class="headerlink" title="1.3加入噪音的原则"></a>1.3加入噪音的原则</h4><img src="/2024/04/23/11-01-13/13-01.png" class>
<ul>
<li>例如模型的功能是识别猫猫，加入噪音可以是输入模糊的猫猫图片，但尽量不要是狗狗的图片。</li>
</ul>
<h3 id="2-丢弃法内容"><a href="#2-丢弃法内容" class="headerlink" title="2.丢弃法内容"></a>2.丢弃法内容</h3><ul>
<li>丢弃法对每个元素作如下扰动</li>
</ul>
<img src="/2024/04/23/11-01-13/13-04.png" class>
<ul>
<li>能够满足加入噪音的期望相同原则</li>
<li>一定概率变为0，一定概率变得很大</li>
</ul>
<img src="/2024/04/23/11-01-13/13-05.png" class>
<ul>
<li>期望没有发生变化，分母的意义</li>
</ul>
<h3 id="3-丢弃法使用"><a href="#3-丢弃法使用" class="headerlink" title="3.丢弃法使用"></a>3.丢弃法使用</h3><h4 id="3-1丢弃法的使用位置"><a href="#3-1丢弃法的使用位置" class="headerlink" title="3.1丢弃法的使用位置"></a>3.1丢弃法的使用位置</h4><ul>
<li>通常将丢弃法作用在隐藏全连接层的输出上</li>
</ul>
<img src="/2024/04/23/11-01-13/13-06.png" class>
<ul>
<li>随机选中某些神经元将其输出置位0，因此模型不会过分依赖某些神经元</li>
</ul>
<img src="/2024/04/23/11-01-13/13-07.png" class>
<h4 id="3-2训练中的丢弃法"><a href="#3-2训练中的丢弃法" class="headerlink" title="3.2训练中的丢弃法"></a>3.2训练中的丢弃法</h4><ul>
<li>正则项（丢弃法）仅在训练中使用：影响模型参数的更新，预测的时候便不再使用</li>
</ul>
<h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h3><ul>
<li>丢弃法将一些输出项随机置0来控制模型复杂度</li>
<li>常作用在多层感知机的隐藏层输出上</li>
<li>丢弃概率是控制模型复杂度的超参数（常取0.9，0.5，0.1）</li>
</ul>
<h3 id="5-代码部分"><a href="#5-代码部分" class="headerlink" title="5.代码部分"></a>5.代码部分</h3><h4 id="5-1Dropout部分"><a href="#5-1Dropout部分" class="headerlink" title="5.1Dropout部分"></a>5.1Dropout部分</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span> (X,dropout)：   <span class="comment">#X为dropout层的输入，dropout为设置的丢弃概率</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span>&lt;=dropout&lt;=<span class="number">1</span>        <span class="comment">#丢弃概率介于0，1之间</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">       <span class="keyword">return</span> torch.zeros_like(x) <span class="comment">#若丢弃概率为1，则X的全部项均被置0</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">       <span class="keyword">return</span> X                   <span class="comment">#若丢弃概率为0，不对X作丢弃操作，直接返回X</span></span><br><span class="line">    mask=(torch.Tensor(X.shape).uniform_(<span class="number">0</span>,<span class="number">1</span>)&gt;dropout).<span class="built_in">float</span>() <span class="comment">#用uniform函数生成0-1间的随机实数，利用”&gt;&quot;，将大于dropout的记为1，小于dropout的记为0，实现丢弃操作</span></span><br><span class="line">    <span class="keyword">return</span> mask*X/(<span class="number">1</span>-dropout) <span class="comment">#将mask与X相乘实现丢弃操作，并除以(1-dropout)，这里不使用选中X中元素置0的原因是相乘操作相比选中操作更快</span></span><br></pre></td></tr></table></figure>
<h4 id="5-2在神经网络中使用丢弃法"><a href="#5-2在神经网络中使用丢弃法" class="headerlink" title="5.2在神经网络中使用丢弃法"></a>5.2在神经网络中使用丢弃法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_</span>(<span class="params">self,num_inputs,num_outputs,num_outputs,num_hiddens1,num_hiddens2,is_training=<span class="literal">True</span></span>):</span><br><span class="line">       <span class="built_in">super</span>(Net,self)._init_()</span><br><span class="line">       self.num_inputs=num_inputs</span><br><span class="line">       self.training=is_training</span><br><span class="line">       self.lin1=nn.Linear(num_inputs,num_hiddens1)</span><br><span class="line">       self.lin2=nn.Linear(num_hiddens1,num_hiddens2)</span><br><span class="line">       self.lin2=nn.Linear(num_hiddens2,num_outputs)</span><br><span class="line">       self.relu=nn.ReLU()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,X</span>):</span><br><span class="line">       H1=self.relu(self.lin1(X.reshape((-<span class="number">1</span>,self.num_inputs))))</span><br><span class="line">       <span class="keyword">if</span> self.training == <span class="literal">True</span>:  <span class="comment">#丢弃法仅在训练中使用</span></span><br><span class="line">           H1=dropout_layer(H1,dropout1)</span><br><span class="line">       H2=self.relu(self.lin2(H1))</span><br><span class="line">       <span class="keyword">if</span> self.training == <span class="literal">True</span>: <span class="comment">#丢弃法仅在训练中使用</span></span><br><span class="line">           H2=dropout_layer(H2,dropout2)</span><br><span class="line">       out=self.lin3(H2)  <span class="comment">#output层不再使用丢弃法</span></span><br><span class="line">       <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>12 权重衰退 Weight Decay</title>
    <url>/2024/04/23/11-01-12/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="12-权重衰退-Weight-Decay"><a href="#12-权重衰退-Weight-Decay" class="headerlink" title="12 权重衰退 Weight Decay"></a>12 权重衰退 Weight Decay</h2><p>权重衰退是最常见的一种处理过拟合的方法，是最广泛使用的正则化技术之一。</p>
<h4 id="复习：控制模型容量"><a href="#复习：控制模型容量" class="headerlink" title="复习：控制模型容量"></a>复习：控制模型容量</h4><blockquote>
<ol>
<li>使用更少参数</li>
<li>控制每个参数（取值/可选择的值）范围较小</li>
</ol>
</blockquote>
<p>其中权重衰退属于第二种方法。</p>
<h3 id="1-硬性限制-直观理解"><a href="#1-硬性限制-直观理解" class="headerlink" title="1. 硬性限制/直观理解"></a>1. 硬性限制/直观理解</h3><p>我们的优化目标仍然是$min\space\ell(\boldsymbol{w},b)$，只是额外对$\boldsymbol{w}$添加一个限制条件$||\boldsymbol{w}||^2\leqslant\theta$，即权重的各项平方和小于一个特定的常数$\theta$。那么设定一个较小的$\theta$就会使得$\boldsymbol{w}$中每个元素的值都不会太大。</p>
<p>通常不会限制偏移b，理论上讲b表示整个数据在零点上的偏移，因此是不应该限制的，但实践中限制与否对结果都没什么影响。</p>
<p><strong>吴恩达课程中对这一现象的解释是w是高维向量，已经包含了绝大多数参数足以表达高方差问题，b作为单个数字对结果的影响就会很小.</strong></p>
<p>小的$\theta$意味着更强的正则项，对于相同的$\theta$，$\boldsymbol{w}$中元素越多则单个元素的值会越小。</p>
<h3 id="2-柔性限制-实际应用"><a href="#2-柔性限制-实际应用" class="headerlink" title="2. 柔性限制/实际应用"></a>2. 柔性限制/实际应用</h3><p>上文说的硬性限制在实际使用时比较麻烦，实际上常用的函数是</p>
<p>$$<br>\begin{split}<br>min\space\ell(\boldsymbol{w},b)+\frac{\lambda}{2}||\boldsymbol{w}||^2<br>\end{split}<br>$$</p>
<p>可以通过拉格朗日乘子证明对于每个$\theta$都可以找到$\lambda$使得硬性限制的目标函数等价于上式。</p>
<p>其中$\frac{\lambda}{2}||\boldsymbol{w}||^2$这一项被称为罚(penalty)，$\lambda$是超参数，控制了正则项的重要程度。</p>
<p>当 $\lambda=0$ 时无作用，$\lambda\rightarrow\infty$ 时最优解 $\boldsymbol{w}^*\rightarrow0$，也就是说 $\lambda$ 越大模型复杂度就被控制的越低。</p>
<p>下面是老师给出的演示图</p>
<img src="/2024/04/23/11-01-12/12-01.JPG" class>
<p>以$\boldsymbol{w}$中只有两个参数为例，其中绿色的部分是原本损失函数函数值的“等高线”，黄色部分可以看作是正则项对应函数值的“等高线” ，使用权重衰减后需要优化的损失函数相当于图中两组等高线叠加。原本最优解位于绿色中心，现在这一位置在对于正则项有很高的损失，而正则项最小值位于原点，因此现在的最终优化解会更靠近原点，而当所有参数都更靠近原点时模型的规模也就更小。</p>
<h3 id="3-参数更新"><a href="#3-参数更新" class="headerlink" title="3. 参数更新"></a>3. 参数更新</h3><h4 id="3-1-计算梯度"><a href="#3-1-计算梯度" class="headerlink" title="3.1 计算梯度"></a>3.1 计算梯度</h4><p>$$<br>\begin{split}<br>\frac{\partial{} }{\partial{\boldsymbol{w} } }(\ell(\boldsymbol{w},b)+\frac{\lambda}{2}||\boldsymbol{w}||^2)=\frac{\partial{\ell(\boldsymbol{w},b)} }{ {\partial{\boldsymbol{w} } } }+\lambda\boldsymbol{w}<br>\end{split}<br>$$</p>
<h4 id="3-2-更新参数"><a href="#3-2-更新参数" class="headerlink" title="3.2 更新参数"></a>3.2 更新参数</h4><p>将上式结果带入更新参数公式整理可得</p>
<p>$$<br>\begin{split}<br>\boldsymbol{w}_{t+1}=(1-\eta\lambda)\boldsymbol{w}_{t}-\eta\frac{\partial{\ell(\boldsymbol{w}_t,b_t)} }{ {\partial{\boldsymbol{w}_{t} } } }<br>\end{split}<br>$$</p>
<p>注意到这个公式中后一项与原来更新参数的公式没有区别，仅仅是在前一项$\boldsymbol{w}_{t}$ 上加了一个系数$(1-\eta\lambda)$。通常$\eta\lambda&lt;1$ ，也就是说由于引入了$\lambda$，每次更新参数前先给待更新参数乘上一个小于1的权重再更新，权重衰退由此得名。</p>
<h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h3><ul>
<li>权重衰退通过L2正则项使得模型参数不会过大，从而控制模型复杂度</li>
<li>正则项权重（$\lambda$）是控制模型复杂度的超参数                                                                                                                                       </li>
</ul>
<h3 id="5-Q-amp-A"><a href="#5-Q-amp-A" class="headerlink" title="5. Q&amp;A"></a>5. Q&amp;A</h3><ul>
<li><p>Q：Pytorch是否支持复数神经网络？</p>
</li>
<li><p>A：应该不支持，但复数可以看作是二维的数，可以尝试将对应结构变成二维来实现需要的效果。</p>
</li>
<li><p>Q：为什么参数不过大复杂度就低呢？</p>
</li>
<li><p>A：确切的说是限制模型优化时只能在很小范围内取参数会使模型复杂度降低，见下图</p>
</li>
</ul>
<img src="/2024/04/23/11-01-12/12-02.JPG" class>
<p>参数选择范围大时可拟合出很复杂的曲线，限制后只能学到更平滑的曲线/选择更简单的模型，那么模型复杂度就变低了。</p>
<ul>
<li><p>Q：如果使用L1范数如何更新权重？</p>
</li>
<li><p>A：编写代码时只需把罚项改成如</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l1_penalty</span>(<span class="params">w</span>):</span><br><span class="line">  <span class="keyword">return</span> torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(w))</span><br></pre></td></tr></table></figure>
<p>老师解答就到这里，但实操不应该只改罚项函数，还需重新定义带正则项的损失函数并求导化简。</p>
<div align="center">

$\frac{\partial{} }{\partial{\mathbf{w} } }(\ell(\mathbf{w},b)+\lambda||\mathbf{w}||_1)=\frac{\partial{\ell(\mathbf{w},b)} }{ {\partial{\mathbf{w} } } }+I'\lambda$

</div>

<p>其中$I’=(a_1,…,a_n)$,当$\mathbf{w}$中第i个元素为正时$a_i=1$，反之$a_i=-1$.（=0时随意）</p>
<p>代入公式化简得</p>
<div align="center">

$\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta\frac{\partial{\ell(\mathbf{w}_t,b_t)} }{ {\partial{\mathbf{w}_{t} } } }-I'\eta\lambda$

</div>

<p>从这个式子可以看出使用L1正则化时只能对所有同号的参数施加一个相同大小的正则项（增减一个定值），而反观L2正则化对参数的影响是与参数本身的值有关的（乘上一个系数）似乎是更好的选择。不过L1正则化在特征提取上会有用处。</p>
<ul>
<li><p>Q：实践中权重衰减的值设置为多少好？跑代码时感觉效果不明显。</p>
</li>
<li><p>A：一般取1e-2,1e-3,1e-4，权重衰退的效果确实有限，之后还会讲解更多方法。如果模型真的很复杂那么权重衰退一般不会带来特别好的效果。</p>
</li>
<li><p>Q：关于L2范数的记法</p>
</li>
<li><p>A：完整的写法是$||\boldsymbol{w}||^2_2$，上标的2表示平方，下标的2表示是L2范数，下标有时省略。</p>
</li>
<li><p>Q：为什么要把$\boldsymbol{w}$往小拉？如果最优解的$\boldsymbol{w}$本来就较大权重衰减是否会起反作用？/正则项使得$\boldsymbol{w}$变得更平均没有突出的值为什么可以拟合的更好呢？</p>
</li>
<li><p>A：实际训练的数据都是有噪音的，而这些噪音可能会被拟合进去使得我们实际求解时得不到数学上的最优解，正则化起到将结果拉向最优解的作用。当然如果$\lambda$选取过大可能会拉小的过多，如果没有过拟合那权重衰减就不起作用。</p>
<p><strong>笔者注：这部分老师花了较长时间解释，建议大家自己去看视频。我的个人理解是重点不在于w大小/是否平均，而是由于数据有噪声，而噪声引起过拟合使得求出的w比数学上的最优解更大/更不平均，这时就需要正则化起到一个将结果拉向更小/平均/接近最优解的作用。</strong></p>
</li>
<li><p>Q：噪音大会使得$\boldsymbol{w}$较大是经验所得还是可以证明？</p>
</li>
<li>A：可以证明，但本课程中不讲，可以自己尝试。</li>
<li>Q：怎样调整$\lambda$？</li>
<li>A：不能确定什么时候是最优，但可以用前面讲的验证集/k折交叉验证，先取$\lambda=0$看训练结果，再改变$\lambda$看是否有改善。</li>
</ul>
<p><strong>代码和部分课后题参考答案见本讲的ipynb文件。</strong></p>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>11-模型选择+过拟合和欠拟合</title>
    <url>/2024/04/23/11-01-11/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="11-模型选择-过拟合和欠拟合"><a href="#11-模型选择-过拟合和欠拟合" class="headerlink" title="11-模型选择+过拟合和欠拟合"></a>11-模型选择+过拟合和欠拟合</h2><h3 id="1-模型选择"><a href="#1-模型选择" class="headerlink" title="1. 模型选择"></a>1. 模型选择</h3><p>本小节主要介绍了评估模型的一些指标和方法</p>
<h4 id="1-1-实例分析：预测谁会偿还贷款"><a href="#1-1-实例分析：预测谁会偿还贷款" class="headerlink" title="1.1 实例分析：预测谁会偿还贷款"></a>1.1 实例分析：预测谁会偿还贷款</h4><ul>
<li>银行雇你来调查谁会偿还贷款，你得到了100个申请人的信息，其中五个人在3年内违约了。然后你惊讶的发现，<strong>所有的五个人在面试时都穿了蓝色衬衫</strong>。显然，你的模型也发现了这个强信号，这会有什么问题？</li>
</ul>
<p><strong>答案是，你的模型很有可能会认为所有来面试的人都会穿蓝色衬衫，而这当然是不对的。</strong></p>
<h4 id="1-2-训练误差和泛化误差"><a href="#1-2-训练误差和泛化误差" class="headerlink" title="1.2 训练误差和泛化误差"></a>1.2 训练误差和泛化误差</h4><ul>
<li>训练误差：模型在训练数据上的误差</li>
<li>泛化误差：模型在新数据上的误差</li>
<li><p>例子：根据模考成绩来预测未来考试分数</p>
<ul>
<li>在过去的考试中表现很好（<strong>训练误差</strong>）不代表未来会好（<strong>泛化误差</strong>）</li>
<li>学生A通过背书在模考中拿到很好成绩</li>
<li>学生B知道答案后面的原因</li>
</ul>
</li>
<li><p><strong>其中，泛化误差是我们所最关心的</strong></p>
</li>
</ul>
<h4 id="1-3-验证数据集和测试数据集"><a href="#1-3-验证数据集和测试数据集" class="headerlink" title="1.3 验证数据集和测试数据集"></a>1.3 验证数据集和测试数据集</h4><ul>
<li>验证数据集：一个用来评估模型好坏的数据集<ul>
<li>例如拿出50%的训练数据</li>
<li>不要跟训练数据混在一起（常犯错误）</li>
</ul>
</li>
<li><p>测试数据集：只用一次的数据集。例如：</p>
<ul>
<li>未来的考试</li>
<li>我出价的房子的实际成交价</li>
<li>用在kaggle私有排行榜中的数据集</li>
</ul>
</li>
<li><p><strong>二者最大的区别就是，验证数据集可以那来用很多次，相当于平时的模拟考，而测试数据集则只能用一次来评估模型的性能，相当于最终的考试。</strong></p>
</li>
</ul>
<h4 id="1-4-K-则交叉验证"><a href="#1-4-K-则交叉验证" class="headerlink" title="1.4 K-则交叉验证"></a>1.4 K-则交叉验证</h4><ul>
<li>在没有足够多数据时使用（这是常态）</li>
<li>算法：<ul>
<li>将训练数据分割k块</li>
<li>For i = 1，……，k<ul>
<li>使用第i块作为验证数据集，其余的作为训练数据集</li>
</ul>
</li>
<li>报告k个验证集误差的平均</li>
</ul>
</li>
<li><p>常用：k = 5或10</p>
<blockquote>
<p>随机打散，分割K块，k次计算，第i次，用第i块作为验证集，剩下的作为训练数据集。这样会获得k次误差，算误差平均值。</p>
</blockquote>
</li>
<li>K-则交叉验证的目的是在没有足够多数据使用时评估模型和超参数的性能，也就是说，<strong>K次训练和验证使用的是相同的超参数和模型</strong></li>
</ul>
<h4 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5 总结"></a>1.5 总结</h4><ul>
<li>训练数据集：训练模型参数</li>
<li>验证数据集：选择模型超参数,保留最好的参数</li>
<li>非大数据集上通常使用k-则交叉验证</li>
</ul>
<h3 id="2-过拟合和欠拟合"><a href="#2-过拟合和欠拟合" class="headerlink" title="2. 过拟合和欠拟合"></a>2. 过拟合和欠拟合</h3><h4 id="2-1-什么是过拟合和欠拟合？"><a href="#2-1-什么是过拟合和欠拟合？" class="headerlink" title="2.1 什么是过拟合和欠拟合？"></a>2.1 什么是过拟合和欠拟合？</h4><div class="table-container">
<table>
<thead>
<tr>
<th>模型容量\数据</th>
<th>简单</th>
<th style="text-align:center">复杂</th>
</tr>
</thead>
<tbody>
<tr>
<td>低</td>
<td>正常</td>
<td style="text-align:center">欠拟合</td>
</tr>
<tr>
<td>高</td>
<td>过拟合</td>
<td style="text-align:center">正常</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>tips：模型容量即模型的复杂度，也代表了模型拟合各种函数的能力</li>
</ul>
<h4 id="2-2-模型容量"><a href="#2-2-模型容量" class="headerlink" title="2.2 模型容量"></a>2.2 模型容量</h4><ul>
<li>拟合各种函数的能力</li>
<li>低容量的模型难以拟合训练数据</li>
<li>高容量的模型可以记住所有的训练数据</li>
</ul>
<img src="/2024/04/23/11-01-11/11-01.png" class>
<ul>
<li>显然，模型容量太低或太高都不好。太低（对应第一种）过于简单，模型分类效果差，太高（对应第二种）则过于复杂，把噪声全部都拟合住了，这是我们所不希望的。</li>
</ul>
<h4 id="2-3-模型容量的影响"><a href="#2-3-模型容量的影响" class="headerlink" title="2.3 模型容量的影响"></a>2.3 模型容量的影响</h4><img src="/2024/04/23/11-01-11/11-02.png" class>
<ul>
<li>我们的核心任务就是把泛化误差往下降</li>
</ul>
<h4 id="2-4-估计模型容量"><a href="#2-4-估计模型容量" class="headerlink" title="2.4 估计模型容量"></a>2.4 估计模型容量</h4><ul>
<li>难以在不同的种类算法之间比较<ul>
<li>例如树模型和神经网络</li>
</ul>
</li>
<li>给定一个模型种类，将有两个主要因素<ul>
<li>参数的个数</li>
<li>参数的选择范围</li>
</ul>
</li>
</ul>
<img src="/2024/04/23/11-01-11/11-03.png" class>
<h4 id="2-5-VC维"><a href="#2-5-VC维" class="headerlink" title="2.5 VC维"></a>2.5 VC维</h4><p>VC维是统计学习理论的一个核心思想，这里大致了解就行，因为很难计算之后学习的模型（如CNN,RNN)的VC维，故并不经常用</p>
<ul>
<li>定义：对于一个分类模型，VC维等于一个最大的数据集的大小，不管如何给定标号，都存在一个模型对它进行完美分类。即存在H个样本，模型能把H个样本的2^H种标号方式打散的H的最大值。</li>
<li>例子：线性分类器的VC维<ul>
<li>2维输入的感知机，VC维=3（对于三个点的任意标号都能分类，而任意四个点的样本都存在不能被打散的标号形式个，如之前讲过的XOR）</li>
</ul>
</li>
</ul>
<p>3个点：</p>
<img src="/2024/04/23/11-01-11/11-04.png" class>
<p>4个点：</p>
<img src="/2024/04/23/11-01-11/11-05.png" class>
<ul>
<li>支持N维输入的感知机的VC维是N+1</li>
</ul>
<h4 id="2-6-VC维的用处"><a href="#2-6-VC维的用处" class="headerlink" title="2.6 VC维的用处"></a>2.6 VC维的用处</h4><ul>
<li>提供为什么一个模型好的理论依据<ul>
<li>它可以衡量训练误差和泛化误差之间的间隔</li>
</ul>
</li>
<li>但深度学习中很少使用<ul>
<li>衡量不是很准确</li>
<li>计算深度学习模型的VC维很困难</li>
</ul>
</li>
</ul>
<h4 id="2-7-数据复杂度"><a href="#2-7-数据复杂度" class="headerlink" title="2.7 数据复杂度"></a>2.7 数据复杂度</h4><ul>
<li>多个重要因素<ul>
<li>样本的元素个数</li>
<li>每个样本的元素个数</li>
<li>时间、空间结构</li>
<li>多样性</li>
</ul>
</li>
</ul>
<h4 id="2-8-总结"><a href="#2-8-总结" class="headerlink" title="2.8 总结"></a>2.8 总结</h4><ul>
<li>模型容量需要匹配数据复杂度，否则可能导致欠拟合和过拟合</li>
<li>统计机器学习提供数学工具来衡量模型复杂度</li>
<li>实际中一般考观察训练误差和验证误差</li>
</ul>
<h3 id="3-多项式回归"><a href="#3-多项式回归" class="headerlink" title="3. 多项式回归"></a>3. 多项式回归</h3><ul>
<li>本小节使用多项式回归为例子，在pytorch上展示过拟合和欠拟合的实际表现</li>
</ul>
<h4 id="3-1-导入库"><a href="#3-1-导入库" class="headerlink" title="3.1 导入库"></a>3.1 导入库</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<h4 id="3-2-生成数据集"><a href="#3-2-生成数据集" class="headerlink" title="3.2 生成数据集"></a>3.2 生成数据集</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_degree = <span class="number">20</span>  <span class="comment"># 多项式的最大阶数</span></span><br><span class="line">n_train, n_test = <span class="number">100</span>, <span class="number">100</span>  <span class="comment"># 训练和测试数据集大小</span></span><br><span class="line">true_w = np.zeros(max_degree)  <span class="comment"># 分配大量的空间</span></span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>] = np.array([<span class="number">5</span>, <span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>])<span class="comment">#前五个参数是有用的已知的参数，其他都是0，是不希望被学习的参数</span></span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, <span class="number">1</span>))<span class="comment">#创建特征值</span></span><br><span class="line">np.random.shuffle(features)<span class="comment">#打乱顺序</span></span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape(<span class="number">1</span>, -<span class="number">1</span>))<span class="comment">#通过广播机制得到每个特征值的所有多项式值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i + <span class="number">1</span>)  <span class="comment"># gamma(n)=(n-1)!，除以gamma防止梯度过大</span></span><br><span class="line"><span class="comment"># labels的维度:(n_train+n_test,)</span></span><br><span class="line">labels = np.dot(poly_features, true_w)<span class="comment">#将对应多项式值与其系数相乘</span></span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>, size=labels.shape)<span class="comment">#加上噪声项</span></span><br></pre></td></tr></table></figure>
<h4 id="3-3-NumPyndarray转换为tensor"><a href="#3-3-NumPyndarray转换为tensor" class="headerlink" title="3.3 NumPyndarray转换为tensor"></a>3.3 NumPyndarray转换为tensor</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w, features, poly_features, labels = [torch.tensor(x, dtype=</span><br><span class="line">    torch.float32) <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]]</span><br></pre></td></tr></table></figure>
<h4 id="3-4-对模型进行训练和测试"><a href="#3-4-对模型进行训练和测试" class="headerlink" title="3.4 对模型进行训练和测试"></a>3.4 对模型进行训练和测试</h4><p>首先让我们[实现一个函数来评估模型在给定数据集上的损失]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 损失的总和,样本数量</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)<span class="comment">#预测值</span></span><br><span class="line">        y = y.reshape(out.shape)<span class="comment">#将y维度变为与out一样</span></span><br><span class="line">        l = loss(out, y)<span class="comment">#计算损失</span></span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())<span class="comment">#加入到迭代器中，进入下一个batch</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]<span class="comment">#返回平均损失</span></span><br></pre></td></tr></table></figure>
<p>现在[定义训练函数]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features, test_features, train_labels, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss()<span class="comment">#定义损失</span></span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 不设置偏置，因为我们已经在多项式特征中实现了它（即x^0）</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, <span class="number">1</span>, bias=<span class="literal">False</span>))<span class="comment">#创建模型</span></span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                                batch_size)<span class="comment">#训练集</span></span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                               batch_size, is_train=<span class="literal">False</span>)<span class="comment">#测试集</span></span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.001</span>)<span class="comment">#设置优化器，这里使用SGD</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">1e-3</span>, <span class="number">1e2</span>],</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])<span class="comment">#动画</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)<span class="comment">#训练</span></span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     evaluate_loss(net, test_iter, loss)))<span class="comment">#将当前的训练集和测试集的损失存入animator中，用于绘图</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight.data.numpy())<span class="comment">#打印训练后的参数</span></span><br></pre></td></tr></table></figure>
<h4 id="3-5-三阶多项式函数拟合-正态"><a href="#3-5-三阶多项式函数拟合-正态" class="headerlink" title="3.5 [三阶多项式函数拟合(正态)]"></a>3.5 [<strong>三阶多项式函数拟合(正态)</strong>]</h4><p>我们将首先使用三阶多项式函数，它与数据生成函数的阶数相同。 结果表明，该模型能有效降低训练损失和测试损失。 学习到的模型参数也接近真实值𝑤=[5,1.2,−3.4,5.6]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-11/11-06.png" class>
<h4 id="3-6-线性函数拟合-欠拟合"><a href="#3-6-线性函数拟合-欠拟合" class="headerlink" title="3.6 [线性函数拟合(欠拟合)]"></a>3.6 [<strong>线性函数拟合(欠拟合)</strong>]</h4><p>让我们再看看线性函数拟合，减少该模型的训练损失相对困难。 在最后一个迭代周期完成后，训练损失仍然很高。 当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选择前2个维度，即1和x</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-11/11-07.png" class>
<h4 id="3-7-高阶多项式函数拟合-过拟合"><a href="#3-7-高阶多项式函数拟合-过拟合" class="headerlink" title="3.7 [高阶多项式函数拟合(过拟合)]"></a>3.7 [<strong>高阶多项式函数拟合(过拟合)</strong>]</h4><p>现在，让我们尝试使用一个阶数过高的多项式来训练模型。 在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。 因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。 虽然训练损失可以有效地降低，但测试损失仍然很高。 结果表明，复杂模型对数据造成了过拟合。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从多项式特征中选取所有维度</span></span><br><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-11/11-08.png" class>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>10-多层感知机</title>
    <url>/2024/04/23/11-01-10/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>从现在的观点来看，感知机实际上就是神经网络中的一个神经单元</p>
<img src="/2024/04/23/11-01-10/%E6%84%9F%E7%9F%A5%E6%9C%BA.png" class>
<p>感知机能解决二分类问题，但与线性回归和softmax回归有所区别：线性回归与softmax回归的输出均为实数，softmax回归的输出同时还满足概率公理。</p>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>训练感知机的伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">initialize w = <span class="number">0</span> <span class="keyword">and</span> b = <span class="number">0</span></span><br><span class="line">repeat</span><br><span class="line">    <span class="comment">#此处表达式小于0代表预测结果错误</span></span><br><span class="line">    <span class="keyword">if</span> y_i[&lt;w,x_i&gt;+b] &lt;= <span class="number">0</span> then</span><br><span class="line">        w=w + yixi</span><br><span class="line">        b=b + yi</span><br><span class="line">    end <span class="keyword">if</span></span><br><span class="line">until <span class="built_in">all</span> classified correctly</span><br></pre></td></tr></table></figure>
<p>可以看出这等价于使用如下损失函数的随机梯度下降（batch_size=1）:<br>$$<br>\ell(y, \boldsymbol x,\boldsymbol w)=max(0,-y&lt;\boldsymbol w,\boldsymbol x&gt;)\\<br>=max(0,-y\boldsymbol w^T\boldsymbol x)<br>$$<br>当预测错误时，偏导数为<br>$$<br>\frac{\partial \ell}{\partial \boldsymbol w}=-y\cdot \boldsymbol x<br>$$</p>
<p>注：此处为了方便计算，将偏置项b归入w中的最后一维，并在特征x中相应的最后一维加入常数1</p>
<h4 id="收敛定理"><a href="#收敛定理" class="headerlink" title="收敛定理"></a>收敛定理</h4><p>设数据在特征空间能被半径为r的圆（球）覆盖，并且分类时有余量（即$\sigma$函数的输入不会取使输出模棱两可的值）$y(\boldsymbol x^T\boldsymbol w)\geq \rho$，若初始参数满足$\|\boldsymbol w\|^2+b^2 \leq 1$，则感知机保证在$\frac{r^2+1}{\rho ^2}$步内收敛</p>
<p><a href="https://zhuanlan.zhihu.com/p/46762820">收敛性的证明</a></p>
<h3 id="线性模型的缺陷"><a href="#线性模型的缺陷" class="headerlink" title="线性模型的缺陷"></a>线性模型的缺陷</h3><p>在前面的课程中我们学习了softmax回归，线性回归，他们有将输入向量与一个权重向量做内积再与一个偏置相加得到一个值的过程：<br>$$<br>O =W^TX+b<br>$$<br>这个过程被称为仿射变换，它是一个带有偏置项的线性变换，它最终产生的模型被称为线性模型，线性模型的特点是只能以线性的方式对特征空间进行划分：</p>
<img src="/2024/04/23/11-01-10/%E7%BA%BF%E6%80%A7%E5%88%92%E5%88%86.png" class>
<p>然而，这种线性划分依赖于线性假设，是非常不可靠的</p>
<ul>
<li>线性假设意味着单调假设，这是不可靠的：<ul>
<li>对于人体的体温与健康情况的建模，人体在37℃时最为健康，过小过大均有风险，然而这不是单调的</li>
</ul>
</li>
<li>线性假设意味着特征与预测存在线性相关性，这也是不可靠的：<ul>
<li>如果预测一个人偿还债务的可能性，那这个人的资产从0万元增至5万元和从100万元增至105万元对应的偿还债务的可能性的增幅肯定是不相等的，也就是不线性相关的</li>
</ul>
</li>
<li>线性模型的评估标准是有位置依赖性的，这是不可靠的：<ul>
<li>如果需要判断图片中的动物是猫还是狗，对于图片中一个像素的权重的改变永远是不可靠的，因为如果将图片翻转，它的类别不会改变，但是线性模型不具备这种性质，像素的权重将会失效</li>
</ul>
</li>
</ul>
<p>课程中所提到的例子是XOR问题，即希望模型能预测出XOR分类（分割图片中的一三象限与二四象限）：</p>
<img src="/2024/04/23/11-01-10/XOR%E9%97%AE%E9%A2%98.png" class>
<h3 id="多层感知机-1"><a href="#多层感知机-1" class="headerlink" title="多层感知机"></a>多层感知机</h3><h4 id="XOR问题的多层次解决"><a href="#XOR问题的多层次解决" class="headerlink" title="XOR问题的多层次解决"></a>XOR问题的多层次解决</h4><p>仍以XOR问题为例，XOR问题的一个解决思路是分类两次，先按x轴分类为+和-，再按y轴分类为+和-，最后将两个分类结果相乘，+即为一三象限，-即为二四象限：</p>
<img src="/2024/04/23/11-01-10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR1.png" class>
<img src="/2024/04/23/11-01-10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR2.png" class>
<p>这实际上将信息进行了多层次的传递：</p>
<img src="/2024/04/23/11-01-10/XOR%E4%BF%A1%E6%81%AF%E5%A4%9A%E5%B1%82%E6%AC%A1%E4%BC%A0%E9%80%92.png" class>
<p>其中蓝色为按X坐标的正负进行的分类，橙色为按Y坐标的正负进行的分类，灰色为将二者信息的综合，这就实现了用多层次的线性模型对非线性进行预测</p>
<h4 id="多层感知机-2"><a href="#多层感知机-2" class="headerlink" title="多层感知机"></a>多层感知机</h4><p>有了XOR问题的解决经验，可以想到如果将多个感知机堆叠起来，形成具有多个层次的结构，如图：</p>
<img src="/2024/04/23/11-01-10/%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82.png" class>
<p>这里的模型称为多层感知机，第一层圆圈$x_1,x_2,x_3,x_4$称为输入（实际上他并非感知机），之后的一层称为隐藏层，由5个感知机构成，他们均以前一层的信息作为输入，最后是输出层，以前一层隐藏层的结果作为输入。除了输入的信息和最后一层的感知机以外，其余的层均称为隐藏层，隐藏层的设置为模型一个重要的超参数，这里的模型有一个隐藏层。</p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>但是仅仅有线性变换是不够的，如果我们简单的将多个线性变换按层次叠加，由于线性变换的结果仍为线性变换，所以最终的结果等价于线性变换，与单个感知机并无区别，反而加大了模型，浪费了资源，为了防止这个问题，需要对每个单元（感知机）的输出通过激活函数进行处理再交由下一层的感知机进行运算，这些激活函数就是解决非线性问题的关键。</p>
<p><em>激活函数</em>（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。</p>
<p>主要的激活函数有：</p>
<h5 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h5><p>最受欢迎的激活函数是<em>修正线性单元</em>（Rectified linear<br>unit，<em>ReLU</em>），因为它实现简单，同时在各种预测任务中表现良好。<strong>ReLU提供了一种非常简单的非线性变换</strong>。给定元素$x$，ReLU函数被定义为该元素与$0$的最大值：<br>$$<br>\operatorname{ReLU}(x) = \max(x, 0)<br>$$<br>ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。为了直观感受一下，我们可以画出函数的曲线图。正如从图中所看到，激活函数是分段线性的。使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现的更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题</p>
<h5 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h5><p><strong>对于一个定义域在$\mathbb{R}$中的输入，<em>sigmoid函数</em>将输入变换为区间(0,1)上的输出</strong>。 因此，sigmoid通常称为<em>挤压函数</em>（squashing function）：它将范围$（-\infty, \infty）$中的任意输入压缩到区间（0,1）中的某个值：<br>$$<br>\operatorname{sigmoid}(x) = \frac{1}{1 + e^{-x}}.<br>$$<br>在基于梯度的学习中，sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。当我们想要将输出视作二元分类问题的概率时，sigmoid仍然被广泛用作输出单元上的激活函数（你可以将sigmoid视为softmax的特例）。然而，sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的ReLU所取代。</p>
<h5 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h5><p>与sigmoid函数类似，<strong>tanh(双曲正切)函数也能将其输入压缩转换到区间(-1,1)上</strong>。tanh函数的公式如下：<br>$$<br>\operatorname{tanh}(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}<br>$$</p>
<h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p>还可以使用更多隐藏层的感知机和softmax函数解决分类问题</p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><ol>
<li>证明一个仅使用ReLU（或pReLU）的多层感知机构造了一个连续的分段线性函数。</li>
</ol>
<blockquote>
<p>绘制出RELU的图像后，我们可以发现，输出值在经过下一层隐藏层的计算后，如果结果小于等于0，则这个数据被舍弃，结果大于0则被保留，类似一个筛选的过程。相当于上一层的输出经过线性变换后在下一层被筛选，线性变换和上述筛选的过程都是连续的，因此就会产生连续而且分段的结果。</p>
</blockquote>
<ol>
<li>构建多个超参数的搜索方法。</li>
</ol>
<blockquote>
<p>有四种主要的策略可用于搜索最佳配置。</p>
<ul>
<li>试错</li>
<li>网格搜索</li>
<li>随机搜索</li>
<li>贝叶斯优化</li>
</ul>
</blockquote>
<p>详见<a href="https://www.jiqizhixin.com/articles/101401">超参数搜索不够高效？这几大策略了解一下</a></p>
<ol>
<li>权重初始化方法</li>
</ol>
<blockquote>
<ol>
<li>全零初始化：在神经网络中，把w初始化为0是不可以的。这是因为如果把w初始化0，那么每一层的神经元学到的东西都是一样的（输出是一样的），而且在BP的时候，每一层内的神经元也是相同的，因为他们的gradient相同，weight<br>update也相同。</li>
<li>随机初始化</li>
<li>Xavier初始化：保持输入和输出的方差一致（服从相同的分布），这样就避免了所有输出值都趋向于0。</li>
<li>He<br>initialization：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0（x负半轴中是不激活的），所以要保持variance不变，只需要在Xavier的基础上再除以2。</li>
<li>pre-training</li>
</ol>
</blockquote>
<p>详见<a href="https://zhuanlan.zhihu.com/p/72374385">权重/参数初始化</a></p>
<ol>
<li>超参数的调节</li>
</ol>
<blockquote>
<ol>
<li>在mlp中，第一个隐藏的的单元数可能大于输入的个数，每个隐藏层中的单元数由前至后递减，逐渐接近输出的个数。</li>
<li>多数情况下，将mlp的深度设置得较深，而每层的单元数相对较少，这样易于训练，不易过拟合，也利于逐步学习样本特征。</li>
<li>激活函数种类的选择对训练的影响小于其余的因素。</li>
</ol>
</blockquote>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>多层感知机使用隐藏层和激活函数来得到非线性模型</p>
</li>
<li><p>常用激活函数：Sigmoid，Tanh，ReLU</p>
</li>
<li><p>使用softmax进行多分类</p>
</li>
<li><p>隐藏层数、大小为超参数</p>
</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>09-softmax回归</title>
    <url>/2024/04/23/11-01-09/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h1 id="09-softmax回归"><a href="#09-softmax回归" class="headerlink" title="09-softmax回归"></a>09-softmax回归</h1><h3 id="1-回归VS分类："><a href="#1-回归VS分类：" class="headerlink" title="1.回归VS分类："></a>1.回归VS分类：</h3><ul>
<li>回归估计一个连续值</li>
<li>分类预测一个离散类别</li>
</ul>
<img src="/2024/04/23/11-01-09/09-01.png" class>
<h4 id="1-1-从回归到多类分类："><a href="#1-1-从回归到多类分类：" class="headerlink" title="1.1 从回归到多类分类："></a>1.1 从回归到多类分类：</h4><h5 id="回归："><a href="#回归：" class="headerlink" title="回归："></a>回归：</h5><ul>
<li>单连续数值输出</li>
<li>自然区间R</li>
<li>跟真实值的区别作为损失</li>
</ul>
<img src="/2024/04/23/11-01-09/09-02.png" class>
<h5 id="分类："><a href="#分类：" class="headerlink" title="分类："></a>分类：</h5><ul>
<li><p>通常多个输出</p>
</li>
<li><p>输出i是预测为第i类的置信度</p>
</li>
</ul>
<img src="/2024/04/23/11-01-09/09-03.png" class>
<h5 id="均方损失："><a href="#均方损失：" class="headerlink" title="均方损失："></a>均方损失：</h5><ul>
<li><p>对类别进行一位有效编码</p>
<p>$y=[y_{1},y_{2},…,y_{n}]^{T}$<br>$y_{i}=\begin{cases}<br>1&amp;i=y\\<br>2&amp;otherwise<br>\end{cases}$</p>
</li>
<li><p>使用均方损失训练</p>
</li>
<li><p>最大值为预测<br>$<br>\hat{y}=\underset {i}{argmax}\quad o^{i}<br>$</p>
</li>
</ul>
<h5 id="无校验比例"><a href="#无校验比例" class="headerlink" title="无校验比例"></a>无校验比例</h5><ul>
<li><p>对类别进行一位有效编码</p>
</li>
<li><p>最大值为预测<br>$<br>\hat{y}=\underset {i}{argmax}\quad o^{i}<br>$</p>
</li>
<li><p>需要更置信的识别正确类（大余量）<br>$<br>o_y-o_i\geq\Delta(y,i)<br>$</p>
</li>
</ul>
<h5 id="校验比例"><a href="#校验比例" class="headerlink" title="校验比例"></a>校验比例</h5><ul>
<li><p>输出匹配概率（$\hat{y}$非负，和为1）<br>$<br>\hat{y}=softmax(o)<br>$</p>
<p>$<br>\hat{y_i}=\frac{exp(o_i)}{\sum_{k} exp(o_k)}<br>$</p>
</li>
</ul>
<ul>
<li>概率$y$和$\hat{y}$的区别作为损失</li>
</ul>
<h4 id="1-2-Softmax和交叉熵损失"><a href="#1-2-Softmax和交叉熵损失" class="headerlink" title="1.2 Softmax和交叉熵损失"></a>1.2 Softmax和交叉熵损失</h4><ul>
<li><p>交叉熵用来衡量两个概率的区别$H(p,q)=\sum_{i} -p_{i}log(q_i)$</p>
</li>
<li><p>将它作为损失<br>$<br>l(y,\hat{y})=-\sum_{i}y_{i}log\hat{y_{i}}=-log\hat{y_y}<br>$</p>
<blockquote>
<p>推导过程？</p>
</blockquote>
</li>
<li>其梯度是真实概率和预测概率的区别<br>$<br>\partial_{o_{i}}l(y,\hat{y})=softmax(o)_{i}-y_{i}<br>$</li>
</ul>
<blockquote>
<p>Softmax回归是一个多类分类模型</p>
<p>使用Softmax操作子得到每个类的预测置信度</p>
<p>使用交叉熵来衡量和预测标号的区别</p>
</blockquote>
<h3 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2.损失函数"></a>2.损失函数</h3><img src="/2024/04/23/11-01-09/09-04.png" class>
<h4 id="2-1-L2-Loss-均方损失"><a href="#2-1-L2-Loss-均方损失" class="headerlink" title="2.1 L2 Loss  均方损失"></a>2.1 L2 Loss  均方损失</h4><p>$<br>l(y,y^{‘})=\frac{1}{2}(y-y^{‘})^2<br>$</p>
<img src="/2024/04/23/11-01-09/09-05.png" class>
<blockquote>
<p>蓝色：y=0<br>绿色似然函数,高斯分布<br>损失函数的梯度，一次函数<br>梯度会随着结果逼近而下降</p>
</blockquote>
<h4 id="2-2-L1-Loss-绝对值损失函数"><a href="#2-2-L1-Loss-绝对值损失函数" class="headerlink" title="2.2 L1 Loss  绝对值损失函数"></a>2.2 L1 Loss  绝对值损失函数</h4><p>$<br>l(y,y^{‘})=\lvert y-y^{‘}\rvert<br>$</p>
<img src="/2024/04/23/11-01-09/09-06.png" class>
<blockquote>
<p>梯度保持不变，但在0处梯度随机</p>
</blockquote>
<h4 id="2-3Huber’s-Robust-Loss-鲁棒损失"><a href="#2-3Huber’s-Robust-Loss-鲁棒损失" class="headerlink" title="2.3Huber’s Robust Loss   鲁棒损失"></a>2.3Huber’s Robust Loss   鲁棒损失</h4><img src="/2024/04/23/11-01-09/09-07.png" class>
<blockquote>
<p>结合L1 Loss和L2 Loss的优点</p>
</blockquote>
<h3 id="3-图片分类数据集"><a href="#3-图片分类数据集" class="headerlink" title="3.图片分类数据集"></a>3.图片分类数据集</h3><h4 id="3-1-Fashion-MNIST数据集："><a href="#3-1-Fashion-MNIST数据集：" class="headerlink" title="3.1 Fashion-MNIST数据集："></a>3.1 Fashion-MNIST数据集：</h4><ul>
<li><p>读取数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，</span></span><br><span class="line"><span class="comment"># 并除以255使得所有像素的数值均在0～1之间</span></span><br><span class="line"><span class="comment"># 转换为张量形式 trans</span></span><br><span class="line"><span class="comment"># 读取为训练集，测试集，读取为张量形式还不是图片形式</span></span><br><span class="line">trans=transforms.ToTensor()</span><br><span class="line">mnist_train=torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">True</span>,                                              transform=trans,download=<span class="literal">True</span>)</span><br><span class="line">mnist_test=torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,                                             transform=trans,download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据集内图片大小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第一张图片的shape</span></span><br><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure>
<p>表示图片为单通道（黑白）的28X28的图片</p>
</li>
<li><p>两个可视化函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):  </span><br><span class="line">  <span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot;</span></span><br><span class="line">  text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">  <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):  </span><br><span class="line">  <span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line">  figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">  _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">  axes = axes.flatten()</span><br><span class="line">  <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">      <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">          <span class="comment"># 图片张量</span></span><br><span class="line">          ax.imshow(img.numpy())</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># PIL图片</span></span><br><span class="line">          ax.imshow(img)</span><br><span class="line">      ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">      ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">      <span class="keyword">if</span> titles:</span><br><span class="line">          ax.set_title(titles[i])</span><br><span class="line">  <span class="keyword">return</span> axes</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>显示数据集图像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 画2行每行9个，titles拿出</span><br><span class="line">X,y = next(iter(data.DataLoader(mnist_train,batch_size=18)))</span><br><span class="line">show_images(X.reshape(18,28,28),2,9,titles=get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-09/09-08.png" class>
</li>
</ul>
<ul>
<li>定义load_data_fashion_mnist函数<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):  </span><br><span class="line">  <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">  trans = [transforms.ToTensor()]</span><br><span class="line">  <span class="keyword">if</span> resize:</span><br><span class="line">      trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">  trans = transforms.Compose(trans)</span><br><span class="line">  mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">      root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">  mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">      root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                          num_workers=get_dataloader_workers()),</span><br><span class="line">          data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                          num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>我们通过指定resize参数来测试load_data_fashion_mnist函数的图像大小调整功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iter, test_iter = load_data_fashion_mnist(<span class="number">32</span>, resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">  <span class="built_in">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class="line">  <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</blockquote>
</li>
</ul>
<h3 id="4-从零实现softmax回归"><a href="#4-从零实现softmax回归" class="headerlink" title="4.从零实现softmax回归"></a>4.从零实现softmax回归</h3><h4 id="softmax"><a href="#softmax" class="headerlink" title="softmax:"></a>softmax:</h4><p>$$<br>softmax(X)_{ij}=\frac{exp(X_{ij})}{\sum_{k} exp(X_{ik})}<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="comment"># 求每一个元素的指数计算</span></span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    <span class="comment"># 按行求和</span></span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition</span><br></pre></td></tr></table></figure>
<ol>
<li><p>将图像展平，每个图像看做长度为784的向量，因为数据集有十个类别，所以网络输出维度为10。以此设定参数大小并初始化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 图片是一个三维的，拉长为一个向量,softmax回归输入为向量</span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"><span class="comment"># 用高斯回归初始权重，均值0，方差0.01，形状行列为输入输出个数，需要计算梯度</span></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 输出需要一个偏移</span></span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现softmax回归模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现交叉熵损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算正确率：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>评估net精度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义训练模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br></pre></td></tr></table></figure>
</li>
<li><p>预测：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测标签（定义见第3章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true +<span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<img src="/2024/04/23/11-01-09/09-09.png" class>
<ol>
<li>动画<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>: </span><br><span class="line">  <span class="string">&quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">               fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">               figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">      <span class="comment"># 增量地绘制多条线</span></span><br><span class="line">      <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          legend = []</span><br><span class="line">      d2l.use_svg_display()</span><br><span class="line">      self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">      <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">          self.axes = [self.axes, ]</span><br><span class="line">      <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">      self.config_axes = <span class="keyword">lambda</span>: d2l.set_axes(</span><br><span class="line">          self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">      self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">      <span class="comment"># 向图表中添加多个数据点</span></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">          y = [y]</span><br><span class="line">      n = <span class="built_in">len</span>(y)</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">          x = [x] * n</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">          self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">          self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">      <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">          <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">              self.X[i].append(a)</span><br><span class="line">              self.Y[i].append(b)</span><br><span class="line">      self.axes[<span class="number">0</span>].cla()</span><br><span class="line">      <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">          self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">      self.config_axes()</span><br><span class="line">      display.display(self.fig)</span><br><span class="line">      display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="5-softmax的简洁实现"><a href="#5-softmax的简洁实现" class="headerlink" title="5.softmax的简洁实现"></a>5.softmax的简洁实现</h3><blockquote>
<p>调用torch内的网络层</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#train_iter 返回一个训练集测试集的迭代器</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line">batch_size=<span class="number">256</span></span><br><span class="line">train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">net=nn.Sequential(nn.Flatten(),nn.Linear(<span class="number">784</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight,std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">loss=nn.CrossEntropyLoss()</span><br><span class="line">trainer=torch.optim.SGD(net.parameters(),lr=<span class="number">0.1</span>)</span><br><span class="line">num_epochs=<span class="number">10</span></span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure>
<h3 id="6-softmax回归Q-amp-A"><a href="#6-softmax回归Q-amp-A" class="headerlink" title="6.softmax回归Q&amp;A"></a>6.softmax回归Q&amp;A</h3><p><strong>Q1:softlabel训练策略以及为什么有效？</strong></p>
<blockquote>
<p>softmax用指数很难逼近1，softlabel将正例和负例分别标记为0.9和0.1使结果逼近变得可能，这是一个常用的小技巧。</p>
</blockquote>
<h5 id="Q2-softmax回归和logistic回归？"><a href="#Q2-softmax回归和logistic回归？" class="headerlink" title="Q2:softmax回归和logistic回归？"></a>Q2:softmax回归和logistic回归？</h5><blockquote>
<p>logistic回归为二分类问题，是softmax回归的特例</p>
</blockquote>
<h5 id="Q3-为什么使用交叉熵，而不用相对熵，互信息熵等其他基于信息量的度量？"><a href="#Q3-为什么使用交叉熵，而不用相对熵，互信息熵等其他基于信息量的度量？" class="headerlink" title="Q3:为什么使用交叉熵，而不用相对熵，互信息熵等其他基于信息量的度量？"></a>Q3:为什么使用交叉熵，而不用相对熵，互信息熵等其他基于信息量的度量？</h5><blockquote>
<p>实际上使用哪一种熵的效果区别不大，所以哪种简单就用哪种</p>
</blockquote>
<h5 id="Q4-y-log-hat-y-为什么我们只关心正确类，而不关心不正确的类呢？"><a href="#Q4-y-log-hat-y-为什么我们只关心正确类，而不关心不正确的类呢？" class="headerlink" title="Q4:$y*log\hat{y}$   为什么我们只关心正确类，而不关心不正确的类呢？"></a>Q4:$y*log\hat{y}$   为什么我们只关心正确类，而不关心不正确的类呢？</h5><blockquote>
<p>并不是不关心，而是不正确的的类标号为零，所以算式中不体现，如果使用softlabel策略，就会体现出不正确的类。</p>
</blockquote>
<h5 id="Q5-似然函数曲线是怎么得出来的？有什么参考意义？"><a href="#Q5-似然函数曲线是怎么得出来的？有什么参考意义？" class="headerlink" title="Q5:似然函数曲线是怎么得出来的？有什么参考意义？"></a>Q5:似然函数曲线是怎么得出来的？有什么参考意义？</h5><blockquote>
<p>最小化损失函数也意味着最大化似然函数，似然函数表示统计概率和模型的拟合程度。</p>
</blockquote>
<h5 id="Q6-在多次迭代之后欧如果测试精度出现上升后再下降是过拟合了吗？可以提前终止吗？"><a href="#Q6-在多次迭代之后欧如果测试精度出现上升后再下降是过拟合了吗？可以提前终止吗？" class="headerlink" title="Q6:在多次迭代之后欧如果测试精度出现上升后再下降是过拟合了吗？可以提前终止吗？"></a>Q6:在多次迭代之后欧如果测试精度出现上升后再下降是过拟合了吗？可以提前终止吗？</h5><blockquote>
<p>很有可能是过拟合，可以继续训练来观察是否持续下降</p>
</blockquote>
<h5 id="Q7-cnn网络主要学习到的是纹理还是轮廓还是所有内容的综合？"><a href="#Q7-cnn网络主要学习到的是纹理还是轮廓还是所有内容的综合？" class="headerlink" title="Q7:cnn网络主要学习到的是纹理还是轮廓还是所有内容的综合？"></a>Q7:cnn网络主要学习到的是纹理还是轮廓还是所有内容的综合？</h5><blockquote>
<p>目前认为主要学习到的是纹理信息</p>
</blockquote>
<h5 id="Q8-softmax可解释吗？"><a href="#Q8-softmax可解释吗？" class="headerlink" title="Q8:softmax可解释吗？"></a>Q8:softmax可解释吗？</h5><blockquote>
<p>单纯softmax是可解释的，可以在统计书籍中找到相关的解释。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>08-线性回归+基础优化算法</title>
    <url>/2024/04/23/11-01-08/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="线性回归-基础优化算法"><a href="#线性回归-基础优化算法" class="headerlink" title="线性回归+基础优化算法"></a>线性回归+基础优化算法</h2><h3 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h3><ul>
<li><p>房价预测例子</p>
</li>
<li><p><strong>线性模型</strong></p>
<ul>
<li><p>输入：$x=[x_1,x_2,…,x_n]^T$</p>
</li>
<li><p>线性模型需要确定一个n维权重和一个标量偏差$\omega=[\omega_1,\omega_2,…,\omega_n]^T,b$</p>
</li>
<li><p>输出 ：$y=\omega_1x_1+\omega_2x_2+…+\omega_nx_n+b$，</p>
<p>向量版本的是 $y=&lt;\omega,x&gt;+b$</p>
</li>
<li><p>线性模型可以看作是单层神经网络</p>
<blockquote>
<ul>
<li>神经网络源于神经科学<ul>
<li>最早的神经网络是源自神经科学的，但是时至今日，很多神经网络已经远远高于神经科学，可解释性也不是很强，不必纠结</li>
</ul>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>衡量估计质量</p>
<ul>
<li><p>我们需要估计模型的预估值和真实值之间的差距，例如房屋售价和股价</p>
</li>
<li><p>假设$y$是真实值，$\tilde{y}$是估计值，我们可以比较</p>
<p>$l(y,\tilde{y})=\frac{1}{2}(y-\tilde{y})^2$，这个叫做<strong>平方损失</strong></p>
</li>
</ul>
</li>
<li><p><strong>训练数据</strong></p>
<ul>
<li><p>收集一些数据点来决定参数值（权重$\omega$和偏差$b$），例如6个月内被卖掉的房子。</p>
</li>
<li><p>这被称之为训练数据</p>
</li>
<li><p>通常越多越好。需要注意的是，现实世界的数据都是有限的，但是为了训练出精确的参数往往需要训练数据越多越好，当训练数据不足的时候，我们还需要进行额外处理。</p>
</li>
<li><p>假设我们有n个样本，记为</p>
<p>$X=[x_1,x_2,…,x_n]^T,y=[y_1,y_2,…y_n]^T$</p>
<p>$X$的每一行是一个样本，$y$的每一行是一个输出的实数值。</p>
</li>
</ul>
</li>
<li><p><strong>参数学习</strong></p>
<ul>
<li><p><strong>训练损失</strong>。但我们训练参数的时候，需要定义一个损失函数来衡量参数的好坏，应用前文提过的平方损失有公式：</p>
<p>$l(X,x,\omega,b)=\frac{1}{2n}\sum_{i=1}^n(y_i-{&lt; x_i, w &gt;}-b)^2=\frac{1}{2n}||y-X\omega-b||^2$</p>
</li>
<li><p><strong>最小化损失来学习参数</strong>。训练参数的目的就是使损失函数的值尽可能小（这意味着预估值和真实值更接近）。最后求得的参数值可表示为：</p>
<p>$\omega^*,b^*=argmin_{\omega,b}l(X,x,\omega,b)$</p>
</li>
</ul>
</li>
<li><p><strong>显示解</strong></p>
<ul>
<li><p>线性回归有显示解，即可以直接矩阵数学运算，得到参数w和b的最优解，而不是用梯度下降，牛顿法等参数优化方式一点点逼近最优解。</p>
</li>
<li><p><strong>推导过程</strong>：</p>
<ul>
<li><p>为了方便矩阵表示和计算，将偏差加入权重，$X\gets[X,1],\omega\gets[\omega,b]$</p>
</li>
<li><p>损失函数是凸函数，最优解满足导数为0，可解出显示解</p>
<p>令$\frac{\partial}{\partial\omega} l(X,y,\omega)=0$</p>
<p>有$\frac{1}{n}(y-X\omega)^TX=0$</p>
<p>解得$\omega^*=(X^TX)^{-1}X^Ty$</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>线性回归是对n维输入的加权，外加偏差</li>
<li>使用<strong>平方损失</strong>来衡量预测值和真实值之间的误差</li>
<li><strong>线性回归有显示解</strong></li>
<li>线性回归可以看作单层神经网络</li>
</ul>
</li>
</ul>
<h3 id="2-基础优化算法"><a href="#2-基础优化算法" class="headerlink" title="2.基础优化算法"></a>2.基础优化算法</h3><ul>
<li><p><strong>梯度下降</strong></p>
<ul>
<li>当模型没有显示解的时候，应用梯度下降法逼近最优解。</li>
<li>梯度下降法的具体步骤：<ul>
<li>挑选一个初始值$\omega_0$</li>
<li>重复迭代参数，迭代公式为：$\omega_t=\omega_{t-1}-\lambda\frac{\partial l}{\partial\omega_{t-1} } $<ul>
<li><strong>$-\frac{\partial l}{\partial\omega_{t-1}}$为函数值下降最快的方向，学习率$\lambda$为学习步长。</strong></li>
</ul>
</li>
</ul>
</li>
<li>选择学习率<ul>
<li>学习率$\lambda$为学习步长，代表了沿负梯度方向走了多远，这是超参数（人为指定的的值，不是训练得到的）</li>
<li>学习率不能太大，也不能太小，需要选取适当。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>小批量随机梯度下降</strong></p>
<ul>
<li><p>在整个训练集上算梯度太贵了</p>
<ul>
<li>在实际应用中，很少直接应用梯度下降法，这是因为每次更新都需要计算训练集上所有的样本，耗费时间太长。一个深度神经网络模型，迭代一次可能需要数分钟甚至数小时。</li>
</ul>
</li>
<li><p>为了减少运算代价，我们可以==随机采样==b个样本$i_1,i_2,…,i_b$来近似损失，损失函数为：</p>
<p>​    $\frac{1}{b}\sum_{i\in I_b}l(x_i,y_i,\omega)$ , </p>
<p>其中<strong>b是批量大小(batch size)，也是超参数</strong></p>
</li>
<li><p><strong>选择批量大小</strong></p>
<ul>
<li>b也不能太大：内存消耗增加；浪费计算资源，一个极端的情况是可能会重复选取很多差不多的样本，浪费计算资源</li>
<li>b也不能太小：每次计算量太小，很难以并行，不能最大限度利用GPU资源</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>总结</strong></p>
<ul>
<li>梯度下降通过不断<strong>沿着负梯度方向</strong>更新参数求解</li>
<li>小批量随机梯度下降是深度学习默认的求解算法（简单，稳定）</li>
<li><strong>两个重要的超参数：批量大小（batch size），学习率（lr）</strong></li>
</ul>
</li>
</ul>
<h3 id="3-线性回归的从零开始实现"><a href="#3-线性回归的从零开始实现" class="headerlink" title="3.线性回归的从零开始实现"></a>3.线性回归的从零开始实现</h3><ul>
<li>代码</li>
</ul>
<h3 id="4-新型回归的简洁实现"><a href="#4-新型回归的简洁实现" class="headerlink" title="4.新型回归的简洁实现"></a>4.新型回归的简洁实现</h3><ul>
<li>代码</li>
</ul>
<ul>
<li><strong>1.为什么使用平方损失而不是绝对差值？</strong><ul>
<li>其实差别不大，最开始使用平方损失是因为它可导，现在其实都可以使用。</li>
</ul>
</li>
<li><strong>2.损失为什么要求平均？</strong><ul>
<li>本质上没有关系，但是如果不求平均，梯度的数值会比较大，这时需要学习率除以n。如果不除以n，可能会随着样本数量的增大而让梯度变得很大。</li>
</ul>
</li>
<li><strong>3.不管是梯度下降还是随机梯度下降，怎么找到合适的学习率？</strong><ul>
<li>选择对学习率不敏感的优化方法，比如Adam</li>
<li>合理参数初始化</li>
</ul>
</li>
<li><strong>4.训练过程中，过拟合和欠拟合情况下，学习率和batch_size应该如何调整？</strong><ul>
<li>理论上学习率和batch_size对最后的拟合结果不会有影响</li>
</ul>
</li>
<li><strong>5.深度学习上，设置损失函数的时候，需要考虑正则吗？</strong><ul>
<li>会考虑，但是和损失函数是分开的，深度学习中正则没有太大的用处，有很多其他的技术可以有正则的效果。</li>
</ul>
</li>
<li><strong>6.如果样本大小不是批量数的整数倍，需要随机剔除多余的样本吗？</strong><ul>
<li>就取多余的样本作为一个批次</li>
<li>直接丢弃</li>
<li>从下一个epoch里面补少的样本</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>07-链式法则与自动求导</title>
    <url>/2024/04/23/11-01-07/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="07-链式法则与自动求导"><a href="#07-链式法则与自动求导" class="headerlink" title="07-链式法则与自动求导"></a>07-链式法则与自动求导</h2><h3 id="1-向量链式法则"><a href="#1-向量链式法则" class="headerlink" title="1. 向量链式法则"></a>1. 向量链式法则</h3><ul>
<li><h4 id="1-1-标量链式法则"><a href="#1-1-标量链式法则" class="headerlink" title="1.1 标量链式法则"></a>1.1 标量链式法则</h4></li>
</ul>
<img src="/2024/04/23/11-01-07/image-01.png" class>
<ul>
<li><h4 id="1-2-拓展到向量"><a href="#1-2-拓展到向量" class="headerlink" title="1.2 拓展到向量"></a>1.2 拓展到向量</h4><blockquote>
<p>需要注意维数的变化</p>
<p>下图三种情况分别对应：</p>
<ol>
<li>y为标量，x为向量</li>
<li>y为标量，x为矩阵</li>
<li>y、x为矩阵</li>
</ol>
</blockquote>
</li>
</ul>
<img src="/2024/04/23/11-01-07/image-02.png" class>
<hr>
<h5 id="例1（标量对向量求导）"><a href="#例1（标量对向量求导）" class="headerlink" title="例1（标量对向量求导）"></a>例1（标量对向量求导）</h5><blockquote>
<p>这里应该是用分子布局，所以是X转置</p>
</blockquote>
<p>​                                 <img src="/2024/04/23/11-01-07/image-03.png" class>   </p>
<h5 id="例2（涉及到矩阵的情况）"><a href="#例2（涉及到矩阵的情况）" class="headerlink" title="例2（涉及到矩阵的情况）"></a>例2（涉及到矩阵的情况）</h5><blockquote>
<p>X是mxn的矩阵,w为n维向量，y为m维向量；<br>z对Xw-y做L2 norm,为标量；<br>过程与例一大体一致；</p>
</blockquote>
<p>​                                   <img src="/2024/04/23/11-01-07/image-04.png" class></p>
<hr>
<blockquote>
<p>由于在神经网络动辄几百层，手动进行链式求导是很困难的，因此我们需要借助自动求导</p>
</blockquote>
<hr>
<h3 id="2-自动求导"><a href="#2-自动求导" class="headerlink" title="2. 自动求导"></a>2. 自动求导</h3><ul>
<li><p>含义：计算一个函数在指定值上的导数</p>
</li>
<li><p>自动求导有别于</p>
<ul>
<li><p>符号求导</p>
<img src="/2024/04/23/11-01-07/image-05.png" class>
</li>
<li><p>数值求导</p>
<img src="/2024/04/23/11-01-07/image-06.png" class>
</li>
</ul>
</li>
</ul>
<p>为了更好地理解自动求导，下面引入计算图的概念</p>
<h4 id="2-1-计算图"><a href="#2-1-计算图" class="headerlink" title="2.1 计算图"></a>2.1 计算图</h4><ul>
<li><p>将代码分解成操作子</p>
</li>
<li><p>将计算表示成一个<strong>无环图</strong></p>
<blockquote>
<p>下图自底向上其实就类似于链式求导过程</p>
</blockquote>
</li>
</ul>
<img src="/2024/04/23/11-01-07/image-07.png" class>
<p>​     </p>
<ul>
<li><p>计算图有两种构造方式<br>计算与上图无关</p>
<ul>
<li><p>显示构造</p>
<blockquote>
<p>可以理解为先定义公式再代值</p>
<p>Tensorflow/Theano/MXNet</p>
</blockquote>
<img src="/2024/04/23/11-01-07/image-08.png" class>
</li>
<li><p>隐式构造</p>
<blockquote>
<p>系统将所有的计算记录下来</p>
<p>Pytorch/MXNet</p>
</blockquote>
<img src="/2024/04/23/11-01-07/image-09.png" class>
</li>
</ul>
</li>
</ul>
<h4 id="2-2-自动求导的两种模式"><a href="#2-2-自动求导的两种模式" class="headerlink" title="2.2 自动求导的两种模式"></a>2.2 自动求导的两种模式</h4><ul>
<li><p>正向累积</p>
 <img src="/2024/04/23/11-01-07/image-10.png" class>
</li>
<li><p>反向累积（反向传递back propagation）</p>
<img src="/2024/04/23/11-01-07/image-11.png" class>
</li>
</ul>
<p>​    <strong>反向累积计算过程</strong></p>
<img src="/2024/04/23/11-01-07/image-12.png" class>
<blockquote>
<p>反向累积的正向过程：自底向上，需要存储中间结果</p>
<p>反向累积的反向过程：自顶向下，可以去除不需要的枝（图中的x应为w）</p>
<img src="/2024/04/23/11-01-07/image-13.png" class>
</blockquote>
<h4 id="2-3-复杂度比较"><a href="#2-3-复杂度比较" class="headerlink" title="2.3 复杂度比较"></a>2.3 复杂度比较</h4><ul>
<li><p>反向累积</p>
<ul>
<li>时间复杂度：O(n),n是操作子数<ul>
<li>通常正向和反向的代价类似</li>
</ul>
</li>
<li>空间复杂度：O(n)<ul>
<li>存储正向过程所有的中间结果</li>
</ul>
</li>
</ul>
</li>
<li><p>正向累积</p>
<blockquote>
<p>每次计算一个变量的梯度时都需要将所有节点扫一遍</p>
</blockquote>
<ul>
<li>时间复杂度：O(n)</li>
<li>空间复杂度：O(1)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-代码部分"><a href="#3-代码部分" class="headerlink" title="3. 代码部分"></a>3. 代码部分</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对y = x.Tx关于列向量x求导</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<pre><code>tensor([0., 1., 2., 3.])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#存储梯度</span></span><br><span class="line">x.requires_grad_(<span class="literal">True</span>)<span class="comment">#等价于x = torch.arange(4.0,requires_grad=True)</span></span><br><span class="line">x.grad<span class="comment">#默认值是None</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.dot(x,x)</span><br><span class="line">y</span><br><span class="line"><span class="comment">#PyTorch隐式地构造计算图，grad_fn用于记录梯度计算</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor(14., grad_fn=&lt;DotBackward0&gt;)
</code></pre><p><strong>通过调用反向传播函数来自动计算y关于x每个分量的梯度</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([0., 2., 4., 6.])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad==<span class="number">2</span>*x<span class="comment">#验证</span></span><br></pre></td></tr></table></figure>
<pre><code>tensor([True, True, True, True])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class="line">x.grad.zero_()<span class="comment">#如果没有这一步结果就会加累上之前的梯度值，变为[1,5,9,13]</span></span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([1., 1., 1., 1.])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y=x*x<span class="comment">#哈达玛积，对应元素相乘</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在深度学习中我们一般不计算微分矩阵</span></span><br><span class="line"><span class="comment">#而是计算批量中每个样本单独计算的偏导数之和</span></span><br><span class="line"></span><br><span class="line">y.<span class="built_in">sum</span>().backward()<span class="comment">#等价于y.backword(torch.ones(len(x)))</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([0., 2., 4., 6.])
</code></pre><p><strong>将某些计算移动到记录的计算图之外</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 后可用于用于将神经网络的一些参数固定住</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x*x</span><br><span class="line">u = y.detach()<span class="comment">#把y当作常数</span></span><br><span class="line">z = u*x</span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u</span><br></pre></td></tr></table></figure>
<pre><code>tensor([True, True, True, True])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == <span class="number">2</span>*x</span><br></pre></td></tr></table></figure>
<pre><code>tensor([True, True, True, True])
</code></pre><p><strong>即使构建函数的计算图需要用过Python控制流，仍然可以计算得到的变量的梯度</strong></p>
<p><strong>这也是隐式构造的优势，因为它会存储梯度计算的计算图，再次计算时执行反向过程就可以</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm()&lt;<span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">a = torch.randn(size=(),requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br><span class="line"></span><br><span class="line">a.grad == d / a</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="4-自动求导-Q-amp-A"><a href="#4-自动求导-Q-amp-A" class="headerlink" title="4. 自动求导 Q&amp;A"></a>4. 自动求导 Q&amp;A</h3><p><strong><code>Q1：ppt上隐式构造和显式构造看起来为啥差不多？</code></strong></p>
<blockquote>
<p>显式和隐式的差别其实就是数学上求梯度和python求梯度计算上的差别，不用深究</p>
<p>显式构造就是我们数学上正常求导数的求法，先把所有求导的表达式选出来再代值</p>
</blockquote>
<p><strong><code>Q2:需要正向和反向都算一遍吗？</code></strong></p>
<blockquote>
<p>需要正向先算一遍，自动求导时只进行反向就可以，因为正向的结果已经存储</p>
</blockquote>
<p><strong><code>Q3:为什么PyTorch会默认累积梯度</code></strong></p>
<blockquote>
<p>便于计算大批量；方便进一步设计</p>
</blockquote>
<p><strong><code>Q4:为什么深度学习中一般对标量求导而不是对矩阵或向量求导</code></strong></p>
<blockquote>
<p>loss一般都是标量</p>
</blockquote>
<p><strong><code>Q5:为什么获取.grad前需要backward</code></strong></p>
<blockquote>
<p>相当于告诉程序需要计算梯度，因为计算梯度的代价很大，默认不计算</p>
</blockquote>
<p><strong><code>Q6:pytorch或mxnet框架设计上可以实现矢量的求导吗</code></strong></p>
<blockquote>
<p>可以</p>
</blockquote>
<h3 id="5-练习"><a href="#5-练习" class="headerlink" title="5. 练习"></a>5. 练习</h3><p><strong>1.为什么计算二阶导数比一阶导数的开销要更大？</strong></p>
<p>二阶导数是在一阶导数的基础上进行的，开销自然更大</p>
<p><strong>2.在运行反向传播函数之后，立即再次运行它，看看会发生什么。</strong></p>
<p>“RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.”</p>
<p>说明不能连续两次运行,pytorch使用的是动态计算图,反向传播函数运行一次后计算图就被释放了</p>
<p><strong>只需要在函数接口将参数retain_graph设为True即可</strong></p>
<p>In [51]:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def f(a):</span><br><span class="line">    b = a * 2</span><br><span class="line">    while b.norm()&lt;1000:</span><br><span class="line">        b = b * 2</span><br><span class="line">    if b.sum() &gt; 0:</span><br><span class="line">        c = b</span><br><span class="line">    else:</span><br><span class="line">        c = 100 * b</span><br><span class="line">    return c</span><br><span class="line">a.grad.zero_()</span><br><span class="line">a = torch.randn(size=(),requires_grad=True)#size=0表示a是标量</span><br><span class="line">d = f(a)</span><br><span class="line">#d.backward(retain_graph=True)</span><br><span class="line">#a.grad</span><br><span class="line">d.backward()</span><br><span class="line">a.grad</span><br></pre></td></tr></table></figure>
<p>Out[51]:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor(4096.)</span><br></pre></td></tr></table></figure>
<p><strong>3.在控制流的例子中，我们计算d关于a的导数，如果我们将变量a更改为随机向量或矩阵，会发生什么？此时，计算结果f(a)不再是标量。结果会发生什么？我们如何分析这个结果？</strong></p>
<p>backward函数的机制本身不允许张量对张量求导，如果输入是向量或矩阵，需要将其在各个分量上求和，变为标量；所以还需要传入一个与输入同型的张量</p>
<p>In [53]:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def f(a):</span><br><span class="line">    b = a * 2</span><br><span class="line">    while b.norm()&lt;1000:</span><br><span class="line">        b = b * 2</span><br><span class="line">    if b.sum() &gt; 0:</span><br><span class="line">        c = b</span><br><span class="line">    else:</span><br><span class="line">        c = 100 * b</span><br><span class="line">    return c</span><br><span class="line">a.grad.zero_()</span><br><span class="line">a = torch.randn(10,requires_grad=True)</span><br><span class="line">d = f(a)</span><br><span class="line">#d.backward(retain_graph=True)</span><br><span class="line">#a.grad</span><br><span class="line">#d.backward()#RuntimeError: grad can be implicitly created only for scalar outputs</span><br><span class="line">d.sum().backward()#需要加上.sum()否则会报错 </span><br><span class="line">a.grad</span><br></pre></td></tr></table></figure>
<p>Out[53]:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([51200., 51200., 51200., 51200., 51200., 51200., 51200., 51200., 51200.,</span><br><span class="line">        51200.])</span><br></pre></td></tr></table></figure>
<p><strong>4.重新设计一个求控制流梯度的例子。运行并分析结果。</strong></p>
<p>In [56]:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def h(x):</span><br><span class="line">    y = x * x</span><br><span class="line">    while y.norm() &lt; 2500:</span><br><span class="line">        y = y * 2</span><br><span class="line">    if y.sum() &lt; 0:</span><br><span class="line">        c = 100*y</span><br><span class="line">    else:</span><br><span class="line">        c = y</span><br><span class="line">    return c</span><br><span class="line">x.grad.zero_()</span><br><span class="line">x = torch.randn(size=(),requires_grad=True)</span><br><span class="line">y = h(x)</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<p>Out[56]:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor(-3311.5398)</span><br></pre></td></tr></table></figure>
<p><strong>5.使f(x)=sin(x)，绘制f(x)和df(x)/dx的图像，其中后者不使用f’(x)=\cos(x)。</strong></p>
<p>In [66]:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x = torch.arange(-20,20,0.1,requires_grad=True,dtype=torch.float32)</span><br><span class="line">y = torch.sin(x)</span><br><span class="line">y.sum().backward()</span><br><span class="line">plt.plot(x.detach(),y.detach(),label=&#x27;y=sinx&#x27;)</span><br><span class="line">plt.plot(x.detach(),x.grad,label=&#x27;dy/dx&#x27;)</span><br><span class="line">plt.legend(loc=&#x27;lower right&#x27;)</span><br></pre></td></tr></table></figure>
<p>Out[66]:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;matplotlib.legend.Legend at 0x1d627d00280&gt;</span><br></pre></td></tr></table></figure>
<img src="/2024/04/23/11-01-07/image-14.png" class>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>06-矩阵计算</title>
    <url>/2024/04/23/11-01-06/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="06-矩阵计算"><a href="#06-矩阵计算" class="headerlink" title="06-矩阵计算"></a>06-矩阵计算</h2><h3 id="1-导数的概念及几何意义"><a href="#1-导数的概念及几何意义" class="headerlink" title="1. 导数的概念及几何意义"></a>1. 导数的概念及几何意义</h3><h4 id="1-1-标量导数"><a href="#1-1-标量导数" class="headerlink" title="1.1 标量导数"></a>1.1 标量导数</h4><ul>
<li>导数是切线的斜率</li>
</ul>
<img src="/2024/04/23/11-01-06/06-01.png" class>
<ul>
<li>指向值变化最大的方向</li>
</ul>
<img src="/2024/04/23/11-01-06/06-02.png" class>
<h4 id="1-2-亚导数"><a href="#1-2-亚导数" class="headerlink" title="1.2 亚导数"></a>1.2 亚导数</h4><ul>
<li>将导数拓展到不可微的函数，在不可导的点的导数可以用一个范围内的数表示</li>
</ul>
<img src="/2024/04/23/11-01-06/06-03.png" class>
<h3 id="2-函数与标量，向量，矩阵"><a href="#2-函数与标量，向量，矩阵" class="headerlink" title="2. 函数与标量，向量，矩阵"></a>2. 函数与标量，向量，矩阵</h3><p>该部分结合课程视频和参考文章进行总结（参考了知乎文章：<a href="https://zhuanlan.zhihu.com/p/263777564">矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇） - 知乎 (zhihu.com)</a>）</p>
<ul>
<li>当f，input为不同形式时，f(input)结果的表达形式</li>
</ul>
<h4 id="2-1-f-为是一个标量"><a href="#2-1-f-为是一个标量" class="headerlink" title="2.1 f 为是一个标量"></a>2.1 f 为是一个标量</h4><h5 id="2-1-1-input是一个标量"><a href="#2-1-1-input是一个标量" class="headerlink" title="2.1.1 input是一个标量"></a>2.1.1 input是一个标量</h5><img src="/2024/04/23/11-01-06/06-04.png" class>
<h5 id="2-1-2-input是一个向量"><a href="#2-1-2-input是一个向量" class="headerlink" title="2.1.2 input是一个向量"></a>2.1.2 input是一个向量</h5><img src="/2024/04/23/11-01-06/06-05.png" class>
<img src="/2024/04/23/11-01-06/06-06.png" class>
<h5 id="2-1-3-input是一个矩阵"><a href="#2-1-3-input是一个矩阵" class="headerlink" title="2.1.3 input是一个矩阵"></a>2.1.3 input是一个矩阵</h5><img src="/2024/04/23/11-01-06/06-07.png" class>
<img src="/2024/04/23/11-01-06/06-08.png" class>
<h4 id="2-2-f是一个向量"><a href="#2-2-f是一个向量" class="headerlink" title="2.2 f是一个向量"></a>2.2 f是一个向量</h4><ul>
<li><strong>f</strong>是由若干个f(标量)组成的向量</li>
</ul>
<h5 id="2-2-1-input是一个标量"><a href="#2-2-1-input是一个标量" class="headerlink" title="2.2.1 input是一个标量"></a>2.2.1 input是一个标量</h5><img src="/2024/04/23/11-01-06/06-09.png" class>
<h5 id="2-2-2-input是一个向量"><a href="#2-2-2-input是一个向量" class="headerlink" title="2.2.2 input是一个向量"></a>2.2.2 input是一个向量</h5><img src="/2024/04/23/11-01-06/06-10.png" class>
<img src="/2024/04/23/11-01-06/06-11.png" class>
<h5 id="2-2-3-input是一个矩阵"><a href="#2-2-3-input是一个矩阵" class="headerlink" title="2.2.3 input是一个矩阵"></a>2.2.3 input是一个矩阵</h5><img src="/2024/04/23/11-01-06/06-12.png" class>
<img src="/2024/04/23/11-01-06/06-13.png" class>
<h4 id="2-3-F是一个矩阵"><a href="#2-3-F是一个矩阵" class="headerlink" title="2.3 F是一个矩阵"></a>2.3 F是一个矩阵</h4><ul>
<li><strong>F</strong>是一个由若干<strong>f</strong>组成的一个矩阵</li>
</ul>
<h5 id="2-3-1-input是一个标量"><a href="#2-3-1-input是一个标量" class="headerlink" title="2.3.1 input是一个标量"></a>2.3.1 input是一个标量</h5><img src="/2024/04/23/11-01-06/06-14.png" class>
<h5 id="2-3-2-input是一个向量"><a href="#2-3-2-input是一个向量" class="headerlink" title="2.3.2 input是一个向量"></a>2.3.2 input是一个向量</h5><img src="/2024/04/23/11-01-06/06-15.png" class>
<img src="/2024/04/23/11-01-06/06-16.png" class>
<h5 id="2-3-3-input是一个向量"><a href="#2-3-3-input是一个向量" class="headerlink" title="2.3.3 input是一个向量"></a>2.3.3 input是一个向量</h5><img src="/2024/04/23/11-01-06/06-17.png" class>
<img src="/2024/04/23/11-01-06/06-18.png" class>
<h3 id="3-求导的本质"><a href="#3-求导的本质" class="headerlink" title="3. 求导的本质"></a>3. 求导的本质</h3><img src="/2024/04/23/11-01-06/06-19.png" class>
<p><strong>可以将f对x1，x2，x3的偏导分别求出来，即</strong></p>
<img src="/2024/04/23/11-01-06/06-20.png" class>
<ul>
<li>矩阵求导也是一样的，<strong>本质就是</strong> $function$ 中的<strong>每个</strong>$f$ <strong>分别对变元中的每个元素逐个求偏导，只不过写成了向量、矩阵形式而已。</strong></li>
</ul>
<img src="/2024/04/23/11-01-06/06-21.png" class>
<p>（课上是按行向量展开的）</p>
<img src="/2024/04/23/11-01-06/06-22.png" class>
<p><strong>X为矩阵时</strong>，先把矩阵变元$X$进行<strong>转置</strong>，再对<strong>转置后</strong>的<strong>每个位置</strong>的元素逐个求偏导，结果布局和<strong>转置布局一样</strong>。（课上讲的是这种展开方式）</p>
<img src="/2024/04/23/11-01-06/06-23.png" class>
<ul>
<li>所以，如果 $function$中有 $m$个$f$ (标量)，变元中有 $n$个元素，那么，每个 $f$对变元中的每个元素逐个求偏导后，我们就会产生 $m*n$个结果。</li>
</ul>
<h3 id="4-矩阵求导的布局"><a href="#4-矩阵求导的布局" class="headerlink" title="4. 矩阵求导的布局"></a>4. 矩阵求导的布局</h3><ul>
<li>经过上述对求导本质的推导，关于矩阵求导的问题，实质上就是对求导结果的进一步排布问题<br><strong>对于2.2（f为向量，input也为向量）中的情况，其求导结果有两种排布方式，一种是<code>分子布局</code>，一种是<code>分母布局</code></strong></li>
</ul>
<h5 id="4-1-分子布局"><a href="#4-1-分子布局" class="headerlink" title="4.1 分子布局"></a>4.1 分子布局</h5><p><strong>分子布局</strong>，就是分子是<strong>列向量</strong>形式，分母是<strong>行向量</strong>形式 （课上讲的）</p>
<img src="/2024/04/23/11-01-06/06-24.png" class>
<h5 id="4-2-分母布局"><a href="#4-2-分母布局" class="headerlink" title="4.2 分母布局"></a>4.2 分母布局</h5><p>2.<strong>分母布局</strong>，就是分母是<strong>列向量</strong>形式，分子是<strong>行向量</strong>形式</p>
<img src="/2024/04/23/11-01-06/06-25.png" class>
<p><strong>将求导推广到矩阵，由于矩阵可以看作由多个向量所组成，因此对矩阵的求导可以看作先对每个向量进行求导，然后再增加一个维度存放求导结果。</strong></p>
<ul>
<li>例如当F为矩阵，input为矩阵时，F中的每个元素f(标量）求导后均为一个矩阵（按照课上的展开方式），因此每个<strong>f</strong>（包含多个f（标量））求导后为存放多个矩阵的三维形状，再由于矩阵F由多个<strong>f</strong>组成，因此F求导后为存放多个<strong>f</strong>求导结果的四维形状。<br><strong>对于不同f和input求导后的维度情况总结如下图所示（课程中的截图）</strong></li>
</ul>
<img src="/2024/04/23/11-01-06/06-26.png" class>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>05-线性代数</title>
    <url>/2024/04/23/11-01-05/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="05-线性代数"><a href="#05-线性代数" class="headerlink" title="05-线性代数"></a>05-线性代数</h2><h3 id="1-线性代数基础知识"><a href="#1-线性代数基础知识" class="headerlink" title="1. 线性代数基础知识"></a>1. 线性代数基础知识</h3><p>这部分主要是由标量过渡到向量，再从向量拓展到矩阵操作，重点在于理解矩阵层面上的操作</p>
<h4 id="1-1-标量"><a href="#1-1-标量" class="headerlink" title="1.1 标量"></a>1.1 标量</h4><img src="/2024/04/23/11-01-05/05-01.png" class>
<h4 id="1-2-向量"><a href="#1-2-向量" class="headerlink" title="1.2 向量"></a>1.2 向量</h4><img src="/2024/04/23/11-01-05/05-02.png" class>
<img src="/2024/04/23/11-01-05/05-03.png" class>
<h4 id="1-3-矩阵"><a href="#1-3-矩阵" class="headerlink" title="1.3 矩阵"></a>1.3 矩阵</h4><h5 id="1-3-1-矩阵的操作"><a href="#1-3-1-矩阵的操作" class="headerlink" title="1.3.1 矩阵的操作"></a>1.3.1 矩阵的操作</h5><img src="/2024/04/23/11-01-05/05-04.png" class>
<img src="/2024/04/23/11-01-05/05-05.png" class>
<img src="/2024/04/23/11-01-05/05-06.png" class>
<img src="/2024/04/23/11-01-05/05-07.png" class>
<p>​    (矩阵范数麻烦且不常用，一般用F范数)</p>
<h5 id="1-3-2-特殊矩阵"><a href="#1-3-2-特殊矩阵" class="headerlink" title="1.3.2 特殊矩阵"></a>1.3.2 特殊矩阵</h5><img src="/2024/04/23/11-01-05/05-08.png" class>
<img src="/2024/04/23/11-01-05/05-09.png" class>
<img src="/2024/04/23/11-01-05/05-10.png" class>
<p>​    (深度学习里基本不会涉及到正定、置换矩阵，这里明确个概念就行)</p>
<h5 id="1-3-3-特征向量和特征值"><a href="#1-3-3-特征向量和特征值" class="headerlink" title="1.3.3 特征向量和特征值"></a>1.3.3 特征向量和特征值</h5><ul>
<li><p>数学定义：设A是n阶方阵，如果存在常数<img src="https://images0.cnblogs.com/blog/650633/201407/141700142243782.png" alt="img">及非零n向量x，使得<img src="https://images0.cnblogs.com/blog/650633/201407/141700145536982.png" alt="img">，则称<img src="https://images0.cnblogs.com/blog/650633/201407/141700149439396.png" alt="img">是矩阵A的特征值，x是A属于特征值<img src="https://images0.cnblogs.com/blog/650633/201407/141700153035353.png" alt="img">的特征向量</p>
</li>
<li><p>直观理解：不被矩阵A改变方向的向量x就是A的一个特征向量</p>
<img src="/2024/04/23/11-01-05/05-11.png" class>
</li>
<li><p>矩阵不一定有特征向量，但是对称矩阵总是可以找到特征向量</p>
</li>
</ul>
<h3 id="2-线性代数实现"><a href="#2-线性代数实现" class="headerlink" title="2. 线性代数实现"></a>2. 线性代数实现</h3><p>这部分主要是应用pytorch实现基本矩阵操作，同样由标量过渡到向量最后拓展到矩阵</p>
<h4 id="2-1-标量"><a href="#2-1-标量" class="headerlink" title="2.1 标量"></a>2.1 标量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch    <span class="comment"># 应用pytorch框架</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 标量由只有一个元素的张量表示</span></span><br><span class="line">x = torch.tensor([<span class="number">3.0</span>])     <span class="comment"># 单独一个数字表示标量也可以</span></span><br><span class="line">y = torch.tensor([<span class="number">2.0</span>])     <span class="comment"># 单独一个数字表示标量也可以</span></span><br><span class="line"><span class="built_in">print</span>(x + y)    <span class="comment"># tensor([5.])</span></span><br><span class="line"><span class="built_in">print</span>(x * y)    <span class="comment"># tensor([6.])</span></span><br><span class="line"><span class="built_in">print</span>(x / y)    <span class="comment"># tensor([1.5000])</span></span><br><span class="line"><span class="built_in">print</span>(x ** y)   <span class="comment"># tensor([9.]) 指数运算</span></span><br></pre></td></tr></table></figure>
<h4 id="2-2-向量"><a href="#2-2-向量" class="headerlink" title="2.2 向量"></a>2.2 向量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 向量可以看作是若干标量值组成的列表</span></span><br><span class="line">x = torch.arange(<span class="number">4</span>)     <span class="comment"># tensor([0, 1, 2, 3])</span></span><br><span class="line">                        <span class="comment"># 生成[0, 4)范围内所有整数构成的张量tensor</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">3</span>])             <span class="comment"># tensor(3)</span></span><br><span class="line">                        <span class="comment"># 和列表相似，通过张量的索引访问元素</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x))           <span class="comment"># 4</span></span><br><span class="line">                        <span class="comment"># 获取张量x的长度</span></span><br><span class="line"><span class="built_in">print</span>(x.shape)          <span class="comment"># torch.Size([4])</span></span><br><span class="line">                        <span class="comment"># 获取张量形状，这里x是只有一个轴的张量因此形状只有一个元素</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="2-3-矩阵"><a href="#2-3-矩阵" class="headerlink" title="2.3 矩阵"></a>2.3 矩阵</h4><h5 id="2-3-1-创建"><a href="#2-3-1-创建" class="headerlink" title="2.3.1 创建"></a>2.3.1 创建</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">6</span>)     <span class="comment"># tensor([0, 1, 2, 3, 4, 5])</span></span><br><span class="line">B = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">0</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">C = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">                   [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">                   [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]],</span><br><span class="line">                  [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                   [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]])</span><br><span class="line">D = torch.arange(<span class="number">20</span>, dtype=torch.float32)</span><br></pre></td></tr></table></figure>
<h5 id="2-3-2-转置"><a href="#2-3-2-转置" class="headerlink" title="2.3.2 转置"></a>2.3.2 转置</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">6</span>)     <span class="comment"># tensor([0, 1, 2, 3, 4, 5])</span></span><br><span class="line">A = A.reshape(<span class="number">3</span>,<span class="number">2</span>)      <span class="comment"># tensor([[0, 1],</span></span><br><span class="line">                        <span class="comment">#         [2, 3],</span></span><br><span class="line">                        <span class="comment">#         [4, 5]])</span></span><br><span class="line"></span><br><span class="line">A = A.T                 <span class="comment"># 转置 A.T</span></span><br><span class="line">                        <span class="comment"># tensor([[0, 2, 4],</span></span><br><span class="line">                        <span class="comment">#         [1, 3, 5]])</span></span><br></pre></td></tr></table></figure>
<h5 id="2-3-3-reshape"><a href="#2-3-3-reshape" class="headerlink" title="2.3.3 reshape"></a>2.3.3 reshape</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用reshape方法创建一个形状为3 x 2的矩阵A</span></span><br><span class="line">A = torch.arange(<span class="number">6</span>)     <span class="comment"># tensor([0, 1, 2, 3, 4, 5])</span></span><br><span class="line">A = A.reshape(<span class="number">3</span>,<span class="number">2</span>)      <span class="comment"># tensor([[0, 1],</span></span><br><span class="line">                        <span class="comment">#         [2, 3],</span></span><br><span class="line">                        <span class="comment">#         [4, 5]])</span></span><br></pre></td></tr></table></figure>
<p>tips(确定矩阵shape)：</p>
<p>由外层到内层依次去中括号，并记下去掉中括号后此时元素的个数，任选其中一个元素重复上述去括号的操作直到该元素中无中括号，记下的数字从左到右依次排序中间用x连接即为矩阵shape</p>
<h5 id="2-3-4-clone"><a href="#2-3-4-clone" class="headerlink" title="2.3.4 clone"></a>2.3.4 clone</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32)</span><br><span class="line">A = A.reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">B = A.clone()   <span class="comment"># 通过分配新内存，将A的一个副本分给B，该边B并不影响A的值</span></span><br><span class="line"><span class="built_in">print</span>(B)        <span class="comment"># tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class="line">                <span class="comment">#         [ 4.,  5.,  6.,  7.],</span></span><br><span class="line">                <span class="comment">#         [ 8.,  9., 10., 11.],</span></span><br><span class="line">                <span class="comment">#         [12., 13., 14., 15.],</span></span><br><span class="line">                <span class="comment">#         [16., 17., 18., 19.]])</span></span><br></pre></td></tr></table></figure>
<h5 id="2-3-5-sum"><a href="#2-3-5-sum" class="headerlink" title="2.3.5 sum"></a>2.3.5 sum</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">                   [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">                   [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]],</span><br><span class="line">                  [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                   [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                   [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]])</span><br><span class="line"><span class="built_in">print</span>(A.shape)</span><br><span class="line"><span class="comment"># torch.Size([2, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>())</span><br><span class="line"><span class="comment"># tensor(54)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">0</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[ 1,  2,  3],</span></span><br><span class="line"><span class="string">        [ 5,  6,  7],</span></span><br><span class="line"><span class="string">        [ 9, 10, 11]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[ 1,  2,  3],</span></span><br><span class="line"><span class="string">         [ 5,  6,  7],</span></span><br><span class="line"><span class="string">         [ 9, 10, 11]]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">1</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[12, 15, 18],</span></span><br><span class="line"><span class="string">        [ 3,  3,  3]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[12, 15, 18]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 3,  3,  3]]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">2</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[ 6, 15, 24],</span></span><br><span class="line"><span class="string">        [ 0,  3,  6]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">2</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[ 6],</span></span><br><span class="line"><span class="string">         [15],</span></span><br><span class="line"><span class="string">         [24]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 0],</span></span><br><span class="line"><span class="string">         [ 3],</span></span><br><span class="line"><span class="string">         [ 6]]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>,<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># tensor([15, 18, 21])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>,<span class="number">1</span>], keepdims=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># tensor([[[15, 18, 21]]])</span></span><br></pre></td></tr></table></figure>
<h5 id="2-3-6-numel"><a href="#2-3-6-numel" class="headerlink" title="2.3.6 numel"></a>2.3.6 numel</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.tensor([[<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">0.</span>],[<span class="number">1.</span>,<span class="number">1.</span>,<span class="number">1.</span>]])</span><br><span class="line"><span class="built_in">print</span>(A.numel())    <span class="comment"># 6 元素个数</span></span><br></pre></td></tr></table></figure>
<h5 id="2-3-7-mean"><a href="#2-3-7-mean" class="headerlink" title="2.3.7 mean"></a>2.3.7 mean</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.tensor([[<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">0.</span>],[<span class="number">1.</span>,<span class="number">1.</span>,<span class="number">1.</span>]])</span><br><span class="line"><span class="built_in">print</span>(A.numel())    <span class="comment"># 6 元素个数</span></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>())      <span class="comment"># tensor(3.)</span></span><br><span class="line"><span class="built_in">print</span>(A.mean())     <span class="comment"># tensor(0.5000)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特定轴</span></span><br><span class="line">A = torch.tensor([[<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">0.</span>],[<span class="number">1.</span>,<span class="number">1.</span>,<span class="number">1.</span>]])</span><br><span class="line"><span class="built_in">print</span>(A.shape[<span class="number">0</span>])       <span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span>(A.<span class="built_in">sum</span>(axis=<span class="number">0</span>))    <span class="comment"># tensor([1., 1., 1.])</span></span><br><span class="line"><span class="built_in">print</span>(A.mean(axis=<span class="number">0</span>))   <span class="comment"># tensor([0.5000, 0.5000, 0.5000])  平均值</span></span><br></pre></td></tr></table></figure>
<h5 id="2-3-8-dot"><a href="#2-3-8-dot" class="headerlink" title="2.3.8 dot"></a>2.3.8 dot</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">0.</span>,<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>])</span><br><span class="line">y = torch.tensor([<span class="number">1.</span>,<span class="number">1.</span>,<span class="number">1.</span>,<span class="number">1.</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.dot(x, y))  <span class="comment"># tensor(6.)</span></span><br></pre></td></tr></table></figure>
<h5 id="2-3-9-mm、mv"><a href="#2-3-9-mm、mv" class="headerlink" title="2.3.9 mm、mv"></a>2.3.9 mm、mv</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.tensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],</span><br><span class="line">                  [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">B = torch.tensor([[<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">                  [<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                  [<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">x = torch.tensor([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.mv(A, x))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">向量积</span></span><br><span class="line"><span class="string">tensor([ 9, 36])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.mm(A, B))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">矩阵积</span></span><br><span class="line"><span class="string">tensor([[ 1,  1],</span></span><br><span class="line"><span class="string">        [10, 10]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h5 id="2-3-10-L1、L2、F范数"><a href="#2-3-10-L1、L2、F范数" class="headerlink" title="2.3.10  L1、L2、F范数"></a>2.3.10  L1、L2、F范数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">abs</span>(x).<span class="built_in">sum</span>())   <span class="comment"># 向量的L1范数: tensor(7.)  x中的每个元素绝对值的和</span></span><br><span class="line"><span class="built_in">print</span>(torch.norm(x))        <span class="comment"># 向量的L2范数: tensor(5.)  x中的每个元素平方的和开根号</span></span><br><span class="line"></span><br><span class="line">A = torch.ones((<span class="number">4</span>, <span class="number">9</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.norm(A))        <span class="comment"># 矩阵的F范数:  tensor(6.)  A中的每个元素平方的和开根号</span></span><br></pre></td></tr></table></figure>
<h5 id="2-3-11-运算"><a href="#2-3-11-运算" class="headerlink" title="2.3.11  运算"></a>2.3.11  运算</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32)</span><br><span class="line">A = A.reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">B = A.clone()   </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(B)        <span class="comment"># tensor([[ 0.,  1.,  2.,  3.],</span></span><br><span class="line">                <span class="comment">#         [ 4.,  5.,  6.,  7.],</span></span><br><span class="line">                <span class="comment">#         [ 8.,  9., 10., 11.],</span></span><br><span class="line">                <span class="comment">#         [12., 13., 14., 15.],</span></span><br><span class="line">                <span class="comment">#         [16., 17., 18., 19.]])</span></span><br><span class="line">                </span><br><span class="line"><span class="built_in">print</span>(A == B)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[True, True, True, True],</span></span><br><span class="line"><span class="string">        [True, True, True, True],</span></span><br><span class="line"><span class="string">        [True, True, True, True],</span></span><br><span class="line"><span class="string">        [True, True, True, True],</span></span><br><span class="line"><span class="string">        [True, True, True, True]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A + B)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[ 0.,  2.,  4.,  6.],</span></span><br><span class="line"><span class="string">        [ 8., 10., 12., 14.],</span></span><br><span class="line"><span class="string">        [16., 18., 20., 22.],</span></span><br><span class="line"><span class="string">        [24., 26., 28., 30.],</span></span><br><span class="line"><span class="string">        [32., 34., 36., 38.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A * B)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[  0.,   1.,   4.,   9.],</span></span><br><span class="line"><span class="string">        [ 16.,  25.,  36.,  49.],</span></span><br><span class="line"><span class="string">        [ 64.,  81., 100., 121.],</span></span><br><span class="line"><span class="string">        [144., 169., 196., 225.],</span></span><br><span class="line"><span class="string">        [256., 289., 324., 361.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h5 id="2-3-12-广播"><a href="#2-3-12-广播" class="headerlink" title="2.3.12 广播"></a>2.3.12 广播</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = torch.tensor([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],</span><br><span class="line">                  [<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>]])</span><br><span class="line">B = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(B)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[ 6.],</span></span><br><span class="line"><span class="string">        [15.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A / B)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0.1667, 0.3333, 0.5000],</span></span><br><span class="line"><span class="string">        [0.2667, 0.3333, 0.4000]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A + B)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[ 7.,  8.,  9.],</span></span><br><span class="line"><span class="string">        [19., 20., 21.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(A * B)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[ 6., 12., 18.],</span></span><br><span class="line"><span class="string">        [60., 75., 90.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>04-数据读取和操作</title>
    <url>/2024/04/23/11-01-04/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="04-数据读取和操作"><a href="#04-数据读取和操作" class="headerlink" title="04-数据读取和操作"></a>04-数据读取和操作</h2><h3 id="1-数据操作"><a href="#1-数据操作" class="headerlink" title="1. 数据操作"></a>1. 数据操作</h3><p>为了能够完成各种数据操作，我们需要某种方法来存储和操作数据。通常，我们需要做两件重要的事：</p>
<ol>
<li>获取数据；</li>
<li>将数据读入计算机后对其进行处理。</li>
</ol>
<p>如果没有某种方法来存储数据，那么获取数据是没有意义的。</p>
<p>首先，我们介绍 n 维数组，也称为<strong>张量</strong>（tensor）。PyTorch的<strong>张量类</strong>与Numpy的<code>ndarray</code>类似。但在深度学习框架中应用PyTorch的<strong>张量类</strong>，又比Numpy的<code>ndarray</code>多一些重要功能：</p>
<ol>
<li>tensor可以在很好地支持GPU加速计算，而NumPy仅支持CPU计算；</li>
<li>tensor支持自动微分。</li>
</ol>
<p>这些功能使得张量类更适合深度学习。</p>
<h4 id="1-1-基本操作"><a href="#1-1-基本操作" class="headerlink" title="1.1 基本操作"></a>1.1 基本操作</h4><p>[<strong>张量表示由一些数值组成的数组，这个数组可能有多个维度</strong>]。具有一个轴的张量对应数学上的<strong>向量</strong>（vector）；具有两个轴的张量对应数学上的<strong>矩阵</strong>（matrix）；具有两个轴以上的张量没有特殊的数学名称。</p>
<p>我们可以使用<code>arange</code>创建一个行向量<code>x</code>。这个行向量包含从0开始的前12个整数，它们被<strong>默认创建为浮点数</strong>。张量中的每个值都称为张量的<strong>元素</strong>（element）。例如，张量<code>x</code>中有12个元素。除非额外指定，新的张量默认将存储在内存中，并采用基于CPU的计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.arange(<span class="number">12</span>)</span><br><span class="line">  tensor([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>])</span><br></pre></td></tr></table></figure>
<p>[<strong>可以通过张量的<code>shape</code>属性来访问张量（沿每个轴的长度）的<em>形状</em></strong>]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.shape</span><br><span class="line">  torch.Size([<span class="number">12</span>])</span><br></pre></td></tr></table></figure>
<p>如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。</p>
<p>因为这里在处理的是一个向量，所以它的<code>shape</code>与它的<code>size</code>相同。在处理更高维度的的张量时，可以用这种方法获取张量中元素的个数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.numel()</span><br><span class="line">  <span class="number">12</span></span><br></pre></td></tr></table></figure>
<p>[<strong>要想改变一个张量的形状而不改变元素数量和元素值，可以调用<code>reshape</code>函数。</strong>]</p>
<p>例如，可以把张量<code>x</code>从形状为（12,）的行向量转换为形状为（3,4）的矩阵。这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。值得注意的是，虽然张量的形状发生了改变，但其元素值并没有变。改变张量的形状时，张量中元素的个数不会改变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = x.reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">  tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">          [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">          [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br></pre></td></tr></table></figure>
<p>我们不需要通过手动指定每个维度来改变形状。如果我们的目标形状是（高度 x 宽度），那么在知道宽度后，高度会被自动计算得出，不必我们自己做除法。我们可以通过<code>-1</code>来调用此自动计算出维度的功能即可以用<code>x.reshape(-1,4)</code>或<code>x.reshape(3,-1)</code>来取代<code>x.reshape(3,4)</code>。</p>
<p>有时，我们希望[<strong>使用全0、全1、其他常量，或者从特定分布中随机采样的数字</strong>]来初始化矩阵。我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0或者1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">  tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">          [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">  tensor([[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">          [[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]])</span><br></pre></td></tr></table></figure>
<p>有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。以下代码创建一个形状为（3,4）的张量。其中的每个元素都从均值为0、标准差为1的标准正态分布中随机采样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">  tensor([[ <span class="number">0.1364</span>,  <span class="number">0.3546</span>, -<span class="number">0.9091</span>, -<span class="number">1.8926</span>],</span><br><span class="line">          [ <span class="number">0.5786</span>, -<span class="number">0.9019</span>, -<span class="number">0.1305</span>, -<span class="number">0.1899</span>],</span><br><span class="line">          [ <span class="number">0.5696</span>,  <span class="number">1.1626</span>, -<span class="number">0.5987</span>,  <span class="number">0.4085</span>]])</span><br></pre></td></tr></table></figure>
<p>我们还可以[<strong>通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值</strong>]。在这里，最外层的列表对应于轴0，内层的列表对应于轴1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">  tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>],</span><br><span class="line">          [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">          [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="1-2-简单运算"><a href="#1-2-简单运算" class="headerlink" title="1.2 简单运算"></a>1.2 简单运算</h4><p>我们想在这些数据上执行数学运算，其中最简单且最有用的操作是<strong>按元素</strong>（elementwise）运算。它们将标准标量运算符应用于数组的每个元素。对于将两个数组作为输入的函数，按元素运算将二元运算符应用于两个数组中的每对位置对应的元素。我们可以基于任何从标量到标量的函数来创建按元素函数。我们通过将标量函数升级为按元素向量运算来生成向量值$F: \mathbb{R}^d, \mathbb{R}^d \rightarrow \mathbb{R}^d$。</p>
<p>对于任意具有相同形状的张量，[<strong>常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和`</strong>`）都可以被升级为按元素运算**]。我们可以在同一形状的任意两个张量上调用按元素操作。我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x + y, x - y, x * y, x / y, x ** y</span><br><span class="line">  (tensor([ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>, <span class="number">10.</span>]),</span><br><span class="line">    tensor([-<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">6.</span>]),</span><br><span class="line">    tensor([ <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">16.</span>]),</span><br><span class="line">    tensor([<span class="number">0.5000</span>, <span class="number">1.0000</span>, <span class="number">2.0000</span>, <span class="number">4.0000</span>]),</span><br><span class="line">    tensor([ <span class="number">1.</span>,  <span class="number">4.</span>, <span class="number">16.</span>, <span class="number">64.</span>]))</span><br></pre></td></tr></table></figure>
<p>(<strong>“按元素”方式可以应用更多的计算</strong>)，包括像求幂这样的一元运算符。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.exp(x)</span><br><span class="line">  tensor([<span class="number">2.7183e+00</span>, <span class="number">7.3891e+00</span>, <span class="number">5.4598e+01</span>, <span class="number">2.9810e+03</span>])</span><br></pre></td></tr></table></figure>
<p>[<strong>我们也可以把多个张量<em>连结</em>（concatenate）在一起</strong>]，把它们端对端地叠起来形成一个更大的张量。我们只需要提供张量列表，并给出沿哪个轴连结。下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素）和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。我们可以看到，第一个输出张量的轴-0长度（$6$）是两个输入张量轴-0长度的总和（$3 + 3$）；第二个输出张量的轴-1长度（$8$）是两个输入张量轴-1长度的总和（$4 + 4$）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((X, Y), dim=<span class="number">0</span>), torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">  (tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">            [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">            [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">            [ <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">            [ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">            [ <span class="number">4.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>]]),</span><br><span class="line">    tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">            [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">            [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>]]))</span><br></pre></td></tr></table></figure>
<p>由上述例子可见，当需要按轴-x连结两个张量时，我们就在第x+1层括号内将两张量中的元素相组合。类似地，我们将两个三维张量相连结。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Y = torch.tensor([[[<span class="number">2.0</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">3</span>]], [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], [[<span class="number">4</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((X, Y), dim=<span class="number">0</span>), torch.cat((X, Y), dim=<span class="number">1</span>), torch.cat((X, Y), dim=<span class="number">2</span>)</span><br><span class="line">  (tensor([[[ <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">              [ <span class="number">2.</span>,  <span class="number">3.</span>]],</span><br><span class="line"> </span><br><span class="line">            [[ <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">              [ <span class="number">6.</span>,  <span class="number">7.</span>]],</span><br><span class="line"> </span><br><span class="line">            [[ <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">              [<span class="number">10.</span>, <span class="number">11.</span>]],</span><br><span class="line"> </span><br><span class="line">            [[ <span class="number">2.</span>,  <span class="number">1.</span>],</span><br><span class="line">              [ <span class="number">4.</span>,  <span class="number">3.</span>]],</span><br><span class="line"> </span><br><span class="line">            [[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">              [ <span class="number">3.</span>,  <span class="number">4.</span>]],</span><br><span class="line"> </span><br><span class="line">            [[ <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">              [ <span class="number">2.</span>,  <span class="number">1.</span>]]]),</span><br><span class="line">    tensor([[[ <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">              [ <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">              [ <span class="number">2.</span>,  <span class="number">1.</span>],</span><br><span class="line">              [ <span class="number">4.</span>,  <span class="number">3.</span>]],</span><br><span class="line"> </span><br><span class="line">            [[ <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">              [ <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">              [ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">              [ <span class="number">3.</span>,  <span class="number">4.</span>]],</span><br><span class="line"> </span><br><span class="line">            [[ <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">             [<span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">             [ <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">              [ <span class="number">2.</span>,  <span class="number">1.</span>]]]),</span><br><span class="line">    tensor([[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>],</span><br><span class="line">              [ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>]],</span><br><span class="line"> </span><br><span class="line">           [[ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">              [ <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>]],</span><br><span class="line"> </span><br><span class="line">            [[ <span class="number">8.</span>,  <span class="number">9.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">             [<span class="number">10.</span>, <span class="number">11.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>]]]))</span><br></pre></td></tr></table></figure>
<p>有时，我们想[<strong>通过<em>逻辑运算符</em>构建二元张量</strong>]。 以<code>X == Y</code>为例： 对于每个位置，如果<code>X</code>和<code>Y</code>在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句<code>X == Y</code>在该位置处为真，否则该位置为0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X == Y</span><br><span class="line">  tensor([[<span class="literal">False</span>,  <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">          [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">            [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]])</span><br></pre></td></tr></table></figure>
<p>[<strong>对张量中的所有元素进行求和，会产生一个单元素张量。</strong>]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X.<span class="built_in">sum</span>()</span><br><span class="line">  tensor(<span class="number">66.</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-3-广播机制"><a href="#1-3-广播机制" class="headerlink" title="1.3 广播机制"></a>1.3 广播机制</h4><p>在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。在某些情况下，[<strong>即使形状不同，我们仍然可以通过调用<em>广播机制</em>（broadcasting mechanism）来执行按元素操作</strong>]。这种机制的工作条件是：两个张量从后开始数，每个维度相等或者其中一个为1。这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a, b, a + b</span><br><span class="line">  (tensor([[<span class="number">0</span>],</span><br><span class="line">            [<span class="number">1</span>],</span><br><span class="line">            [<span class="number">2</span>]]),</span><br><span class="line">    tensor([[<span class="number">0</span>, <span class="number">1</span>]]))</span><br><span class="line">    tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">           [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">           [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
<p>由于<code>a</code>和<code>b</code>分别是$3\times1$和$1\times2$矩阵，如果让它们相加，它们的形状不匹配。我们将两个矩阵<strong>广播</strong>为一个更大的$3\times2$矩阵，矩阵<code>a</code>复制列，矩阵<code>b</code>复制行，然后再按元素相加。需要注意的是，广播机制只能扩展维度，而不能凭空增加张量的维度，例如在计算沿某个轴的均值时，若张量维度不同，则会报错：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>C = torch.arange(<span class="number">24</span>, dtype=torch.float32).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C / C.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">  RuntimeError: The size of tensor a (<span class="number">3</span>) must <span class="keyword">match</span> the size of tensor b (<span class="number">2</span>) at non-singleton dimension <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>此时我们需要将<code>keepdims</code>设为True，才能正确利用广播机制扩展<code>C.sum(axis=1)</code>的维度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>C / C.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">  tensor([[[<span class="number">0.0000</span>, <span class="number">0.0667</span>, <span class="number">0.1111</span>, <span class="number">0.1429</span>],</span><br><span class="line">            [<span class="number">0.3333</span>, <span class="number">0.3333</span>, <span class="number">0.3333</span>, <span class="number">0.3333</span>],</span><br><span class="line">            [<span class="number">0.6667</span>, <span class="number">0.6000</span>, <span class="number">0.5556</span>, <span class="number">0.5238</span>]],</span><br><span class="line"></span><br><span class="line">          [[<span class="number">0.2500</span>, <span class="number">0.2549</span>, <span class="number">0.2593</span>, <span class="number">0.2632</span>],</span><br><span class="line">            [<span class="number">0.3333</span>, <span class="number">0.3333</span>, <span class="number">0.3333</span>, <span class="number">0.3333</span>],</span><br><span class="line">            [<span class="number">0.4167</span>, <span class="number">0.4118</span>, <span class="number">0.4074</span>, <span class="number">0.4035</span>]]])</span><br></pre></td></tr></table></figure>
<h4 id="1-4-索引和切片"><a href="#1-4-索引和切片" class="headerlink" title="1.4 索引和切片"></a>1.4 索引和切片</h4><p>就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1；可以指定范围以包含第一个元素和最后一个之前的元素。如下所示，我们[<strong>可以用<code>[-1]</code>选择最后一个元素，可以用<code>[1:3]</code>选择第二个和第三个元素</strong>]：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X[-<span class="number">1</span>], X[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">  (tensor([ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]),</span><br><span class="line">    tensor([[ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">            [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]]))</span><br></pre></td></tr></table></figure>
<p>我们[<strong>可以用<code>[::2]</code>每间隔一个元素选择一个元素，可以用<code>[::3]</code>每间隔两个元素选择一个元素</strong>]：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X[::<span class="number">2</span>, ::<span class="number">3</span>]</span><br><span class="line">  tensor([[ <span class="number">0.</span>,  <span class="number">3.</span>],</span><br><span class="line">            [ <span class="number">8.</span>, <span class="number">11.</span>]])</span><br></pre></td></tr></table></figure>
<p>[<strong>除读取外，我们还可以通过指定索引来将元素写入矩阵。</strong>]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X[<span class="number">1</span>, <span class="number">2</span>] = <span class="number">9</span></span><br><span class="line">  tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">          [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">9.</span>,  <span class="number">7.</span>],</span><br><span class="line">          	[ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]])</span><br></pre></td></tr></table></figure>
<p>如果我们想[<strong>为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。</strong>]例如，<code>[0:2, :]</code>访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span></span><br><span class="line">  tensor([[<span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>],</span><br><span class="line">          [<span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>],</span><br><span class="line">          	[ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="1-5-节约内存"><a href="#1-5-节约内存" class="headerlink" title="1.5 节约内存"></a>1.5 节约内存</h4><p>[<strong>如果在后续计算中没有重复使用<code>X</code>，我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销。</strong>]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>before = <span class="built_in">id</span>(X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X += Y</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">id</span>(X) == before</span><br><span class="line">  <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4 id="1-6-转换为其他Python对象"><a href="#1-6-转换为其他Python对象" class="headerlink" title="1.6 转换为其他Python对象"></a>1.6 转换为其他Python对象</h4><p>将深度学习框架定义的张量[<strong>转换为NumPy张量（<code>ndarray</code>）</strong>]很容易，反之也同样容易。torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = X.numpy()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B = torch.tensor(A)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(A), <span class="built_in">type</span>(B)</span><br><span class="line">  (numpy.ndarray, torch.Tensor)</span><br></pre></td></tr></table></figure>
<p>要(<strong>将大小为1的张量转换为Python标量</strong>)，我们可以调用<code>item</code>函数或Python的内置函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br><span class="line">  (tensor([<span class="number">3.5000</span>]), <span class="number">3.5</span>, <span class="number">3.5</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h3><p>为了能用深度学习来解决现实世界的问题，我们经常从预处理原始数据开始，而不是从那些准备好的张量格式数据开始。在Python中常用的数据分析工具中，我们通常使用<code>pandas</code>软件包。像庞大的Python生态系统中的许多其他扩展包一样，<code>pandas</code>可以与张量兼容。本节我们将简要介绍使用<code>pandas</code>预处理原始数据，并将原始数据转换为张量格式的步骤。</p>
<h4 id="2-1-读取数据集"><a href="#2-1-读取数据集" class="headerlink" title="2.1 读取数据集"></a>2.1 读取数据集</h4><p>举一个例子，我们首先(<strong>创建一个人工数据集，并存储在CSV（逗号分隔值）文件</strong>)<code>../data/house_tiny.csv</code>中。以其他格式存储的数据也可以通过类似的方式进行处理。下面我们将数据集按行写入CSV文件中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="comment"># 列名</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)  <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>要[<strong>从创建的CSV文件中加载原始数据集</strong>]，我们导入<code>pandas</code>包并调用<code>read_csv</code>函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = pd.read_csv(data_file)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(data)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">NumRooms</th>
<th style="text-align:center">Alley</th>
<th style="text-align:center">Price</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">NaN</td>
<td style="text-align:center">Pave</td>
<td style="text-align:center">127500</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">NaN</td>
<td style="text-align:center">106000</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">4.0</td>
<td style="text-align:center">NaN</td>
<td style="text-align:center">178100</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">NaN</td>
<td style="text-align:center">NaN</td>
<td style="text-align:center">140000</td>
</tr>
</tbody>
</table>
</div>
<h4 id="2-2-处理缺失值"><a href="#2-2-处理缺失值" class="headerlink" title="2.2 处理缺失值"></a>2.2 处理缺失值</h4><p>“NaN”项代表缺失值。[<strong>为了处理缺失的数据，典型的方法包括<em>插值法</em>和<em>删除法</em>，</strong>]其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。通过位置索引<code>iloc</code>，我们将<code>data</code>分成<code>inputs</code>和<code>outputs</code>，其中前者为<code>data</code>的前两列，而后者为<code>data</code>的最后一列。对于<code>inputs</code>中缺少的数值，我们用同一列的均值替换“NaN”项。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>inputs = inputs.fillna(inputs.mean(numeric_only=<span class="literal">True</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>numeric_only=True,处理运行bug</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">NumRooms</th>
<th style="text-align:center">Alley</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">Pave</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">NaN</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">4.0</td>
<td style="text-align:center">NaN</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">NaN</td>
</tr>
</tbody>
</table>
</div>
<p>利用删除法，我们删除缺失元素最多的一个样本。首先，<code>data.isnull()</code>矩阵统计每个元素是否缺失，之后在轴-1的方向上将<code>data.isnull()</code>元素求和，得到每个样本缺失元素个数，取得缺失元素个数最大的样本的序号，并将其删除。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>nan_numer = data.isnull().<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>nan_max_id = nan_numer.idxmax()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data_delete = data.drop([nan_max_id], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">NumRooms</th>
<th style="text-align:center">Alley</th>
<th style="text-align:center">Price</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">NaN</td>
<td style="text-align:center">Pave</td>
<td style="text-align:center">127500</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">NaN</td>
<td style="text-align:center">106000</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">4.0</td>
<td style="text-align:center">NaN</td>
<td style="text-align:center">178100</td>
</tr>
</tbody>
</table>
</div>
<p>一般情况下，利用<code>dropna</code>删除数据，其中•Axis哪个维度How如何删除，‘any’表示有nan即删除，‘all’表示全为nan删除，Thresh有多少个nan删除，Subset在哪些列中查找nan，Inplace是否原地修改。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dropna( axis=<span class="number">0</span>, how=‘<span class="built_in">any</span>’, thresh=<span class="literal">None</span>, subset=<span class="literal">None</span>, inplace=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>[<strong>对于<code>inputs</code>中的类别值或离散值，我们将“NaN”视为一个类别。</strong>]由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，<code>pandas</code>可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">NumRooms</th>
<th style="text-align:center">Alley_Pave</th>
<th style="text-align:center">Alley_nan</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">4.0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<h4 id="2-3-转换为张量格式"><a href="#2-3-转换为张量格式" class="headerlink" title="2.3 转换为张量格式"></a>2.3 转换为张量格式</h4><p>[<strong>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式。</strong>]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class="line">  (tensor([[<span class="number">3.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">2.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">4.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">3.</span>, <span class="number">0.</span>, <span class="number">1.</span>]], dtype=torch.float64),</span><br><span class="line">    tensor([<span class="number">127500</span>, <span class="number">106000</span>, <span class="number">178100</span>, <span class="number">140000</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="3-Q-amp-A"><a href="#3-Q-amp-A" class="headerlink" title="3. Q&amp;A"></a>3. Q&amp;A</h3><p><strong><code>Q1：reshape和view的区别？</code></strong></p>
<blockquote>
<p>View为浅拷贝，只能作用于连续型张量；Contiguous函数将张量做深拷贝并转为连续型；Reshape在张量连续时和view相同，不连续时等价于先contiguous再view。</p>
</blockquote>
<p><strong><code>Q2：数组计算吃力怎么办？</code></strong></p>
<blockquote>
<p>学习numpy的知识。</p>
</blockquote>
<p><strong><code>Q3：如何快速区分维度？</code></strong></p>
<blockquote>
<p>利用<code>a.shape</code>或<code>a.dim()</code>。</p>
</blockquote>
<p><strong><code>Q4：Tensor和Array有什么区别？</code></strong></p>
<blockquote>
<p>Tensor是数学上定义的张量，Array是计算机概念数组，但在深度学习中有时将Tensor视为多维数组。</p>
</blockquote>
<p><strong><code>Q5：新分配了y的内存，那么之前y对应的内存会自动释放吗？</code></strong></p>
<blockquote>
<p>Python会在不需要时自动释放内存。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>03-深度学习安装</title>
    <url>/2024/04/23/11-01-03/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="03-安装"><a href="#03-安装" class="headerlink" title="03-安装"></a>03-安装</h2><h3 id="1-安装python"><a href="#1-安装python" class="headerlink" title="1.安装python"></a>1.安装python</h3><p>首先前提是安装python，这里推荐安装python3.8 输入命令 <strong><em>sudo apt install python3.8</em></strong> 即可</p>
<h3 id="2-安装Miniconda-Anaconda"><a href="#2-安装Miniconda-Anaconda" class="headerlink" title="2.安装Miniconda/Anaconda"></a>2.安装Miniconda/Anaconda</h3><ul>
<li><p>然后第二步，安装 Miniconda（如果已经安装conda或者Miniconda，则可以跳过该步骤)。</p>
<h4 id="2-1-安装Miniconda"><a href="#2-1-安装Miniconda" class="headerlink" title="2.1 安装Miniconda"></a>2.1 安装Miniconda</h4><ul>
<li>安装MIniconda的好处是可以创建很多虚拟环境，并且不同环境之间互相不会有依赖关系，对日后的项目有帮助，如果只想在本地安装的话，不装Miniconda只使用pip即可，第二步可以跳过。</li>
<li>如果是Windows系统，输入命令 <strong><em>wget <a href="https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-Windows-x86_64.exe">https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-Windows-x86_64.exe</a></em></strong></li>
<li>如果是macOS，输入命令 <strong><em>wget <a href="https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-MacOSX-x86_64.sh">https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-MacOSX-x86_64.sh</a></em></strong> 之后要输入命令 <strong><em>sh Miniconda3-py38_4.10.3-MacOSX-x86_64.sh -b</em></strong></li>
<li>如果是Linux系统，输入命令 <strong><em>wget <a href="https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-Linux-x86_64.sh">https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-Linux-x86_64.sh</a></em></strong> 之后输入命令 <strong><em>sh Miniconda3-py38_4.10.3-Linux-x86_64.sh -b</em></strong></li>
<li>以上都是基于python3.8版本，对于其他版本，可以访问 <strong><em><a href="https://docs.conda.io/en/latest/miniconda.html">https://docs.conda.io/en/latest/miniconda.html</a></em></strong> ，下载对应版本即可。</li>
</ul>
<h4 id="2-2-Miniconda环境操作"><a href="#2-2-Miniconda环境操作" class="headerlink" title="2.2 Miniconda环境操作"></a>2.2 Miniconda环境操作</h4><ul>
<li>对于第一次安装Miniconda的，要初始化终端shell，输入命令 <strong><em>~/miniconda3/bin/conda init</em></strong></li>
<li>这样我们就可以使用 <strong><em>conda create —name d2l python=3.8 -y</em></strong> 来创建一个名为xxx的环境，这里命名为d2l</li>
<li>打开xxx环境命令: <strong><em>conda activate xxx</em></strong> ；关闭命令：<strong><em>conda deactivate xxx</em></strong>。对于基础conda环境不用添加名</li>
</ul>
</li>
</ul>
<h3 id="3-安装Pytorch-d2l-jupyter包"><a href="#3-安装Pytorch-d2l-jupyter包" class="headerlink" title="3.安装Pytorch, d2l, jupyter包"></a>3.安装Pytorch, d2l, jupyter包</h3><ul>
<li><p>第三步，安装深度学习框架和<code>d2l</code>软件包</p>
<p>在安装深度学习框架之前，请先检查你的计算机上是否有可用的GPU（为笔记本电脑上显示器提供输出的GPU不算）。 例如，你可以查看计算机是否装有NVIDIA GPU并已安装<a href="https://developer.nvidia.com/cuda-downloads">CUDA</a>。 如果你的机器没有任何GPU，没有必要担心，因为你的CPU在前几章完全够用。 但是，如果你想流畅地学习全部章节，请提早获取GPU并且安装深度学习框架的GPU版本。</p>
<ul>
<li><p>你可以按如下方式安装PyTorch的CPU或GPU版本：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install torch==1.8.1</span><br><span class="line">pip install torchvision==0.9.1</span><br></pre></td></tr></table></figure>
</li>
<li><p>也可以访问官网 <strong><em><a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></em></strong> 选择适合自己电脑pytorch版本下载!</p>
</li>
<li>
</li>
<li><p>本课程的jupyter notebook代码详见 <strong><em><a href="https://zh-v2.d2l.ai/d2l-zh.zip">https://zh-v2.d2l.ai/d2l-zh.zip</a></em></strong></p>
</li>
<li><p>下载jupyter notebook ：输入命令 <strong><em>pip install jupyter notebook</em></strong> （若pip失灵可以尝试pip3），输入密命令 <strong><em>jupyter notebook</em></strong> 即可打开。</p>
</li>
</ul>
</li>
</ul>
<h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h3><ul>
<li>本节主要介绍<strong>安装Miniconda</strong>、<strong>CPU环境下的Pytorch</strong>和其它课程所需<strong>软件包</strong>(d2l, jupyter)。对于前面几节来说，CPU已经够用了。<ul>
<li>如果您<strong>已经安装</strong>了Miniconda/Anaconda, Pytorch框架和jupyter记事本, 您只需再安装<strong>d2l包</strong>，就可以跳过本节视频了<strong>开启深度学习之旅</strong>了; 如果希望后续章节在<strong>GPU下跑深度学习</strong>, 可以<strong>新建环境</strong>安装<strong>CUDA版本的Pytorch</strong>。</li>
<li>如果需要在Windows下<strong>安装CUDA和Pytorch</strong>(cuda版本)，用<strong>本地GPU跑深度学习</strong>，可以参考李沐老师<a href="https://www.zhihu.com/zvideo/1363284223420436480">Windows下安装CUDA和Pytorch跑深度学习</a>，如果网慢总失败的同学可以参考<a href="https://www.zhihu.com/question/425647129/answer/2278290137">cuda11.0如何安装pytorch？ - Glenn1Q84的回答 - 知乎</a>。当然，如果不方便在本地进行配置(如无GPU, GPU显存过低等)，也可以选择<a href="https://colab.research.google.com/">Colab</a>(需要科学上网)，或其它<strong>云服务器</strong>GPU跑深度学习。</li>
<li>如果pip安装比较慢，可以用镜像源安装：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install torch torchvision -i http://mirrors.aliyun.com/pypi/simple/  --trusted-host mirrors.aliyun.com</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>如果安装时经常报错, 可以参考课程评论区部分。</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>02-深度学习介绍</title>
    <url>/2024/04/23/11-01-02/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="02-深度学习介绍"><a href="#02-深度学习介绍" class="headerlink" title="02-深度学习介绍"></a>02-深度学习介绍</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h4 id="1-1-AI地图"><a href="#1-1-AI地图" class="headerlink" title="1.1 AI地图"></a>1.1 AI地图</h4><p>首先画一个简单的人工智能地图：</p>
<img src="/2024/04/23/11-01-02/02-01.png" class>
<ul>
<li><p>x轴表示不同的模式or方法：最早的是符号学，接下来是概率模型，之后是机器学习</p>
</li>
<li><p>y轴表示可以达到的层次：由底部向上依次是</p>
<blockquote>
<p>感知：了解是什么，比如能够可以看到物体，如面前的一块屏幕</p>
<p>推理：基于感知到的现象，想象或推测未来会发生什么</p>
<p>知识：根据看到的数据或者现象，形成自己的知识</p>
<p>规划：根据学习到的知识，做出长远的规划</p>
</blockquote>
</li>
</ul>
<h4 id="1-2-AI地图解读"><a href="#1-2-AI地图解读" class="headerlink" title="1.2 AI地图解读"></a>1.2 AI地图解读</h4><ul>
<li><p>问题领域的一个简单分类</p>
<ul>
<li><p><strong>自然语言处理</strong>：</p>
<ul>
<li>停留在比较简单的<strong>感知</strong>层面，比如自然语言处理用的比较多的机器翻译，给一句中文翻译成英文，很多时候是人的潜意识里面大脑感知的一个问题。一般来说，人可以几秒钟内反应过来的东西，属于感知范围。</li>
<li>自然语言处理最早使用的方法是<strong>符号学</strong>，由于语言具有符号性；之后一段时间比较流行的有<strong>概率模型</strong>，以及现在也用的比较多的<strong>机器学习</strong>。</li>
</ul>
</li>
<li><p><strong>计算机视觉</strong>：</p>
<ul>
<li>在简单的感知层次之上，可以对图片做一些<strong>推理</strong>。 </li>
<li>图片里都是一些像素，很难用符号学解释，所以一般采用<strong>概率模型</strong>和<strong>机器学习</strong>。</li>
</ul>
</li>
<li><p><strong>深度学习</strong></p>
<ul>
<li>机器学习的一种，更深层的神经网络。</li>
<li>可以做计算机视觉，自然语言处理，强化学习等。</li>
</ul>
</li>
</ul>
</li>
<li><p>过去八年最热的方向，也是本课程关心的重点：</p>
<ul>
<li>深度学习+计算机视觉 / 自然语言处理</li>
</ul>
</li>
</ul>
<p>本节课只关注AI中的一小部分领域，即深度学习背景下的视觉和自然语言处理相关的基础应用。</p>
<hr>
<h3 id="2-深度学习的应用"><a href="#2-深度学习的应用" class="headerlink" title="2. 深度学习的应用"></a>2. 深度学习的应用</h3><h4 id="2-1-图片分类"><a href="#2-1-图片分类" class="headerlink" title="2.1 图片分类"></a>2.1 图片分类</h4><p>深度学习最早是在图片分类上有比较大的突破，ImageNet是一个比较大的图片分类数据集，</p>
<img src="/2024/04/23/11-01-02/02-02.png" class>
<p><code>x轴</code>：年份      <code>y轴</code>：错误率     <code>圆点</code>：表示某年份某研究工作/paper的错误率  <a href="https://image-net.org/">IMAGENET</a>   <a href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/">数据来源</a></p>
<blockquote>
<p>在2010年时，错误率比较高，最好的工作错误率也在26%、%27左右；</p>
<p>在2012年，有团队首次使用深度学习将错误率降到25%以下；</p>
<p>在接下来几年中，使用深度学习可以将误差降到很低。</p>
<p>2017年基本所有的团队可以将错误率降到5%以下，基本可以达到人类识别图片的精度。</p>
</blockquote>
<h4 id="2-2-物体检测和分割"><a href="#2-2-物体检测和分割" class="headerlink" title="2.2 物体检测和分割"></a>2.2 物体检测和分割</h4><p>当你不仅仅想知道图片里有什么内容，还想知道物体是什么，在什么位置，这就是<strong>物体检测</strong>。<strong>物体分割</strong>是指每一个像素属于什么，属于飞机还是属于人(如下图)，这是图像领域更深层次的一个应用。<br><img src="/2024/04/23/11-01-02/02-03.png" class></p>
<h4 id="2-3-样式迁移"><a href="#2-3-样式迁移" class="headerlink" title="2.3 样式迁移"></a>2.3 样式迁移</h4><p>原图片+想要迁移的风格=风格迁移后的图片，加了一个可以根据输入改变图片风格的滤镜。<br><img src="/2024/04/23/11-01-02/02-04.png" class></p>
<h4 id="2-4-人脸合成"><a href="#2-4-人脸合成" class="headerlink" title="2.4 人脸合成"></a>2.4 人脸合成</h4><p>下图中所有的人脸都是假的，由机器合成的图片：<br><img src="/2024/04/23/11-01-02/02-05.png" class></p>
<h4 id="2-5-文字生成图片"><a href="#2-5-文字生成图片" class="headerlink" title="2.5 文字生成图片"></a>2.5 文字生成图片</h4><ol>
<li><p>描述：一个胡萝卜宝宝遛狗的图片。</p>
</li>
<li><p>描述：一个牛油果形状的靠背椅。</p>
<img src="/2024/04/23/11-01-02/02-06.png" class>
</li>
</ol>
<h4 id="2-6-文字生成"><a href="#2-6-文字生成" class="headerlink" title="2.6 文字生成"></a>2.6 文字生成</h4><blockquote>
<p>示例1：</p>
<p>问题输入：如何举行一个有效的董事会议 </p>
<p>机器输出：生成篇章回答</p>
<p>示例2：</p>
<p>输入：将Students从School这个table中选出来</p>
<p>输出：用于查询的SQL语言</p>
</blockquote>
<img src="/2024/04/23/11-01-02/02-07.png" class>
<h4 id="2-7-无人驾驶"><a href="#2-7-无人驾驶" class="headerlink" title="2.7 无人驾驶"></a>2.7 无人驾驶</h4><p>识别车、道路以及各种障碍物等，并规划路线。</p>
<img src="/2024/04/23/11-01-02/02-08.png" class>
<h4 id="2-8-案例研究——广告点击"><a href="#2-8-案例研究——广告点击" class="headerlink" title="2.8 案例研究——广告点击"></a>2.8 案例研究——广告点击</h4><blockquote>
<p>用户输入想要搜索的广告内容，如：baby toy</p>
<p>网站呈现最具有效益的广告(用户更可能点击，且给网站带来更高经济效益)</p>
</blockquote>
<img src="/2024/04/23/11-01-02/02-09.png" class>
<h5 id="2-8-1-步骤"><a href="#2-8-1-步骤" class="headerlink" title="2.8.1 步骤"></a>2.8.1 步骤</h5><img src="/2024/04/23/11-01-02/02-10.png" class>
<ol>
<li>触发：用户输入关键词，机器先找到一些相关的广告</li>
<li>点击率预估：   利用机器学习的模型预测用户对广告的点击率</li>
<li>排序：利用点击率x竞价的结果进行排序呈现广告，排名高的在前面呈现</li>
</ol>
<h5 id="2-8-2-模型的预测与训练"><a href="#2-8-2-模型的预测与训练" class="headerlink" title="2.8.2 模型的预测与训练"></a>2.8.2 模型的<mark>预测与训练</mark></h5><p>上述步骤的第二步中涉及到模型预测用户的点击率，具体过程如下：<br><img src="/2024/04/23/11-01-02/02-11.png" class></p>
<p><strong>模型预测</strong></p>
<p>数据 (待预测广告) → 特征提取 → 模型 → 点击率预测</p>
<p><strong>模型训练</strong></p>
<p>训练数据 (过去广告展现和用户点击) → 特征(X)和用户点击(Y) → 喂给模型训练</p>
<h5 id="2-8-3-完整的故事"><a href="#2-8-3-完整的故事" class="headerlink" title="2.8.3 完整的故事"></a>2.8.3 完整的故事</h5><img src="/2024/04/23/11-01-02/02-12.png" class>
<p><strong><font color="Navy">领域专家</font></strong>：对特定的应用有比较深的了解，根据展现情况以及用户点击分析用户的行为，期望模型对应用做一些拟合，符合真实数据和分析情况。</p>
<p><strong><font color="Navy">数据科学家</font></strong>：利用数据训练模型，训练后模型投入使用，进行预测呈现。</p>
<p><strong><font color="Navy">AI专家</font></strong>：应用规模扩大，用户数量增多，模型更加复杂，需要进一步提升精度和性能。</p>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><ul>
<li>通过AI地图，课程从纵向和横向两个维度解读了深度学习在重要问题领域的概况。</li>
<li>介绍了深度学习在CV和NLP方面的一些应用</li>
<li>简单分析并研究了深度学习实例——广告点击。</li>
</ul>
<hr>
<h3 id="4-深度学习介绍-Q-amp-A"><a href="#4-深度学习介绍-Q-amp-A" class="headerlink" title="4. 深度学习介绍 Q&amp;A"></a>4. 深度学习介绍 Q&amp;A</h3><p>◆ <strong>Q1：机器学习的可解释性？机器学习在图片分割为什么有效，目前有没有定论？</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A1</font>：模型的可解释性不管是深度学习还是机器学习都是非常受人关注的点，因为这像是一个黑盒，训练了一个模型，也不知道为什么work或者为什么不work。对于深度学习来说，模型的<strong>可解释性</strong>是做得不够好的；对于机器学习来说，我们对简单的模型可以理解，但当模型变得很深的时候，几乎只能放弃理解的过程。</p>
<p>特别地，<strong>为什么有效</strong>和<strong>可解释性</strong>是两个不同的概念，所有提出的新模型都会解释一下为什么有效，可解释性是说人是不是能理解这个模型，理解模型为什么工作是一个方面，还需要知道它什么时候不工作以及在什么地方会出现偏差。</p>
<p>目前来说，一个模型在一个应用/领域上为什么可以工作，会有一些解释，我们会在解释不同的模型的时候给大家进行讲解。</p>
</blockquote>
<p>◆ <strong>Q2：领域专家是什么意思？</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A2</font>：举个例子，比如我要做农业上的物体识别，我种了一棵树，想要看今年的收成怎么样，我有很多很多土地，用人去一个个查看很费力，于是我用一个无人机，将农作物的情况拍下来，假设得到了树的一些图片，而数据科学家不知道农作物什么样的情况是好，什么样是坏，于是<strong>领域专家进行解释</strong>，比如多少叶子算是好，什么样不好。同时<strong>数据科学家</strong>将领域专家的<strong>问题翻译</strong>成机器学习能做的任务。所以可以认为<strong>领域专家</strong>是<strong>提需求</strong>的人<strong>甲方</strong>，而<strong>数据科学家</strong>是<strong>乙方</strong>。</p>
</blockquote>
<p>◆ <strong>Q3：MXNet的GPU版本的安装必须要卸载CPU版本的吗？</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A3</font>：是的，但我们的课程是<strong>基于Pytorch</strong>，Pytorch会不一样一些。</p>
</blockquote>
<p>◆ <strong>Q4：深度学习无法用数学规范表述，只能用直觉理解是吗？</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A4</font>：不一定，深度学习模型可以用<strong>数学形式表示</strong>，接下来也会讲到很多数学的东西，但是说具体用数学解释它<strong>为什么工作</strong>，<strong>为什么不工作</strong>，这个是目前我们做的不好的地方。</p>
</blockquote>
<p>◆ <strong>Q5：符号学可以和机器学习融合起来吗？</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A5</font>：确实是可以的。目前来说，<strong>符号学</strong>在<strong>深度学习</strong>有一些新的进展，以前说符号学就是做一些符号上的推理，目前<strong>深度学习如图神经网络</strong>，可以做一些比较复杂的推理。</p>
</blockquote>
<p>◆ <strong>Q6：数据科学家和AI专家的主要区别在哪里？</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A6</font>：我觉得没有太多区别。数据科学家很多时候关注的是给一个数据，赶紧出一个模型，能工作就好了，<strong>关注</strong>的是如何把领域专家的一个实际的业务问题，变成一个机器学习能做的任务，训练成一个还不错的模型。<strong>AI专家也可能是数据科学家</strong>，也就是说其不仅要训练出来一个模型能用，而且会<strong>关心</strong>如何把模型精度做得很高，或者可以说，<strong>资深数据科学家可以认为是AI专家</strong>。</p>
<p>数据科学家可以有<strong>两条路线</strong>，一个是<strong>不断开拓新领域</strong>，比如机器学习在农业上的应用，在医疗上的应用等，这个是<strong>往广的走</strong>，<strong>往深的走</strong>的话可以称为是AI专家，在某一块方面了解得很深。</p>
</blockquote>
<p>◆ <strong>Q7：MAC是不是可以支持Pytorch？</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A7</font>：可以。</p>
</blockquote>
<p>◆ <strong>Q8：说自然语言处理仅仅停留在感知层面似乎不太合适？因为语言的理解和产出不仅仅是感知，也涉及到语言知识和世界知识，也涉及到规划，比如机器规划下一步要做什么。</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A8</font>：确实是这样，语言当然是一个很复杂的过程，我只是想说，自然语言处理我们做得还很一般，虽然能做一些感知以外的东西，但是我感觉是说，<strong>不如</strong>深度学习特别机器学习，在图片上的应用做得好一些。当然AI地图上也只是一个<strong>大致的分类</strong>，大家不用特别纠结。</p>
</blockquote>
<p>◆ <strong>Q9：请问老师有考虑后面讲一讲如何寻找自己领域的paper的经验吗？</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A9</font>：这是一个很好的问题，因为大家如果现在去读paper的话，可能每天都有一百篇paper出来，你怎么样去找到你想要的paper，总不能天天看朋友圈推文，这样只能知道别人读过的paper，不会有自己<strong>独特的见解</strong>，怎么样找到自己感兴趣和有启发性的论文，后面有机会和大家分享一下我的做法。</p>
</blockquote>
<p>◆ <strong>Q10：以无人驾驶为例，误判率在不断下降，但误判的影响还是很严重的，有可能从已有的判断case(样例)得到修正，从而完全避免这样的错误吗？</strong></p>
<blockquote>
<p><font color="MediumSeaGreen">A10</font>：<strong>无人驾驶</strong>中，任何一次出现的错误，都可能带来毁灭性的灾难。大家可能看到，特斯拉今天撞了，明天又撞了。所以说，无人驾驶对于<strong>错误率</strong>确实是非常注重的。</p>
<p>机器学习在学术界现在有很多关于<strong>uncertainty</strong>或者<strong>robustness</strong>的研究，就是说模型在数据偏移或者极端情况下会不会给出很不好的答案，我们不会特别深入去讲这个事情，但是无人驾驶这一块确实会通过大量的技术，比如说把不同的模型融合在一起，不是仅仅train一个模型，用多个模型来做投票。汽车有很多雷达、摄像头，它会通过不同的传感器来进行模型的融合，从而降低误差。</p>
<p>我们这个课程不会特别地讲，因为涉及到评价无人驾驶的特别技术，但在竞赛中我们会给大家看到如何通过<strong>融合多个模型提升精度</strong>的做法。</p>
</blockquote>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>01-深度学习课程安排</title>
    <url>/2024/04/23/11-01-01/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="01-课程安排"><a href="#01-课程安排" class="headerlink" title="01-课程安排"></a>01-课程安排</h2><h3 id="1-课程目标"><a href="#1-课程目标" class="headerlink" title="1. 课程目标"></a>1. 课程目标</h3><ul>
<li>介绍深度学习经典和最新模型<ul>
<li>LeNet, ResNet, LSTM, BERT, …</li>
</ul>
</li>
<li>机器学习基础<ul>
<li>损失函数、目标函数、过拟合、优化</li>
</ul>
</li>
<li>实践<ul>
<li>使用Pytorch实现介绍的知识点</li>
<li>在真实数据上体验算法效果</li>
</ul>
</li>
</ul>
<h3 id="2-内容"><a href="#2-内容" class="headerlink" title="2. 内容"></a>2. 内容</h3><blockquote>
<p>深度学习基础：线性神经网络，多层感知机</p>
<p>卷积神经网络：LeNet, AlexNet, VGG, Inception, ResNet</p>
<p>循环神经网络：RNN，GRU，LSTM，seq2seq</p>
<p>注意力机制：Attention， Transformer</p>
<p>优化算法：SGD，Momentum，Adam</p>
<p>高性能计算：并行，多GPU，分布式</p>
<p>计算机视觉：目标检测，语义分割</p>
<p>自然语言处理：词嵌入，BERT</p>
</blockquote>
<h3 id="3-学到什么"><a href="#3-学到什么" class="headerlink" title="3. 学到什么"></a>3. 学到什么</h3><ul>
<li>What：深度学习有哪些技术，以及哪些技术可以帮你解决问题</li>
<li>How：如何实现（产品 or paper）和调参（精度or速度）</li>
<li>Why：背后的原因（直觉、数学）</li>
</ul>
<h3 id="4-基本要求"><a href="#4-基本要求" class="headerlink" title="4. 基本要求"></a>4. 基本要求</h3><ul>
<li><strong>AI相关从业人员</strong>（产品经理等）：掌握What，知道名词，能干什么</li>
<li><strong>数据科学家、工程师</strong>：掌握What、How，手要快，能出活</li>
<li><strong>研究员、学生</strong>：掌握What、How、Why，除了知道有什么和怎么做，还要知道为什么，思考背后的原因，做出新的突破</li>
</ul>
<h3 id="5-课程资源"><a href="#5-课程资源" class="headerlink" title="5. 课程资源"></a>5. 课程资源</h3><ul>
<li>课程主页：<a href="https://courses.d2l.ai/zh-v2/">https://courses.d2l.ai/zh-v2/</a></li>
<li>教材：<a href="https://zh-v2.d2l.ai/">https://zh-v2.d2l.ai/</a></li>
<li>课程论坛讨论：<a href="https://discuss.d2l.ai/c/chinese-version/16">https://discuss.d2l.ai/c/chinese-version/16</a></li>
<li>Pytorch论坛：<a href="https://discuss.pytorch.org/">https://discuss.pytorch.org/</a></li>
<li>b站视频合集：[<a href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497](">https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497](</a></li>
</ul>
<h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h3><ul>
<li>课程目标、内容和要求</li>
<li>相关课程资源链接</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>00-深度学习预告</title>
    <url>/2024/04/23/11-01-00/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.24：初稿</li>
</ul>
<h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><ul>
<li><a href="/2024/04/24/12-18-34/" title="动手学深度学习笔记汇总">动手学深度学习笔记汇总</a>
</li>
</ul>
<h2 id="00-预告"><a href="#00-预告" class="headerlink" title="00-预告"></a>00-预告</h2><h3 id="1-学习深度学习关键是动手"><a href="#1-学习深度学习关键是动手" class="headerlink" title="1. 学习深度学习关键是动手"></a>1. 学习深度学习关键是动手</h3><ul>
<li>深度学习是人工智能最热的领域</li>
<li>核心是神经网络</li>
<li>神经网络是一门语言</li>
<li><p>应该像学习Python/C++一样学习深度学习</p>
<h3 id="2-参考书目《动手学深度学习》"><a href="#2-参考书目《动手学深度学习》" class="headerlink" title="2. 参考书目《动手学深度学习》"></a>2. 参考书目《动手学深度学习》</h3></li>
<li><p>是一本深度学习的教科书</p>
<ul>
<li><p>覆盖90年代至今的重要模型</p>
</li>
<li><p>每一章是一个Jupyter记事本</p>
<ul>
<li>提供所有模型的完整实现</li>
<li>在真实数据上运行</li>
</ul>
</li>
<li><p><strong>原书链接</strong>：<a href="https://zh.d2l.ai/">动手学深度学习(原书中文版)</a></p>
</li>
<li><p><strong>项目链接</strong>：<a href="https://github.com/d2l-ai/d2l-zh">Github项目</a></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>英文版：2019年推出<br>重写了所有章节</p>
<ul>
<li>加入50%新内容(如Transformer)<br>Numpy/MXNet, Pytorch和TensorFlow2.0的代码实现</li>
</ul>
</li>
</ul>
<ul>
<li>课程对应网站：<a href="https://courses.d2l.ai/zh-v2/">课程主页</a>(包含原视频<strong>课件</strong>等资源)</li>
</ul>
<ul>
<li>b站<strong>视频</strong>网站：<a href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497">视频合集</a></li>
</ul>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h3><ul>
<li>如何学习深度学习——动手跑代码</li>
<li>课程资源：书籍、视频、课件等</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记四 神经网络的表达</title>
    <url>/2024/04/22/13-37-13/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.22：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2024/04/18/15-40-33/" title="机器笔记汇总">吴恩达机器学习 - 笔记汇总</a>
</li>
</ul>
<h1 id="8-神经网络：表达（Neural-Networks-Representation）"><a href="#8-神经网络：表达（Neural-Networks-Representation）" class="headerlink" title="8 神经网络：表达（Neural Networks: Representation）"></a>8 神经网络：表达（Neural Networks: Representation）</h1><h2 id="8-1-非线性假设（Non-linear-Hypotheses）"><a href="#8-1-非线性假设（Non-linear-Hypotheses）" class="headerlink" title="8.1 非线性假设（Non-linear Hypotheses）"></a>8.1 非线性假设（Non-linear Hypotheses）</h2><p>理论上我们可以用多项式函数去近似任意函数（泰勒极数（Taylor series）），从而可得到任意问题的拟合曲线。</p>
<p>在实际处理时，特征量通常会很多，如果再构造高阶多项式等，特征数量将会急剧增加，这使得回归模型的复杂度太高，可见并不合适。神经网络无需构造高阶多项式，在特征量很大时也可以处理的很好。</p>
<p>那特征能有多大呢？下面是一个计算机视觉中的例子：</p>
<img src="/2024/04/22/13-37-13/20180115_084326.png" class>
<p>如上图，如果选取一小块 $50 * 50$ 像素的灰度图片（一个像素只有亮度一个值），选择每个像素点作为特征，则特征总量 $n=2500$（换成 RGB（一个像素有三个值），则 $n = 7500$），如果将其两两组合作为新特征，则特征数量将为 $C_{2500}^{2} \approx 3\ million$。</p>
<h2 id="8-2-神经网络和大脑（Neurons-and-the-Brain）"><a href="#8-2-神经网络和大脑（Neurons-and-the-Brain）" class="headerlink" title="8.2 神经网络和大脑（Neurons and the Brain）"></a>8.2 神经网络和大脑（Neurons and the Brain）</h2><p>脑科学家通过对动物实验，发现大脑中专用于处理听觉信号的脑皮层也能处理其他诸如视觉等信号，即如果切断其与耳朵的联系，将其与眼睛相连，则这块负责听觉的脑皮层区域也能接受并处理视觉信号，从而学会“看”。脑科学家通过这类换源实验，就推论假设大脑的学习算法只有一种（“one learning algorithm” hypothesis）。那么如果能找出这种学习算法并应用于计算机中，那梦想中和人一样的人工智能就成真了。</p>
<p>神经网络就源于<strong>模拟人类大脑</strong>，但其需要的计算量很大。随着计算机硬件性能的提高，神经网络逐渐从衰落变为流行，如今已广泛地被应用在各行各业中。</p>
<p>下图是根据研究做的一些应用（有兴趣可回顾视频）：</p>
<img src="/2024/04/22/13-37-13/20180115_101441.png" class>
<p>BrainPort  系统：帮助失明人士通过摄像头以及舌尖感官“看”东西</p>
<img src="/2024/04/22/13-37-13/20180115_101442.png" class>
<p>触觉皮带：在朝北时蜂鸣器会发出声响，可使人拥有方向感（声音信号转换为方向信号）。</p>
<h2 id="8-3-模型表示1（Model-Representation-I）"><a href="#8-3-模型表示1（Model-Representation-I）" class="headerlink" title="8.3 模型表示1（Model Representation I）"></a>8.3 模型表示1（Model Representation I）</h2><p>大脑神经元</p>
<img src="/2024/04/22/13-37-13/20141213201613758.jpg" class>
<p>想象一下印刷厂中流水线的工人，每个工人都有特定的任务，比如装订，塑封，贴防伪标识等等，工人们看到书本并处理完自己的任务后，就回放回传送带，紧接着传送带就传给下一个环节的工人，如此不断重复从而完成一个又一个环节，直到一本书印制完成。</p>
<p>那么类比一下，把上图中的<strong>细胞核（nucleus）</strong>类比成工人，<strong>轴突（axon）</strong>类比传送带，<strong>树突（dendrite）</strong>则比类比成工人的双眼。一个又一个细胞体，从树突接收需要处理的信息，对其进行处理后，再经由轴突通过电信号把处理完的信息传递出去，直到理解信息的内容。</p>
<p>人工神经网络中，树突对应<strong>输入（input）</strong>，细胞核对应<strong>激活单元（activation unit）</strong>，轴突对应<strong>输出（output）</strong>。</p>
<p>我们一般把神经网络划分为三部分（注意，不是只有三层！），即输入层（input layer），隐藏层（hidden layer）和输出层（output layer）。</p>
<img src="/2024/04/22/13-37-13/20180116_001543.png" class>
<p>图中的一个圈表示神经网络中的一个激活单元，输入层对应输入单元，隐藏层对应中间单元，输出层则对应输出单元。中间激活单元应用<strong>激活函数</strong>处理数据。</p>
<p>下面列出一些已有概念在神经网络中的别称：</p>
<ul>
<li>$x_0$: 偏置单元（bias unit），$x_0$=1</li>
<li>$\Theta$: 权重（weight），即参数。</li>
<li>激活函数: $g$，即逻辑函数等。</li>
<li>输入层: 对应于训练集中的特征 $x$。</li>
<li>输出层: 对应于训练集中的结果 $y$。</li>
</ul>
<blockquote>
<p>$a^{(j)}_i$: 第 $j$ 层的第 $i$ 个激活单元</p>
<p>$\Theta^{(j)}$: 从第 $j$ 层映射到第 $j+1$ 层时的权重矩阵。</p>
<p>$\Theta^{(j)}_{v,u}$: 从第 $j$ 层的第 $u$ 个单元映射到第 $j+1$ 层的第 $v$ 个单元的权重</p>
<p>$s_j$: 第 $j$ 层的激活单元数目（不包含偏置单元）</p>
</blockquote>
<p>注意：</p>
<ul>
<li><strong>每个单元会作用于下一层的所有单元</strong>（矩阵乘法运算）。</li>
<li>如果第 $j$ 层有 $s_j$ 个单元，第 $j+1$ 层有 $s_{j+1}$ 个单元，$\Theta^{(j)}$ 是一个 $s_{j+1} \times (s_j+1)$ 维的权重矩阵。即每一层的权重矩阵大小都是非固定的。</li>
<li>其中，$+1$ 来自于偏置单元，这样意味着输出层不包含偏置单元，输入层和隐藏层需要增加偏置单元。</li>
</ul>
<p>依据本节所给模型，有：</p>
<p>$Size(\Theta^{(1)})=s_{j+1} \times (s_j + 1) =s_2 \times (s_1 + 1) = 3 \times 4$</p>
<p>$Size(\Theta^{(2)})=s_3 \times (s_2 + 1) = 1 \times 4$</p>
<h2 id="8-4-模型表示2（Model-Representation-II）"><a href="#8-4-模型表示2（Model-Representation-II）" class="headerlink" title="8.4 模型表示2（Model Representation II）"></a>8.4 模型表示2（Model Representation II）</h2><img src="/2024/04/22/13-37-13/20180116_001543.png" class>
<p>对输入层（Layer 1）的所有激活单元应用激活函数，从而得到隐藏层（Layer 2）中激活单元的值：</p>
<p>$$<br>\begin{split} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3)\\<br>a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3)\\<br>a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3)<br>\end{split}<br>$$<br>对 Layer 2 中的所有激活单元应用激活函数，从而得到输出：</p>
<p>$h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})$</p>
<p>上面的计算过程被称为<strong>前向传播（Forward propagation）</strong>，即从输入层开始，一层一层地向下计算并传递结果。</p>
<p>再回顾一下逻辑回归：</p>
<p>${h_\theta}\left( x \right)=g\left( {\theta_0}+{\theta_1}{x_1}+{\theta_{2} }{x_{2} }+{\theta_{3} }x_3 \right)$</p>
<p>是不是除了符号表示，其他都完全一样？其实神经网络就好似回归模型，只不过输入变成了中间单元 $a_1^{(j)}, a_2^{(j)}, \dots, a_n^{(j)}$。从输入 $x$ 开始，下一层的每个激活单元都包含了上一层的所有信息（单元值），通过最优化算法不断迭代计算，激活单元能得出关于输入 $x$ 的更多信息，这就好像是在给假设函数加多项式。隐藏层的这些单元好似升级版的初始特征，从而能给出更好的预测。</p>
<p><strong>向量化实现</strong></p>
<p>定义 $a^{(1)}=x=\left[ \begin{matrix}x_0\\ x_1 \\ x_2 \\ x_3 \end{matrix} \right]$，$\Theta^{(1)}=\left[\begin{matrix}\Theta^{(1)}_{10}&amp; \Theta^{(1)}_{11}&amp; \Theta^{(1)}_{12}&amp; \Theta^{(1)}_{13}\\ \Theta^{(1)}_{20}&amp; \Theta^{(1)}_{21}&amp; \Theta^{(1)}_{22}&amp; \Theta^{(1)}_{23}\\ \Theta^{(1)}_{30}&amp; \Theta^{(1)}_{31}&amp; \Theta^{(1)}_{32} &amp; \Theta^{(1)}_{33}\end{matrix}\right]$，</p>
<p>$\begin{split}a_1^{(2)} = g(z_1^{(2)}) \\ a_2^{(2)} = g(z_2^{(2)}) \newline a_3^{(2)} = g(z_3^{(2)}) \newline \end{split}$，$z^{(2)}=\left[ \begin{matrix}z_1^{(2)}\\ z_1^{(2)} \\ z_1^{(2)}\end{matrix} \right]$</p>
<p>则有 $a^{(2)}= g(\Theta^{(1)}a^{(1)})=g(z^{(2)})$</p>
<p>预测结果即 $h_\Theta(x) = a^{(3)} = g(\Theta^{(2)}a^{(2)}) = g(z^{(3)})$</p>
<p>即有 $z^{(j)}_i = \Theta^{(j-1)}_{i,0}a^{(j-1)}_{0}+ \Theta^{(j-1)}_{i,1}a^{(j-1)}_{1}+\dots+ \Theta^{(j-1)}_{i,n}a^{(j-1)}_{n}$，</p>
<p> $z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$，$a^{(j)} = g(z^{(j)})$，通过该式即可计算神经网络中每一层的值。</p>
<p>扩展到所有样本实例：</p>
<p>${ {z}^{\left( 2 \right)} }={ {\Theta }^{\left( 1 \right)} } { {X}^{T} }$，这时 $z^{(2)}$ 是一个 $s_2 \times m$ 维矩阵。</p>
<blockquote>
<p>$m$: 训练集中的样本实例数量</p>
<p>$s_2$: 第二层神经网络中激活单元的数量</p>
</blockquote>
<p>当然，神经网络可有多层，每层的激活单元数量也并不固定：</p>
<img src="/2024/04/22/13-37-13/20180116_105545.png" class>
<blockquote>
<p>我们习惯于将输入层称为神经网络的第 0 层，如上图的神经网络被称为三层网络。</p>
</blockquote>
<h2 id="8-5-例子和直观理解1（Examples-and-Intuitions-I）"><a href="#8-5-例子和直观理解1（Examples-and-Intuitions-I）" class="headerlink" title="8.5 例子和直观理解1（Examples and Intuitions I）"></a>8.5 例子和直观理解1（Examples and Intuitions I）</h2><p>为了更好的理解神经网络，举例单层神经网络进行逻辑运算的例子。</p>
<p>下面的例子中，$x_1,x_2$ 为二进制数。</p>
<p>逻辑与（AND）运算（都为真值则结果才为真）神经网络：</p>
<img src="/2024/04/22/13-37-13/20180117_000612.png" class>
<p>$\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix}$，$h_\Theta(x) = g(-30+20x_1+20x_2)$。</p>
<p>回顾 sigmoid 函数图像，根据输入则有上图中右边的表格，即 $h_\theta(x)\approx x_1\ \text{AND}\ x_2$。这样就实现了一个能够进行与运算的神经网络。 </p>
<img src="/2024/04/22/13-37-13/2413fbec8ff9fa1f19aaf78265b8a33b_Logistic_function.png" class>
<p>再举一例，逻辑或（OR）运算（有一个真值则结果就为真）神经网络：</p>
<img src="/2024/04/22/13-37-13/20180117_000349.png" class>
<h2 id="8-6-例子和直观理解2（Examples-and-Intuitions-II）"><a href="#8-6-例子和直观理解2（Examples-and-Intuitions-II）" class="headerlink" title="8.6 例子和直观理解2（Examples and Intuitions II）"></a>8.6 例子和直观理解2（Examples and Intuitions II）</h2><p>下面逐步构建复杂一点的神经网络</p>
<img src="/2024/04/22/13-37-13/20180117_004820.png" class>
<p>如上图，我们分别构建了三个单层神经网络，将这三个网络组合起来，可得到一个新的神经网络，其可完成逻辑运算中的异或（XNOR）操作：</p>
<img src="/2024/04/22/13-37-13/20180116_235545.png" class>
<p>这里的组合即为 $\text{XNOR}=( \text{x}_1\, \text{AND}\, \text{x}_2 )\, \text{OR} \left( \left( \text{NOT}\, \text{x}_1 \right) \text{AND} \left( \text{NOT}\, \text{x}_2 \right) \right)$</p>
<p>$\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20 \newline 10 &amp; -20 &amp; -20\end{bmatrix}$，$\Theta^{(2)} =\begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix}$，$\begin{split}&amp; a^{(2)} = g(\Theta^{(1)} \cdot x) \newline&amp; a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)}) \newline&amp; h_\Theta(x) = a^{(3)}\end{split}$</p>
<p>可见，特征值能不断升级，并抽取出更多信息，直到计算出结果。而如此不断组合，我们就可以逐渐构造出越来越复杂、强大的神经网络，比如用于手写识别的神经网络。</p>
<h2 id="8-7-多类别分类（Multiclass-Classification）"><a href="#8-7-多类别分类（Multiclass-Classification）" class="headerlink" title="8.7 多类别分类（Multiclass Classification）"></a>8.7 多类别分类（Multiclass Classification）</h2><p>之前讨论的都是预测结果为单值情况下的神经网络，要实现多类别分类，其实只要修改一下输出层，让输出层包含多个输出单元即可。</p>
<p>举一个 4 分类问题的实例：</p>
<img src="/2024/04/22/13-37-13/20180117_010904.png" class>
<p>有四种分类情况，那么就让输出层包含 4 个输出单元即可，则 $h_\Theta$ 为 4 维向量。 </p>
<p>神经网络中的多分类算法算是对 one-vs-all 思想的扩展，定义预测结果一共有 4 种情况：</p>
<img src="/2024/04/22/13-37-13/20180117_011331.png" class>
<p>如果预测结果 $h_\Theta(x) =\begin{bmatrix}0 \newline 0 \newline 1 \newline 0 \newline\end{bmatrix}$，那么表示 $h_\Theta(x)_3$，即分为第 3 类，对应于图中的摩托车（Motorcycle）。</p>
<p><strong>总结一下</strong></p>
<p>多分类问题，要分为 $K$ 类，就在输出层放置 $K$ 个输出单元，对于单个样本实例，预测向量 $h_\Theta(x)$ 为 $K$ 维向量，我们则依据这个预测向量，得出该实例属于哪个类 $y^{(i)}$。注意，神经网络中的预测和结果都是 $K$ 维向量，而不再只是一个实数了。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习, 深度学习, 吴恩达</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记三 多变量线性回归</title>
    <url>/2024/04/22/13-37-10/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.22：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2024/04/18/15-40-33/" title="机器笔记汇总">吴恩达机器学习 - 笔记汇总</a>
</li>
</ul>
<h1 id="6-逻辑回归（Logistic-Regression）"><a href="#6-逻辑回归（Logistic-Regression）" class="headerlink" title="6 逻辑回归（Logistic Regression）"></a>6 逻辑回归（Logistic Regression）</h1><h2 id="6-1-分类（Classification）"><a href="#6-1-分类（Classification）" class="headerlink" title="6.1 分类（Classification）"></a>6.1 分类（Classification）</h2><p>在分类问题中，预测的结果是离散值（结果是否属于某一类），逻辑回归算法（Logistic Regression）被用于解决这类分类问题。</p>
<ul>
<li>垃圾邮件判断</li>
<li>金融欺诈判断</li>
<li>肿瘤诊断</li>
</ul>
<p>讨论肿瘤诊断问题：</p>
<img src="/2024/04/22/13-37-10/20180109_144040.png" class>
<p>肿瘤诊断问题的目的是告诉病人<strong>是否</strong>为恶性肿瘤，是一个<strong>二元分类问题（binary class problems）</strong>，则定义 $ y \in\lbrace 0, 1\rbrace$，其中 0 表示<strong>负向类（negative class）</strong>，代表恶性肿瘤（”-“），1 为<strong>正向类（positive class）</strong>，代表良性肿瘤（”+”）。如图，定义最右边的样本为<strong>偏差项</strong>。</p>
<p>在未加入偏差项时，线性回归算法给出了品红色的拟合直线，若规定</p>
<p>$h_\theta(x) \geqslant 0.5$ ，预测为 $y = 1$，即正向类；</p>
<p>$h_\theta(x) \lt 0.5$ ，预测为 $y = 0$，即负向类。</p>
<p>即以 0.5 为<strong>阈值（threshold）</strong>，则我们就可以根据线性回归结果，得到相对正确的分类结果 $y$。</p>
<p>接下来加入偏差项，线性回归算法给出了靛青色的拟合直线，如果阈值仍然为 0.5，可以看到算法在某些情况下会给出完全错误的结果的。</p>
<p>不仅如此，线性回归算法的值域为全体实数集（$h_\theta(x) \in R$）</p>
<p>逻辑回归算法是一个分类算法，<strong>其输出值永远在 0 到 1 之间</strong>，即 $h_\theta(x) \in (0,1)$。</p>
<h2 id="6-2-假设函数表示（Hypothesis-Representation"><a href="#6-2-假设函数表示（Hypothesis-Representation" class="headerlink" title="6.2 假设函数表示（Hypothesis Representation"></a>6.2 假设函数表示（Hypothesis Representation</h2><p>为了使 $h_\theta(x) \in \left(0, 1\right)$，引入逻辑回归模型，定义假设函数<br>$$<br>h_\theta \left( x \right)=g(z)=g\left(\theta^{T}x \right)<br>$$<br>对比线性回归函数 $h_\theta \left( x \right)=\theta^{T}x$，$g$ 表示逻辑函数logistic function，复合起来，则称为逻辑回归函数。</p>
<p>逻辑函数是 S 形函数，会将所有实数映射到 $(0, 1)$ 范围。</p>
<p>sigmoid 函数（如下图）是逻辑函数的特殊情 况，其公式为 $g\left( z \right)=\frac{1}{1+{ {e}^{-z} } }$。 </p>
<img src="/2024/04/22/13-37-10/2413fbec8ff9fa1f19aaf78265b8a33b_Logistic_function.png" class>
<p>应用 sigmoid 函数，则逻辑回归模型：$$h_{\theta}(x)=g(\theta^Tx) =\frac{1}{1+e^{-\theta^Tx} }$$</p>
<p>逻辑回归模型中，$h_\theta \left( x \right)$ 的作用是，根据输入 $x$ 以及参数 $\theta$，计算得出”输出 $y=1$“的可能性（estimated probability），概率学中表示为：</p>
<p>$$<br>\begin{split}<br>&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \\<br>&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1<br>\end{split}<br>$$<br>以肿瘤诊断为例，$h_\theta \left( x \right)=0.7$ 表示病人有 $70\%$ 的概率得了恶性肿瘤。</p>
<h2 id="6-3-决策边界（Decision-Boundary）"><a href="#6-3-决策边界（Decision-Boundary）" class="headerlink" title="6.3 决策边界（Decision Boundary）"></a>6.3 决策边界（Decision Boundary）</h2><p>决策边界的概念，可帮助我们更好地理解逻辑回归模型的拟合原理。</p>
<p>在逻辑回归中，有假设函数 $h_\theta \left( x \right)=g(z)=g\left(\theta^{T}x \right)$。</p>
<p>为了得出分类的结果，这里和前面一样，规定以 $0.5$ 为阈值：</p>
<p>$$<br>\begin{split}<br>&amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \\<br>&amp; h_\theta(x) &lt; 0.5 \rightarrow y = 0 \\<br>\end{split}<br>$$<br>回忆一下 sigmoid 函数的图像：</p>
<img src="/2024/04/22/13-37-10/2413fbec8ff9fa1f19aaf78265b8a33b_Logistic_function.png" class>
<p>观察可得当 $g(z) \geq 0.5$ 时，有 $z \geq 0$，即 $\theta^Tx \geq 0$。</p>
<p>同线性回归模型的不同点在于：<br>$$<br>\begin{split}<br>z \to +\infty, e^{-\infty} \to 0 \Rightarrow g(z)=1 \\<br>z \to -\infty, e^{\infty}\to \infty \Rightarrow g(z)=0<br>\end{split}<br>$$<br>直观一点来个例子，${h_\theta}\left( x \right)=g\left( {\theta_0}+{\theta_1}{x_1}+{\theta_{2} }{x_{2} }\right)$ 是下图模型的假设函数：</p>
<img src="/2024/04/22/13-37-10/20180111_000814.png" class>
<p>根据上面的讨论，要进行分类，那么只要 $ {\theta_0}+{\theta_1}{x_1}+{\theta_{2} }{x_{2} }\geq0$ 时，就预测 $y = 1$，即预测为正向类。</p>
<p>如果取 $\theta = \begin{bmatrix} -3\\1\\1\end{bmatrix}$，则有 $z = -3+{x_1}+{x_2}$，当 $z \geq 0$ 即 ${x_1}+{x_2} \geq 3$ 时，易绘制图中的品红色直线即<strong>决策边界</strong>，为正向类（以红叉标注的数据）给出 $y=1$ 的分类预测结果。</p>
<p>上面讨论了逻辑回归模型中线性拟合的例子，下面则是一个多项式拟合的例子，和线性回归中的情况也是类似的。</p>
<p>为了拟合下图数据，建模多项式假设函数：</p>
<p>$$<br>{h_\theta}\left( x \right)=g\left( {\theta_0}+{\theta_1}{x_1}+{\theta_{2} }{x_{2} }+{\theta_{3} }x_{1}^{2}+{\theta_{4} }x_{2}^{2} \right)<br>$$<br>这里取 $\theta = \begin{bmatrix} -1\\0\\0\\1\\1\end{bmatrix}$，决策边界对应了一个在原点处的单位圆（${x_1}^2+{x_2}^2 = 1$），如此便可给出分类结果，如图中品红色曲线：</p>
<img src="/2024/04/22/13-37-10/20180111_000653.png" class>
<p>当然，通过一些更为复杂的多项式，还能拟合那些图像显得非常怪异的数据，使得决策边界形似碗状、爱心状等等。</p>
<p>简单来说，决策边界就是<strong>分类的分界线</strong>。</p>
<h2 id="6-4-代价函数（Cost-Function）"><a href="#6-4-代价函数（Cost-Function）" class="headerlink" title="6.4 代价函数（Cost Function）"></a>6.4 代价函数（Cost Function）</h2><p>那我们怎么知道决策边界是啥样？$\theta$ 多少时能很好的拟合数据？当然，见招拆招，总要来个 $J(\theta)$。</p>
<p>如果直接套用线性回归的代价函数： $J\left( {\theta} \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ { {\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2} } }$</p>
<p>其中 $h_\theta(x) = g\left(\theta^{T}x \right)$，可绘制关于 $J(\theta)$ 的图像，如下图</p>
<img src="/2024/04/22/13-37-10/20180111_080314.png" class>
<p><strong>同济高数教材关于凸函数的定义是反的</strong><br>回忆线性回归中的平方损失函数，其是一个二次凸函数（碗状），二次凸函数的重要性质是只有一个局部最小点即全局最小点。上图中有许多局部最小点，这样将使得梯度下降算法无法确定收敛点是全局最优。</p>
<img src="/2024/04/22/13-37-10/20180111_080514.png" class>
<p>如果此处的损失函数也是一个凸函数，是否也有同样的性质，从而最优化？这类讨论凸函数最优值的问题，被称为<strong>凸优化问题（Convex optimization）</strong>。</p>
<p>当然，损失函数不止平方损失函数一种。</p>
<p>对于逻辑回归，更换平方损失函数为<strong>对数损失函数</strong>，可由统计学中的最大似然估计方法推出代价函数 $J(\theta)$：</p>
<p>$$<br>\begin{split}<br>&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \\<br>&amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \\<br>&amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}<br>\end{split}<br>$$<br>则有关于 $J(\theta)$ 的图像如下：</p>
<img src="/2024/04/22/13-37-10/20180111_080614.png" class>
<p>如左图，当训练集的结果为 $y=1$（正样本）时，随着假设函数趋向于 $1$，代价函数的值会趋于 $0$，即意味着拟合程度很好。如果假设函数此时趋于 $0$，则会给出一个<strong>很高的代价</strong>，拟合程度<strong>差</strong>，算法会根据其迅速纠正 $\theta$ 值，右图 $y=0$ 同理。</p>
<p>区别于平方损失函数，对数损失函数也是一个凸函数，但没有局部最优值。</p>
<h2 id="6-5-简化的成本函数和梯度下降（Simplified-Cost-Function-and-Gradient-Descent）"><a href="#6-5-简化的成本函数和梯度下降（Simplified-Cost-Function-and-Gradient-Descent）" class="headerlink" title="6.5 简化的成本函数和梯度下降（Simplified Cost Function and Gradient Descent）"></a>6.5 简化的成本函数和梯度下降（Simplified Cost Function and Gradient Descent）</h2><p>不分类讨论，对于二元分类问题，我们可把代价函数<strong>简化</strong>为一个函数：<br>$Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$</p>
<p>成本函数：<br>$J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]$</p>
<p>向量化实现：$J(\theta)$应该是一个对角线矩阵</p>
<p>$h = g(X\theta)$，$J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)$</p>
<p>为了最优化 $\theta$，仍使用梯度下降法，算法同线性回归中一致：</p>
<p>$$<br>\begin{split}<br>&amp; \text{Repeat until convergence:} \; \lbrace \\<br>&amp;{ {\theta }_{j} }:={ {\theta }_{j} }-\alpha \frac{\partial }{\partial { {\theta }_{j} } }J\left( {\theta}  \right) \\<br>\rbrace<br>\end{split}<br>$$</p>
<p>解出偏导得：</p>
<p>$$<br>\begin{split}<br>&amp; \text{Repeat until convergence:} \; \lbrace \\<br>&amp; \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; &amp; \text{for j := 0,1…n}\\<br>\rbrace<br>\end{split}<br>$$</p>
<p>注意，虽然形式上梯度下降算法同线性回归一样，但其中的假设函不同，即$h_\theta(x) = g\left(\theta^{T}x \right)$，不过求导后的结果也相同。</p>
<p>向量化实现：$\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - y)$</p>
<p><strong>逻辑回归中代价函数求导的推导过程：</strong><br>$$<br>J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]<br>$$<br>令 $f(\theta) = { {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)$</p>
<p>以及, $h_\theta(x) = g(z)$，$g(z) = \frac{1}{1+e^{(-z)} }$，则</p>
<p>$$<br>\begin{split}<br>f(\theta) &amp;= { {y}^{(i)} }\log \left( \frac{1}{1+{ {e}^{-z} } } \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-\frac{1}{1+{ {e}^{-z} } } \right) \\<br>&amp;= -{ {y}^{(i)} }\log \left( 1+{ {e}^{-z} } \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1+{ {e}^{z} } \right)<br>\end{split}<br>$$</p>
<p>以及，$z=\theta^Tx^{(i)}$，对 $\theta_j$ 求偏导，则没有 $\theta_j$ 的项求偏导即为 $0$，都消去，则得：</p>
<p>$$<br>\frac{\partial z}{\partial {\theta_{j} } }=\frac{\partial }{\partial {\theta_{j} } }\left( \theta^Tx^{(i)}  \right)=x^{(i)}_j<br>$$<br>所以有：</p>
<p>$$<br>\begin{split}<br>\frac{\partial }{\partial {\theta_{j} } }f\left( \theta  \right)&amp;=\frac{\partial }{\partial {\theta_{j} } }[-{ {y}^{(i)} }\log \left( 1+{ {e}^{-z} } \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1+{ {e}^{z} } \right)] \\<br>&amp;=-{ {y}^{(i)} }\frac{\frac{\partial }{\partial {\theta_{j} } }\left(-z \right) e^{-z} }{1+e^{-z} }-\left( 1-{ {y}^{(i)} } \right)\frac{\frac{\partial }{\partial {\theta_{j} } }\left(z \right){e^{z} } }{1+e^{z} } \\<br>&amp;=-{ {y}^{(i)} }\frac{-x^{(i)}_je^{-z} }{1+e^{-z} }-\left( 1-{ {y}^{(i)} } \right)\frac{x^{(i)}_j}{1+e^{-z} } \\<br>&amp;=\left({ {y}^{(i)} }\frac{e^{-z} }{1+e^{-z} }-\left( 1-{ {y}^{(i)} } \right)\frac{1}{1+e^{-z} }\right)x^{(i)}_j \\<br>&amp;=\left({ {y}^{(i)} }\frac{e^{-z} }{1+e^{-z} }-\left( 1-{ {y}^{(i)} } \right)\frac{1}{1+e^{-z} }\right)x^{(i)}_j \\<br>&amp;=\left(\frac{ { {y}^{(i)} }(e^{-z}+1)-1}{1+e^{-z} }\right)x^{(i)}_j \\<br>&amp;={({ {y}^{(i)} }-\frac{1}{1+{ {e}^{-z} } })x_j^{(i)} } \\<br>&amp;={\left({ {y}^{(i)} }-{h_\theta}\left( { {x}^{(i)} } \right)\right)x_j^{(i)} } \\<br>&amp;=-{\left({h_\theta}\left( { {x}^{(i)} } \right)-{ {y}^{(i)} }\right)x_j^{(i)} }<br>\end{split}<br>$$</p>
<p>则可得代价函数的导数：</p>
<p>$$<br>\frac{\partial }{\partial {\theta_{j} } }J(\theta) = -\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{\partial }{\partial {\theta_{j} } }f(\theta)}=\frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}<br>$$</p>
<h2 id="6-6-进阶优化（Advanced-Optimization）"><a href="#6-6-进阶优化（Advanced-Optimization）" class="headerlink" title="6.6 进阶优化（Advanced Optimization）"></a>6.6 进阶优化（Advanced Optimization）</h2><p>我们编写代码给出代价函数及其偏导数然后传入梯度下降算法中，接下来算法则会为我们最小化代价函数给出参数的最优解。这类算法被称为<strong>最优化算法（Optimization Algorithms）</strong>，梯度下降算法不是唯一的最小化算法。</p>
<p>一些最优化算法：</p>
<ul>
<li>梯度下降法（Gradient Descent）</li>
<li>共轭梯度算法（Conjugate gradient）</li>
<li>牛顿法和拟牛顿法（Newton’s method &amp; Quasi-Newton Methods）<ul>
<li>DFP算法</li>
<li>局部优化法（BFGS）</li>
<li>有限内存局部优化法（L-BFGS）</li>
</ul>
</li>
<li>拉格朗日乘数法（Lagrange multiplier）</li>
</ul>
<p>比较梯度下降算法：一些最优化算法虽然会更为复杂，难以调试，不过这些算法通常效率更高，并无需选择学习速率 $\alpha$。</p>
<p>Octave/Matlab 中对这类高级算法做了封装，易于调用。</p>
<p>假设有 $J(\theta) = (\theta_1-5)^2 + (\theta_2-5)^2$，要求参数 $\theta=\begin{bmatrix} \theta_1\\\theta_2\end{bmatrix}$的最优值。</p>
<p>下面为 Octave/Matlab 求解最优化问题的代码实例：</p>
<ol>
<li>创建一个函数以返回代价函数及其偏导数：</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[jVal, gradient]</span> = <span class="title">costFunction</span><span class="params">(theta)</span></span></span><br><span class="line">  <span class="comment">% code to compute J(theta)</span></span><br><span class="line">  jVal=(theta(<span class="number">1</span>)<span class="number">-5</span>)^<span class="number">2</span>+(theta(<span class="number">2</span>)<span class="number">-5</span>)^<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">% code to compute derivative of J(theta)</span></span><br><span class="line">  gradient=<span class="built_in">zeros</span>(<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">  </span><br><span class="line">  gradient(<span class="number">1</span>)=<span class="number">2</span>*(theta(<span class="number">1</span>)<span class="number">-5</span>);</span><br><span class="line">  gradient(<span class="number">2</span>)=<span class="number">2</span>*(theta(<span class="number">2</span>)<span class="number">-5</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ol>
<li>将 <code>costFunction</code> 函数及所需参数传入最优化函数 <code>fminunc</code>，以求解最优化问题：</li>
</ol>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">options = optimset(<span class="string">&#x27;GradObj&#x27;</span>, <span class="string">&#x27;on&#x27;</span>, <span class="string">&#x27;MaxIter&#x27;</span>, <span class="number">100</span>);</span><br><span class="line">initialTheta = <span class="built_in">zeros</span>(<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>&#39;GradObj&#39;, &#39;on&#39;</code>: 启用梯度目标参数（则需要将梯度传入算法）</p>
<p><code>&#39;MaxIter&#39;, 100</code>: 最大迭代次数为 100 次</p>
<p><code>@xxx</code>: Octave/Matlab 中的函数指针</p>
<p><code>optTheta</code>: 最优化得到的参数向量</p>
<p><code>functionVal</code>: 引用函数最后一次的返回值</p>
<p><code>exitFlag</code>: 标记代价函数是否收敛</p>
</blockquote>
<p>注：Octave/Matlab 中可以使用 <code>help fminunc</code> 命令随时查看函数的帮助文档。</p>
<ol>
<li>返回结果</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optTheta =</span><br><span class="line"></span><br><span class="line">     5</span><br><span class="line">     5</span><br><span class="line"></span><br><span class="line">functionVal = 0</span><br><span class="line"></span><br><span class="line">exitFlag = 1</span><br></pre></td></tr></table></figure>
<h2 id="6-7-多类别分类-一对多（Multiclass-Classification-One-vs-all）"><a href="#6-7-多类别分类-一对多（Multiclass-Classification-One-vs-all）" class="headerlink" title="6.7 多类别分类: 一对多（Multiclass Classification: One-vs-all）"></a>6.7 多类别分类: 一对多（Multiclass Classification: One-vs-all）</h2><p>一直在讨论二元分类问题，这里谈谈多类别分类问题（比如天气预报）。</p>
<img src="/2024/04/22/13-37-10/20180112_001720.png" class>
<p>原理是，转化多类别分类问题为<strong>多个二元分类问题</strong>，这种方法被称为 One-vs-all。</p>
<p>正式定义：$h_\theta^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta  \right), i=\left( 1,2,3….k \right)$</p>
<blockquote>
<p>$h_\theta^{\left( i \right)}\left( x \right)$: 输出 $y=i$（属于第 $i$ 个分类）的可能性</p>
<p>$k$: 类别总数，如上图 $k=3$。</p>
</blockquote>
<p>注意多类别分类问题中 $h_\theta(x)$ 的结果不再只是一个实数而是一个向量，如果类别总数为 $k$，现在 $h_\theta(x)$ 就是一个 $k$ 维向量。</p>
<p>对于某个样本实例，需计算所有的 $k$ 种分类情况得到 $h_\theta(x)$，然后看分为哪个类别时预测输出的值最大，就说它输出属于哪个类别，即 $y = \mathop{\max}\limits_i\,h_\theta^{\left( i \right)}\left( x \right)$。</p>
<h1 id="7-正则化（Regularization）"><a href="#7-正则化（Regularization）" class="headerlink" title="7 正则化（Regularization）"></a>7 正则化（Regularization）</h1><h2 id="7-1-过拟合问题（The-Problem-of-Overfitting）"><a href="#7-1-过拟合问题（The-Problem-of-Overfitting）" class="headerlink" title="7.1 过拟合问题（The Problem of Overfitting）"></a>7.1 过拟合问题（The Problem of Overfitting）</h2><p>对于拟合的表现，可以分为三类情况：</p>
<ul>
<li><p><strong>欠拟合（Underfitting）</strong></p>
<p>无法很好的拟合训练集中的数据，预测值和实际值的误差很大，这类情况被称为欠拟合。拟合模型比较简单（特征选少了）时易出现这类情况。</p>
</li>
<li><p><strong>优良的拟合（Just right）</strong></p>
<p>不论是训练集数据还是不在训练集中的预测数据，都能给出较为正确的结果。</p>
</li>
<li><p><strong>过拟合（Overfitting）</strong></p>
<p>能很好甚至完美拟合训练集中的数据，即 $J(\theta) \to 0$，但是对于不在训练集中的<strong>新数据</strong>，预测值和实际值的误差会很大，<strong>泛化能力弱</strong>，这类情况被称为过拟合。拟合模型过于复杂（特征选多了）时易出现这类情况。</p>
</li>
</ul>
<p>线性模型中的拟合情况（左图欠拟合，右图过拟合）：<br><img src="/2024/04/22/13-37-10/20180112_091654.png" class></p>
<p>逻辑分类模型中的拟合情况：<br><img src="/2024/04/22/13-37-10/20180112_092027.png" class></p>
<p>为了度量拟合表现，引入：</p>
<ul>
<li><p>偏差（bias）</p>
<p>指模型的预测值与真实值的<strong>偏离程度</strong>。偏差越大，预测值偏离真实值越厉害。偏差低意味着能较好地反应训练集中的数据情况。</p>
</li>
<li><p>方差（Variance）</p>
<p>指模型预测值的<strong>离散程度或者变化范围</strong>。方差越大，数据的分布越分散，函数波动越大，泛化能力越差。方差低意味着拟合曲线的稳定性高，波动小。</p>
</li>
</ul>
<p>据此，我们有对同一数据的各类拟合情况如下图：<br><img src="/2024/04/22/13-37-10/20180112_085630.png" class></p>
<p>据上图，高偏差意味着欠拟合，高方差意味着过拟合。</p>
<p>我们应尽量使得拟合模型处于低方差（较好地拟合数据）状态且同时处于低偏差（较好地预测新值）的状态。</p>
<p>避免过拟合的方法有：</p>
<ul>
<li>减少特征的数量<ul>
<li>手动选取需保留的特征</li>
<li>使用模型选择算法来选取合适的特征（如 PCA 算法）</li>
<li>减少特征的方式易丢失有用的特征信息</li>
</ul>
</li>
<li>正则化（Regularization）<ul>
<li>可保留所有参数（许多有用的特征都能轻微影响结果）</li>
<li>减少/惩罚各参数大小（magnitude），以减轻各参数对模型的影响程度</li>
<li>当有很多参数对于模型只有轻微影响时，正则化方法的表现很好</li>
</ul>
</li>
</ul>
<h2 id="7-2-代价函数（Cost-Function）"><a href="#7-2-代价函数（Cost-Function）" class="headerlink" title="7.2 代价函数（Cost Function）"></a>7.2 代价函数（Cost Function）</h2><p>很多时候由于特征数量过多，过拟合时我们很难选出要保留的特征，这时候应用正则化方法则是很好的选择。</p>
<p>上文中，$\theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4$ 这样一个复杂的多项式较易过拟合，在不减少特征的情况下，<strong>如果能消除类似于 $\theta_3x^3$、$\theta_4x^4$ 等复杂部分，那复杂函数就变得简单了</strong>。</p>
<p>为了保留各个参数的信息，不修改假设函数，改而修改代价函数：</p>
<ul>
<li>消除方式为如果使代价函数足够小，$\lambda$等于1000时，$\theta_3$就需要约等于0，消除了这个特征的影响。</li>
</ul>
<p>$$<br>min_\theta\ \dfrac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + 1000\cdot\theta_3^2 + 1000\cdot\theta_4^2<br>$$</p>
<p>上式中，我们在代价函数中增加了 $\theta_3$、$\theta_4$ 的惩罚项（penalty term）$1000\cdot\theta_3^2 + 1000\cdot\theta_4^2$，如果要最小化代价函数，那么势必需要极大地<strong>减小 $\theta_3$、$\theta_4$</strong>，从而使得假设函数中的 $\theta_3x^3$、$\theta_4x^4$ 这两项的参数非常小，就相当于没有了，假设函数也就<strong>变得简单</strong>了，从而在保留各参数的情况下避免了过拟合问题。</p>
<img src="/2024/04/22/13-37-10/20180114_090054.png" class>
<p>根据上面的讨论，有时也无法决定要减少哪个参数，故统一惩罚除了 $\theta_0$ 外的所有参数。</p>
<p>代价函数：</p>
<p>$$<br>J\left( \theta  \right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}{ { {({h_\theta}({ {x}^{(i)} })-{ {y}^{(i)} })}^{2} }+\lambda \sum\limits_{j=1}^{n}{\theta_{j}^{2} }]}<br>$$</p>
<blockquote>
<p>$\lambda$: 正则化参数（Regularization Parameter），$\lambda &gt; 0$</p>
<p>$\sum\limits_{j=1}^{n}$: 不惩罚基础参数 $\theta_0$</p>
<p>$\lambda \sum\limits_{j=1}^{n}{\theta_{j}^{2} }$: 正则化项</p>
</blockquote>
<p>$\lambda$ 正则化参数类似于学习速率，也需要我们自行对其选择一个合适的值。</p>
<ul>
<li>过大<ul>
<li>导致模型欠拟合（假设可能会变成近乎 $x = \theta_0$ 的直线）</li>
<li>无法正常去过拟问题</li>
<li>梯度下降可能无法收敛</li>
</ul>
</li>
<li>过小<ul>
<li>无法避免过拟合（等于没有）</li>
</ul>
</li>
</ul>
<blockquote>
<p>正则化符合奥卡姆剃刀（Occam’s razor）原理。在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较大的先验概率，简单的模型有较小的先验概率。</p>
<p>正则化是结构风险最小化策略的实现，是去过拟合问题的典型方法，虽然看起来多了个一参数多了一重麻烦，后文会介绍自动选取正则化参数的方法。模型越复杂，正则化参数值就越大。比如，正则化项可以是模型参数向量的范数。</p>
</blockquote>
<h2 id="7-3-线性回归正则化（Regularized-Linear-Regression）"><a href="#7-3-线性回归正则化（Regularized-Linear-Regression）" class="headerlink" title="7.3 线性回归正则化（Regularized Linear Regression）"></a>7.3 线性回归正则化（Regularized Linear Regression）</h2><p>应用正则化的线性回归梯度下降算法：</p>
<p>$$<br>\begin{split}<br>&amp; \text{Repeat}\ \lbrace \\<br>&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\<br>&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right], \ \ \ j \in \lbrace 1,2…n\rbrace\\<br>&amp; \rbrace<br>\end{split}<br>$$<br>也可以移项得到更新表达式的另一种表示形式</p>
<p>$$<br>\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br>$$</p>
<blockquote>
<p>$\frac{\lambda}{m}\theta_j$: 正则化项</p>
</blockquote>
<p>应用正则化的正规方程法：</p>
<p>$$<br>\begin{split}<br>&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \\<br>&amp; \text{where}\ \ L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \\<br>&amp; 1 &amp; &amp; &amp; \\<br>&amp; &amp; 1 &amp; &amp; \\<br>&amp; &amp; &amp; \ddots &amp; \\<br>&amp; &amp; &amp; &amp; 1 \\ \end{bmatrix}<br>\end{split}<br>$$</p>
<blockquote>
<p>$\lambda\cdot L$: 正则化项</p>
<p>$L$: 第一行第一列为 $0$ 的 $n+1$ 维单位矩阵</p>
</blockquote>
<p>Octave 代码：<br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; L = <span class="built_in">eye</span>(<span class="number">5</span>)</span><br><span class="line">&gt;&gt; L(<span class="number">1</span>,<span class="number">1</span>) = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">L =</span><br><span class="line"></span><br><span class="line">     <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span>     <span class="number">1</span>     <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span>     <span class="number">0</span>     <span class="number">1</span>     <span class="number">0</span>     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span>     <span class="number">1</span>     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span>     <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>前文提到正则化可以解决正规方程法中不可逆的问题，即增加了 $\lambda \cdot L$ 正则化项后，可以保证 $X^TX + \lambda \cdot L$ 可逆（invertible），即便 $X^TX$ 不可逆（non-invertible）。 </p>
<h2 id="7-4-逻辑回归正则化（Regularized-Logistic-Regression）"><a href="#7-4-逻辑回归正则化（Regularized-Logistic-Regression）" class="headerlink" title="7.4 逻辑回归正则化（Regularized Logistic Regression）"></a>7.4 逻辑回归正则化（Regularized Logistic Regression）</h2><p>为逻辑回归的代价函数添加正则化项：</p>
<p>$$<br>J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2<br>$$<br>前文已经证明过逻辑回归和线性回归的代价函数的求导结果是一样的，此处通过给正则化项添加常数 $\frac{1}{2}$，则其求导结果也就一样了。</p>
<p>从而有应用正则化的逻辑回归梯度下降算法：</p>
<p>$$<br>\begin{split}<br>&amp; \text{Repeat}\ \lbrace \\<br>&amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\<br>&amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right], \ \ \ j \in \lbrace 1,2…n\rbrace\\<br>&amp; \rbrace \end{split}<br>$$</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习, 深度学习, 吴恩达</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记二 多变量线性回归</title>
    <url>/2024/04/21/07-10-14/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.21：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2024/04/18/15-40-33/" title="机器笔记汇总">吴恩达机器学习 - 笔记汇总</a>
</li>
</ul>
<h1 id="4-多变量线性回归（Linear-Regression-with-Multiple-Variables）"><a href="#4-多变量线性回归（Linear-Regression-with-Multiple-Variables）" class="headerlink" title="4 多变量线性回归（Linear Regression with Multiple Variables）"></a>4 多变量线性回归（Linear Regression with Multiple Variables）</h1><h2 id="4-1-多特征（Multiple-Features）"><a href="#4-1-多特征（Multiple-Features）" class="headerlink" title="4.1 多特征（Multiple Features）"></a>4.1 多特征（Multiple Features）</h2><p>不同维度的多个特征。</p>
<img src="/2024/04/21/07-10-14/20180107_234509.png" class>
<p>这里由于特征不再只有一个，引入一些新的记号</p>
<blockquote>
<p>$n$: 特征的总数</p>
<p> ${x}^{\left( i \right)}$: 代表样本矩阵中第 $i$ 行，也就是第 $i$ 个训练实例。</p>
<p> ${x}_{j}^{\left( i \right)}$: 代表样本矩阵中第 $i$ 行的第 $j$ 列，也就是第 $i$ 个训练实例的第 $j$ 个特征。</p>
</blockquote>
<p>参照上图，则有 ${x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}, {x}^{(2)}_{1} = 1416$</p>
<blockquote>
<p>全部写成一维向量</p>
</blockquote>
<p>多变量假设函数 $h$ 表示为：$h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2} }+…+{\theta_{n} }{x_{n} }$</p>
<p>对于 $\theta_0$，和单特征中一样，我们将其看作基础数值。</p>
<p>$$<br>h_\theta\left(x\right)=\begin{bmatrix}\theta_0\; \theta_1\; … \;\theta_n \end{bmatrix}\begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix}= \theta^T x<br>$$</p>
<blockquote>
<p>$\theta^T$: $\theta$ 矩阵的转置</p>
<p>$x$: 某个样本的特征向量，$n+1$ 维特征量向量</p>
<p>$x_0$: 为了计算方便我们会假设 $x_0^{(i)} = 1$</p>
</blockquote>
<h2 id="4-2-多变量梯度下降（Gradient-Descent-for-Multiple-Variables）"><a href="#4-2-多变量梯度下降（Gradient-Descent-for-Multiple-Variables）" class="headerlink" title="4.2 多变量梯度下降（Gradient Descent for Multiple Variables）"></a>4.2 多变量梯度下降（Gradient Descent for Multiple Variables）</h2><p>多变量代价函数类似于单变量代价函数，</p>
<p>即 $J\left( {\theta_{0} },{\theta_{1} }…{\theta_{n} } \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ { {\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2} } }$ ，其中 $h_\theta\left(x\right)= \theta^T x$。</p>
<p>前文提到梯度下降对于最小化代价函数的通用性，则多变量梯度下降公式即</p>
<p>\begin{split}<br>&amp; \text{Repeat until convergence:} \; \lbrace \\<br>&amp;{ {\theta }_{j} }:={ {\theta }_{j} }-\alpha \frac{\partial }{\partial { {\theta }_{j} } }J\left( {\theta_{0} },{\theta_{1} }…{\theta_{n} }  \right) \\<br>\rbrace<br>\end{split}</p>
<p>解出偏导得：</p>
<p>\begin{split}<br>&amp; \text{repeat until convergence:} \; \lbrace \\<br>&amp; \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; &amp; \text{for j := 0,1…n}\\<br>\rbrace<br>\end{split}</p>
<p>可展开为：</p>
<p>\begin{split}<br>&amp; \text{repeat until convergence:} \; \lbrace \\<br>&amp; \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}\\<br>&amp; \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} \\<br>&amp; \theta_2 := \theta_2 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} \\<br>&amp; \vdots \\<br>&amp; \theta_n := \theta_n - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_n^{(i)} &amp;\\<br>\rbrace<br>\end{split}</p>
<p>当然，同单变量梯度下降一样，计算时需要<strong>同时更新</strong>所有参数。</p>
<p>$h_\theta\left(x\right)= \theta^T x$，则得到同时更新参数的向量化（Vectorization）实现：<br>$$<br>\theta = \theta - \alpha \frac{1}{m}(X^T(X\theta-y))<br>$$</p>
<p><strong>向量化推导过程：</strong></p>
<p>\begin{split}<br>&amp;X = \begin{bmatrix}{ {x}^{(0)} } ^ T \newline { {x}^{(1)} } ^ T \newline \vdots \newline { {x}^{(m)} } ^ T\ \end{bmatrix}<br>&amp;X^T = \begin{bmatrix}{x}^{(0)}\; {x}^{(1)}\; … \;{x}^{(m)} \end{bmatrix}<br>\\<br>&amp;X\theta - y = \begin{bmatrix}{ {x}^{(0)} } ^ T \theta - y\newline { {x}^{(1)} } ^ T \theta -y \newline \vdots \newline { {x}^{(m)} } ^ T\theta -y\ \end{bmatrix} \\<br>&amp;h_\theta\left(x\right)= X\theta<br>\end{split}</p>
<blockquote>
<p>$X$: 是一个秩为1的矩阵？  一次更新一个相同的特征向量？？<br>$X$: 训练集数据，$m\times(n+1)$ 维矩阵（包含基本特征 $x_0=1$）</p>
</blockquote>
<h2 id="4-3-梯度下降实践1-特征值缩放（Gradient-Descent-in-Practice-I-Feature-Scaling）"><a href="#4-3-梯度下降实践1-特征值缩放（Gradient-Descent-in-Practice-I-Feature-Scaling）" class="headerlink" title="4.3 梯度下降实践1-特征值缩放（Gradient Descent in Practice I - Feature Scaling）"></a>4.3 梯度下降实践1-特征值缩放（Gradient Descent in Practice I - Feature Scaling）</h2><p>在应用梯度下降算法实践时，由于各特征值的范围不一，可能会影响代价函数收敛速度。</p>
<p>房屋面积大小和房间数量这两个特征。</p>
<img src="/2024/04/21/07-10-14/20180108_100751.png" class>
<p>为了优化梯度下降的收敛速度，采用特征缩放的技巧，使各特征值的<strong>范围尽量一致</strong>。</p>
<p>除了以上图人工选择并除以一个参数的方式，<strong>均值归一化（Mean normalization</strong>方法更为便捷，可采用它来对所有特征值统一缩放：</p>
<p> $x_i:=\frac{x_i-average(x)}{maximum(x)-minimum(x)}$, 使得  $x_i \in (-1,1)$</p>
<p>对于特征的范围，并不一定需要使得 $-1 \leqslant x \leqslant 1$，类似于 $1\leqslant x \leqslant 3$ 等也是可取的，而诸如 $-100 \leqslant x \leqslant 100 $，$-0.00001 \leqslant x \leqslant 0.00001$，就显得过大/过小了。</p>
<p>另外注意，一旦采用特征缩放，我们就需对所有的输入采用特征缩放，包括训练集、测试集、预测输入等。</p>
<h2 id="4-4-梯度下降实践2-学习速率（Gradient-Descent-in-Practice-II-Learning-Rate）"><a href="#4-4-梯度下降实践2-学习速率（Gradient-Descent-in-Practice-II-Learning-Rate）" class="headerlink" title="4.4 梯度下降实践2-学习速率（Gradient Descent in Practice II - Learning Rate）"></a>4.4 梯度下降实践2-学习速率（Gradient Descent in Practice II - Learning Rate）</h2><p>通常，有两种方法来确定函数是否收敛</p>
<ul>
<li>多次迭代收敛法<ul>
<li>无法确定需要多少次迭代</li>
<li>较易绘制关于迭代次数的图像</li>
<li>根据图像易预测所需的迭代次数</li>
</ul>
</li>
<li>自动化测试收敛法（比较阈值）<ul>
<li>不易选取阈值</li>
<li>代价函数近乎直线时无法确定收敛情况</li>
</ul>
</li>
</ul>
<p>对于梯度下降，一般采用多次迭代收敛法来得出最小化代价函数的参数值，自动化测试收敛法（如设定 $J\left(\theta\right) &lt; {10}^{-3}$ 时判定收敛）则几乎不会被使用。</p>
<p>我们可以通过绘制<strong>代价函数关于迭代次数的图像</strong>，可视化梯度下降的执行过程，借助直观的图形来发现代价函数趋向于多少时能趋于收敛，依据图像变化情况，确定诸如学习速率的取值，迭代次数的大小等问题。</p>
<img src="/2024/04/21/07-10-14/20180108_103357.png" class>
<p>对于学习速率 $\alpha$，一般上图展现的为适中情况，下图中，左图可能表明 <strong>$\alpha$ 过大</strong>，代价函数<strong>无法收敛</strong>，右图可能表明 <strong>$\alpha$ 过小</strong>，代价函数<strong>收敛的太慢</strong>。当然，$\alpha$ 足够小时，代价函数在每轮迭代后一定会减少。</p>
<img src="/2024/04/21/07-10-14/20180108_104701.png" class>
<p>通过不断改变 $\alpha$ 值，绘制并观察图像，并以此来确定合适的学习速率。 尝试时可取 $\alpha$ 如 $\dots\;0,001,\;0.003,\;0.01,\;0.03,\;0.1,\;\dots$</p>
<h2 id="4-5-特征和多项式回归（Features-and-Polynomial-Regression）"><a href="#4-5-特征和多项式回归（Features-and-Polynomial-Regression）" class="headerlink" title="4.5 特征和多项式回归（Features and Polynomial Regression）"></a>4.5 特征和多项式回归（Features and Polynomial Regression）</h2><p>在特征选取时，我们也可以自己归纳总结，定义一个新的特征，用来<strong>取代或拆分</strong>旧的一个或多个特征。比如，对于房屋面积特征来说，我们可以将其拆分为长度和宽度两个特征，反之，我们也可以合并长度和宽度这两个特征为面积这一个特征。</p>
<p>线性回归只能以直线来对数据进行拟合，有时候需要使用<strong>曲线</strong>来对数据进行拟合，即<strong>多项式回归（Polynomial Regression）</strong>。</p>
<p>比如一个二次方模型：$h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2}^2}$</p>
<p>或者三次方模型：$h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2}^2}+{\theta_{3} }{x_{3}^3}$ </p>
<p>或者平方根模型： $h_{\theta}\left( x \right)={\theta_{0} }+{\theta_{1} }{x_{1} }+{\theta_{2} }{x_{2}^2}+{\theta_{3} }{\sqrt{x_3} }$</p>
<img src="/2024/04/21/07-10-14/20180108_113132.png" class>
<p>在使用多项式回归时，要记住非常有必要进行特征缩放，比如 $x_1$ 的范围为 1-1000，那么 $x_1^2$ 的范围则为 1- 1000000，不使用特征缩放的话，范围不一致，也更易影响效率。</p>
<blockquote>
<p>多特征下的特征值缩放按照$x$还是$x^3$ ？</p>
</blockquote>
<h2 id="4-6-正规方程（Normal-Equation）"><a href="#4-6-正规方程（Normal-Equation）" class="headerlink" title="4.6 正规方程（Normal Equation）"></a>4.6 正规方程（Normal Equation）</h2><p>对于一些线性回归问题来说，正规方程法给出了一个更好的解决问题的方式。</p>
<p>正规方程法，即令 $\frac{\partial}{\partial{\theta_{j} } }J\left( {\theta_{j} } \right)=0$ ，通过解析函数的方式直接计算得出参数向量的值  $\theta ={ {\left( {X^T}X \right)}^{-1} }{X^{T} }y$ ，Octave/Matlab 代码： <code>theta = inv(X&#39;*X)*X&#39;*y</code>。</p>
<blockquote>
<p>${X}^{-1}$: 矩阵 $X$ 的逆，在 Octave 中，<code>inv</code> 函数用于计算矩阵的逆，类似的还有 <code>pinv</code> 函数。</p>
<p><code>X&#39;</code>: 在 Octave 中表示矩阵 X 的转置，即 $X^T$</p>
</blockquote>
<p>下表列出了正规方程法与梯度下降算法的对比</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>条件</th>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>是否需要选取 $\alpha$</td>
<td>需要</td>
<td>不需要</td>
</tr>
<tr>
<td>是否需要迭代运算</td>
<td>需要</td>
<td>不需要</td>
</tr>
<tr>
<td>特征量大时</td>
<td>适用，$O\left(kn^2\right)$</td>
<td>不适用，$(X^TX)^{-1}$ 复杂度 $O\left( { {n}^{3} } \right)$</td>
</tr>
<tr>
<td>适用范围</td>
<td>各类模型</td>
<td>只适用线性模型，且矩阵需可逆</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>一般来说，当 $n$ 超过 10000 时，对于正规方程而言，特征量较大。</li>
<li>梯度下降算法的普适性好，而对于特定的线性回归模型，正规方程是很好的替代品。</li>
</ul>
<p><strong>正规方程法的推导过程</strong>：(同4.2)</p>
<p>\begin{split}<br>J\left( \theta  \right)&amp; =\frac{1}{2m}\sum\limits_{i=1}^{m}{ { {\left( {h_{\theta} }\left( {x^{(i)} } \right)-{y^{(i)} } \right)}^{2} } }\\<br>&amp; =\frac{1}{2m}||X\theta-y||^2 \\<br>&amp; =\frac{1}{2m}(X\theta-y)^T(X\theta-y) \hspace{15cm}<br>\end{split}</p>
<p>展开上式可得</p>
<p>$<br>(X\theta-y)^T = \theta^T X^T - y^T<br>$</p>
<p>$J(\theta )= \frac{1}{2m}\left( { {\theta }^{T} }{ {X}^{T} }X\theta -{ {\theta}^{T} }{ {X}^{T} }y-{ {y}^{T} }X\theta + { {y}^{T} }y \right)$</p>
<p>注意到 ${ {\theta}^{T} }{ {X}^{T} }y$ 与 ${ {y}^{T} }X\theta$ 都为标量，实际上是等价的，则：</p>
<p>$J(\theta) = \frac{1}{2m}[{\theta }^{T}X^TX\theta-2\theta^TX^Ty+y^Ty]$</p>
<p>接下来对$J(\theta )$ 求偏导，根据矩阵的求导法则:</p>
<p>$\frac{d X^TAX}{d X}=2AX$</p>
<p>$\frac{d X^T}{d X}={E}$</p>
<p>$\frac{d AB}{d B}={A^T}$</p>
<p>所以有:</p>
<p>$\frac{\partial J\left( \theta  \right)}{\partial \theta }=\frac{1}{2m}\left(2{ {X}^{T} }X\theta -2{ {X}^{T} }y \right)={ {X}^{T} }X\theta -{ {X}^{T} }y$</p>
<p>令$\frac{\partial J\left( \theta  \right)}{\partial \theta }=0$, 则有<br>$$<br>\theta ={ {\left( {X^{T} }X \right)}^{-1} }{X^{T} }y<br>$$</p>
<blockquote>
<p>求最小值, 极值一定是最小值, 具有实际意义</p>
</blockquote>
<h2 id="4-7-不可逆性正规方程（Normal-Equation-Noninvertibility）"><a href="#4-7-不可逆性正规方程（Normal-Equation-Noninvertibility）" class="headerlink" title="4.7 不可逆性正规方程（Normal Equation Noninvertibility）"></a>4.7 不可逆性正规方程（Normal Equation Noninvertibility）</h2><p>（本部分内容为选讲）</p>
<p>正规方程无法应用于不可逆的矩阵，发生这种问题的概率很小，通常由于</p>
<ul>
<li><p>特征之间线性相关</p>
<p>比如同时包含英寸的尺寸和米为单位的尺寸两个特征，它们是线性相关的</p>
<p>即 ${x_{1} }={x_{2} }*{ {\left( 3.28 \right)}^{2} }$。</p>
</li>
<li><p>特征数量大于训练集的数量 $\left(m \leqslant n \right)$。</p>
</li>
</ul>
<p>如果发现 $X^TX$ 的结果不可逆，可尝试：</p>
<ul>
<li>减少多余/重复特征</li>
<li>增加训练集数量</li>
<li>使用正规化（后文）</li>
</ul>
<p>对于这类不可逆的矩阵，我们称之为<strong>奇异矩阵</strong>或<strong>退化矩阵</strong>。</p>
<p>这种情况下，如果还想使用正规方程法，在Octave中，可以选用 <code>pinv</code> 函数，<code>pinv</code> 区别于 <code>inv</code>，<code>pinv</code> 函数被称为伪逆函数，在矩阵不可逆的时候，使用这个函数仍可正确地计算出 $\theta$ 的值。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习, 深度学习, 吴恩达</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown语法</title>
    <url>/2024/04/19/22-19-04/</url>
    <content><![CDATA[<h1 id="markdown"><a href="#markdown" class="headerlink" title="markdown"></a>markdown</h1><h2 id="latex特殊转义"><a href="#latex特殊转义" class="headerlink" title="latex特殊转义"></a>latex特殊转义</h2><p>选用hexo-renderer-kramed渲染器</p>
<blockquote>
<p>作为行内代码：$ a+b $<br>行内公式：<code>\$\$ a+b \$\$</code><br>大部分公式中，符号之前要有空格，不然无法通过hexo渲染。</p>
</blockquote>
<h3 id="换行"><a href="#换行" class="headerlink" title="换行"></a>换行</h3><ul>
<li><code>\begin&#123;aligned&#125;</code> 可以使用</li>
<li><code>\begin&#123;equation&#125;</code> 自动给公式编号</li>
<li>换行：<code>\begin&#123;align\*&#125;  //  \end&#123;align\*&#125;</code>  转为 <code>\begin&#123;split&#125;</code></li>
<li>换行中间不能有空行</li>
<li>多行公式对齐，使用<code>&amp;</code>，所有的符号都会在每一行对齐</li>
</ul>
<h3 id="乘法"><a href="#乘法" class="headerlink" title="乘法*"></a>乘法*</h3><ul>
<li>乘法需要加<code>a\\*b</code>转义 $a*b$</li>
</ul>
<h3 id="求导偏导"><a href="#求导偏导" class="headerlink" title="求导偏导"></a>求导偏导</h3><ul>
<li>求导 <code>d</code>  直接写 $d$</li>
<li>偏导 <code>\partial</code>  $\partial$</li>
</ul>
<h3 id="大写bold"><a href="#大写bold" class="headerlink" title="大写bold"></a>大写bold</h3><ul>
<li>改为<code>\boldsymbol&#123;x&#125;</code> $\boldsymbol{x}$</li>
</ul>
<h3 id="lim极限-求和"><a href="#lim极限-求和" class="headerlink" title="lim极限 求和"></a>lim极限 求和</h3><ul>
<li><code>\displaystyle\lim_x</code>  显示 $\displaystyle\lim_x$</li>
<li><code>\lim_x</code>  显示 $\lim_x$<h3 id="空心字"><a href="#空心字" class="headerlink" title="空心字"></a>空心字</h3></li>
<li><code>\mathbb&#123;1&#125;</code>, $\mathbb{1}$</li>
</ul>
<h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><p>与正文空一行，才能正常转义</p>
<h3 id="跳转1"><a href="#跳转1" class="headerlink" title="跳转1"></a>跳转<sup><a href="#fn_1" id="reffn_1">1</a></sup></h3><ul>
<li><blockquote id="fn_1">
<sup>1</sup>. <code>[^1]</code>设置标签， 在需要跳转的地方写入<code>[^1]:</code>生成跳转箭头<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
</li>
</ul>
<h3 id="mermaid-绘图"><a href="#mermaid-绘图" class="headerlink" title="mermaid 绘图"></a>mermaid 绘图</h3><p>饼状图：pie<br>流程图：graph<br>序列图：sequenceDiagram<br>甘特图：grantt<br>类图：classDiagram<br>状态图：stateDiagram<br>用户旅程图：journey</p>
<p>线：<br><code>-.-&gt;</code>: 虚线带箭头<br><code>==&gt;</code>: 粗实线</p>
<p>其他：<br><code>%%</code>：注释<br><code>[&quot;内容&quot;]</code>：可以添加内容</p>
<h4 id="graph流程图"><a href="#graph流程图" class="headerlink" title="graph流程图"></a>graph流程图</h4><blockquote>
<pre class="mermaid">> graph TD;
   A & B-->C;
   B-->A;
   C-->D;
</pre>



</blockquote>
<pre><code>•    graph TD;：graph 表示这是一个图表定义，TD 表示布局方向。
•    TD：布局方向代码，代表 Top to Down（从上到下）。
•    其他布局选项包括：
•    LR：从左到右（Left to Right）
•    RL：从右到左（Right to Left）
•    BT：从下到上（Bottom to Top）
</code></pre><p>效果：</p>
<pre class="mermaid">graph TD;
    A & B-->C;
    B-->A;
    C-->D;

graph TB
    A
    %%过程
    B[矩形节点]
    %%开始
    C(圆形矩形节点)
    %%链接
    D((圆形节点))
    %% 判断
    E{菱形节点}  
    F>旗帜形状节点]

效果：
```mermaid
graph TB
    A
    B[矩形节点]
    C(圆形矩形节点)
    D((圆形节点))
    E{菱形节点}
    F>旗帜形状节点]</pre>


<h5 id="子表图"><a href="#子表图" class="headerlink" title="子表图"></a>子表图</h5><p>先用subgraph写进去所有类函数，<br>然后在外层，写调用过程。</p>
<p>子表图指定流程图方向：<br><code>direction TB</code></p>
<p>graph RL<br>    c1—&gt;a2<br>    subgraph one<br>    a1—&gt;a2<br>    end<br>    subgraph two<br>    b1—&gt;b2<br>    end<br>    subgraph three<br>    c1—&gt;c2<br>    end<br>    one—&gt;two<br>    three—&gt;two<br>    two—&gt;c2</p>
<pre class="mermaid">graph RL
    c1-->a2
    subgraph one
    a1-->a2
    end
    subgraph two
    b1-->b2
    end
    subgraph three
    c1-->c2
    end
    one-->two
    three-->two
    two-->c2</pre>


<h1 id="hexo-创建文件夹内文章"><a href="#hexo-创建文件夹内文章" class="headerlink" title="hexo 创建文件夹内文章"></a>hexo 创建文件夹内文章</h1><ul>
<li><code>hexo new post -p /n/m</code></li>
</ul>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>markdown, hexo, Latex</tag>
      </tags>
  </entry>
  <entry>
    <title>vim实用技巧</title>
    <url>/2024/04/19/11-47-11/</url>
    <content><![CDATA[<h1 id="按键说明"><a href="#按键说明" class="headerlink" title="按键说明"></a>按键说明</h1><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><h3 id="动词"><a href="#动词" class="headerlink" title="动词"></a>动词</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">d 删除 delete</span><br><span class="line">r 替换 replace</span><br><span class="line">c 修改 change</span><br><span class="line">y 复制 yank</span><br><span class="line">v 选取 visual select</span><br></pre></td></tr></table></figure>
<h3 id="名词-text-object"><a href="#名词-text-object" class="headerlink" title="名词 text object"></a>名词 text object</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">w 单词word</span><br><span class="line">s 句子 sentence</span><br><span class="line">p 段落paragraph</span><br><span class="line">t HTML标签tag</span><br><span class="line">u 撤销(undo)</span><br><span class="line">. 重复最后一个命令</span><br><span class="line">引号或者各种括号所包含的文本称作一个文本块。</span><br></pre></td></tr></table></figure>
<h3 id="介词"><a href="#介词" class="headerlink" title="介词"></a>介词</h3><p>介词界定了待编辑文本的范围或者位置。例如：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">i “在…之内”inside</span><br><span class="line">a “环绕…”around</span><br><span class="line">t “到…位置前”to</span><br><span class="line">f “到…位置上”forward</span><br></pre></td></tr></table></figure></p>
<h3 id="组词为句"><a href="#组词为句" class="headerlink" title="组词为句"></a>组词为句</h3><p>动词+介词+名词<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">删除一个段落 delete inside paragraph: dip</span><br><span class="line">选取一个句子 visual select inside sentence: vis</span><br><span class="line">修改一个单词 change inside word: ciw</span><br><span class="line">修改一个单词 change around word: caw</span><br><span class="line">删除文本直到字符“x”（不包括字符“x”）delete to x: dtx</span><br><span class="line">删除文本直到字符“x”（包括字符“x”）delete forward x:dfx</span><br></pre></td></tr></table></figure></p>
<h3 id="数词"><a href="#数词" class="headerlink" title="数词"></a>数词</h3><p>动词+介词/数词+名词<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">修改三个单词 change three words: c3w</span><br><span class="line">删除两个单词 delete two words: d2w</span><br></pre></td></tr></table></figure></p>
<p>数词也可以修饰动词，表示将操作执行 n 次<br>数词+动词+名词<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">两次删除单词（等价于删除两个单词） twice delete word: <span class="number">2</span>dw</span><br><span class="line">三次删除字符（等价于删除三个字符）three times delete character: <span class="number">3</span>x</span><br></pre></td></tr></table></figure></p>
<h2 id="移动光标"><a href="#移动光标" class="headerlink" title="移动光标"></a>移动光标</h2><h3 id="行"><a href="#行" class="headerlink" title="行"></a>行</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span> 跳转到当前行首</span><br><span class="line">^ 跳转到当前行的第一个非空字符(空格/TAB)</span><br><span class="line">$ 跳转到当前行的末尾</span><br><span class="line">gg 跳转到文件第一行(<span class="keyword">goto</span>)</span><br><span class="line">G 跳转到文件最后一行</span><br><span class="line"><span class="number">47</span>G 跳转到文件第<span class="number">47</span>行</span><br><span class="line">:<span class="number">47</span> 跳转到文件第<span class="number">47</span>行(同<span class="string">&quot;47G&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="字符"><a href="#字符" class="headerlink" title="字符"></a>字符</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">f[<span class="type">char</span>] 跳转到第一个[<span class="type">char</span>]字符(find)</span><br><span class="line"><span class="number">3f</span>[<span class="type">char</span>] 跳转到第三个[<span class="type">char</span>]字符</span><br><span class="line">F[<span class="type">char</span>] 向左跳转到第一个[<span class="type">char</span>]字符</span><br><span class="line">t[<span class="type">char</span>] 跳转到第一个[<span class="type">char</span>]字符的前一个字符(till before - right)</span><br><span class="line">T[<span class="type">char</span>] 向左跳转到第一个[<span class="type">char</span>]字符的后一个字符(till after - left)</span><br><span class="line">; 重复最后一次的 f/F/t/T 移动命令</span><br><span class="line"> </span><br><span class="line">这里加 shift 大写，意为反向：向左</span><br></pre></td></tr></table></figure>
<h3 id="单词和文本块"><a href="#单词和文本块" class="headerlink" title="单词和文本块"></a>单词和文本块</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">w 跳转到下一个单词的开头(word)</span><br><span class="line"><span class="number">3</span>w 跳转到第三个单词的开头</span><br><span class="line">e 跳转到下一个单词的结尾(end of word)</span><br><span class="line">b 跳转到上一个单词的开头(backward beginning)</span><br><span class="line">( 跳转到上一个句子的开头</span><br><span class="line">) 跳转到下一个句子的开头</span><br><span class="line">&#123; 跳转到上一个段落的开头</span><br><span class="line">&#125; 跳转到下一个段落的开头</span><br></pre></td></tr></table></figure>
<h3 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">% 在当前大括号/中括号/小括号的开始位置<span class="string">&quot;&#123;/[/(&quot;</span>和结束位置<span class="string">&quot;&#125;/]/)&quot;</span>之间跳转</span><br><span class="line">[[ 跳转到上一个函数的开头(如果光标在函数体内则跳转到当前函数的开头)</span><br><span class="line">]] 跳转到下一个函数的开头</span><br><span class="line">[&#123; 跳转到当前程序块的开头(当前程序块为当前程序的上一层，不是固定的)</span><br><span class="line">]&#125; 跳转到当前程序块的结尾</span><br></pre></td></tr></table></figure>
<h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">/[word] 搜索[word]字符串</span><br><span class="line">?[word] 向上搜索[word]字符串</span><br><span class="line">n 跳转到下一个匹配的字符串(保持最后一个搜索命令的方向)(next match)</span><br><span class="line">N 跳转到上一个匹配的字符串(保持最后一个搜索命令的方向)</span><br><span class="line">* 搜索当前光标下的单词</span><br><span class="line"># 向上搜索当前光标下的单词</span><br></pre></td></tr></table></figure>
<h2 id="插入模式"><a href="#插入模式" class="headerlink" title="插入模式"></a>插入模式</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">s 删除光标位置下的字符并进入插入模式</span><br><span class="line">S 删除当前行内容并进入插入模式</span><br></pre></td></tr></table></figure>
<h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">x 删除光标位置下的字符</span><br><span class="line">dw 删除光标之后的单词剩余部分(delete word)</span><br><span class="line">diw 删除一个单词</span><br><span class="line">dd 删除当前行</span><br><span class="line">D 删除从光标位置到当前行的末尾(同<span class="string">&quot;d$&quot;</span>)</span><br><span class="line">df[<span class="type">char</span>] 删除从光标位置到[<span class="type">char</span>]字符(delete find [<span class="type">char</span>])</span><br><span class="line">d) 删除从光标位置到下一个句子的开始</span><br><span class="line">d&#125; 删除从光标位置到该段落的末尾</span><br><span class="line">di&#123; 删除花括号之间的内容(delete inner &#123;&#125;)(同<span class="string">&quot;diB&quot;</span>)</span><br><span class="line">di( 删除小括号之间的内容(delete inner ())(同<span class="string">&quot;dib&quot;</span>)</span><br><span class="line">dit 删除闭合标签之间的内容(html/xml等标签，delete inner tag)</span><br><span class="line">dat 删除左右尖括号及之间的内容(delete a tag)</span><br><span class="line">da&lt; 删除左右尖括号及之间的内容(delete a &lt;&gt;)</span><br><span class="line">di<span class="string">&quot; 删除引号之间的内容(delete inner &quot;</span><span class="string">&quot;)</span></span><br><span class="line"><span class="string">da&quot;</span> 删除左右引号及之间的内容(delete a <span class="string">&quot;&quot;</span>)</span><br><span class="line">:<span class="number">5</span>,<span class="number">10</span>d 删除<span class="number">5</span><span class="number">-10</span>行</span><br><span class="line"><span class="number">3</span>dd 删除从当前行开始的<span class="number">3</span>行</span><br><span class="line">&lt;C-w&gt; 删除光标前的一个单词(插入模式)</span><br><span class="line">&lt;C-u&gt; 从光标位置删除到行首(插入模式)</span><br><span class="line"> </span><br><span class="line">这里加 shift 大写，意为行尾</span><br><span class="line">注：<span class="string">&quot;d&quot;</span>/<span class="string">&quot;c&quot;</span>开头的命令会将删除的文本放到寄存器(通过<span class="string">&quot;:reg&quot;</span>查看)，可以理解为剪切。</span><br><span class="line">关于<span class="string">&quot;a&quot;</span>n和<span class="string">&quot;i&quot;</span>nner可以参考<span class="string">&quot;:help object-select&quot;</span>文本对象选择部分</span><br><span class="line">另：一般向右的操作包含光标下的字符，向左的操作不包含光标下的字符</span><br></pre></td></tr></table></figure>
<h2 id="更改文本"><a href="#更改文本" class="headerlink" title="更改文本"></a>更改文本</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">J 将下一行合并的当前行的末尾(Join line)</span><br><span class="line"><span class="number">3</span>,<span class="number">9</span>j 合并<span class="number">3</span><span class="number">-9</span>行</span><br><span class="line">~ 切换光标下字符的大小写</span><br><span class="line">u 更改选定的文本为小写(可视模式)</span><br><span class="line">U 更改选定的文本为大写(可视模式)</span><br><span class="line">&lt;C-a&gt; 把当前光标下或之后的数值加<span class="number">1</span></span><br><span class="line">&lt;C-x&gt; 把当前光标下或之后的数值减<span class="number">1</span></span><br><span class="line">r[<span class="type">char</span>] 替换光标下的字符为[<span class="type">char</span>]. (replace)</span><br><span class="line">R 进入替换模式</span><br><span class="line">cw 删除光标之后的单词剩余部分并进入插入模式(change word)</span><br><span class="line">cc 删除当前行内容并进入插入模式(同<span class="string">&quot;S&quot;</span>)</span><br><span class="line">C 删除从光标位置到当前行的末尾并进入插入模式(同<span class="string">&quot;c$&quot;</span>)</span><br><span class="line">cf[<span class="type">char</span>] 删除从光标位置到[<span class="type">char</span>]字符并进入插入模式</span><br><span class="line"> </span><br><span class="line">这里<span class="string">&quot;c&quot;</span>开头加 shift 大写，意为行尾</span><br><span class="line"><span class="string">&quot;c&quot;</span>开头<span class="string">&quot;change&quot;</span>更改(删除并插入)，<span class="string">&quot;d&quot;</span>开头<span class="string">&quot;delete&quot;</span>删除，和<span class="string">&quot;y&quot;</span>开头<span class="string">&quot;yank/copy&quot;</span>，格式相同，可相互参考使用</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="缩进"><a href="#缩进" class="headerlink" title="缩进"></a>缩进</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">&gt;&gt; 缩进当前行</span><br><span class="line">&lt;&lt; 向左缩进当前行</span><br><span class="line">&lt;C-d&gt; 缩进当前行(插入模式)</span><br><span class="line">&lt;C-t&gt; 向左缩进当前行(插入模式)</span><br><span class="line">:<span class="number">3</span>,<span class="number">9</span>&gt;&gt;&gt;&gt;&gt; 将<span class="number">3</span><span class="number">-9</span>行缩进<span class="number">5</span>个TAB</span><br><span class="line">&gt; 缩进选定的行(可视模式)</span><br><span class="line">&lt; 向左缩进选定的行(可视模式)</span><br><span class="line">&gt;i&#123; 缩进花括号之间的内容(indent inner &#123;&#125;)(同<span class="string">&quot;&gt;iB&quot;</span>)</span><br><span class="line">&gt;a&#123; 缩进花括号及之间的内容(indent a &#123;&#125;)(同<span class="string">&quot;&gt;aB&quot;</span>)</span><br><span class="line">=&#125; 缩进当前段落</span><br><span class="line">gg=G 全文缩进/格式化</span><br></pre></td></tr></table></figure>
<h2 id="复制粘贴"><a href="#复制粘贴" class="headerlink" title="复制粘贴"></a>复制粘贴</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">v 进入可视模式，以字符为单位选择</span><br><span class="line">V 进入可视模式，以行为单位选择</span><br><span class="line">&lt;C-v&gt; 进入列块可视模式(如果映射<span class="string">&quot;&lt;C-v&gt;&quot;</span>为<span class="string">&quot;粘贴&quot;</span>时请注意)</span><br><span class="line">gv 重新选择最后选定的区域</span><br><span class="line">y 抽出选择的文本到寄存器(可视模式)(yank/copy)</span><br><span class="line"><span class="string">&quot;+y 抽出选择的文本到系统剪切板(可视模式)(好像不太好使)</span></span><br><span class="line"><span class="string">:co 10 复制当前行到第11行(copy)</span></span><br><span class="line"><span class="string">:co . 复制当前行到下一行(同&quot;</span>yyp<span class="string">&quot;)</span></span><br><span class="line"><span class="string">:5,10co 20 复制5-10行到第21行</span></span><br><span class="line"><span class="string">yy 复制当前行</span></span><br><span class="line"><span class="string">y$ 复制到行尾</span></span><br><span class="line"><span class="string">yw 复制光标之后的单词剩余部分(yank word)</span></span><br><span class="line"><span class="string">yb 复制光标之前的单词剩余部分</span></span><br><span class="line"><span class="string">yiw 复制一个单词</span></span><br><span class="line"><span class="string">yip 复制当前段落(yank inner paragraph)</span></span><br><span class="line"><span class="string">yas 复制一个句子(yank a sentence)</span></span><br><span class="line"><span class="string">yi&lt; 复制尖括号之间的内容(Yank inner &lt;&gt;)</span></span><br><span class="line"><span class="string">11y 复制11行</span></span><br><span class="line"><span class="string">p 粘贴(paste)</span></span><br><span class="line"><span class="string">P 粘贴到光标前</span></span><br><span class="line"><span class="string">&lt;C-r&gt;&quot;</span> 粘贴(插入模式)</span><br><span class="line"><span class="string">&quot;ayy 复制当前行到寄存器&quot;</span>a<span class="string">&quot;(可使用范围&quot;</span>a-z<span class="string">&quot;)</span></span><br><span class="line"><span class="string">&quot;</span>ap 粘贴从寄存器<span class="string">&quot;a&quot;</span></span><br><span class="line"> </span><br><span class="line">这里的<span class="string">&quot;寄存器&quot;</span>即VIM剪切板</span><br></pre></td></tr></table></figure>
<h2 id="滚动屏幕"><a href="#滚动屏幕" class="headerlink" title="滚动屏幕"></a>滚动屏幕</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">H 跳转到屏幕的顶部(home)</span><br><span class="line">M 跳转到屏幕的中间(middle)</span><br><span class="line">L 跳转到屏幕的底部(low)</span><br><span class="line">zt 将当前行滚动至屏幕顶部(top)</span><br><span class="line">zz 将当前行滚动至屏幕中间(同<span class="string">&quot;z.&quot;</span>)</span><br><span class="line">zb 将当前行滚动至屏幕中间(bottom)(同<span class="string">&quot;z-&quot;</span>)</span><br><span class="line">&lt;C-f&gt; 滚动至下一页(forwards)</span><br><span class="line">&lt;C-b&gt; 滚动至上一页(backwards)</span><br><span class="line">&lt;C-d&gt; 向下滚动半屏(downwards)</span><br><span class="line">&lt;C-u&gt; 向下滚动半屏(upwards)</span><br></pre></td></tr></table></figure>
<h2 id="键映射"><a href="#键映射" class="headerlink" title="键映射"></a>键映射</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">:<span class="built_in">map</span> 查看已映射的键列表</span><br><span class="line">:imap 查看插入模式下已映射的键列表</span><br><span class="line">:nmap 查看普通模式下已映射的键列表</span><br><span class="line">:imap jj &lt;Esc&gt; 插入模式下键入<span class="string">&quot;jj&quot;</span>映射到<span class="string">&quot;&lt;ESC&gt;&quot;</span>(返回普通模式)</span><br><span class="line">:nmap &lt;C-h&gt; &lt;C-w&gt;h 普通模式下映射<span class="string">&quot;&lt;C-h&gt;&quot;</span>到<span class="string">&quot;&lt;C-w&gt;h&quot;</span>(光标移动到左边一个窗口)</span><br></pre></td></tr></table></figure>
<h1 id="键位图"><a href="#键位图" class="headerlink" title="键位图"></a>键位图</h1><ul>
<li>绿色键：motion，移动光标，或定义操作的范围</li>
<li>黄色键：command，直接执行的命令，红色命令进入编辑模式</li>
<li>橙色键：operator，后面跟随表示操作范围的指令</li>
<li>灰色键：extra，特殊功能，需要额外的输入</li>
</ul>
<img src="/2024/04/19/11-47-11/592892-20180328123208117-408079142.gif" class>
<img src="/2024/04/19/11-47-11/57cbaf4d4db0e734254abe2717da0360.jpeg" class>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具, vim</tag>
      </tags>
  </entry>
  <entry>
    <title>27-GooLeNet</title>
    <url>/2024/04/18/15-40-53/</url>
    <content><![CDATA[<h2 id="GooLeNet"><a href="#GooLeNet" class="headerlink" title="GooLeNet"></a>GooLeNet</h2><h4 id="含并行连结的网络"><a href="#含并行连结的网络" class="headerlink" title="含并行连结的网络"></a>含并行连结的网络</h4><ul>
<li>GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。我们往往不确定到底选取什么样的层效果更好，到底是3X3卷积层还是5X5的卷积层，诸如此类的问题是GooLeNet选择了另一种思路“小学生才做选择，我全都要”，这也使得GooLeNet成为了第一个模型中超过1000个层的模型。</li>
</ul>
<h4 id="Inception块"><a href="#Inception块" class="headerlink" title="Inception块"></a>Inception块</h4><ul>
<li><p>在GoogLeNet中，基本的卷积块被称为<em>Inception块</em>（Inception block）</p>
<p><img src="https://github.com/kinza99/DeepLearning-MuLi-Notes/blob/main/imgs/27/27-1.png" alt="截屏2022-01-23 上午10.11.18"></p>
</li>
<li><p>Inception块由四条并行路径组成。 前三条路径使用窗口大小为1×11×1、3×33×3和5×55×5的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行1×11×1卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用3×33×3最大汇聚层，然后使用1×11×1卷积层来改变通道数。 这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。</p>
</li>
</ul>
<h4 id="GooLeNet模型"><a href="#GooLeNet模型" class="headerlink" title="GooLeNet模型"></a>GooLeNet模型</h4><ul>
<li>GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。<img src="https://github.com/kinza99/DeepLearning-MuLi-Notes/blob/main/imgs/27/27-2.png" alt="截屏2022-01-23 上午10.17.11"></li>
<li>第一个模块是7×7卷积层。</li>
<li>第二个模块使用两个卷积层：第一个卷积层是1×1卷积层；第二个卷积层使用将通道数量增加三倍的3×3卷积层。 这对应于Inception块中的第二条路径。</li>
<li>第三个模块串联两个完整的Inception块。 第一个Inception块的输出通道数为64+128+32+32=25664+128+32+32=256，四个路径之间的输出通道数量比为64:128:32:32=2:4:1:164:128:32:32=2:4:1:1。 第二个和第三个路径首先将输入通道的数量分别减少到96/192=1/296/192=1/2和16/192=1/1216/192=1/12，然后连接第二个卷积层。第二个Inception块的输出通道数增加到128+192+96+64=480128+192+96+64=480，四个路径之间的输出通道数量比为128:192:96:64=4:6:3:2128:192:96:64=4:6:3:2。 第二条和第三条路径首先将输入通道的数量分别减少到128/256=1/2128/256=1/2和32/256=1/832/256=1/8。</li>
<li>第四模块更加复杂， 它串联了5个Inception块，其输出通道数分别是192+208+48+64=512192+208+48+64=512、160+224+64+64=512160+224+64+64=512、128+256+64+64=512128+256+64+64=512、112+288+64+64=528112+288+64+64=528和256+320+128+128=832256+320+128+128=832。 这些路径的通道数分配和第三模块中的类似，首先是含3×3卷积层的第二条路径输出最多通道，其次是仅含1×1卷积层的第一条路径，之后是含5×5卷积层的第三条路径和含3×3最大汇聚层的第四条路径。 其中第二、第三条路径都会先按比例减小通道数。 这些比例在各个Inception块中都略有不同。</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li><p>Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用1×1卷积层减少每像素级别上的通道维数从而降低模型复杂度。</p>
</li>
<li><p>GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</p>
</li>
<li><p>GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>动手学深度学习</category>
      </categories>
      <tags>
        <tag>动手学深度学习, 深度学习, 李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>机器笔记汇总</title>
    <url>/2024/04/18/15-40-33/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.18：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2024/04/18/15-34-48/" title="机器学习笔记一 单变量线性回归">机器学习笔记一 单变量线性回归</a></li>
<li><a href="/2024/04/21/07-10-14/" title="机器学习笔记二 多变量线性回归">机器学习笔记二 多变量线性回归</a></li>
<li><a href="/2024/04/22/13-37-10/" title="机器学习笔记三 多变量线性回归">机器学习笔记三 多变量线性回归</a></li>
<li><a href="/2024/04/22/13-37-13/" title="机器学习笔记四 神经网络的表达">机器学习笔记四 神经网络的表达</a></li>
<li><a href="/2024/04/23/12-43-40/" title="机器学习笔记五 神经网络的学习">机器学习笔记五 神经网络的学习</a>
</li>
</ul>
<blockquote>
<p>先学习一部分，之后遇到了再具体学习，暂停之后学习动手学深度学习。</p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习, 深度学习, 吴恩达</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记一 单变量线性回归</title>
    <url>/2024/04/18/15-34-48/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.04.18：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2024/04/18/15-40-33/" title="机器笔记汇总">吴恩达机器学习 - 笔记汇总</a>
</li>
</ul>
<h1 id="1-引言（Introduction）"><a href="#1-引言（Introduction）" class="headerlink" title="1 引言（Introduction）"></a>1 引言（Introduction）</h1><h2 id="1-1-Welcome"><a href="#1-1-Welcome" class="headerlink" title="1.1 Welcome"></a>1.1 Welcome</h2><p>随着互联网数据不断累积，硬件不断升级迭代，在这个信息爆炸的时代，机器学习已被应用在各行各业中，可谓无处不在。</p>
<h2 id="1-2-什么是机器学习（What-is-Machine-Learning）"><a href="#1-2-什么是机器学习（What-is-Machine-Learning）" class="headerlink" title="1.2 什么是机器学习（What is Machine Learning）"></a>1.2 什么是机器学习（What is Machine Learning）</h2><ol>
<li><p>机器学习定义<br> 这里主要有两种定义：</p>
<ul>
<li><p>Arthur Samuel (1959). Machine Learning:<br>Field of study that gives computers the ability to learn without being explicitly programmed.<br>这个定义有点不正式但提出的时间最早，来自于一个懂得计算机编程的下棋菜鸟。他编写了一个程序，但没有显式地编程每一步该怎么走，而是让计算机自己和自己对弈，并不断地计算布局的好坏，来判断什么情况下获胜的概率高，从而积累经验，好似学习，最后，这个计算机程序成为了一个比他自己还厉害的棋手。</p>
</li>
<li><p>Tom Mitchell (1998) Well-posed Learning Problem:<br>A computer program is said to learn from experience E with respect to some <strong>task T</strong> and some <strong>performance measure P</strong>, if its performance on T, as measured by P, improves with <strong>experience E</strong>.<br>Tom Mitchell 的定义更为现代和正式。在过滤垃圾邮件这个例子中，电子邮件系统会根据用户对电子邮件的标记（是/不是垃圾邮件）不断学习，从而提升过滤垃圾邮件的准确率，定义中的三个字母分别代表：</p>
<ul>
<li>T(Task): 过滤垃圾邮件任务。</li>
<li>P(Performance): 电子邮件系统过滤垃圾邮件的准确率。</li>
<li>E(Experience): 用户对电子邮件的标记。<blockquote>
<p>类似监督学习</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li>机器学习算法<br>主要有两种机器学习的算法分类<ol>
<li>监督学习</li>
<li>无监督学习<br>还有一些算法也属于机器学习领域，诸如：</li>
</ol>
<ul>
<li>半监督学习: 介于监督学习于无监督学习之间</li>
<li>推荐算法: </li>
<li>强化学习: 通过观察来学习如何做出动作，每个动作都会对环境有所影响，而环境的反馈又可以引导该学习算法。</li>
<li>迁移学习: 某个领域应用到另个领域</li>
</ul>
</li>
</ol>
<h2 id="1-3-监督学习（Supervised-Learning）"><a href="#1-3-监督学习（Supervised-Learning）" class="headerlink" title="1.3 监督学习（Supervised Learning）"></a>1.3 监督学习（Supervised Learning）</h2><p>监督学习，即为教计算机如何去完成预测任务（有反馈），预先给一定数据量的输入<strong>和对应的结果</strong>即训练集，建模拟合，最后让计算机预测未知数据的结果。<br>监督学习一般有两种：</p>
<ol>
<li><p>回归问题（Regression）<br>回归问题即为预测一系列的<strong>连续值</strong>。<br>在房屋价格预测的例子中，给出了一系列的房屋面积数据，根据这些数据来预测任意面积的房屋价格。给出照片-年龄数据集，预测给定照片的年龄。</p>

</li>
<li><p>分类问题（Classification）<br>分类问题即为预测一系列的<strong>离散值</strong>。<br>即根据数据预测被预测对象属于哪个分类。<br>视频中举了癌症肿瘤这个例子，针对诊断结果，分别分类为良性或恶性。还例如垃圾邮件分类问题，也同样属于监督学习中的分类问题。</p>
<img src="/2024/04/18/15-34-48/20180105_194839.png" class>
</li>
</ol>
<p>视频中提到<strong>支持向量机</strong>这个算法，旨在解决当特征量很大的时候（特征即如癌症例子中的肿块大小，颜色，气味等各种特征），计算机内存一定会不够用的情况。<strong>支持向量机能让计算机处理无限多个特征。</strong></p>
<h2 id="1-4-无监督学习（Unsupervised-Learning）"><a href="#1-4-无监督学习（Unsupervised-Learning）" class="headerlink" title="1.4 无监督学习（Unsupervised Learning）"></a>1.4 无监督学习（Unsupervised Learning）</h2><p>相对于监督学习，训练集不会有人为标注的结果（无反馈），我们<strong>不会给出</strong>结果或<strong>无法得知</strong>训练集的结果是什么样，而是单纯由计算机通过无监督学习算法自行分析，从而“得出结果”。计算机可能会把特定的数据集归为几个不同的簇，故叫做聚类算法。</p>
<p>无监督学习一般分为两种：</p>
<ol>
<li>聚类（Clustering）<ul>
<li>新闻聚合</li>
<li>DNA 个体聚类</li>
<li>天文数据分析</li>
<li>市场细分</li>
<li>社交网络分析</li>
</ul>
</li>
<li>非聚类（Non-clustering）<ul>
<li>鸡尾酒问题</li>
</ul>
</li>
</ol>
<p><strong>新闻聚合</strong></p>
<p>在例如谷歌新闻这样的网站中，每天后台都会收集成千上万的新闻，然后将这些新闻分组成一个个的新闻专题，这样一个又一个聚类，就是应用了无监督学习的结果。</p>
<p><strong>鸡尾酒问题</strong></p>
 <img src="/2024/04/18/15-34-48/20180105_201639.png" class>
<p>在鸡尾酒会上，大家说话声音彼此重叠，几乎很难分辨出面前的人说了什么。我们很难对于这个问题进行数据标注，而这里的通过机器学习的无监督学习算法，就可以将说话者的声音同背景音乐分离出来。</p>
<h1 id="2-单变量线性回归（Linear-Regression-with-One-Variable）"><a href="#2-单变量线性回归（Linear-Regression-with-One-Variable）" class="headerlink" title="2 单变量线性回归（Linear Regression with One Variable）"></a>2 单变量线性回归（Linear Regression with One Variable）</h1><h2 id="2-1-模型表示（Model-Representation）"><a href="#2-1-模型表示（Model-Representation）" class="headerlink" title="2.1 模型表示（Model Representation）"></a>2.1 模型表示（Model Representation）</h2><ol>
<li>房价预测训练集</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>Size in $feet^2$ ($x$)</th>
<th>Price ($) in 1000’s($y$)</th>
</tr>
</thead>
<tbody>
<tr>
<td>2104</td>
<td>460</td>
</tr>
<tr>
<td>1416</td>
<td>232</td>
</tr>
<tr>
<td>1534</td>
<td>315</td>
</tr>
<tr>
<td>852</td>
<td>178</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
<p>房价预测训练集中，同时给出了输入 $x$ 和输出结果 $y$，即给出了人为标注的<strong>“正确结果”</strong>，且预测的量是连续的，属于监督学习中的回归问题。</p>
<ol>
<li><p><strong>问题解决模型</strong></p>
<img src="/2024/04/18/15-34-48/20180105_212048.png" class>
</li>
</ol>
<p>其中 $h$ 代表结果函数，也称为<strong>假设（hypothesis）</strong> 。假设函数根据输入（房屋的面积），给出预测结果输出（房屋的价格），即是一个 $X\to Y$ 的映射。</p>
<p>$h_\theta(x)=\theta_0+\theta_1x$，为解决房价问题的一种可行表达式。</p>
<blockquote>
<p>$x$: 特征/输入变量。</p>
</blockquote>
<p>上式中，$\theta$ 为参数，$\theta$ 的变化才决定了输出结果，不同以往，这里的 $x$ 被我们<strong>视作已知</strong>（不论是数据集还是预测时的输入），所以怎样解得 $\theta$ 以更好地拟合数据，成了求解该问题的最终问题。</p>
<p>单变量，即只有一个特征（如例子中房屋的面积这个特征）。</p>
<h2 id="2-2-代价函数（Cost-Function）"><a href="#2-2-代价函数（Cost-Function）" class="headerlink" title="2.2 代价函数（Cost Function）"></a>2.2 代价函数（Cost Function）</h2><blockquote>
<p>李航《统计学习方法》一书中，损失函数与代价函数两者为<strong>同一概念</strong>，未作细分区别，全书没有和《深度学习》一书一样混用，而是统一使用<strong>损失函数</strong>来指代这类类似概念。</p>
<p>吴恩达（Andrew Ng）老师在其公开课中对两者做了细分。</p>
<p><strong>损失函数</strong>（Loss/Error Function）: 计算<strong>单个</strong>样本的误差。</p>
<p><strong>代价函数</strong>（Cost Function）: 计算整个训练集<strong>所有损失函数之和的平均值</strong></p>
</blockquote>
<p>我们的目的在于求解预测结果 $h$ 最接近于实际结果 $y$ 时 $\theta$ 的取值，则问题可表达为<strong>求解 $\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})$ 的最小值</strong>。</p>
<blockquote>
<p>$m$: 训练集中的样本总数</p>
<p>$y$: 目标变量/输出变量</p>
<p>$\left(x, y\right)$: 训练集中的实例</p>
<p>$\left(x^{\left(i\right)},y^{\left(i\right)}\right)$: 训练集中的第 $i$ 个样本实例</p>
</blockquote>
<img src="/2024/04/18/15-34-48/20180105_224648.png" class>
<p>上图展示了当 $\theta$ 取不同值时，$h_\theta\left(x\right)$ 对数据集的拟合情况，蓝色虚线部分代表<strong>建模误差</strong>。</p>
<p>为了求解最小值，引入代价函数（Cost Function）概念，用于度量建模误差。考虑到要计算最小值，应用二次函数对求和式建模，即应用统计学中的平方损失函数（最小二乘法）：</p>
<p>$$<br>J(\theta_0, \theta_1)= \dfrac{ 1 }{ 2m } \displaystyle \sum_ {i=1}^m\left(\hat{y}_{i}-y_{i} \right)^2=\dfrac{1}{2m}\displaystyle\sum_{i=1}^m\left(h_\theta(x_{i})-y_{i}\right)^2<br>$$</p>
<blockquote>
<p>$\hat{y}$: $y$ 的预测值</p>
<p>系数 $\frac{1}{2}$ 存在与否都不会影响结果，这里是为了在应用梯度下降时便于求解，平方的导数会抵消掉 $\frac{1}{2}$ 。</p>
</blockquote>
<p>讨论到这里，我们的问题就转化成了<strong>求解 $J\left( \theta_0, \theta_1  \right)$ 的最小值</strong>。</p>
<h2 id="2-3-代价函数-直观理解1（Cost-Function-Intuition-I）"><a href="#2-3-代价函数-直观理解1（Cost-Function-Intuition-I）" class="headerlink" title="2.3 代价函数 - 直观理解1（Cost Function - Intuition I）"></a>2.3 代价函数 - 直观理解1（Cost Function - Intuition I）</h2><p>根据上节视频，列出如下定义：</p>
<ul>
<li>假设函数（Hypothesis）: $h_\theta(x)=\theta_0+\theta_1x$</li>
<li>参数（Parameters）: $\theta_0, \theta_1$</li>
<li>代价函数（Cost Function）: $J\left( \theta_0, \theta_1  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ { {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)}^{2} } }$</li>
<li>目标（Goal）: $\underset{\theta_0, \theta_1}{\text{minimize} } J \left(\theta_0, \theta_1 \right)$</li>
</ul>
<p>为了直观理解代价函数到底是在做什么，先假设 $\theta_0 = 0$，并假设训练集有三个数据，分别为$\left(1, 1\right), \left(2, 2\right), \left(3, 3\right)$，这样在平面坐标系中绘制出 $h_\theta\left(x\right)$ ，并分析 $J\left(\theta_0, \theta_1\right)$ 的变化。</p>
<img src="/2024/04/18/15-34-48/20180106_085915.png" class>
<p>右图 $J\left(\theta_0, \theta_1\right)$ 随着 $\theta_1$ 的变化而变化，可见<strong>当 $\theta_1 = 1$ 时，$J\left(\theta_0, \theta_1 \right) = 0$，取得最小值，</strong>对应于左图青色直线，即函数 $h$ 拟合程度最好的情况。</p>
<h2 id="2-4-代价函数-直观理解2（Cost-Function-Intuition-II）"><a href="#2-4-代价函数-直观理解2（Cost-Function-Intuition-II）" class="headerlink" title="2.4 代价函数 - 直观理解2（Cost Function - Intuition II）"></a>2.4 代价函数 - 直观理解2（Cost Function - Intuition II）</h2><p>给定数据集：<br><img src="/2024/04/18/15-34-48/20180106_091307.png" class></p>
<p>参数在 $\theta_0$ 不恒为 $0$ 时代价函数 $J\left(\theta\right)$ 关于 $\theta_0, \theta_1$ 的3-D图像，图像中的高度为代价函数的值。</p>
<img src="/2024/04/18/15-34-48/20180106_090904.png" class>
<p>由于3-D图形不便于标注，所以将3-D图形转换为<strong>轮廓图（contour plot）</strong>，下面用轮廓图（下图中的右图）来作直观理解，其中相同颜色的一个圈代表着同一高度（同一 $J\left(\theta\right)$ 值）。</p>
<p>$\theta_0 = 360, \theta_1 =0$ 时：</p>
<img src="/2024/04/18/15-34-48/0f38a99c8ceb8aa5b90a5f12136fdf43.png" class>
<p>大概在 $\theta_0 = 0.12, \theta_1 =250$ 时：</p>
<img src="/2024/04/18/15-34-48/20180106_092119.png" class>
<p>上图中最中心的点（红点），近乎为图像中的最低点，也即代价函数的最小值，此时对应 $h_\theta\left(x\right)$ 对数据的拟合情况如左图所示。</p>
<h2 id="2-5-梯度下降（Gradient-Descent）"><a href="#2-5-梯度下降（Gradient-Descent）" class="headerlink" title="2.5 梯度下降（Gradient Descent）"></a>2.5 梯度下降（Gradient Descent）</h2><p>在特征量很大的情况下，即便是借用计算机来生成图像，人工的方法也很难读出 $J\left(\theta\right)$ 的最小值，并且大多数情况无法进行可视化，故引入<strong>梯度下降（Gradient Descent）方法，让计算机自动找出最小化代价函数时对应的 $\theta$ 值。</strong></p>
<p>梯度下降背后的思想是：开始时，我们随机选择一个参数组合$\left( {\theta_{0} },{\theta_{1} },……,{\theta_{n} } \right)$即起始点，计算代价函数，然后寻找下一个能使得代价函数下降最多的参数组合。不断迭代，直到找到一个<strong>局部最小值（local minimum）</strong>，由于下降的情况只考虑当前参数组合周围的情况，所以无法确定当前的局部最小值是否就是<strong>全局最小值（global minimum）</strong>，不同的初始参数组合，可能会产生不同的局部最小值。</p>
<p>下图根据不同的起始点，产生了两个不同的局部最小值。</p>
<img src="/2024/04/18/15-34-48/db48c81304317847870d486ba5bb2015.jpg" class>
<p>视频中举了下山的例子，即我们在山顶上的某个位置，为了下山，就不断地看一下周围<strong>下一步往哪走</strong>下山比较快，然后就<strong>迈出那一步</strong>，一直重复，直到我们到达山下的某一处<strong>陆地</strong>。</p>
<p>梯度下降公式：</p>
<p>$$<br>\begin{split}<br>&amp; \text{Repeat until convergence:} \; \lbrace \\<br>&amp;{ {\theta }_{j} }:={ {\theta }_{j} }-\alpha \frac{\partial }{\partial { {\theta }_{j} } }J\left( {\theta_{0} },{\theta_{1} }  \right) \\<br>\rbrace<br>\end{split}<br>$$</p>
<blockquote>
<p>“-”来源于：<strong>正值向左移动减少，负值向右移动增加</strong><br>${\theta }_{j}$: 第 $j$ 个特征参数</p>
<p>”:=“: 赋值操作符</p>
<p>$\alpha$: 学习速率（learning rate）, $\alpha &gt; 0$</p>
<p>$\frac{\partial }{\partial { {\theta }_{j} } }J\left( \theta_0, \theta_j  \right)$: $J\left( \theta_0, \theta_j \right)$ 的偏导</p>
</blockquote>
<p>公式中，学习速率决定了参数值变化的速率即”<strong>走多少距离</strong>“，而偏导这部分决定了下降的方向即”<strong>下一步往哪里</strong>“走（当然实际上的走多少距离是由偏导值给出的，学习速率起到调整后决定的作用），收敛处的局部最小值又叫做极小值，即”<strong>陆地</strong>“。</p>
<img src="/2024/04/18/15-34-48/20180106_101659.png" class>
<p>注意，在计算时要<strong>批量更新 $\theta$ 值</strong>，即如上图中的左图所示，否则结果上会有所出入，原因不做细究。</p>
<h2 id="2-6-梯度下降直观理解（Gradient-Descent-Intuition）"><a href="#2-6-梯度下降直观理解（Gradient-Descent-Intuition）" class="headerlink" title="2.6 梯度下降直观理解（Gradient Descent Intuition）"></a>2.6 梯度下降直观理解（Gradient Descent Intuition）</h2><p>该节探讨 $\theta_1$ 的梯度下降更新过程，即 $\theta_1 := \theta_1 - \alpha\frac{d}{d\theta_1}J\left(\theta_1\right)$。</p>
<img src="/2024/04/18/15-34-48/20180106_184926.png" class>
<p>直线的斜率，表示了函数 $J\left(\theta\right)$ 在初始点处有<strong>正斜率</strong>，也就是说它有<strong>正导数</strong>，会<strong>向左边移动</strong>。不断重复，直到收敛。</p>
<p>初始 $\theta$ 值（初始点）是任意的。</p>
<p>对于学习速率 $\alpha$，需要选取一个合适的值才能使得梯度下降算法运行良好。</p>
<ul>
<li><p>学习速率过小图示：</p>
<img src="/2024/04/18/15-34-48/20180106_190944.png" class>
<p>收敛的太慢，需要更多次的迭代。</p>
</li>
<li><p>学习速率过大图示：</p>
<img src="/2024/04/18/15-34-48/20180106_191023.png" class>
<p>可能越过最低点，甚至导致无法收敛。</p>
</li>
</ul>
<p><strong>学习速率只需选定即可</strong>，不需要动态改变，随着斜率越来越接近于0，代价函数的变化幅度会越来越小，直到收敛到局部极小值。</p>
<p>代价函数随着迭代的进行，变化的幅度越来越小。</p>
<img src="/2024/04/18/15-34-48/20180106_191956.png" class>
<h2 id="2-7-线性回归中的梯度下降（Gradient-Descent-For-Linear-Regression）"><a href="#2-7-线性回归中的梯度下降（Gradient-Descent-For-Linear-Regression）" class="headerlink" title="2.7 线性回归中的梯度下降（Gradient Descent For Linear Regression）"></a>2.7 线性回归中的梯度下降（Gradient Descent For Linear Regression）</h2><p>线性回归模型</p>
<ul>
<li>$h_\theta(x)=\theta_0+\theta_1x$</li>
<li>$J\left( \theta_0, \theta_1  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ { {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)}^{2} } }$</li>
</ul>
<p>梯度下降算法<br>$$<br>\begin{split}<br>  &amp; \text{Repeat until convergence:} \; \lbrace \\<br>  &amp;{ {\theta }_{j} }:={ {\theta }_{j} }-\alpha \frac{\partial }{\partial { {\theta }_{j} } }J\left( {\theta_{0} },{\theta_{1} }  \right) \\<br>  \rbrace<br>  \end{split}<br>$$</p>
<p>直接将线性回归模型公式代入梯度下降公式可得出公式</p>
<img src="/2024/04/18/15-34-48/20180106_203726.png" class>
<p>当 $j = 0, j = 1$ 时，<strong>线性回归中代价函数求导的推导过程（：</strong></p>
<p>$$<br>\begin{split}<br>\frac{\partial}{\partial\theta_j} J(\theta_1, \theta_1)&amp;=\frac{\partial}{\partial\theta_j} \left(\frac{1}{2m}\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)}^{2} } \right)\\<br>&amp;=\left(\frac{1}{2m}*2\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} } \right)*\frac{\partial}{\partial\theta_j}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} } \\<br>&amp;=\left(\frac{1}{m}\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} } \right)*\frac{\partial}{\partial\theta_j} {\left( \theta_0 + \theta_1{x_1^{(i)} }-{ {y}^{(i)} } \right)}<br>\end{split}<br>$$</p>
<p>所以当 $j = 0$ 时：</p>
<p>$$<br>\frac{\partial}{\partial\theta_0} J(\theta)=\frac{1}{m}\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} }<br>$$</p>
<p>所以当 $j = 1$ 时：</p>
<p>$$<br>\frac{\partial}{\partial\theta_1} J(\theta)=\frac{1}{m}\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} } *x_1^{(i)}<br>$$</p>
<p>上文中所提到的梯度下降，都为批量梯度下降（Batch Gradient Descent），即每次计算都使用<strong>所有</strong>的数据集 $\left(\sum\limits_{i=1}^{m}\right)$ 更新。</p>
<p>由于线性回归函数呈现<strong>碗状</strong>，且<strong>只有一个</strong>全局的最优值，所以函数<strong>一定总会</strong>收敛到全局最小值（学习速率不可过大）。同时，函数 $J$ 被称为<strong>凸二次函数</strong>，而线性回归函数求解最小值问题属于<strong>凸函数优化问题</strong>。</p>
<img src="/2024/04/18/15-34-48/24e9420f16fdd758ccb7097788f879e7.png" class>
<p>另外，使用循环求解，代码较为冗余，后面会讲到如何使用<strong>向量化（Vectorization）</strong>来简化代码并优化计算，使梯度下降运行的更快更好。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习, 深度学习, 吴恩达</tag>
      </tags>
  </entry>
  <entry>
    <title>天梯赛刷题记录</title>
    <url>/2024/04/04/12-45-10/</url>
    <content><![CDATA[<h1 id="L1-039-古风排版"><a href="#L1-039-古风排版" class="headerlink" title="L1-039. 古风排版"></a>L1-039. 古风排版</h1><p>中国的古人写文字，是从右向左竖向排版的。本题就请你编写程序，把一段文字按古风排版。</p>
<p>输入格式：</p>
<p>输入在第一行给出一个正整数N（&lt;100），是每一列的字符数。第二行给出一个长度不超过1000的非空字符串，以回车结束。</p>
<p>输出格式：</p>
<p>按古风格式排版给定的字符串，每列N个字符（除了最后一列可能不足N个）</p>
<p>输入样例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">4</span><br><span class="line">This is a test case</span><br></pre></td></tr></table></figure>
<p>输出样例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">asa T</span><br><span class="line">st ih</span><br><span class="line">e tsi</span><br><span class="line"> ce s</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>算法比赛</tag>
        <tag>c++</tag>
        <tag>算法模板</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP 实验V ShellLab</title>
    <url>/2024/03/14/12-34-42/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.03.14：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2023/03/15/10-59-10/" title="csapp 笔记汇总">CSAPP - 笔记汇总</a></li>
<li><a href="/2023/03/09/11-54-50/" title="CSAPP 实验I Data Lab">I Data Lab - 位操作，数据表示</a></li>
<li><a href="/2023/03/14/09-43-26/" title="CSAPP 实验II Bomb Lab">II Bomb Lab - 汇编，栈帧与 gdb</a></li>
<li><a href="/2023/03/18/15-54-47/" title="CSAPP 实验III Attack Lab">III Attack Lab - 漏洞是如何被攻击的</a></li>
<li><a href="/2024/03/09/16-34-34/" title="CSAPP 实验IV CacheLab">IV Cache Lab - 实现一个缓存系统</a></li>
<li><a href="/2024/03/14/12-34-42/" title="CSAPP 实验V ShellLab">V Shell Lab - 实现一个Shell</a>
</li>
</ul>
<h1 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h1><p>ShellLab实验，理解程序控制和信号，完成一个简单Shell。</p>
<h1 id="相关内容"><a href="#相关内容" class="headerlink" title="相关内容"></a>相关内容</h1><ul>
<li><p><code>tsh&gt; /bin/ls -l -d</code>，<code>&amp;</code>后台运行</p>
</li>
<li><p><code>int main(int argc, char *argv[])</code></p>
<ul>
<li><p><code>argc</code>：参数个数  <code>argv[]</code>：从左往右的参数</p>
</li>
<li><p><code>ctrl-c</code> SIGINT终止信号，<code>ctrl-z</code>SIGTSTP停止信号，SIGCONT继续信号</p>
</li>
<li><p><code>jobs</code>：列出所有作业</p>
</li>
<li><p><code>bg &lt;job&gt;</code>：后台暂停的作业开始运行</p>
</li>
<li><p><code>fg &lt;job&gt;</code>：前台暂停的作业开始运行</p>
</li>
<li><p><code>kill &lt;job&gt;</code>：杀死作业</p>
</li>
</ul>
</li>
<li><p><strong>tsh</strong>的规格</p>
<ul>
<li><p>提示符为<code>tsh&gt;</code></p>
</li>
<li><p>输入命令为<code>name arguments</code>：如果name是内置命令，直接执行，否则视为可执行文件路径，并调用初始子进程运行</p>
</li>
<li><p>不需要支持<code>|</code>和<code>&lt;</code>，<code>&gt;</code></p>
</li>
<li><p>信号需要传递到所有子进程</p>
</li>
<li><p>命令末尾带<code>&amp;</code>，后台运行</p>
</li>
<li><p>进程PID<code>%5</code>，作业JID<code>5</code></p>
</li>
<li><p>需要支持内置命令<code>quit</code>，<code>jobs</code>，<code>bg &lt;job&gt;</code>，<code>fg &lt;job&gt;</code></p>
</li>
<li><p>如果僵尸进程没有收到信号就终止了，需要打印出异常信息</p>
</li>
</ul>
</li>
<li><p>检查作业</p>
<ul>
<li><p><code>tshref</code>：样例tsh，可以对比自己的shell</p>
</li>
<li><p><code>sdriver.pl</code>：打印输入的shell命令和输出信息<br> <code>./sdriver.pl -t trace01.txt -s ./tsh -a &quot;-p&quot;</code>：<code>-t</code>追踪文件，<code>-s</code>指定shell，<code>-a &quot;-p&quot;</code>不输出提示符</p>
</li>
<li><p>也可以使用<code>make test01</code>测试自己的shell，<code>make rtest01</code>测试样例shell</p>
</li>
<li><p><code>trace01.txt</code>序号从01-16，越小测试越简单</p>
</li>
<li><p><code>tshref.out</code>所有的输出信息以供参考</p>
</li>
</ul>
</li>
<li><p>提示</p>
<ul>
<li><p>根据追踪文件来设计shell，依次从简单开始，按照输出完成设计</p>
</li>
<li><p>要检测出<code>kill -pid</code>的错误</p>
</li>
<li><p>多使用下列函数</p>
<ul>
<li><p><code>pid_t waitpid (pid_t pid, int* statusp, int options)</code>：</p>
<blockquote>
<p>成功时返回子进程的PID，  </p>
<p>options = WNOHANG时立即返回0     表示没有任何子进程终止</p>
<p>options = WUNTRACED时，接收到SIGSTOP信号，终止或暂停。</p>
<p>如果发生其他错误返回-1。</p>
</blockquote>
</li>
<li><p><code>kill</code>:<code>kill -s &lt;信号&gt;</code></p>
<blockquote>
<p>用指定信号形式终止进程，可以杀死可以暂停</p>
<p>-s，会将s的进程组全部执行</p>
</blockquote>
</li>
<li><p><code>int fork(void)</code>:</p>
<blockquote>
<p>创建一个子进程，子进程返回0，父进程返回子进程的ID，调用一次返回两次，先返回子进程，再返回父进程</p>
</blockquote>
</li>
<li><p><code>int execve(const char *filename, char *const argv[], char *const envp[]);</code>: </p>
<blockquote>
<p>参数为新程序的路径名称，命令行参数数组，环境变量参数数组</p>
<p>替换进程的状态和上下文，切换另一个进程</p>
<p>执行成功不会返回到调用，错误返回-1</p>
</blockquote>
</li>
<li><p><code>int setpgid(pid_t pid, pid_t pgid)</code>:</p>
<blockquote>
<p>给pid进程设置为pgid进程组识别码，如果pid为0设置当前进程，pgid为0，用当前组识别码取代，返回为组识别码，-1错误</p>
</blockquote>
</li>
<li><p><code>sigprocmash</code>:</p>
<blockquote>
<p>int sigprocmask(int how, const sigset_t <em>set, sigset_t </em>oldset);<br>参数：<br>how：用于指定信号修改的方式，可能选择有三种：<br>SIG_BLOCK //加入信号到进程屏蔽。<br>SIG_UNBLOCK //从进程屏蔽里将信号删除。<br>SIG_SETMASK //将set的值设定为新的进程屏蔽。</p>
<p>set：为指向信号集的指针，在此专指新设的信号集，如果仅想读取现在的屏蔽值，可将其置为NULL。<br>oldset：也是指向信号集的指针，在此存放原来的信号集。<br>返回说明：<br>成功执行时，返回0。失败返回-1，errno被设为EINVAL。</p>
</blockquote>
</li>
<li><p><code>exit(int status)</code>：</p>
<blockquote>
<p>终止进程，正常返回状态0</p>
</blockquote>
</li>
<li><p>尽量不要在自己的shell中使用more, less, vi, emacs，多使用/bin/ls,/bin/ps, /bin/echo</p>
</li>
<li><p>如果在Unix shell中像自己的shell输入 ctrl-c：fork之后执行之前，子进程先调用<code>setpgid(0, 0)</code>，放入一个新的进程组，保证Unix shell的进程组中只有shell。不会把命令传递给所有Unix shell的进程组，只传递给shell。</p>
</li>
<li><p>建议在<code>waitfg</code>的循环中用<code>sleep</code>函数，在<code>sigchld_handler</code>中调用<code>waitpid</code></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>常用信号量</p>
<img src="/2024/03/14/12-34-42/QQ20240314-160705@2x.png" class>
<img src="/2024/03/14/12-34-42/QQ20240314-154949@2x.png" class>
</li>
</ul>
<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><h2 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h2><p>trace01-03比较简单，用课本中代码写入就可以，修改一下fork和execve封装。</p>
<h2 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h2><h3 id="trace04"><a href="#trace04" class="headerlink" title="trace04"></a>trace04</h3><p>根据标准输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./sdriver.pl -t trace04.txt -s ./tsh -a &quot;-p&quot;</span><br><span class="line">#</span><br><span class="line"># trace04.txt - Run a background job.</span><br><span class="line">#</span><br><span class="line">tsh&gt; ./myspin 1 &amp;</span><br><span class="line">[1] (26252) ./myspin 1 &amp;</span><br></pre></td></tr></table></figure>
<p>在主函数执行中增加添加作业函数<code>addjob</code>，修改一下原来的输出格式，<code>printf(&quot;[%d] (%d)  %s&quot;, pid2jid(pid), pid, cmdline);</code>。</p>
<p>问题：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tsh&gt; ./myspin 1 &amp;</span><br><span class="line">[2] (920)  ./myspin 1 &amp;</span><br></pre></td></tr></table></figure>
<p>作业从[2]标号开始，可能是重复添加作业了。查看输出后，发现echo函数添加作业后，运行结束时没有删除作业，导致作业号多一个。</p>
<p>解决办法：</p>
<p>在waitpid之后，增加一个删除作业函数<code>deletejob</code></p>
<h2 id="trace05"><a href="#trace05" class="headerlink" title="trace05"></a>trace05</h2><p>增加打印jobs列表</p>
<h3 id="trace06"><a href="#trace06" class="headerlink" title="trace06"></a>trace06</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#</span><br><span class="line"># trace06.txt - Forward SIGINT to foreground job.</span><br><span class="line">#</span><br><span class="line">/bin/echo -e tsh&gt; ./myspin 4</span><br><span class="line">./myspin 4 </span><br><span class="line"></span><br><span class="line">SLEEP 2</span><br><span class="line">INT</span><br></pre></td></tr></table></figure>
<blockquote>
<p>SLEEP命令不是输入进去的字符串，主要是获取作业的终止状态</p>
</blockquote>
<p>相关内容</p>
<ul>
<li><p>waitpid返回状态status</p>
<ul>
<li><p>WIFEXITED(status)：非0正常结束，退出状态WEXITSTATUS(status)</p>
</li>
<li><p>WIFSIGNALED(status)：非0异常终止，退出的信号编号WTERMSIG(status)</p>
</li>
<li><p>WIFSTOPPED(status)：非0暂停状态，暂停的信号编号WSTOPSIG(status)</p>
</li>
<li><p>WIFCONTINUED(status)：非0 暂停后以继续运行</p>
</li>
</ul>
</li>
<li><p>options变量</p>
<ul>
<li><p><code>WNOHANG</code>：没有子进程终止时立即返回，而不会阻塞。</p>
</li>
<li><p><code>WUNTRACED</code>：挂起，返回终止或暂停的进程pid</p>
</li>
<li><p><code>WCONTINUED</code>：挂起，返回一个正在运行的进程终止，或一个暂停的收到SIGCONT信号继续执行。</p>
</li>
</ul>
</li>
<li><p>信号量相关</p>
<ul>
<li><p><code>sigfillset(&amp;mask_all);</code>返回当前进程的信号阻塞集合</p>
</li>
<li><p><code>sigemptyset(&amp;mask_one);</code>初试空值</p>
</li>
<li><p><code>sigaddset(&amp;mask_one, SIGCHLD);</code>，添加一位阻塞位</p>
</li>
<li><p><code>sigprocmask(int how, sigset_t *set, sigset_t *oldset)</code></p>
<blockquote>
<p>how: <code>SIG_SETMASK</code>赋值  </p>
<pre><code>      `SIG_BLOCK`阻塞 做或运算   与原来的集合

      `SIG_UNBLOCK` 解除  做与运算
</code></pre><p>set，操作集合</p>
<p>oldset，保存原集合，可以为NULL</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p>对add函数和del函数阻塞信号。</p>
<p>检测INT键，绑定响应函数，用kill杀死进程，系统发出<code>SIGCHLD</code>信号，在<code>SIGCHLD</code>响应函数中删除作业。</p>
<h3 id="trace07"><a href="#trace07" class="headerlink" title="trace07"></a>trace07</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#</span><br><span class="line"># trace07.txt - Forward SIGINT only to foreground job.</span><br><span class="line">#</span><br><span class="line">/bin/echo -e tsh&gt; ./myspin 4 \046</span><br><span class="line">./myspin 4 &amp;</span><br><span class="line"></span><br><span class="line">/bin/echo -e tsh&gt; ./myspin 5</span><br><span class="line">./myspin 5 </span><br><span class="line"></span><br><span class="line">SLEEP 2</span><br><span class="line">INT</span><br><span class="line"></span><br><span class="line">/bin/echo tsh&gt; jobs</span><br><span class="line">jobs</span><br></pre></td></tr></table></figure>
<p>06已经做完了，输出完整。</p>
<h3 id="trace08"><a href="#trace08" class="headerlink" title="trace08"></a>trace08</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/bin/echo -e tsh&gt; ./myspin 4 \046</span><br><span class="line">./myspin 4 &amp;</span><br><span class="line"></span><br><span class="line">/bin/echo -e tsh&gt; ./myspin 5</span><br><span class="line">./myspin 5 </span><br><span class="line"></span><br><span class="line">SLEEP 2</span><br><span class="line">TSTP</span><br><span class="line"></span><br><span class="line">/bin/echo tsh&gt; jobs</span><br><span class="line">jobs</span><br></pre></td></tr></table></figure>
<p>完成前台进程暂停信号响应。kill函数可以暂停进程。</p>
<p>代码可以正常暂停，但是暂停之后不会终止，无法退出。</p>
<p>解决：</p>
<p>用kill(-pid,sig);函数对进程组整体操作。</p>
<h3 id="trace09"><a href="#trace09" class="headerlink" title="trace09"></a>trace09</h3><p>问题：</p>
<p>需要完成bg内置命令，暂停程序后台运行。</p>
<blockquote>
<p>不太清楚中间一次输出信息，为什么不需要换行。</p>
</blockquote>
<h3 id="trace10"><a href="#trace10" class="headerlink" title="trace10"></a>trace10</h3><p>完成fg内置命令</p>
<h3 id="trace11-13"><a href="#trace11-13" class="headerlink" title="trace11-13"></a>trace11-13</h3><img src="/2024/03/14/12-34-42/QQ20240316-185955@2x.png" class>
<p>输出信息与样例shell相同，但和答案不同。</p>
<h3 id="trace14"><a href="#trace14" class="headerlink" title="trace14"></a>trace14</h3><p>问题：</p>
<p>简单的错误处理，处理输入输出格式错误。</p>
<h3 id="trace15-16"><a href="#trace15-16" class="headerlink" title="trace15-16"></a>trace15-16</h3><p>运行结果都对</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>不处理行数问题了，没有评分程序。只有所有例子都输出相同，不太确定是不是都是正确的。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * tsh - A tiny shell program with job control</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;Put your name and login ID here&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;ctype.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Misc manifest constants */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXLINE 1024   <span class="comment">/* max line size */</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXARGS 128    <span class="comment">/* max args on a command line */</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXJOBS 16     <span class="comment">/* max jobs at any point in time */</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXJID 1 &lt;&lt; 16 <span class="comment">/* max job ID */</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Job states */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> UNDEF 0 <span class="comment">/* undefined */</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FG 1    <span class="comment">/* running in foreground */</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> BG 2    <span class="comment">/* running in background */</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ST 3    <span class="comment">/* stopped */</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Jobs states: FG (foreground), BG (background), ST (stopped)</span></span><br><span class="line"><span class="comment"> * Job state transitions and enabling actions:</span></span><br><span class="line"><span class="comment"> *     FG -&gt; ST  : ctrl-z</span></span><br><span class="line"><span class="comment"> *     ST -&gt; FG  : fg command</span></span><br><span class="line"><span class="comment"> *     ST -&gt; BG  : bg command</span></span><br><span class="line"><span class="comment"> *     BG -&gt; FG  : fg command</span></span><br><span class="line"><span class="comment"> * At most 1 job can be in the FG state.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Global variables */</span></span><br><span class="line"><span class="keyword">extern</span> <span class="type">char</span> **environ;   <span class="comment">/* defined in libc */</span></span><br><span class="line"><span class="type">char</span> prompt[] = <span class="string">&quot;tsh&gt; &quot;</span>; <span class="comment">/* command line prompt (DO NOT CHANGE) */</span></span><br><span class="line"><span class="type">int</span> verbose = <span class="number">0</span>;         <span class="comment">/* if true, print additional output */</span></span><br><span class="line"><span class="type">int</span> nextjid = <span class="number">1</span>;         <span class="comment">/* next job ID to allocate */</span></span><br><span class="line"><span class="type">char</span> sbuf[MAXLINE];      <span class="comment">/* for composing sprintf messages */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">job_t</span> &#123;</span>           <span class="comment">/* The job struct */</span></span><br><span class="line">  <span class="type">pid_t</span> pid;             <span class="comment">/* job PID */</span></span><br><span class="line">  <span class="type">int</span> jid;               <span class="comment">/* job ID [1, 2, ...] */</span></span><br><span class="line">  <span class="type">int</span> state;             <span class="comment">/* UNDEF, BG, FG, or ST */</span></span><br><span class="line">  <span class="type">char</span> cmdline[MAXLINE]; <span class="comment">/* command line */</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">job_t</span> <span class="title">jobs</span>[<span class="title">MAXJOBS</span>];</span> <span class="comment">/* The job list */</span></span><br><span class="line"><span class="comment">/* End global variables */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Function prototypes */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//补充函数</span></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">Fork</span><span class="params">(<span class="type">void</span>)</span>;                                                              <span class="comment">//封装fork</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Execve</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *filename, <span class="type">char</span> *<span class="type">const</span> argv[], <span class="type">char</span> *<span class="type">const</span> environ[])</span>;  <span class="comment">//封装execv函数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Here are the functions that you will implement */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">eval</span><span class="params">(<span class="type">char</span> *cmdline)</span>;  <span class="comment">//分析命令</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">builtin_cmd</span><span class="params">(<span class="type">char</span> **argv)</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">do_bgfg</span><span class="params">(<span class="type">char</span> **argv)</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">waitfg</span><span class="params">(<span class="type">pid_t</span> pid)</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">sigchld_handler</span><span class="params">(<span class="type">int</span> sig)</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">sigtstp_handler</span><span class="params">(<span class="type">int</span> sig)</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">sigint_handler</span><span class="params">(<span class="type">int</span> sig)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Here are helper routines that we&#x27;ve provided for you */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">parseline</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *cmdline, <span class="type">char</span> **argv)</span>;  <span class="comment">//返回bg标志  1 后台</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sigquit_handler</span><span class="params">(<span class="type">int</span> sig)</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">clearjob</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *job)</span>;                                     <span class="comment">//清除队列中的一个作业</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">initjobs</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs)</span>;                                    <span class="comment">//初始化</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">maxjid</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs)</span>;                                       <span class="comment">//返回以分配的最大作业号</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">addjob</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs, <span class="type">pid_t</span> pid, <span class="type">int</span> state, <span class="type">char</span> *cmdline)</span>;  <span class="comment">//添加一个作业  返回1  满了返回0</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">deletejob</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs, <span class="type">pid_t</span> pid)</span>;                         <span class="comment">//删除队列中一个作业   成功返回1</span></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">fgpid</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs)</span>;                                      <span class="comment">//返回一个前台运行的作业号    没有返回0</span></span><br><span class="line"><span class="keyword">struct</span> <span class="type">job_t</span> *<span class="title function_">getjobpid</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs, <span class="type">pid_t</span> pid)</span>;               <span class="comment">//使用pid查找一个作业  没找到 返回空</span></span><br><span class="line"><span class="keyword">struct</span> <span class="type">job_t</span> *<span class="title function_">getjobjid</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs, <span class="type">int</span> jid)</span>;                 <span class="comment">//用jid查找一个作业</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">pid2jid</span><span class="params">(<span class="type">pid_t</span> pid)</span>;                                               <span class="comment">// 用pid 返回 jid,  没找到返回0</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">listjobs</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs)</span>;                                    <span class="comment">//打印作业队列</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">usage</span><span class="params">(<span class="type">void</span>)</span>;                                   <span class="comment">//打印帮助菜单</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">unix_error</span><span class="params">(<span class="type">char</span> *msg)</span>;                         <span class="comment">// unix风格打印错误信息</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">app_error</span><span class="params">(<span class="type">char</span> *msg)</span>;                          <span class="comment">// app风格的错误信息</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">void</span> <span class="title function_">handler_t</span><span class="params">(<span class="type">int</span>)</span>;                        <span class="comment">//一个int参数,无返回</span></span><br><span class="line"><span class="type">handler_t</span> *<span class="title function_">Signal</span><span class="params">(<span class="type">int</span> signum, <span class="type">handler_t</span> *handler)</span>;  <span class="comment">//信号包装器</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * main - The shell&#x27;s main routine</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span> &#123;</span><br><span class="line">  <span class="type">char</span> c;</span><br><span class="line">  <span class="type">char</span> cmdline[MAXLINE];</span><br><span class="line">  <span class="type">int</span> emit_prompt = <span class="number">1</span>; <span class="comment">/* emit prompt (default) */</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//不用管    重定向输出</span></span><br><span class="line">  <span class="comment">/* Redirect stderr to stdout (so that driver will get all output</span></span><br><span class="line"><span class="comment">   * on the pipe connected to stdout) */</span></span><br><span class="line">  dup2(<span class="number">1</span>, <span class="number">2</span>);  <span class="comment">//把1的文件描述符 复制给2</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Parse the command line */</span></span><br><span class="line">  <span class="keyword">while</span> ((c = getopt(argc, argv, <span class="string">&quot;hvp&quot;</span>)) != EOF) &#123;</span><br><span class="line">    <span class="keyword">switch</span> (c) &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&#x27;h&#x27;</span>: <span class="comment">/* print help message */</span></span><br><span class="line">        usage();</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&#x27;v&#x27;</span>: <span class="comment">/* emit additional diagnostic info */</span></span><br><span class="line">        verbose = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&#x27;p&#x27;</span>:          <span class="comment">/* don&#x27;t print a prompt */</span></span><br><span class="line">        emit_prompt = <span class="number">0</span>; <span class="comment">/* handy for automatic testing */</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        usage();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Install the signal handlers */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* These are the ones you will need to implement */</span></span><br><span class="line">  Signal(SIGINT, sigint_handler);   <span class="comment">/* ctrl-c */</span></span><br><span class="line">  Signal(SIGTSTP, sigtstp_handler); <span class="comment">/* ctrl-z */</span></span><br><span class="line">  Signal(SIGCHLD, sigchld_handler); <span class="comment">/* Terminated or stopped child */</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* This one provides a clean way to kill the shell */</span></span><br><span class="line">  Signal(SIGQUIT, sigquit_handler);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Initialize the job list */</span></span><br><span class="line">  initjobs(jobs);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Execute the shell&#x27;s read/eval loop */</span></span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="comment">/* Read command line */</span></span><br><span class="line">    <span class="keyword">if</span> (emit_prompt) &#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;%s&quot;</span>, prompt);</span><br><span class="line">      fflush(<span class="built_in">stdout</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> ((fgets(cmdline, MAXLINE, <span class="built_in">stdin</span>) == <span class="literal">NULL</span>) &amp;&amp; ferror(<span class="built_in">stdin</span>))</span><br><span class="line">      app_error(<span class="string">&quot;fgets error&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (feof(<span class="built_in">stdin</span>)) &#123; <span class="comment">/* End of file (ctrl-d) */</span></span><br><span class="line">      fflush(<span class="built_in">stdout</span>);</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Evaluate the command line */</span></span><br><span class="line">    eval(cmdline);</span><br><span class="line">    fflush(<span class="built_in">stdout</span>);</span><br><span class="line">    fflush(<span class="built_in">stdout</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">0</span>); <span class="comment">/* control never reaches here */</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * eval - Evaluate the command line that the user has just typed in</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * If the user has requested a built-in command (quit, jobs, bg or fg)</span></span><br><span class="line"><span class="comment"> * then execute it immediately. Otherwise, fork a child process and</span></span><br><span class="line"><span class="comment"> * run the job in the context of the child. If the job is running in</span></span><br><span class="line"><span class="comment"> * the foreground, wait for it to terminate and then return.  Note:</span></span><br><span class="line"><span class="comment"> * each child process must have a unique process group ID so that our</span></span><br><span class="line"><span class="comment"> * background children don&#x27;t receive SIGINT (SIGTSTP) from the kernel</span></span><br><span class="line"><span class="comment"> * when we type ctrl-c (ctrl-z) at the keyboard.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">eval</span><span class="params">(<span class="type">char</span> *cmdline)</span> &#123;</span><br><span class="line">  <span class="type">char</span> *argv[MAXARGS]; <span class="comment">/*Argument list execve() */</span></span><br><span class="line">  <span class="type">char</span> buf[MAXLINE];   <span class="comment">/*Holds modified command line */</span></span><br><span class="line">  <span class="type">int</span> bg;              <span class="comment">/*Should the job run in bg or fg? */</span></span><br><span class="line">  <span class="type">pid_t</span> pid;           <span class="comment">/*Process id */</span></span><br><span class="line"></span><br><span class="line">  <span class="type">sigset_t</span> mask_all, mask_one, prev_one;</span><br><span class="line">  sigfillset(&amp;mask_all);</span><br><span class="line">  sigemptyset(&amp;mask_one);</span><br><span class="line">  sigaddset(&amp;mask_one, SIGCHLD);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">strcpy</span>(buf, cmdline);</span><br><span class="line">  bg = parseline(buf, argv);</span><br><span class="line">  <span class="keyword">if</span> (argv[<span class="number">0</span>] == <span class="literal">NULL</span>)</span><br><span class="line">    <span class="keyword">return</span>; <span class="comment">/* Ignore empty lines */</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!builtin_cmd(argv)) &#123;</span><br><span class="line">    sigprocmask(SIG_BLOCK, &amp;mask_one, &amp;prev_one);   <span class="comment">//阻塞SIGCHLD</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((pid = Fork()) == <span class="number">0</span>) &#123;         <span class="comment">/* Child runs user job */</span></span><br><span class="line">      setpgid(<span class="number">0</span>, <span class="number">0</span>);          <span class="comment">//单独加入一个组</span></span><br><span class="line">      sigprocmask(SIG_SETMASK, &amp;prev_one, <span class="literal">NULL</span>);  <span class="comment">//解除SIGCHLD阻塞 赋值新的mask</span></span><br><span class="line">      Execve(argv[<span class="number">0</span>], argv, environ);  <span class="comment">//执行函数不会返回   会结束运行</span></span><br><span class="line">    &#125;                                  <span class="comment">/* Parent waits for foreground job to terminate */</span></span><br><span class="line">    <span class="comment">//添加作业     FG = 0 + 1 = 1    BG = 1 + 1 = 2</span></span><br><span class="line">    sigprocmask(SIG_BLOCK, &amp;mask_all, <span class="literal">NULL</span>);   <span class="comment">//应该是重置作用</span></span><br><span class="line">    addjob(jobs, pid, bg + <span class="number">1</span>, cmdline);</span><br><span class="line">    sigprocmask(SIG_SETMASK, &amp;prev_one, <span class="literal">NULL</span>); </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!bg) &#123;</span><br><span class="line">      waitfg(pid);   <span class="comment">//等待前台运行结束</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;[%d] (%d)  %s&quot;</span>, pid2jid(pid), pid, cmdline);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * parseline - Parse the command line and build the argv array.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Characters enclosed in single quotes are treated as a single</span></span><br><span class="line"><span class="comment"> * argument.  Return true if the user has requested a BG job, false if</span></span><br><span class="line"><span class="comment"> * the user has requested a FG job.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">parseline</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *cmdline, <span class="type">char</span> **argv)</span> &#123;</span><br><span class="line">  <span class="type">static</span> <span class="type">char</span> <span class="built_in">array</span>[MAXLINE]; <span class="comment">/* holds local copy of command line */</span></span><br><span class="line">  <span class="type">char</span> *buf = <span class="built_in">array</span>;          <span class="comment">/* ptr that traverses command line */</span></span><br><span class="line">  <span class="type">char</span> *delim;                <span class="comment">/* points to first space delimiter */</span></span><br><span class="line">  <span class="type">int</span> argc;                   <span class="comment">/* number of args */</span></span><br><span class="line">  <span class="type">int</span> bg;                     <span class="comment">/* background job? */</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">strcpy</span>(buf, cmdline);</span><br><span class="line">  buf[<span class="built_in">strlen</span>(buf) - <span class="number">1</span>] = <span class="string">&#x27; &#x27;</span>;   <span class="comment">/* replace trailing &#x27;\n&#x27; with space */</span></span><br><span class="line">  <span class="keyword">while</span> (*buf &amp;&amp; (*buf == <span class="string">&#x27; &#x27;</span>)) <span class="comment">/* ignore leading spaces */</span></span><br><span class="line">    buf++;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Build the argv list */</span></span><br><span class="line">  argc = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (*buf == <span class="string">&#x27;\&#x27;&#x27;</span>) &#123;</span><br><span class="line">    buf++;</span><br><span class="line">    delim = <span class="built_in">strchr</span>(buf, <span class="string">&#x27;\&#x27;&#x27;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    delim = <span class="built_in">strchr</span>(buf, <span class="string">&#x27; &#x27;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (delim) &#123;</span><br><span class="line">    argv[argc++] = buf;</span><br><span class="line">    *delim = <span class="string">&#x27;\0&#x27;</span>;</span><br><span class="line">    buf = delim + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (*buf &amp;&amp; (*buf == <span class="string">&#x27; &#x27;</span>)) <span class="comment">/* ignore spaces */</span></span><br><span class="line">      buf++;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (*buf == <span class="string">&#x27;\&#x27;&#x27;</span>) &#123;</span><br><span class="line">      buf++;</span><br><span class="line">      delim = <span class="built_in">strchr</span>(buf, <span class="string">&#x27;\&#x27;&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      delim = <span class="built_in">strchr</span>(buf, <span class="string">&#x27; &#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  argv[argc] = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (argc == <span class="number">0</span>) <span class="comment">/* ignore blank line */</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* should the job run in the background? */</span></span><br><span class="line">  <span class="keyword">if</span> ((bg = (*argv[argc - <span class="number">1</span>] == <span class="string">&#x27;&amp;&#x27;</span>)) != <span class="number">0</span>) &#123;</span><br><span class="line">    argv[--argc] = <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> bg;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * builtin_cmd - If the user has typed a built-in command then execute</span></span><br><span class="line"><span class="comment"> *    it immediately.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">builtin_cmd</span><span class="params">(<span class="type">char</span> **argv)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], <span class="string">&quot;quit&quot;</span>))</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], <span class="string">&quot;jobs&quot;</span>)) &#123;</span><br><span class="line">    listjobs(jobs);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], <span class="string">&quot;bg&quot;</span>) || !<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], <span class="string">&quot;fg&quot;</span>)) &#123;</span><br><span class="line">    do_bgfg(argv);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], <span class="string">&quot;&amp;&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; <span class="comment">/* not a builtin command */</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * do_bgfg - Execute the builtin bg and fg commands</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">do_bgfg</span><span class="params">(<span class="type">char</span> **argv)</span> &#123;</span><br><span class="line">  <span class="type">int</span> doid;</span><br><span class="line">  <span class="type">pid_t</span> pid;</span><br><span class="line">  <span class="type">pid_t</span> t_pid;</span><br><span class="line">  <span class="keyword">if</span>(argv[<span class="number">1</span>] == <span class="literal">NULL</span>)&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%s command requires PID or %%jobid argument\n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> jflag = <span class="number">0</span>;  <span class="comment">//组标记</span></span><br><span class="line">  <span class="keyword">if</span>(argv[<span class="number">1</span>][<span class="number">0</span>] == <span class="string">&#x27;%&#x27;</span>)&#123;</span><br><span class="line">    jflag = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">isdigit</span>(argv[<span class="number">1</span>][<span class="number">1</span>]) == <span class="number">0</span>)&#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;%s: argument must be a PID or %%jobid\n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    doid = atoi(&amp;argv[<span class="number">1</span>][<span class="number">1</span>]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">isdigit</span>(argv[<span class="number">1</span>][<span class="number">0</span>]) == <span class="number">0</span>)&#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;%s: argument must be a PID or %%jobid\n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    doid = atoi(&amp;argv[<span class="number">1</span>][<span class="number">0</span>]);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">job_t</span> * <span class="title">job</span>;</span></span><br><span class="line">  <span class="keyword">if</span>(jflag)&#123;</span><br><span class="line">    job = getjobjid(jobs, doid);  <span class="comment">//jid</span></span><br><span class="line">    <span class="keyword">if</span>(job == <span class="literal">NULL</span>)&#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;%d: No such job\n&quot;</span>, doid);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span>&#123;</span><br><span class="line">    job = getjobpid(jobs, doid);  <span class="comment">//pid</span></span><br><span class="line">    <span class="keyword">if</span>(job == <span class="literal">NULL</span>)&#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;(%d): No such process\n&quot;</span>, doid);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(argv[<span class="number">0</span>], <span class="string">&quot;bg&quot;</span>)) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    kill(-job-&gt;pid, SIGCONT); <span class="comment">//发送继续命令</span></span><br><span class="line">    job-&gt;state = BG;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;[%d] (%d) %s&quot;</span>,job-&gt;jid,job-&gt;pid,job-&gt;cmdline);  <span class="comment">//不清楚为什么不换行</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span>&#123;</span><br><span class="line">    </span><br><span class="line">    t_pid = fgpid(jobs);   <span class="comment">//找当前的前台作业</span></span><br><span class="line">    <span class="keyword">if</span>(job-&gt;state == ST)&#123;</span><br><span class="line">      kill(-job-&gt;pid, SIGCONT); <span class="comment">//发送继续命令</span></span><br><span class="line">    &#125;</span><br><span class="line">    job-&gt;state = FG;</span><br><span class="line">    pid = job-&gt;pid;</span><br><span class="line">    <span class="keyword">if</span>(t_pid != <span class="number">0</span>)&#123;  <span class="comment">//有前台作业 转为后台</span></span><br><span class="line">      job = getjobpid(jobs, doid);</span><br><span class="line">      job-&gt;state = BG;</span><br><span class="line">    &#125;</span><br><span class="line">    waitfg(pid);  <span class="comment">//等待前台运行</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * waitfg - Block until process pid is no longer the foreground process</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">同一时间只有一个fg作业</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">waitfg</span><span class="params">(<span class="type">pid_t</span> pid)</span> &#123;</span><br><span class="line">  <span class="keyword">while</span>(pid == fgpid(jobs))</span><br><span class="line">    sleep(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*****************</span></span><br><span class="line"><span class="comment"> * Signal handlers</span></span><br><span class="line"><span class="comment"> *****************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * sigchld_handler - The kernel sends a SIGCHLD to the shell whenever</span></span><br><span class="line"><span class="comment"> *     a child job terminates (becomes a zombie), or stops because it</span></span><br><span class="line"><span class="comment"> *     received a SIGSTOP or SIGTSTP signal. The handler reaps all</span></span><br><span class="line"><span class="comment"> *     available zombie children, but doesn&#x27;t wait for any other</span></span><br><span class="line"><span class="comment"> *     currently running children to terminate.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  响应 停止信号 处理函数  </span></span><br><span class="line"><span class="comment">  回收所有僵尸进程</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sigchld_handler</span><span class="params">(<span class="type">int</span> sig)</span> &#123;</span><br><span class="line">  <span class="type">int</span> olderrno = errno;</span><br><span class="line">  <span class="type">sigset_t</span> mask_all,prev_all;</span><br><span class="line">  <span class="type">pid_t</span> pid;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> status; <span class="comment">//获取返回状态</span></span><br><span class="line">  sigfillset(&amp;mask_all);</span><br><span class="line">  <span class="comment">//回收所有的僵尸进程</span></span><br><span class="line">  <span class="keyword">while</span>((pid = waitpid(<span class="number">-1</span>,&amp;status,WNOHANG | WUNTRACED))&gt;<span class="number">0</span>)&#123;  <span class="comment">//如果都没终止返回0 有一个被终止 返回终止pid</span></span><br><span class="line">    <span class="keyword">if</span> (WIFEXITED(status))  &#123;  <span class="comment">//正常终止</span></span><br><span class="line">      sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all);</span><br><span class="line">      deletejob(jobs, pid);</span><br><span class="line">      sigprocmask(SIG_SETMASK, &amp;prev_all, <span class="literal">NULL</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(WIFSIGNALED(status))  &#123;  <span class="comment">//正常终止</span></span><br><span class="line">      sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all);</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;Job [%d] (%d) terminated by signal %d\n&quot;</span>, pid2jid(pid), pid, WTERMSIG(status));</span><br><span class="line">      deletejob(jobs, pid);</span><br><span class="line">      sigprocmask(SIG_SETMASK, &amp;prev_all, <span class="literal">NULL</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(WIFSTOPPED(status))  &#123;  <span class="comment">//正常暂停</span></span><br><span class="line">      sigprocmask(SIG_BLOCK, &amp;mask_all, &amp;prev_all);</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;Job [%d] (%d) stopped by signal %d\n&quot;</span>, pid2jid(pid), pid, WSTOPSIG(status));</span><br><span class="line">      <span class="class"><span class="keyword">struct</span> <span class="title">job_t</span> * <span class="title">fgjob</span> =</span> getjobpid(jobs, pid);  <span class="comment">//查找前台进程指针</span></span><br><span class="line">      fgjob-&gt;state = ST;</span><br><span class="line">      sigprocmask(SIG_SETMASK, &amp;prev_all, <span class="literal">NULL</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  errno = olderrno;  </span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * sigint_handler - The kernel sends a SIGINT to the shell whenver the</span></span><br><span class="line"><span class="comment"> *    user types ctrl-c at the keyboard.  Catch it and send it along</span></span><br><span class="line"><span class="comment"> *    to the foreground job.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">SIGINT 信号只会传递给前台进程</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sigint_handler</span><span class="params">(<span class="type">int</span> sig)</span> &#123;</span><br><span class="line">  <span class="type">int</span> olderrno = errno;</span><br><span class="line">  <span class="type">pid_t</span> pid;</span><br><span class="line">  pid = fgpid(jobs);</span><br><span class="line">  kill(-pid, sig);  <span class="comment">//发送给前台作业</span></span><br><span class="line">  errno = olderrno;  </span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * sigtstp_handler - The kernel sends a SIGTSTP to the shell whenever</span></span><br><span class="line"><span class="comment"> *     the user types ctrl-z at the keyboard. Catch it and suspend the</span></span><br><span class="line"><span class="comment"> *     foreground job by sending it a SIGTSTP.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sigtstp_handler</span><span class="params">(<span class="type">int</span> sig)</span> &#123;</span><br><span class="line">  <span class="type">int</span> olderrno = errno;</span><br><span class="line">  <span class="type">pid_t</span> pid;</span><br><span class="line">  pid = fgpid(jobs);</span><br><span class="line">  kill(-pid, sig);  <span class="comment">//发送给前台作业</span></span><br><span class="line">  errno = olderrno;</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*********************</span></span><br><span class="line"><span class="comment"> * End signal handlers</span></span><br><span class="line"><span class="comment"> *********************/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//补充函数</span></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">Fork</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">  <span class="type">pid_t</span> pid;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> ((pid = fork()) &lt; <span class="number">0</span>)</span><br><span class="line">    unix_error(<span class="string">&quot;Fork error&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> pid;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">Execve</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *filename, <span class="type">char</span> *<span class="type">const</span> argv[], <span class="type">char</span> *<span class="type">const</span> environ[])</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (execve(filename, argv, environ) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%s: Command not found.\n&quot;</span>, filename);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************************************</span></span><br><span class="line"><span class="comment"> * Helper routines that manipulate the job list</span></span><br><span class="line"><span class="comment"> **********************************************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* clearjob - Clear the entries in a job struct */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">clearjob</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *job)</span> &#123;</span><br><span class="line">  job-&gt;pid = <span class="number">0</span>;</span><br><span class="line">  job-&gt;jid = <span class="number">0</span>;</span><br><span class="line">  job-&gt;state = UNDEF;</span><br><span class="line">  job-&gt;cmdline[<span class="number">0</span>] = <span class="string">&#x27;\0&#x27;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* initjobs - Initialize the job list */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">initjobs</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs)</span> &#123;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAXJOBS; i++)</span><br><span class="line">    clearjob(&amp;jobs[i]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* maxjid - Returns largest allocated job ID */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">maxjid</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs)</span> &#123;</span><br><span class="line">  <span class="type">int</span> i, max = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAXJOBS; i++)</span><br><span class="line">    <span class="keyword">if</span> (jobs[i].jid &gt; max)</span><br><span class="line">      max = jobs[i].jid;</span><br><span class="line">  <span class="keyword">return</span> max;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* addjob - Add a job to the job list */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">addjob</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs, <span class="type">pid_t</span> pid, <span class="type">int</span> state, <span class="type">char</span> *cmdline)</span> &#123;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (pid &lt; <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAXJOBS; i++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (jobs[i].pid == <span class="number">0</span>) &#123;</span><br><span class="line">      jobs[i].pid = pid;</span><br><span class="line">      jobs[i].state = state;</span><br><span class="line">      jobs[i].jid = nextjid++;</span><br><span class="line">      <span class="keyword">if</span> (nextjid &gt; MAXJOBS)</span><br><span class="line">        nextjid = <span class="number">1</span>;</span><br><span class="line">      <span class="built_in">strcpy</span>(jobs[i].cmdline, cmdline);</span><br><span class="line">      <span class="keyword">if</span> (verbose) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Added job [%d] %d %s\n&quot;</span>, jobs[i].jid, jobs[i].pid, jobs[i].cmdline);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Tried to create too many jobs\n&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* deletejob - Delete a job whose PID=pid from the job list */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">deletejob</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs, <span class="type">pid_t</span> pid)</span> &#123;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (pid &lt; <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAXJOBS; i++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (jobs[i].pid == pid) &#123;</span><br><span class="line">      clearjob(&amp;jobs[i]);</span><br><span class="line">      nextjid = maxjid(jobs) + <span class="number">1</span>;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* fgpid - Return PID of current foreground job, 0 if no such job */</span></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">fgpid</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs)</span> &#123;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAXJOBS; i++)</span><br><span class="line">    <span class="keyword">if</span> (jobs[i].state == FG)</span><br><span class="line">      <span class="keyword">return</span> jobs[i].pid;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* getjobpid  - Find a job (by PID) on the job list */</span></span><br><span class="line"><span class="keyword">struct</span> <span class="type">job_t</span> *<span class="title function_">getjobpid</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs, <span class="type">pid_t</span> pid)</span> &#123;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (pid &lt; <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAXJOBS; i++)</span><br><span class="line">    <span class="keyword">if</span> (jobs[i].pid == pid)</span><br><span class="line">      <span class="keyword">return</span> &amp;jobs[i];</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* getjobjid  - Find a job (by JID) on the job list */</span></span><br><span class="line"><span class="keyword">struct</span> <span class="type">job_t</span> *<span class="title function_">getjobjid</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs, <span class="type">int</span> jid)</span> &#123;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (jid &lt; <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAXJOBS; i++)</span><br><span class="line">    <span class="keyword">if</span> (jobs[i].jid == jid)</span><br><span class="line">      <span class="keyword">return</span> &amp;jobs[i];</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* pid2jid - Map process ID to job ID */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">pid2jid</span><span class="params">(<span class="type">pid_t</span> pid)</span> &#123;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (pid &lt; <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAXJOBS; i++)</span><br><span class="line">    <span class="keyword">if</span> (jobs[i].pid == pid) &#123;</span><br><span class="line">      <span class="keyword">return</span> jobs[i].jid;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* listjobs - Print the job list */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">listjobs</span><span class="params">(<span class="keyword">struct</span> <span class="type">job_t</span> *jobs)</span> &#123;</span><br><span class="line">  <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; MAXJOBS; i++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (jobs[i].pid != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;[%d] (%d) &quot;</span>, jobs[i].jid, jobs[i].pid);</span><br><span class="line">      <span class="keyword">switch</span> (jobs[i].state) &#123;</span><br><span class="line">        <span class="keyword">case</span> BG:</span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">&quot;Running &quot;</span>);</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> FG:</span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">&quot;Foreground &quot;</span>);</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> ST:</span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">&quot;Stopped &quot;</span>);</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">&quot;listjobs: Internal error: job[%d].state=%d &quot;</span>,</span><br><span class="line">                 i, jobs[i].state);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;%s&quot;</span>, jobs[i].cmdline);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/******************************</span></span><br><span class="line"><span class="comment"> * end job list helper routines</span></span><br><span class="line"><span class="comment"> ******************************/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/***********************</span></span><br><span class="line"><span class="comment"> * Other helper routines</span></span><br><span class="line"><span class="comment"> ***********************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * usage - print a help message</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">usage</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Usage: shell [-hvp]\n&quot;</span>);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;   -h   print this message\n&quot;</span>);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;   -v   print additional diagnostic information\n&quot;</span>);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;   -p   do not emit a command prompt\n&quot;</span>);</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * unix_error - unix-style error routine</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">unix_error</span><span class="params">(<span class="type">char</span> *msg)</span> &#123;</span><br><span class="line">  <span class="built_in">fprintf</span>(<span class="built_in">stdout</span>, <span class="string">&quot;%s: %s\n&quot;</span>, msg, strerror(errno));</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * app_error - application-style error routine</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">app_error</span><span class="params">(<span class="type">char</span> *msg)</span> &#123;</span><br><span class="line">  <span class="built_in">fprintf</span>(<span class="built_in">stdout</span>, <span class="string">&quot;%s\n&quot;</span>, msg);</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Signal - wrapper for the sigaction function</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">handler_t</span> *<span class="title function_">Signal</span><span class="params">(<span class="type">int</span> signum, <span class="type">handler_t</span> *handler)</span> &#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">sigaction</span> <span class="title">action</span>, <span class="title">old_action</span>;</span></span><br><span class="line"></span><br><span class="line">  action.sa_handler = handler;</span><br><span class="line">  sigemptyset(&amp;action.sa_mask); <span class="comment">/* block sigs of type being handled */</span></span><br><span class="line">  action.sa_flags = SA_RESTART; <span class="comment">/* restart syscalls if possible */</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (sigaction(signum, &amp;action, &amp;old_action) &lt; <span class="number">0</span>)</span><br><span class="line">    unix_error(<span class="string">&quot;Signal error&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> (old_action.sa_handler);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * sigquit_handler - The driver program can gracefully terminate the</span></span><br><span class="line"><span class="comment"> *    child shell by sending it a SIGQUIT signal.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sigquit_handler</span><span class="params">(<span class="type">int</span> sig)</span> &#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Terminating after receipt of SIGQUIT signal\n&quot;</span>);</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP 实验IV CacheLab</title>
    <url>/2024/03/09/16-34-34/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>24.03.09：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2023/03/15/10-59-10/" title="csapp 笔记汇总">CSAPP - 笔记汇总</a></li>
<li><a href="/2023/03/09/11-54-50/" title="CSAPP 实验I Data Lab">I Data Lab - 位操作，数据表示</a></li>
<li><a href="/2023/03/14/09-43-26/" title="CSAPP 实验II Bomb Lab">II Bomb Lab - 汇编，栈帧与 gdb</a></li>
<li><a href="/2023/03/18/15-54-47/" title="CSAPP 实验III Attack Lab">III Attack Lab - 漏洞是如何被攻击的</a></li>
<li><a href="/2024/03/09/16-34-34/" title="CSAPP 实验IV CacheLab">IV Cache Lab - 实现一个缓存系统</a></li>
<li><a href="/2024/03/14/12-34-42/" title="CSAPP 实验V ShellLab">V Shell Lab - 实现一个Shell</a>
</li>
</ul>
<h1 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h1><p>CacheLab实验，第一部分需要模拟Cache计算命中，未命中次数，第二部分实现代码优化，减少未命中次数。</p>
<h1 id="相关内容"><a href="#相关内容" class="headerlink" title="相关内容"></a>相关内容</h1><ol>
<li><p>Cache</p>
<blockquote>
<img src="/2024/03/09/16-34-34/QQ20240309-165258@2x.png" class>
<p>不需要关注 <code>b</code> bits，使用LRU替换策略。</p>
<p>Cache是一个二维数组cache[S][E]，S=2^s，组数。</p>
<p>每一个cache行包含：Valid bit， Tag， LRU counter。</p>
<p>用Tag对比E匹配line。</p>
</blockquote>
</li>
<li><p>生成内存调试工具命令: <code>linux&gt; valgrind --log-fd=1 --tool=lackey -v --trace-mem=yes ls -l</code></p>
<blockquote>
<p>参数<code>ls -l</code>：打印输出在文件stdout</p>
<p>格式为<code>[space]operation address,size</code>，除了I操作外，前都有一个空格</p>
<p>操作：I-instruction load，L-data load，S-data store， M-data modify。M包含L，S。</p>
<p>address：64-bit 16进制</p>
<p>size：字节数</p>
</blockquote>
</li>
</ol>
<h1 id="题目及解法"><a href="#题目及解法" class="headerlink" title="题目及解法"></a>题目及解法</h1><h2 id="Part-A-Writing-a-Cache-Simulator"><a href="#Part-A-Writing-a-Cache-Simulator" class="headerlink" title="Part A Writing a Cache Simulator"></a>Part A Writing a Cache Simulator</h2><p>完成<code>csim.c</code>文件，使用<code>valgrind</code>生成测试数据，输出命中，未命中和替换次数，输出格式为<code>hits:4 misses:5 evictions:3</code>。</p>
<h3 id="相关内容-1"><a href="#相关内容-1" class="headerlink" title="相关内容"></a>相关内容</h3><ul>
<li><p>修改<code>csim.c</code>文件</p>
</li>
<li><p>可参考的二进制文件<code>csim-ref</code>，使用命令<code>./csim-ref [-hv] -s &lt;s&gt; -E &lt;E&gt; -b &lt;b&gt; -t &lt;tracefile&gt;</code></p>
<blockquote>
<p>参数：v-打印内存追踪记录</p>
</blockquote>
</li>
<li><p>自动评分工具<code>test-csim</code></p>
</li>
<li><p>建议代码像参考模拟器显示输出，方便测试。</p>
</li>
<li><p>使用<code>getopt</code>分析命令行参数。需包含头文件<getopt.h>，<unistd.h>，<stdlib.h>。可以获取函数参数。</stdlib.h></unistd.h></getopt.h></p>
</li>
<li><p>使用<code>fscanf</code>按行读取文件</p>
</li>
<li><p>使用<code>malloc</code>，<code>free</code>，防止内存泄漏</p>
</li>
<li><p>使用<code>printSummary(hit_count, miss_count, eviction_count);</code>，打印输出</p>
</li>
<li><p>数据是对齐，且不会出现跨块访问。</p>
</li>
</ul>
<h3 id="答案"><a href="#答案" class="headerlink" title="答案"></a>答案</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> MaxSetbits 5</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MaxLine 4</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MaxBytebits 5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Myline</span>&#123;</span></span><br><span class="line">  <span class="type">int</span> valid; <span class="comment">//有效位</span></span><br><span class="line">  <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> tag;   <span class="comment">//tag标记  十进制</span></span><br><span class="line">  <span class="type">int</span> lru_num;   <span class="comment">// 最近使用标记 0 - (E - 1)</span></span><br><span class="line">&#125;Myline;</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Mycache</span>&#123;</span></span><br><span class="line">  <span class="type">int</span> S;            <span class="comment">// set 大小</span></span><br><span class="line">  <span class="type">int</span> E;            <span class="comment">// line 大小</span></span><br><span class="line">  <span class="type">int</span> B;            <span class="comment">//偏移大小</span></span><br><span class="line">  <span class="type">int</span> size_Tag;     <span class="comment">// tag bits = 64 - s- b</span></span><br><span class="line">  Myline **<span class="built_in">set</span>;    <span class="comment">//set数组</span></span><br><span class="line">&#125;Mycache;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//get command line arguments</span></span><br><span class="line"><span class="comment">//p_s s的指针   p_E E的指针   p_b b的指针    fn 文件名</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Mygetopt</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[], <span class="type">int</span> *p_s, <span class="type">int</span> *p_E, <span class="type">int</span> *p_b, <span class="type">char</span> *fn, <span class="type">int</span> *flag_v)</span>;   </span><br><span class="line"><span class="comment">//16进制转换十进制</span></span><br><span class="line"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> <span class="title function_">Hex2dec</span><span class="params">(<span class="type">char</span> *hex)</span>;</span><br><span class="line"><span class="comment">//返回tag</span></span><br><span class="line"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> <span class="title function_">GetDecTag</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec, <span class="type">int</span> s, <span class="type">int</span> b)</span>;</span><br><span class="line"><span class="comment">//返回组号</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">GetDecSetNum</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec, <span class="type">int</span> s, <span class="type">int</span> b)</span>;</span><br><span class="line"><span class="comment">//返回这个dec地址下当前偏移量开始剩余的字节数</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">GetDecRestByte</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec, <span class="type">int</span> s, <span class="type">int</span> b, <span class="type">int</span> B)</span>;</span><br><span class="line"><span class="comment">//初始化set</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Initset</span><span class="params">(Mycache *mc)</span>;</span><br><span class="line"><span class="comment">//处理最近未使用</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">GetminLRU</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E)</span>;</span><br><span class="line"><span class="comment">//是否set满了</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">IsFullSet</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E)</span>;</span><br><span class="line"><span class="comment">//命中处理  </span></span><br><span class="line"><span class="type">void</span> <span class="title function_">HitDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">int</span> Eindex)</span>;</span><br><span class="line"><span class="comment">//未命中 加载cache</span></span><br><span class="line"><span class="comment">// 有空余line加载</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">MissLoadDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">int</span> Eindex, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec_tag)</span>;</span><br><span class="line"><span class="comment">//line替换</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">MissEvictionDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">int</span> delindex, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec_tag)</span>;</span><br><span class="line"><span class="comment">//访问处理   返回1 hit   2  miss  3 miss eviction</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">LoadDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> now_tag, <span class="type">int</span> flag_v)</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">StoreDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> now_tag, <span class="type">int</span> flag_v)</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">ModifyDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> now_tag, <span class="type">int</span> flag_v)</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span> &#123;</span><br><span class="line">  FILE *fp;</span><br><span class="line">  <span class="type">int</span> hit_count, miss_count, eviction_count;</span><br><span class="line">  <span class="type">int</span> s, E, b;</span><br><span class="line">  <span class="type">int</span> flag_v = <span class="number">0</span>;     <span class="comment">//opt v 的标记</span></span><br><span class="line">  <span class="type">char</span> *file_name;</span><br><span class="line">  <span class="type">char</span> optcache = <span class="string">&#x27; &#x27;</span>;</span><br><span class="line">  <span class="type">char</span> ignorechar = <span class="string">&#x27; &#x27;</span>;</span><br><span class="line">  <span class="type">char</span> hex[<span class="number">65</span>] = <span class="string">&quot;&quot;</span>;</span><br><span class="line">  <span class="type">int</span> size = <span class="number">0</span>;        <span class="comment">//个数</span></span><br><span class="line">  <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec;</span><br><span class="line"></span><br><span class="line">  hit_count = <span class="number">0</span>;</span><br><span class="line">  miss_count = <span class="number">0</span>;</span><br><span class="line">  eviction_count = <span class="number">0</span>;</span><br><span class="line">  Mycache cache;</span><br><span class="line">  file_name = (<span class="type">char</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">char</span>) * <span class="number">30</span>);</span><br><span class="line">  Mygetopt(argc, argv, &amp;s, &amp;E, &amp;b, file_name, &amp;flag_v);</span><br><span class="line">  cache.S = <span class="built_in">pow</span>(<span class="number">2</span>, s);</span><br><span class="line">  cache.E = E;</span><br><span class="line">  cache.B = <span class="built_in">pow</span>(<span class="number">2</span>, b);</span><br><span class="line">  cache.size_Tag = <span class="number">64</span> - s - b;</span><br><span class="line">  Initset(&amp;cache);</span><br><span class="line"></span><br><span class="line">  fp = fopen(file_name, <span class="string">&quot;r&quot;</span>);</span><br><span class="line">  <span class="keyword">while</span>(<span class="built_in">fscanf</span>(fp, <span class="string">&quot; %c %[^,]%c%d&quot;</span>, &amp;optcache, hex, &amp;ignorechar, &amp;size) != <span class="number">-1</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(optcache == <span class="string">&#x27;I&#x27;</span>)&#123;</span><br><span class="line">      <span class="built_in">fscanf</span>(fp, <span class="string">&quot;%c&quot;</span>, &amp;ignorechar);</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(flag_v == <span class="number">1</span>)&#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;%c %s,%d&quot;</span>, optcache, hex, size);</span><br><span class="line">    &#125;</span><br><span class="line">    dec = Hex2dec(hex);</span><br><span class="line">    <span class="comment">//========处理内存访问</span></span><br><span class="line">    <span class="type">int</span> setnum = <span class="number">0</span>;  <span class="comment">//组号</span></span><br><span class="line">    <span class="type">int</span> flag_kind = <span class="number">0</span>;   <span class="comment">//cache 类型   hit miss  </span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dectag = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//int restbyte = 0;  // 当前偏移量下剩余的字节数</span></span><br><span class="line">    <span class="comment">//int loadbyte = 0;  //已加载的字节数</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> loaddec = dec;   <span class="comment">//当前加载的地址</span></span><br><span class="line">    setnum = GetDecSetNum(loaddec, s, b);</span><br><span class="line">    dectag = GetDecTag(loaddec, s, b);</span><br><span class="line">    <span class="comment">//restbyte = GetDecRestByte(loaddec, s, b, cache.B);   //剩余加载的字节数</span></span><br><span class="line">    flag_kind = LoadDeal(cache.<span class="built_in">set</span>[setnum], E, dectag, flag_v);</span><br><span class="line">    <span class="comment">/*loadbyte += restbyte;</span></span><br><span class="line"><span class="comment">    loaddec += restbyte;   //当前地址加上剩余字节数  如果加载字节数不够  这就是下一个加载地址</span></span><br><span class="line"><span class="comment">    while(loadbyte &lt; size)&#123;</span></span><br><span class="line"><span class="comment">      setnum = GetDecSetNum(loaddec, s, b);</span></span><br><span class="line"><span class="comment">      dectag = GetDecTag(loaddec, s, b);</span></span><br><span class="line"><span class="comment">      LoadDeal(cache.set[setnum], E, dectag, flag_v);</span></span><br><span class="line"><span class="comment">      loadbyte += cache.B;</span></span><br><span class="line"><span class="comment">      loaddec += cache.B;</span></span><br><span class="line"><span class="comment">    &#125;*/</span></span><br><span class="line">    <span class="keyword">if</span>(flag_kind == <span class="number">1</span>)&#123;</span><br><span class="line">      hit_count++;</span><br><span class="line">      <span class="keyword">if</span>(flag_v == <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot; hit&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="keyword">if</span>(flag_kind == <span class="number">2</span>)&#123;</span><br><span class="line">        miss_count++;</span><br><span class="line">        <span class="keyword">if</span>(flag_v == <span class="number">1</span>)&#123;</span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">&quot; miss&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span>&#123;</span><br><span class="line">        miss_count++;</span><br><span class="line">        eviction_count++;</span><br><span class="line">        <span class="keyword">if</span>(flag_v == <span class="number">1</span>)&#123;</span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">&quot; miss eviction&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(optcache == <span class="string">&#x27;M&#x27;</span>)&#123;</span><br><span class="line">      hit_count++;   <span class="comment">//修改  store一定hit</span></span><br><span class="line">      <span class="keyword">if</span>(flag_v == <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot; hit\n&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="keyword">if</span>(flag_v == <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//=====================</span></span><br><span class="line">    <span class="built_in">fscanf</span>(fp, <span class="string">&quot;%c&quot;</span>, &amp;ignorechar);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  printSummary(hit_count, miss_count, eviction_count);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">Mygetopt</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[], <span class="type">int</span> *p_s, <span class="type">int</span> *p_E, <span class="type">int</span> *p_b, <span class="type">char</span> *fn, <span class="type">int</span> *flag_v)</span>&#123;</span><br><span class="line">  <span class="type">int</span> opt;</span><br><span class="line">  <span class="keyword">while</span> ((opt = getopt(argc, argv, <span class="string">&quot;vh:s:E:b:t:&quot;</span>)) != <span class="number">-1</span>) &#123;</span><br><span class="line">    <span class="keyword">switch</span> (opt) &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&#x27;s&#x27;</span>:</span><br><span class="line">        *p_s = atoi(optarg);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&#x27;E&#x27;</span>:</span><br><span class="line">        *p_E = atoi(optarg);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&#x27;b&#x27;</span>:</span><br><span class="line">        *p_b = atoi(optarg);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&#x27;t&#x27;</span>:</span><br><span class="line">        <span class="built_in">strcpy</span>(fn, optarg);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&#x27;v&#x27;</span>:</span><br><span class="line">        *flag_v = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> <span class="title function_">Hex2dec</span><span class="params">(<span class="type">char</span> *hex)</span>&#123;</span><br><span class="line">  <span class="type">int</span> len = <span class="built_in">strlen</span>(hex);</span><br><span class="line">  <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> pow16 = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = len - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">isdigit</span>(hex[i]))&#123;</span><br><span class="line">      dec += (hex[i] - <span class="string">&#x27;0&#x27;</span>) * pow16;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">      dec += (hex[i] - <span class="number">87</span>) * pow16;</span><br><span class="line">    &#125;</span><br><span class="line">    pow16 *= <span class="number">16</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> dec;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> <span class="title function_">GetDecTag</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec, <span class="type">int</span> s, <span class="type">int</span> b)</span>&#123;</span><br><span class="line">  <span class="keyword">return</span> dec &gt;&gt; (s + b);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">GetDecSetNum</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec, <span class="type">int</span> s, <span class="type">int</span> b)</span>&#123;</span><br><span class="line">  <span class="type">int</span> setbits = (<span class="number">1</span> &lt;&lt; s);</span><br><span class="line">  setbits = ~(~setbits + <span class="number">1</span>);</span><br><span class="line">  dec = dec &gt;&gt; (b);</span><br><span class="line">  <span class="keyword">return</span> dec &amp; setbits;   <span class="comment">//返回set组号</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">GetDecRestByte</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec, <span class="type">int</span> s, <span class="type">int</span> b, <span class="type">int</span> B)</span>&#123;</span><br><span class="line">  <span class="type">int</span> Bytebits = <span class="number">1</span> &lt;&lt; b;</span><br><span class="line">  Bytebits = ~(~Bytebits + <span class="number">1</span>);</span><br><span class="line">  dec = dec &amp; Bytebits;   <span class="comment">//剩余偏移量</span></span><br><span class="line">  <span class="keyword">return</span> B - dec;  <span class="comment">//返回剩余的字节数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">Initset</span><span class="params">(Mycache *mc)</span>&#123;</span><br><span class="line">  <span class="type">int</span> S = (*mc).S;</span><br><span class="line">  <span class="type">int</span> E = (*mc).E;</span><br><span class="line">  (*mc).<span class="built_in">set</span> = (Myline **)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Myline *) * S);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; S; i++)&#123;</span><br><span class="line">    (*mc).<span class="built_in">set</span>[i] = (Myline *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Myline) * E);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; (*mc).S; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; (*mc).E; j++)&#123;</span><br><span class="line">      (*mc).<span class="built_in">set</span>[i][j].valid = <span class="number">0</span>;  <span class="comment">//有效位为0</span></span><br><span class="line">      (*mc).<span class="built_in">set</span>[i][j].lru_num = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">GetminLRU</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E)</span>&#123;</span><br><span class="line">  <span class="type">int</span> min = <span class="number">1000</span>;</span><br><span class="line">  <span class="type">int</span> k = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; E; i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">set</span>[i].valid == <span class="number">0</span>)&#123;   <span class="comment">// 如果存在一个无效位置 直接返回</span></span><br><span class="line">      k = i;</span><br><span class="line">      <span class="keyword">break</span>;  </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">set</span>[i].lru_num &lt; min)&#123;  </span><br><span class="line">      min = <span class="built_in">set</span>[i].lru_num;</span><br><span class="line">      k = i;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> k;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">IsFullSet</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E)</span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; E; i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">set</span>[i].valid == <span class="number">0</span>)&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">HitDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">int</span> Eindex)</span>&#123;</span><br><span class="line">  <span class="type">int</span> nowLRU = <span class="built_in">set</span>[Eindex].lru_num;</span><br><span class="line">  <span class="comment">//把比现在要访问的cache set中lrunum大的全部减一  </span></span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; E; i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">set</span>[i].lru_num &gt; nowLRU)&#123;</span><br><span class="line">      <span class="built_in">set</span>[i].lru_num--;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">set</span>[Eindex].lru_num = E - <span class="number">1</span>;  <span class="comment">//最新访问的cache LRU设置为最大</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">MissLoadDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">int</span> Eindex, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec_tag)</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//把比现在要访问的cache set中lrunum大的全部减一  </span></span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; E; i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">set</span>[i].lru_num &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      <span class="built_in">set</span>[i].lru_num--;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">set</span>[Eindex].tag = dec_tag;</span><br><span class="line">  <span class="built_in">set</span>[Eindex].valid = <span class="number">1</span>;  </span><br><span class="line">  <span class="built_in">set</span>[Eindex].lru_num = E - <span class="number">1</span>;  <span class="comment">//最新访问的cache LRU设置为最大</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">MissEvictionDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">int</span> delindex, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> dec_tag)</span>&#123;</span><br><span class="line">  <span class="comment">//将要替换的line命中  改为最近访问过,然后再替换掉tag值</span></span><br><span class="line">  HitDeal(<span class="built_in">set</span>, E, delindex);</span><br><span class="line">  <span class="built_in">set</span>[delindex].tag = dec_tag;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">LoadDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> now_tag, <span class="type">int</span> flag_v)</span>&#123;</span><br><span class="line">  <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; E; i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">set</span>[i].tag == now_tag &amp;&amp; <span class="built_in">set</span>[i].valid == <span class="number">1</span>)&#123;</span><br><span class="line">      HitDeal(<span class="built_in">set</span>, E, i);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125; </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(i == E)&#123;</span><br><span class="line">    <span class="type">int</span> Lindex = GetminLRU(<span class="built_in">set</span>, E);</span><br><span class="line">    <span class="keyword">if</span>(IsFullSet(<span class="built_in">set</span>, E) == <span class="number">1</span>)&#123;</span><br><span class="line">      <span class="comment">//满的  替换</span></span><br><span class="line">      MissEvictionDeal(<span class="built_in">set</span>, E, Lindex, now_tag);</span><br><span class="line">      <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">      MissLoadDeal(<span class="built_in">set</span>, E, Lindex, now_tag);</span><br><span class="line">      <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">StoreDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> now_tag, <span class="type">int</span> flag_v)</span>&#123;</span><br><span class="line">  <span class="comment">//Store 相当于 Load</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">ModifyDeal</span><span class="params">(Myline *<span class="built_in">set</span>, <span class="type">int</span> E, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">int</span> now_tag, <span class="type">int</span> flag_v)</span>&#123;</span><br><span class="line">  <span class="comment">//int kind = LoadDeal(set, E, now_tag,flag_v);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Part-B-Optimizing-Matrix-Transpose"><a href="#Part-B-Optimizing-Matrix-Transpose" class="headerlink" title="Part B Optimizing Matrix Transpose"></a>Part B Optimizing Matrix Transpose</h2><p>完成<code>trans.c</code>文件，实现一个转置矩阵代码<code>transpose_submit</code>，尽可能少的产生cache未命中。</p>
<h3 id="相关内容-2"><a href="#相关内容-2" class="headerlink" title="相关内容"></a>相关内容</h3><ul>
<li><p>只允许使用最多12个本地int变量，调用函数中一起计算</p>
</li>
<li><p>不允许使用long或者其他存储数值的方法，不允许使用数组</p>
</li>
<li><p>不能递归</p>
</li>
<li><p>使用参考模拟器来测试代码，cache参数为<code>s = 5， E = 1， b = 5</code></p>
</li>
<li><p>int为4个字节。</p>
</li>
<li><p>只有三种形式的矩阵，32*32，64*64，61*67。</p>
</li>
<li><img src="/2024/03/09/16-34-34/QQ20240310-152729@2x.png" class> 
<p>矩阵乘法的分块技术</p>
</li>
<li><p>两个数组的第一个数据，在cache中存储到相同位置，数组按照行优先存储，数据是对齐的</p>
</li>
<li><p><code>./test-trans -M 32 -N 32</code> 测试函数 </p>
</li>
<li><p><code>./tracegen -M 64 -N 64 -F 0</code> 输出错误信息 </p>
</li>
</ul>
<h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>数据是对齐的，所以一个cache行包含32个字节，存储8个int数据，cache能存储256个int数据，根据<del><code>3B^2 &lt;&lt; C</code>，块长为B=8</del>。根据<code>2B^2 &lt;&lt; C</code>，块长&lt;=11，由于cache行可以存8个数据，所以选择块长8。</p>
<img src="/2024/03/09/16-34-34/QQ20240310-191918@2x.png" class>
<p>以32*32的矩阵为例，除了处于对角线上的块会产生冲突不命中，其他块都可以优化不命中情况</p>
<p>以上这种情况对32*32是成立的，64*64和61*67不成立，所以重新思考了这道题的解法，根据data lab中数据的操作都是在内存中进行的。</p>
<p><strong>数组的首地址给出后，行优先数据为一维连续存储形式</strong></p>
<p>这道题可以转换为数组A[N][M]在内存中连续存放，每一个位置x的数根据相同的转换方式保存到数组B[M][N]的位置y，转换公式为<code>i_a = x / M</code>，<code>j_a = x % M</code>，<code>y = j_a * N + i_a * M</code>。于是存储公式为<code>B(y) = A(x)</code>。</p>
<img src="/2024/03/09/16-34-34/QQ20240312-145802@2x.png" class>
<blockquote>
<p>示意图如上，cache行存2个数据，3*3的数组中，无法直观看出内存区别，而cache的分块目的是为了在内存中读取到cache时减少miss次数，所以我们根据内存进行重排数组，改为5*2的数组。</p>
</blockquote>
<p>所以这道题可以把数组转换为32列的任意数组，由于一行只有32列，远远小于cache的容量256个数据，所以相邻行的上下两个数据不会映射的同一个cache行。</p>
<p>需要把文件放入linux的文件夹中，如果在共享文件夹中会出现运行超时。</p>
<h3 id="32-x-32"><a href="#32-x-32" class="headerlink" title="32 x 32"></a>32 x 32</h3><p>基本步骤：</p>
<ul>
<li><p>重排数组</p>
</li>
<li><p>处理AB数组中x，y映射相同的位置，先用局部变量保存，然后处理这一行的其他数据，最后保存在B数组的中，cache行自动换出。</p>
</li>
</ul>
<p><strong>分析原因</strong></p>
<blockquote>
<p>当<code>A[i][j]</code>与<code>B[j][i]</code>的cache映射相同set内时，取出<code>A[i][j]</code>中的值，写入B数组时会换出A的cache行，导致下次访问A的同一行cache数据时需要继续换出一次。</p>
</blockquote>
<p><strong>解决办法</strong></p>
<blockquote>
<p>用局部变量保存访问冲突的值，等循环结束后，在写入B中。</p>
<p>计算所在行，x表示数组一维连续内存的位置(0开始)，cache 组号<code>s=（x%256 ) / 8;</code></p>
</blockquote>
<p><strong>结果</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> M, <span class="type">int</span> N, <span class="type">int</span> A[N][M], <span class="type">int</span> B[M][N])</span> &#123;</span><br><span class="line">  <span class="comment">//int t_1, t_2, t_3;</span></span><br><span class="line">  <span class="type">int</span> t_4, t_5;</span><br><span class="line">  <span class="type">int</span> i, j, ii, jj;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; N; i += <span class="number">8</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; M; j += <span class="number">8</span>) &#123;</span><br><span class="line">      <span class="keyword">for</span> (ii = i; ii &lt; ((i + <span class="number">8</span>) &lt; N ? i + <span class="number">8</span> : N); ii++) &#123;</span><br><span class="line">        <span class="comment">//标记没有访问冲突</span></span><br><span class="line">        t_4 = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span> (jj = j; jj &lt; ((j + <span class="number">8</span>) &lt; M ? j + <span class="number">8</span> : M); jj++) &#123;</span><br><span class="line">          <span class="keyword">if</span> (ii * M + jj == jj * N + ii) &#123;</span><br><span class="line">            <span class="keyword">if</span> (t_4 == <span class="number">-1</span>) &#123;</span><br><span class="line">              t_4 = ii;</span><br><span class="line">              t_5 = jj;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// printf(&quot;**  2-- %d  3--  %d\n&quot;, t_2, t_3);</span></span><br><span class="line">          B[jj][ii] = A[ii][jj];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (t_4 != <span class="number">-1</span>) &#123;</span><br><span class="line">          B[t_5][t_4] = A[t_4][t_5];</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/2024/03/09/16-34-34/QQ20240312-183951@2x.png" class>
<p>miss数量287，满分</p>
<h3 id="64-x-64"><a href="#64-x-64" class="headerlink" title="64 x 64"></a>64 x 64</h3><p>出现一个cache行中两次相同映射的问题</p>
<p><strong>分析原因</strong></p>
<blockquote>
<p>在32 x 32的数组中，一个cache行为8，块为8x8，一个cache行最多只有对角线的的一个元素冲突miss，但是64x64的数组中，8x8的块中，前四行与后四行冲突miss，所以一个cache行的8个数据中，最多有两个元素冲突miss</p>
</blockquote>
<p><strong>解决办法</strong></p>
<blockquote>
<p>继续添加一个局部变量记录冲突位置，一行遍历完后再处理。</p>
<p>继续出现了只遍历到了32，在64 X 64的数组转换为32列时，行数为128。</p>
</blockquote>
<img src="/2024/03/09/16-34-34/QQ20240312-225626@2x.png" class>
<p>miss数量为4611，还需要继续调低。</p>
<p><strong>分析原因</strong></p>

<p><del>错误答案</del></p>
<p>64X64中，改了内存排列后，第二行与第一行的转置不在同一cache行内。</p>
<p>按照左上右上，左下，右下顺序。</p>
<p><strong>结果</strong></p>
<img src="/2024/03/09/16-34-34/QQ20240313-183244@2x.png" class>
<p><strong>分析原因</strong></p>
<blockquote>
<p>没有拿到满分，按照上下划分为两个4X8的数组</p>
<p>下边的4X8数组，一行中的后四列在B数组中会cache冲突行，如图<img src="/2024/03/09/16-34-34/QQ20240313-191649@2x.png" class></p>
<p>注意：</p>
<p>上四行转置之后，B数组的下四行正在cache中，为了不换出，先转置A数组的右下，再左下。</p>
</blockquote>
<p><strong>结果</strong></p>
<img src="/2024/03/09/16-34-34/QQ20240313-193617@2x.png" class>
<p>减少到1547，但是还没有满分。</p>
<p>继续修改</p>
<blockquote>
<p>题目要求减少miss数量，可以利用cache中空闲的地方保存数据  最后在替换出去</p>
<img src="/2024/03/09/16-34-34/QQ20240313-202434@2x.png" class>
<p>如图，A的上半部分的后四列存到B的后四列中</p>
<img src="/2024/03/09/16-34-34/QQ20240313-202818@2x.png" class>
<p>下半部分的第一列，先把B数组的第一行后四列保存到下部分的第一行，这样第一个cache就替换完成，依次完成其他行，最后把右下角转置过去。</p>
</blockquote>
<p><strong>结果</strong></p>
<img src="/2024/03/09/16-34-34/QQ20240313-210554@2x.png" class>
<p>miss数1339，差39次满分，没有找到优化位置。</p>
<h3 id="61-X-67"><a href="#61-X-67" class="headerlink" title="61 X 67"></a>61 X 67</h3><p><strong>结果</strong></p>
<img src="/2024/03/09/16-34-34/QQ20240313-210917@2x.png" class>
<p><strong>最终得分</strong></p>
<img src="/2024/03/09/16-34-34/QQ20240314-144148@2x.png" class>
<p>拿到了51.5分，之后有时间再来查代码优化的地方。</p>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP 笔记六 存储器层次结构</title>
    <url>/2023/07/23/09-27-18/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>23.07.23：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2023/03/15/10-59-10/" title="csapp 笔记汇总">CSAPP - 笔记汇总</a></li>
<li><a href="/2023/05/24/13-04-09/" title="CSAPP 笔记三 程序的机器级表示">第三章 - 程序的机器级表示</a></li>
<li><a href="/2023/07/06/14-25-42/" title="CSAPP 笔记五 优化程序性能">第五章 - 优化程序性能</a></li>
<li><a href="/2023/07/23/09-27-18/" title="CSAPP 笔记六 存储器层次结构">第六章 - 存储器层次结构</a>
</li>
</ul>
<h1 id="The-Memory-Hierarchy"><a href="#The-Memory-Hierarchy" class="headerlink" title="The Memory Hierarchy"></a>The Memory Hierarchy</h1><h2 id="Storage-technologies-and-trends"><a href="#Storage-technologies-and-trends" class="headerlink" title="Storage technologies and trends"></a>Storage technologies and trends</h2><p><strong>Random-Access Memory(RAM)</strong></p>
<p>根据存储单元实现方式区分为：</p>
<p><strong>SRAM(Static)</strong></p>
<ul>
<li><p>4或6个晶体存储 1 bit， 成本很高</p>
</li>
<li><p>访问速度比DRAM快</p>
</li>
<li><p>不太需要EDC(error detection and correction)错误检测和纠正</p>
</li>
<li><p>组成cache</p>
</li>
</ul>
<p><strong>DRAM(Dynamic)</strong></p>
<ul>
<li><p>一个晶体存储 1 bit</p>
</li>
<li><p>需要刷新 </p>
</li>
<li><p>需要EDC</p>
</li>
<li><p>组成主存储器，图形显卡的帧缓存(frame buffers)</p>
</li>
</ul>
<p><strong>Nonvolatile Memories</strong></p>
<p>Read-only memory(ROM):生产期间硬编程一次</p>
<p>Programmable ROM(PROM): 可编程ROM</p>
<p>Eraseable PROM(EPROM): 可擦拭可编程ROM</p>
<p>Electrically eraseable PROM(EEPROM): 电子可擦拭可编程ROM</p>
<p>Flash memory: 闪存，提供了擦拭模块，但是会有损耗。</p>
<p><strong>Disk Drive</strong></p>
<p>因为机械特性，硬盘会比SRAM，DRAM慢很多。</p>
<img src="/2023/07/23/09-27-18/QQ20230723-095730@2x.jpg" class>
<p>Sectors：扇区</p>
<img src="/2023/07/23/09-27-18/QQ20230723-143055@2x.jpg" class>
<p>Cylinder: 柱面</p>
<p>Logical Disk Blocks：逻辑块</p>
<p>实现物理块与逻辑块之间的映射关系。</p>
<p><strong>Solid State Disks(SSDs)</strong></p>
<img src="/2023/07/23/09-27-18/QQ20230723-143913@2x.jpg" class>
<ul>
<li><p>没有机械部件，由闪存翻译层固件控制</p>
</li>
<li><p>以页为单位读写，写入时必须擦拭整个块</p>
</li>
</ul>
<h2 id="Locality-of-reference"><a href="#Locality-of-reference" class="headerlink" title="Locality of reference"></a>Locality of reference</h2><p>程序的局部性</p>
<p>程序需要良好的局部性，有利用于读写内存。</p>
<h2 id="Caching-in-the-Memory-Hierarchy"><a href="#Caching-in-the-Memory-Hierarchy" class="headerlink" title="Caching in the Memory Hierarchy"></a>Caching in the Memory Hierarchy</h2><p>利用缓存和局部性原理构建了存储器层次结构。</p>
<img src="/2023/07/23/09-27-18/QQ20230723-144603@2x.jpg" class>
<blockquote>
<p>高层保存着从低层检索到的数据</p>
</blockquote>
<p><strong>Caches</strong></p>
<img src="/2023/07/23/09-27-18/QQ20230723-144821@2x.jpg" class>
<p>Cold (compulsory) miss: 冷命中，初始时cache为空</p>
<p>Conflict miss: 冲突不命中，位置被占用了</p>
<p>Capacity miss: 容量不命中，不可以容纳超过缓存大小的工作集</p>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP 实验V Performance Lab</title>
    <url>/2023/07/07/16-27-21/</url>
    <content><![CDATA[<p>更新历史：</p>
<ul>
<li>23.07.07：初稿</li>
<li>23.07.22：没学过汇编，复杂代码看不懂，明年有时间再回头做这个实验</li>
<li>24.03.09：实验作废，用cache lab 替代。</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2023/03/15/10-59-10/" title="csapp 笔记汇总">CSAPP - 笔记汇总</a>
</li>
</ul>
<h1 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h1><p>采用代码移动，循环展开以及指令并行对图像的旋转和平滑函数进行优化。</p>
<h1 id="相关内容"><a href="#相关内容" class="headerlink" title="相关内容"></a>相关内容</h1><ol>
<li><p><code>make driver</code>：生成运行文件， 运行<code>./driver</code>参数</p>
<blockquote>
<p><code>-g</code>：尽运行<code>rotate</code>和<code>smooth</code>函数</p>
<p><code>-f</code>：指定文件输入运行</p>
<p><code>-d</code>： 运行写入指定文件</p>
<p><code>-q</code>：写入文件后结束运行</p>
</blockquote>
</li>
<li><p><code>指导文件建议</code>：观看汇编代码，平滑函数计算密集，翻转函数存储密集。</p>
</li>
<li><p>允许添加宏，全局变量和其他函数(过程)。</p>
</li>
<li><p>假定图像矩阵的大小为32的倍数</p>
</li>
</ol>
<h1 id="题目及解法"><a href="#题目及解法" class="headerlink" title="题目及解法"></a>题目及解法</h1><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>课程中讲过的常见优化方式：代码移动，减少重复过程调用，消除不必要的内存引用，循环展开以及提高并行性。先查看汇编代码，根据汇编代码选择合适的方法对函数进行优化。</p>
<h2 id="Phase-Rotate"><a href="#Phase-Rotate" class="headerlink" title="Phase Rotate"></a>Phase Rotate</h2><p>原函数：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">naive_rotate</span><span class="params">(<span class="type">int</span> dim, pixel *src, pixel *dst)</span> </span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i, j;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; dim; i++)</span><br><span class="line">    <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; dim; j++)</span><br><span class="line">        dst[RIDX(dim<span class="number">-1</span>-j, i, dim)] = src[RIDX(i, j, dim)];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>汇编：</p>
<img src="/2023/07/07/16-27-21/QQ20230719-190626@2x.jpg" class>
<img src="/2023/07/07/16-27-21/QQ20230719-190641@2x.jpg" class>
<blockquote>
<p><del>之前的lab里讲过实验环境在32位shark机器下，这里汇编代码在push占用了8个字节，不太明白是不是我的环境不对</del></p>
<p><code>已解决</code>：确实环境不对，没有32位依赖，更新之后push占用4个字节</p>
<p><code>imul multi， src， des</code>三目运算</p>
</blockquote>
<p><strong>1.数据存放方式</strong></p>
<p>数据在栈空间中小端存放，<code>rdi</code>存放在<code>rsp + 0x18</code>，<code>edi</code>存放在加4个字节的位置。所以 0x1c(%esp) 的值就是dim。</p>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
        <tag>shell</tag>
        <tag>gdb</tag>
        <tag>objdump</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP 笔记五 优化程序性能</title>
    <url>/2023/07/06/14-25-42/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>23.07.06：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2023/03/15/10-59-10/" title="csapp 笔记汇总">CSAPP - 笔记汇总</a></li>
<li><a href="/2023/05/24/13-04-09/" title="CSAPP 笔记三 程序的机器级表示">第三章 - 程序的机器级表示</a></li>
<li><a href="/2023/07/06/14-25-42/" title="CSAPP 笔记五 优化程序性能">第五章 - 优化程序性能</a></li>
<li><a href="/2023/07/23/09-27-18/" title="CSAPP 笔记六 存储器层次结构">第六章 - 存储器层次结构</a>
</li>
</ul>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>通过查看汇编代码，对没有优化的代码进行调整。</p>
<h1 id="Generally-Useful-Optimizations"><a href="#Generally-Useful-Optimizations" class="headerlink" title="Generally Useful Optimizations"></a>Generally Useful Optimizations</h1><ul>
<li><p>Code Motion：代码移动，把常用的结果存到循环外。优化等级1以上，编译器会自动完成这个工作。</p>
</li>
<li><p>Reduction in Strength：减少计算量，乘法换左移或加法。</p>
</li>
<li><p>Share Common Subexpressions：寻找公共表达式，减少计算量。</p>
</li>
</ul>
<h1 id="Optimization-Blockers"><a href="#Optimization-Blockers" class="headerlink" title="Optimization Blockers"></a>Optimization Blockers</h1><ul>
<li><p>Procedure Calls：循环中进行相同结果的重复调用。</p>
<blockquote>
<p>编译器无法优化的原因：</p>
<p>1.不知道是否在循环中修改了字符串；2. 文件单独编译，编译器不知道函数使用的是哪个文件中的函数，编译器事先假设函数是一个黑盒。</p>
</blockquote>
</li>
<li><p>Memory Matters: 循环中重复读取内存。</p>
<blockquote>
<p>无法优化的原因：</p>
<p>编译器不确定C语言是否存在内存别名，之前的结果会不会影响下一次读取的内存值。所以每次都进行读取写入操作。</p>
</blockquote>
</li>
<li><p>Memory Aliasing：引入局部变量防止内存重叠。</p>
</li>
</ul>
<h1 id="Exploiting-Instruction-Level-Parallelism"><a href="#Exploiting-Instruction-Level-Parallelism" class="headerlink" title="Exploiting Instruction-Level Parallelism"></a>Exploiting Instruction-Level Parallelism</h1><p>采用指令级并行，适用于各种机器的通用优化方法</p>
<h2 id="Benchmark-Example"><a href="#Benchmark-Example" class="headerlink" title="Benchmark Example"></a>Benchmark Example</h2><p>基准例子，使用一个抽象的数据结构。</p>
<img src="/2023/07/06/14-25-42/WX20230706-153904@2x.png" class>
<p><strong>Basic Optimizations</strong></p>
<ul>
<li><p>计算数组长度移出循环</p>
</li>
<li><p>移除边界检查，用新函数获取数组</p>
</li>
<li><p>引入局部变量</p>
</li>
</ul>
<img src="/2023/07/06/14-25-42/WX20230706-154041@2x.png" class>
<p>优化后性能：</p>
<img src="/2023/07/06/14-25-42/WX20230706-154316@2x.png" class>
<p><code>优化的基本限制</code>: 程序限制了整数和浮点数的乘法必须顺序执行，所需要的周期为3和5。</p>
<p><strong>Modern CPU Design</strong></p>
<img src="/2023/07/06/14-25-42/WX20230706-163939@2x.png" class>
<p> 1995年的CPU，了解流程。</p>
<p> 超标量乱序执行：顺序指令分解重组，实现指令级并行。</p>
<p>Superscalar Processor：超标量处理器</p>
<p><strong>Pipelined Functional Units</strong></p>
<img src="/2023/07/06/14-25-42/WX20230706-170033@2x.png" class>
<p>指令流水线：<code>这里假设指令需要三个阶段完成</code></p>
<h2 id="Loop-Unrolling"><a href="#Loop-Unrolling" class="headerlink" title="Loop Unrolling"></a>Loop Unrolling</h2><p><code>循环展开</code>: 循环中计算多个值，而不是计算一次。</p>
<img src="/2023/07/06/14-25-42/WX20230707-094102@2x.png" class>
<blockquote>
<p>每次循环计算两个操作数</p>
<p><code>limit</code>: n-1, 循环i跳出时满足 i + 1 &lt; (n - 1) + 1 = n; </p>
</blockquote>
<img src="/2023/07/06/14-25-42/WX20230707-094654@2x.png" class>
<blockquote>
<p>只有加法的CPE有了提升，降低了循环计数的消耗。其他没用提升是因为接近了延迟界限。</p>
</blockquote>
<p>调整循环展开的计算顺序：</p>
<p><code>x = x OP (d[i] OP d[i+1])</code></p>
<img src="/2023/07/06/14-25-42/WX20230707-105030@2x.png" class>
<p>吞吐量突破了延迟界限，浮点数两个乘法器，一个加法器。</p>
<img src="/2023/07/06/14-25-42/WX20230707-105355@2x.png" class>
<p>减少了计算的依赖，两个元素的计算不需要等上一个x的结果。浮点数不满足结合律，改变括号可能会发生舍入溢出，结果发生变化。</p>
<p><code>Latency Bound</code>：严格顺序执行时，一条指令需要花费的全部时间。</p>
<p><code>Throughput Bound</code>：基于硬件数量和性能限制，<code>只有两个load单元，吞吐量为0.5</code></p>
<p><strong>Separate Accumulators</strong></p>
<p>多累加器，改变元素组合的顺序，如奇偶分开。</p>
<img src="/2023/07/06/14-25-42/WX20230707-112901@2x.png" class>
<p><strong>Unrolling &amp; Accumulating</strong></p>
<p>以K为因子展开一个L长度的数组，CPE可以接近吞吐界限</p>
<h2 id="Programming-with-AVX2"><a href="#Programming-with-AVX2" class="headerlink" title="Programming with AVX2"></a>Programming with AVX2</h2><p><strong>YMM Registers</strong></p>
<p>总共有16个32字节的寄存器 ，是%XMM的二倍。</p>
<h2 id="SIMD-Operations"><a href="#SIMD-Operations" class="headerlink" title="SIMD Operations"></a>SIMD Operations</h2><img src="/2023/07/06/14-25-42/WX20230707-114005@2x.png" class>
<p><code>Vector Instructions</code>矢量指令：并行执行八次单精度或四次双精度浮点操作</p>
<p>矢量指令是为处理视频，声音，图形引入的</p>
<p>gcc对矢量指令的优化是有限的</p>
<h1 id="Dealing-with-Conditionals"><a href="#Dealing-with-Conditionals" class="headerlink" title="Dealing with Conditionals"></a>Dealing with Conditionals</h1><h2 id="What-About-Branches？"><a href="#What-About-Branches？" class="headerlink" title="What About Branches？"></a>What About Branches？</h2><p><strong>Branch Prediction</strong></p>
<p>分支预测技术：先执行猜测的分支，然后再判断分支是否正确</p>
<blockquote>
<p>所有的分支预测结果都写在寄存器副本中，错误时会回退正确的寄存器结果</p>
</blockquote>
<p><strong>Performance Cost</strong></p>
<p>执行代价就是用了很多时钟周期处理所有的分支指令，可能会执行大量无效操作</p>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP 笔记三 程序的机器级表示</title>
    <url>/2023/05/24/13-04-09/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>23.05.24：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2023/03/15/10-59-10/" title="csapp 笔记汇总">CSAPP - 笔记汇总</a></li>
<li><a href="/2023/05/24/13-04-09/" title="CSAPP 笔记三 程序的机器级表示">第三章 - 程序的机器级表示</a></li>
<li><a href="/2023/07/06/14-25-42/" title="CSAPP 笔记五 优化程序性能">第五章 - 优化程序性能</a></li>
<li><a href="/2023/07/23/09-27-18/" title="CSAPP 笔记六 存储器层次结构">第六章 - 存储器层次结构</a>
</li>
</ul>
<h1 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h1><h2 id="Intel-x86-Processors"><a href="#Intel-x86-Processors" class="headerlink" title="Intel x86 Processors"></a>Intel x86 Processors</h2><ul>
<li><p>CISC - Complex 复杂指令集，RISC - Reduced 精简指令集。</p>
</li>
<li><p>x86-64，Linux的命名</p>
</li>
<li><p>ARM架构- Acorn RISC Machine</p>
</li>
<li><p>两种汇编语法：AT&amp;T和Intel。Linux和课程使用的是AT&amp;T。</p>
</li>
</ul>
<h2 id="C-assembly-machine-code"><a href="#C-assembly-machine-code" class="headerlink" title="C,assembly, machine code"></a>C,assembly, machine code</h2><ul>
<li>Micro architecture：微指令架构，CSAPP中很少涉及</li>
</ul>
<p><strong>Turning C into Object Code</strong></p>
<ul>
<li>编译命令<br><code>gcc -Og p1.c p2.c -o p</code><br><code>gcc -Og -S sum.c</code>：<br><code>-S</code>:编译阶段终止，生成汇编语言<br><code>-Og</code>:Optimize编译优化，<code>O1</code>是过去的优化级别，<code>g</code>级可以使代码更可读</li>
</ul>
<p><strong>Assembly Characteristics</strong></p>
<p>Data Types</p>
<ul>
<li><p>integer: 以1，2，4，8字节存储<br>数据和地址都用整型保存，不区分无符号和有符号。</p>
</li>
<li><p>floating point: 4，8，10字节</p>
</li>
<li><p>没有数组以及结构体，这些是由编译器构造的。</p>
</li>
</ul>
<p><strong>Operations</strong></p>
<ul>
<li><p>执行算术操作</p>
</li>
<li><p>在内存和寄存器之间传送数据</p>
</li>
<li><p>转移控制</p>
</li>
</ul>
<p>Disassembling Object Code</p>
<blockquote>
<p>指令长度在1-15个字节</p>
</blockquote>
<p>反汇编：将目标代码文件反汇编为类汇编语言。</p>
<ul>
<li><code>objdump -d sum</code>: 反汇编</li>
</ul>
<h2 id="Assembly-Basics-Registers-operands-move"><a href="#Assembly-Basics-Registers-operands-move" class="headerlink" title="Assembly Basics: Registers, operands, move"></a>Assembly Basics: Registers, operands, move</h2><p><strong>Integer Registers</strong></p>
<img src="/2023/05/24/13-04-09/WX20230603-145124@2x.png" class>
<ul>
<li><p>参数：1<code>rdi</code>, 2<code>rsi</code>, 3<code>rdx</code>, 4<code>rcx</code>, 5<code>r8</code>, 6<code>r9</code>，只能传递整型或者指针，浮点数由另外一组单独的寄存器传递。超出的参数存在栈里。</p>
</li>
<li><p>返回值：<code>rax</code></p>
</li>
</ul>
<p><strong>Moving Data</strong></p>
<blockquote>
<p><code>mov Source, Dest</code></p>
</blockquote>
<ul>
<li><p>不允许从内存取出，存入内存。从内存取出，只能存入寄存器。</p>
</li>
<li><p><code>mov $0x4, %rax</code>：目的地址是寄存器<br><code>mov $0x4, (%rax)</code>: 目的地址是内存</p>
</li>
<li><p><code>D(R)</code>: Mem[Reg[R] + D]</p>
</li>
<li><p><code>D(Rb, Ri, S)</code>: Mem[Reg[Rb] + S * Reg[Ri] + D]</p>
</li>
</ul>
<h2 id="Arithmetic-amp-logical-operations"><a href="#Arithmetic-amp-logical-operations" class="headerlink" title="Arithmetic &amp; logical operations"></a>Arithmetic &amp; logical operations</h2><p><strong>lea Src, Dst</strong></p>
<blockquote>
<p>C语言中的&amp;，取地址</p>
</blockquote>
<p>实际使用中C语言编译器喜欢用这条指令做算术运算，<code>lea (%rdi, %rdi, 2), %rax</code>, mov是将地址指向的内存值取出，lea指令将内存值所在地址取出，相当于存入的是<code>%rdi + %rdi * 2</code>。</p>
<p>Some ArTwo Operand Instructions</p>
<img src="/2023/05/24/13-04-09/WX20230603-152227@2x.png" class>
<ul>
<li><p>目的操作数作为第一个操作数，类似<code>x+=y</code></p>
</li>
<li><p>算术左移和逻辑左移相同</p>
</li>
</ul>
<p>One Operand Instructions</p>
<img src="/2023/05/24/13-04-09/WX20230603-152823@2x.png" class>
<h1 id="Control"><a href="#Control" class="headerlink" title="Control"></a>Control</h1><h2 id="Condition-Codes"><a href="#Condition-Codes" class="headerlink" title="Condition Codes"></a>Condition Codes</h2><p><strong>Single bit registers</strong></p>
<ul>
<li><p><code>CF</code>: Carry Flag (for unsigned)，同样表示无符号数溢出</p>
</li>
<li><p><code>SF</code>: Sign Flag (for signed)，负数为1</p>
</li>
<li><p><code>ZF</code>: Zero Flag</p>
</li>
<li><p><code>OF</code>: Overflow Flag (for signed)</p>
</li>
</ul>
<p><strong>Explicit Setting</strong></p>
<p>Compare</p>
<blockquote>
<p>cmpq Src2, Src1</p>
</blockquote>
<p>类似减法，Src1 - Src2，但是不保存值，只改变四个条件码</p>
<ul>
<li><p>溢出：<code>cmp b， a</code>，有符号补码<br>同号不会溢出，异号才会溢出</p>
<ul>
<li><p>负溢出：a-，b+， a-b&gt;0</p>
</li>
<li><p>正溢出：a+，b-，a-b&lt;0</p>
</li>
</ul>
</li>
</ul>
<p>Test</p>
<blockquote>
<p>testq Src2, Src1</p>
</blockquote>
<p>与运算，改变SF和ZF。</p>
<p>SetX Instructions</p>
<img src="/2023/05/24/13-04-09/WX20230605-113101@2x.png" class>
<blockquote>
<p>x86-64中4字节计算结果会将高4字节零填充，而2字节操作只会改变2字节。</p>
</blockquote>
<h2 id="Conditional-branches"><a href="#Conditional-branches" class="headerlink" title="Conditional branches"></a>Conditional branches</h2><p><strong>Jumping</strong></p>
<p>JX Instructions</p>
<img src="/2023/05/24/13-04-09/WX20230605-113806@2x.png" class>
<p><strong>Using Conditional Moves</strong></p>
<p>分支预测技术，更多时候分支预测正确率很低，选择执行分支的两部分指令，计算出两个结果，在最后一分钟选择需要的结果。只适用在两个分支都是简单计算。</p>
<blockquote>
<p><code>cmovle</code>: 小于等于的时候移动</p>
</blockquote>
<h2 id="Loops"><a href="#Loops" class="headerlink" title="Loops"></a>Loops</h2><p>C代码中最底层的控制就是跳转和测试。</p>
<h2 id="Switch-Statements"><a href="#Switch-Statements" class="headerlink" title="Switch Statements"></a>Switch Statements</h2><ul>
<li><p>用跳转表保存代码块的地址，可以快速跳转到该地址。</p>
</li>
<li><p>无论最小值是多少，通过增加偏置变为0</p>
</li>
<li><p>跳转表由编译器生成，汇编程序填写。</p>
</li>
<li><p>值范围很大，相对稀疏，会转变为条件树。</p>
</li>
</ul>
<h1 id="Procedures"><a href="#Procedures" class="headerlink" title="Procedures"></a>Procedures</h1><h2 id="Stack-Structure"><a href="#Stack-Structure" class="headerlink" title="Stack Structure"></a>Stack Structure</h2><ul>
<li><p>内存地址从下往上递增</p>
</li>
<li><p>栈底在上，push时rsp减小</p>
</li>
</ul>
<p><strong>Push &amp; Pop</strong></p>
<ul>
<li><p>Push：先减小，在写入</p>
</li>
<li><p>Pop：rsp增加</p>
</li>
</ul>
<h2 id="Calling-Conventions"><a href="#Calling-Conventions" class="headerlink" title="Calling Conventions"></a>Calling Conventions</h2><p><strong>Passing Control</strong></p>
<ul>
<li><p><code>call label</code>: 返回地址入栈，跳转到label</p>
</li>
<li><p><code>ret</code>: 出栈，返回</p>
</li>
<li><p><code>pc</code>寄存器就存在<code>rip</code>。</p>
</li>
</ul>
<p><strong>Passing data</strong></p>
<p>数据在寄存器和内存中传递，采用默认的规则，在不同的编译器下都可以传递参数。</p>
<p><strong>Managing local data</strong></p>
<p>Stack Frame</p>
<p>每个函数使用的内存块称为栈帧。</p>
<ul>
<li><p><code>rbp</code>: 表示基指针，栈底。调用者的<code>rbp</code>保存在被调用者的栈底。</p>
</li>
<li><p><code>rsp</code>: 栈顶，当它被分配了多少字节，就知道需要释放多少字节</p>
</li>
</ul>
<h2 id="Illustration-of-Recursion"><a href="#Illustration-of-Recursion" class="headerlink" title="Illustration of Recursion"></a>Illustration of Recursion</h2><p>栈帧是递归调用的前提。</p>
<h1 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h1><h2 id="Arrays"><a href="#Arrays" class="headerlink" title="Arrays"></a>Arrays</h2><p><strong>One-dimensional</strong></p>
<ul>
<li>指针加常量， 常量会被适当放缩。</li>
</ul>
<p><strong>Multi-dimensional(nested)</strong></p>
<ul>
<li>多维嵌套数组，按行或列连续存储。</li>
</ul>
<p><strong>Multi-level</strong></p>
<ul>
<li>三个数组，一个以为指针数组保存三个数组的起始地址。</li>
</ul>
<h2 id="Structures"><a href="#Structures" class="headerlink" title="Structures"></a>Structures</h2><p><strong>Access</strong></p>
<ul>
<li>通过字节偏移访问对应的结构体成员。</li>
</ul>
<p><strong>Alignment</strong></p>
<ul>
<li><p>字节对齐，访问更方便，数据不会跨越多个块</p>
</li>
<li><p>占m字节的变量，就存放在m的倍数地址处</p>
</li>
<li><p>调整声明顺序，可以优化对齐</p>
</li>
</ul>
<h2 id="Floating-Point"><a href="#Floating-Point" class="headerlink" title="Floating Point"></a>Floating Point</h2><img src="/2023/05/24/13-04-09/WX20230605-164923@2x.png" class>
<h1 id="Advanced-Topics"><a href="#Advanced-Topics" class="headerlink" title="Advanced Topics"></a>Advanced Topics</h1><h2 id="Memory-Layout"><a href="#Memory-Layout" class="headerlink" title="Memory Layout"></a>Memory Layout</h2><p>目前64位的内存只允许使用47位。Linux栈的大小为8MB。</p>
<ul>
<li><p>Stack: 局部变量</p>
</li>
<li><p>Text：执行的机器指令，只可读</p>
</li>
<li><p>Data：存放全局变量，静态变量，字符串常量</p>
</li>
<li><p>Heap：动态申请，malloc，calloc，new</p>
</li>
<li><p>Shared Libraries：存放库函数，动态加载</p>
</li>
</ul>
<p>堆分配的内存是从高位低位向中间分配，中间的一部分没有分配的内存访问段错误。<code>这是由操作系统的管理策略决定</code></p>
<h2 id="Buffer-Overflow"><a href="#Buffer-Overflow" class="headerlink" title="Buffer Overflow"></a>Buffer Overflow</h2><p><strong>Vulnerability</strong></p>
<ul>
<li><p><code>gets()</code>: 函数不检查缓冲区大小，容易越界。 </p>
</li>
<li><p><code>strcpy</code>, <code>strcat</code>,<code>scanf</code>, <code>fscanf</code>, <code>sscanf</code>, 都有溢出风险。</p>
</li>
<li><p>机器指令是小端存放的，低位在前。 </p>
</li>
</ul>
<p><strong>Code Injection Attacks</strong></p>
<p>代码注入攻击，把字符填充到缓冲区中，形成可执行的指令，修改返回指针。</p>
<blockquote>
<p>Worm与Virus：蠕虫可以自己生存，病毒不能独自运行。</p>
</blockquote>
<p><strong>Protection</strong></p>
<p><strong>Randomized stack offsets</strong>栈随机化</p>
<p>栈随机化会让缓冲区随机变化，无法预测下一个地址。</p>
<p><strong>Nonexecutable code segments</strong></p>
<p>标记栈是不可执行的代码。</p>
<p><strong>Stack Canaries</strong></p>
<p><code>gcc -fstack-protector</code> 默认启动栈保护</p>
<p><code>%fs</code>: 特殊寄存器，某块内存的值，如果Canay值改变了说明有溢出。Canay值是<code>小端存放</code>。</p>
<blockquote>
<p>Canay的值最低位字节为0，这是字符串的off-by-one bug，虽然字符串的空字符占用了Canay值，产生了溢出，但是检测不到溢出。</p>
</blockquote>
<p><strong>Return-Oriented Programming Attacks</strong></p>
<p>Canay无法破解，但是我们知道代码在什么地方，全局变量和代码的位置没有改变，通过找到某段代码组合在一起，面向返回编程。</p>
<blockquote>
<p>gadget: 通过截断一些指令，形成新的指令，替换了程序计数器</p>
</blockquote>
<img src="/2023/05/24/13-04-09/WX20230608-112255@2x.png" class>
<p>attack Lab中关闭了canary，可以通过缓冲区溢出设置需要的返回地址。</p>
<h2 id="Unions"><a href="#Unions" class="headerlink" title="Unions"></a>Unions</h2><ul>
<li><p>联合体只会为最大的域分配地址。</p>
</li>
<li><p>联合体可以用来做类型转换，不改变位。</p>
</li>
</ul>
<h2 id="大端小端"><a href="#大端小端" class="headerlink" title="大端小端"></a>大端小端</h2><ul>
<li><p>IA32：字节小端存放</p>
</li>
<li><p>x86-64：字节小端存放</p>
</li>
</ul>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP 实验III Attack Lab</title>
    <url>/2023/03/18/15-54-47/</url>
    <content><![CDATA[<p>更新历史：</p>
<ul>
<li>23.03.18：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2023/03/15/10-59-10/" title="csapp 笔记汇总">CSAPP - 笔记汇总</a>
</li>
<li><a href="/2023/03/09/11-54-50/" title="CSAPP 实验I Data Lab">I Data Lab - 位操作，数据表示</a>
</li>
<li><a href="/2023/03/14/09-43-26/" title="CSAPP 实验II Bomb Lab">II Bomb Lab - 汇编，栈帧与 gdb</a>
</li>
<li><a href="/2023/03/18/15-54-47/" title="CSAPP 实验III Attack Lab">III Attack Lab - 漏洞是如何被攻击的</a>
</li>
<li><a href="/2024/03/09/16-34-34/" title="CSAPP 实验IV CacheLab">IV Cache Lab - 实现一个缓存系统</a>
</li>
<li><a href="/2024/03/14/12-34-42/" title="CSAPP 实验V ShellLab">V Shell Lab - 实现一个Shell</a>
</li>
</ul>
<h1 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h1><p>Attack Lab实验，采用 code-injection 和 return-oriented 两种方法攻击程序。</p>
<h1 id="相关内容"><a href="#相关内容" class="headerlink" title="相关内容"></a>相关内容</h1><ol>
<li><code>./ctarget -q</code>：运行目标文件用参数<code>-q</code>，不使用服务器</li>
<li><code>-i FILE</code>: 文件输入</li>
<li><code>./hex2raw &lt; 1.txt | ./ctarget -q</code>: 答案不换行，用<code>hex2raw</code>转换成字符串，注意需要用小端存储。</li>
<li><code>call</code>指令会将8个字节的<code>rip</code>值入栈，作为<code>ret</code>返回地址。</li>
<li>code-injection：关闭了栈随机化，栈不可执行和canary。</li>
<li>return-oriented：打开栈随机化和不可执行，关闭canary。</li>
</ol>
<h1 id="题目及解法"><a href="#题目及解法" class="headerlink" title="题目及解法"></a>题目及解法</h1><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>和课程中讲解的两种漏洞的破解方式相同：代码注入利用缓冲区溢出，把需要执行的指令输入到栈中。面向返回编程将需要的指令片段返回地址输入到缓冲区中，模拟一个程序计数器，顺序执行需要执行的指令。由于bomb实验对每一条汇编命令都进行了解释，这个实验就不逐条解释了，把之前做过的答案重新整理一遍。</p>
<h2 id="Phase-1"><a href="#Phase-1" class="headerlink" title="Phase 1"></a>Phase 1</h2><p>思路：第一个很简单输入函数开辟了0x28的空间用于保存输入,返回地址在rsp+0x28的位置,所以输入一段0x28字节的字符串后面跟上返回touch1的地址,注意地址是小端存放:<code>C0 17 40 00 00 00 00 00</code>, 以下答案为了方便观看添加了换行，运行程序时需要去掉换行。</p>
<ol>
<li><p>gdb进入程序，对test函数反汇编：</p>
<img src="/2023/03/18/15-54-47/WX20230609-103056@2x.png" class>
</li>
<li><p>调用了getbuf函数，可以查看一下汇编代码，找出缓冲区的大小</p>
<img src="/2023/03/18/15-54-47/WX20230609-103332@2x.png" class>
</li>
<li><p>缓冲区大小为<code>0x28</code>，40个字节，输入的字符串保存在当前<code>rsp</code>处。然后输入40个字节无用数据，充满缓冲区，继续输入8个字节的touch1地址，覆盖掉原地址。</p>
</li>
<li><p>对要跳转的touch1函数反汇编，找一下函数入口地址：</p>
<img src="/2023/03/18/15-54-47/WX20230609-102814@2x.png" class>
</li>
<li><p>所以需要返回的地址为<code>0x00000000004017c0</code>，输入小端存放。</p>
</li>
</ol>
<p>答案：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line">c0 <span class="number">17</span> <span class="number">40</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br></pre></td></tr></table></figure>
<h2 id="Phase-2"><a href="#Phase-2" class="headerlink" title="Phase 2"></a>Phase 2</h2><p>思路：</p>
<p>第二个需要注入可执行代码, 需要把touch2的参数寄存器<code>rdi</code>内容设置为cookie, 这样重置ret到touch2才能正确执行, 所有第一次在getbuf中把返回值重置为一个栈地址, 然后在这个地址内写入一系列操作, 每次ret后返回地址会自动退栈, 所以先把<code>rsp</code>-8, 然后在<code>rdi</code>写入touch2的地址,然后将<code>rdi</code>写入<code>rsp</code>指向的栈地址中,然后把cookie值写入<code>rdi</code>中,最后ret。</p>
<ol>
<li><p>先查看一下touch2函数的入口在哪里：</p>
<img src="/2023/03/18/15-54-47/WX20230609-104729@2x.png" class>
</li>
<li><p>可以看到需要返回地址为<code>0x00000000004017ec</code>， 参数可以进入touch2函数打印一下那个局部变量的值，也可以在上一个题目输出信息那里看到，我的cookie值为<code>0x59b997fa</code>。</p>
</li>
<li><p>这次需要让指令在栈中执行，所以需要知道getbuf时<code>%rsp</code>的值，可以在test处打断点，<code>display $rsp</code>，值为<code>$rsp = (void *) 0x5561dcb0</code>。test函数中分配8个字节，call getbuf 时使用了8个字节，getbuf分配40个字节，一共分配了56个字节，所以在我们输入字符串时，<code>rsp</code>指向的地址值应该是<code>0x5561dc78</code></p>
</li>
<li><p>接下来就是注入一段需要运行的代码，可以先写汇编代码，通过指令<code>gcc -c foo.s</code>，生成二进制代码<code>foo.o</code>， 在通过<code>objdump -d foo.o</code>反汇编，生成可读的机器指令。</p>
</li>
</ol>
<p>答案：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">48</span> <span class="number">83</span> ec <span class="number">08</span>           <span class="comment">/*  sub    $0x8,%rsp */</span></span><br><span class="line"><span class="number">48</span> c7 c7 ec <span class="number">17</span> <span class="number">40</span> <span class="number">00</span>  <span class="comment">/*  mov    $0x4017ec,%rdi */</span></span><br><span class="line"><span class="number">48</span> <span class="number">89</span> <span class="number">3</span>c <span class="number">24</span>           <span class="comment">/*  mov    %rdi,(%rsp) */</span></span><br><span class="line"><span class="number">48</span> c7 c7 fa <span class="number">97</span> b9 <span class="number">59</span>  <span class="comment">/*  mov    $0x59b997fa,%rdi   -   set cookie */</span></span><br><span class="line">c3                    <span class="comment">/*  retq  */</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span></span><br><span class="line"><span class="number">78</span> dc <span class="number">61</span> <span class="number">55</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span><span class="comment">/* return adress, execute exploit code */</span></span><br></pre></td></tr></table></figure>
<h2 id="Phase-3"><a href="#Phase-3" class="headerlink" title="Phase 3"></a>Phase 3</h2><p>思路：</p>
<p>第三题和第二题基本相同return到touch3后要进行一次cookie对比,第二题是十六进制的数字对比,可以提前把cookie写入<code>rdi</code>寄存器里,但是第三题的参数是一个字符串指针,所以要把字符串地址写入<code>rsi</code>第二个参数寄存器里,不能用在栈中写,因为后面还有两次函数调用,栈空间很容易被覆盖,找了一下变量cookie的地址,这个地址<code>0x6044e4</code>的后面有很长一段地址没有使用过,所以可以把字符串写入<code>0x6044ec</code>,<code>rsi</code>的参数传递在<code>0x6044ec</code>,还有一个地方是字符串<code>0x59b997fa</code>,每个字符存在一个字节里,一共需要8个字节的空间,可以用<code>mov</code>,写入十六进制数,但是要注意字符串大端存放,数字是小端存放,翻转一下就可以了.</p>
<ol>
<li><p>根据实验指南中提示的函数<code>hexmatch</code>, 做了一个字符串对比，第二个参数是字符串地址，而且在函数中将传入的cookie值转换成了一个随机地址的字符串，然后进行字符串对比，所以我们需要提前把字符串写入到固定的地址，然参数传进去。</p>
</li>
<li><p><code>disas touch3</code></p>
<img src="/2023/03/18/15-54-47/WX20230609-123834@2x.png" class>
</li>
<li><p>可以看出cookie值存在内存的<code>0x6044e4</code>处，打印一下周围的内存值，<code>x/6xg</code></p>
<img src="/2023/03/18/15-54-47/WX20230609-130233@2x.png" class>
</li>
<li><p>上面可以看到后面有一部分是没有使用的内存，选一个<code>0x6044ec</code>，把我们的字符串写入这里，注意大端存放。</p>
</li>
<li><p>接下来注入要运行的指令，和上一题相同，但是要把字符串写入到选定的地址里，字符串<code>59b997fa</code>转换为16进制为<code>0x35 0x39 0x62 0x39 0x39 0x37 0x66 0x61</code>，但是要注意小端存放的值为<code>0x6166373939623935</code>，所以写入这个值，当做字符串读出时刚好是cookie值。<code>movabs</code>：用于64位立即数赋值。</p>
</li>
</ol>
<p>答案：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">48</span> <span class="number">83</span> ec <span class="number">08</span>           <span class="comment">/*  sub    $0x8,%rsp */</span></span><br><span class="line">bf fa <span class="number">18</span> <span class="number">40</span> <span class="number">00</span>        <span class="comment">/*  mov    $0x4018fa,%edi */</span></span><br><span class="line"><span class="number">48</span> <span class="number">89</span> <span class="number">3</span>c <span class="number">24</span>           <span class="comment">/*  mov    %rdi,(%rsp) */</span></span><br><span class="line">bf ec <span class="number">44</span> <span class="number">60</span> <span class="number">00</span>        <span class="comment">/*  mov    $0x6044ec,%edi  */</span></span><br><span class="line"><span class="number">48</span> be <span class="number">35</span> <span class="number">39</span> <span class="number">62</span> <span class="number">39</span> <span class="number">39</span>  </span><br><span class="line"><span class="number">37</span> <span class="number">66</span> <span class="number">61</span>              <span class="comment">/*  movabs $0x6166373939623935,%rsi */</span></span><br><span class="line"><span class="number">48</span> <span class="number">89</span> <span class="number">37</span>              <span class="comment">/*  mov    %rsi,(%rdi)    */</span></span><br><span class="line">c3                    <span class="comment">/*  retq  */</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span></span><br><span class="line"><span class="number">78</span> dc <span class="number">61</span> <span class="number">55</span> <span class="comment">/* return adress, execute exploit code */</span></span><br></pre></td></tr></table></figure>
<h2 id="Phase-4"><a href="#Phase-4" class="headerlink" title="Phase 4"></a>Phase 4</h2><p>思路：</p>
<p>第四题,和第二题相同,需要把代码返回到touch2里面,但是不能像phase2中代码注入运行代码,因为栈内标记为不可执行了,所以要使用ROP的方法来操作,在给出的gadget中选择两个连续的48 89 c7 c3, mov %rax,%rdi,   还有一个58 c3, popq %rax,将这两条指令的地址用栈溢出的方法写到返回里,在返回地址中间加上cookie码。</p>
<ol>
<li><p><strong><em><code>gcc -c farm.c</code>：生成目标文件<br><code>objdump -d farm.o &gt; farm.s</code>：反汇编生成可读指令<br><code>这里需要注意优化级别为 -Og，与实验的优化级别相同</code></em></strong><br><code>objdump -d rtarget.o &gt; rtarget.s</code>：直接对可执行文件反汇编生成的指令带地址，更方便看一些。</p>
</li>
<li><p>在<code>rtarget.s</code>中找我们需要的指令，</p>
</li>
<li><p><code>setval_426</code>函数中有：地址为<code>4019c5</code></p>
<img src="/2023/03/18/15-54-47/WX20230609-140230@2x.png" class>
</li>
<li><p>找到我们需要的<code>48 89 c7</code>，后续的<code>90</code>是一个nop，什么也不做，计数器加1，最后跟着<code>c3</code>ret。</p>
</li>
<li><p>上面把<code>rax</code>的值存到<code>rdi</code>中了，接下来我们就是找一个pop指令<code>58 90 c3</code>，退栈到<code>rax</code>中，找到<code>getval_280</code>：地址为<code>4019cc</code></p>
<img src="/2023/03/18/15-54-47/WX20230609-140336@2x.png" class>
</li>
<li><p>所以溢出的输入为退栈到<code>rax</code>，栈顶是cookie值，然后执行<code>0x4019c5</code>的mov指令，最后返回touch2。注意所有的操作都是8字节。</p>
</li>
</ol>
<p>答案：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">01</span> <span class="number">02</span> <span class="number">00</span> <span class="number">00</span>   <span class="comment">/*40个字节溢出  */</span></span><br><span class="line">cc <span class="number">19</span> <span class="number">40</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line">fa <span class="number">97</span> b9 <span class="number">59</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line">c5 <span class="number">19</span> <span class="number">40</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line">ec <span class="number">17</span> <span class="number">40</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br></pre></td></tr></table></figure>
<h2 id="Phase-5"><a href="#Phase-5" class="headerlink" title="Phase 5"></a>Phase 5</h2><p>思路：</p>
<p>和Phase 3相同，但是要把代码注入改为target序列执行指令，可能需要对寄存器的低字节进行操作，还没想出怎么做这个题目，后面有时间了再来做。</p>
<p>答案：</p>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
        <tag>shell</tag>
        <tag>gdb</tag>
        <tag>objdump</tag>
      </tags>
  </entry>
  <entry>
    <title>csapp 笔记汇总</title>
    <url>/2023/03/15/10-59-10/</url>
    <content><![CDATA[<h1 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h1><h2 id="CMU-15-213笔记"><a href="#CMU-15-213笔记" class="headerlink" title="CMU 15-213笔记"></a>CMU 15-213笔记</h2><ul>
<li><a href="/2023/05/24/13-04-09/" title="CSAPP 笔记三 程序的机器级表示">第三章 - 程序的机器级表示</a></li>
<li><a href="/2023/07/06/14-25-42/" title="CSAPP 笔记五 优化程序性能">第五章 - 优化程序性能</a></li>
<li><a href="/2023/07/23/09-27-18/" title="CSAPP 笔记六 存储器层次结构">第六章 - 存储器层次结构</a>
</li>
</ul>
<h2 id="代码仓库"><a href="#代码仓库" class="headerlink" title="代码仓库"></a>代码仓库</h2><p><a href="https://github.com/PKunicor/CsappLab">PKunicor/Csapp lab</a></p>
<h2 id="实验笔记"><a href="#实验笔记" class="headerlink" title="实验笔记"></a>实验笔记</h2><ul>
<li><a href="/2023/03/09/11-54-50/" title="CSAPP 实验I Data Lab">I Data Lab - 位操作，数据表示</a></li>
<li><a href="/2023/03/14/09-43-26/" title="CSAPP 实验II Bomb Lab">II Bomb Lab - 汇编，栈帧与 gdb</a></li>
<li><a href="/2023/03/18/15-54-47/" title="CSAPP 实验III Attack Lab">III Attack Lab - 漏洞是如何被攻击的</a></li>
<li><a href="/2024/03/09/16-34-34/" title="CSAPP 实验IV CacheLab">IV Cache Lab - 实现一个缓存系统</a></li>
<li><a href="/2024/03/14/12-34-42/" title="CSAPP 实验V ShellLab">V Shell Lab - 实现一个Shell</a>
</li>
</ul>
<h1 id="实验指南"><a href="#实验指南" class="headerlink" title="实验指南"></a>实验指南</h1><ul>
<li><p><a href="https://www.bilibili.com/video/BV1iW411d7hd/?spm_id_from=333.788.top_right_bar_window_custom_collection.content.click&amp;vd_source=d5a3522259c17126f1b623c977d04c3e">B站CS15-213翻译课程</a> </p>
</li>
<li><p><a href="https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22b96d90ae-9871-4fae-91e2-b1627b43e25e%22">实验课程指导</a> <code>lab重点</code> </p>
</li>
</ul>
<blockquote>
<p>做Lab非常推荐看一下官方给的Lab指导课程，课程里详细介绍了实验上手方式以及需要用到的命令。 </p>
</blockquote>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP 实验II Bomb Lab</title>
    <url>/2023/03/14/09-43-26/</url>
    <content><![CDATA[<p>更新历史：</p>
<ul>
<li>23.03.14：初稿</li>
<li>23.03.18：新增secret_phase</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2023/03/15/10-59-10/" title="csapp 笔记汇总">CSAPP - 笔记汇总</a>
</li>
<li><a href="/2023/03/09/11-54-50/" title="CSAPP 实验I Data Lab">I Data Lab - 位操作，数据表示</a>
</li>
<li><a href="/2023/03/14/09-43-26/" title="CSAPP 实验II Bomb Lab">II Bomb Lab - 汇编，栈帧与 gdb</a>
</li>
<li><a href="/2023/03/18/15-54-47/" title="CSAPP 实验III Attack Lab">III Attack Lab - 漏洞是如何被攻击的</a>
</li>
<li><a href="/2024/03/09/16-34-34/" title="CSAPP 实验IV CacheLab">IV Cache Lab - 实现一个缓存系统</a>
</li>
<li><a href="/2024/03/14/12-34-42/" title="CSAPP 实验V ShellLab">V Shell Lab - 实现一个Shell</a>
</li>
</ul>
<h1 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h1><p>Bomb Lab实验，使用GDB工具拆除六个阶段的炸弹，需要仔细输入，爆炸次数过多实验会失败。</p>
<h1 id="相关内容"><a href="#相关内容" class="headerlink" title="相关内容"></a>相关内容</h1><ol>
<li><p>下载自己的炸弹，不能修改bomb.c代码。</p>
</li>
<li><p>每爆炸一次失去一半的分数。</p>
</li>
<li><p>测试命令</p>
<blockquote>
<p><code>./bomb</code>：运行炸弹程序， <code>psol.txt</code>：文件输入答案</p>
</blockquote>
</li>
<li><p>GDB命令</p>
<blockquote>
<p><code>gdb boom</code>：调试命令。参数：<code>-e</code>：置顶可执行文件名<br><code>quit</code>：退出调试<br><code>run [参数]</code>：运行程序；<code>&lt; psol.txt</code>：文件输入<br><code>continue</code>：断点继续<br><code>print expr</code>：打印表达式值，区别：x打印的内存的值<br><code>nexti</code>：单步执行，不进入函数调用<br><code>stepi</code>：单步执行进入函数调用<br><code>break function</code>：函数处断点<br><code>break *address</code>：内存地址处断点<br><code>delete n</code>：删除n号断点<br><code>delete</code>：删除所有断点<br><code>finish</code>：运行的当前函数返回停止<br><code>x/5cb str</code> ：显示5个单字节；<code>h</code>：两个；<code>w</code>：四个；<code>g</code>：八个<br><code>x/1sb str</code> ：显示字符串str<br><code>x/4dw arr</code> ：取4个整型数字以十进制格式显示<br><code>display arr</code>：每次断点都打印数据，参数<code>/x /s /u /c</code>。<br><code>disas</code>：反汇编当前函数<br><code>info registers</code>：显示寄存器内容</p>
</blockquote>
</li>
<li><p>objdump命令 <code>反汇编</code></p>
<blockquote>
<p><code>objdump -t</code>：打印变量内容的存储位置<br><code>objdump -d bomb &gt; bomb.s</code>：反汇编生成bomb.s文件。<code>需要使用gdb查看函数</code></p>
</blockquote>
</li>
<li><p>screen 命令 <code>分屏看汇编文件，也可以不分屏在GDB中使用disas命令</code></p>
<blockquote>
<p><code>screen -ls</code>：查看所有screen终端<br><code>screen -S name</code> ：创建终端<br><code>screen -r name or id</code> ：连接终端。连接失败用<code>screen -d **</code>：连接<br><code>exit</code>：退出终端<br><code>Ctrl + a，d</code>：暂离；<code>c</code>：创建子会话；<code>w</code>：子会话列表；<code>p</code>：上一个；<code>n</code>：下一个；<code>0-9</code>：切换；<code>：</code>：命令行模式(<code>resize 80</code>)调整窗口大小。<br><code>screen -S id -X quit</code>：关闭视窗</p>
</blockquote>
</li>
<li><p>函数参数传递相关寄存器</p>
<blockquote>
<p><code>%rax</code>：返回值<br><code>%rdi</code>：第一个参数<br><code>%rsi</code>：第二个参数<br><code>%rdx</code>：第三个参数<br><code>%rcx</code>：第四个参数<br><code>%d8</code>，<code>%d9</code>，<code>%d10</code>依次是后面的参数</p>
</blockquote>
</li>
</ol>
<h1 id="题目及解法"><a href="#题目及解法" class="headerlink" title="题目及解法"></a>题目及解法</h1><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>先从源码<code>bomb.c</code>可以看出调用顺序，通过函数<code>explode_bomb</code>让炸弹爆炸，所有我们在调试过程中，在这个函数处打断点，每次只要输入错误密码，炸弹爆炸的时候程序都会进入断点，这样就不会扣分了。从源码里还能看出每一个关卡的函数都是<code>phase_x</code>，所以每次调试之前，在爆炸处打断点，在<code>phase_x</code>处打断点，进入这关之后，用反汇编命令<code>disas</code>，显示函数的汇编代码，然后进行解题。</p>
<h2 id="phase-1"><a href="#phase-1" class="headerlink" title="phase_1"></a>phase_1</h2><p>思路：</p>
<p>第一个比较简单，找到字符串对比函数，参数1放在寄存器rdi里，参数2放在寄存器rsi里。<br>在字符串对比函数内打断点，命令<code>x/1sb $rsi</code>,打印出寄存器里的字符串内容。<br>做法：</p>
<ol>
<li><p><code>break phase_1</code> ：打断点</p>
</li>
<li><p><code>disas</code>：显示汇编指令</p>
<img src="/2023/03/14/09-43-26/WX20230315-120145@2x.png" class>
</li>
<li><p><code>break explode_bomb</code> ：在爆炸函数打断点，防止进入这里引爆炸弹  </p>
</li>
<li><p>根据汇编语言可以看出，输入的字符串在调用字符串对比函数中进行对比，根据返回值进行选择是否爆炸。<br><code>break strings_not_equal</code> ：打断点</p>
</li>
<li><p><code>x/1sb $rsi</code>：打印<code>$rsi</code>内容，根据phase_1函数的汇编指令中，改变了rsi内容，说明我们输入的密码在<code>$rdi</code>中，解密的密码在<code>$rsi</code>中。</p>
<img src="/2023/03/14/09-43-26/WX20230315-122703@2x.png" class>
</li>
</ol>
<p>答案:</p>
<p><code>Border relations with Canada have never been better.</code></p>
<h2 id="phase-2"><a href="#phase-2" class="headerlink" title="phase_2"></a>phase_2</h2><p>思路：</p>
<p>第二个主要是是在函数分配的栈里面进行操作,通过sscanf函数的第二个参数,format,”%d %d %d %d %d %d”，可以看出密码应该是六个数,通过记录的<code>%rsi</code>是phase2函数的<code>%rsp</code>，可以知道最终密码应该是保存在主函数的栈区，第一次和1比较,相同才能继续，第一个数字是1， 后面每一个数字都是和前一个数字*2相比较，那么就是一个等比数列，公比2。<br>做法：</p>
<ol>
<li><p><code>break phase_2</code> ：打断点</p>
</li>
<li><p><code>disas</code>：显示汇编指令</p>
<img src="/2023/03/14/09-43-26/WX20230315-164003@2x.png" class>
<p>发现函数调用了read_six_numbers函数，所以继续在这里打断点，看具体输入了什么。</p>
</li>
<li><p><code>break read_six_numbers</code>，查看这个函数的汇编指令</p>
<img src="/2023/03/14/09-43-26/WX20230315-164335@2x.png" class>
<p>从调用sscanf函数之前可以看出输入格式在第二个参数里 ，以在调用之前打断点，打印<code>rsi</code>的内容。从调用爆炸函数的条件也可以看出来，sscanf返回值大于等于6的时候才会正常跳转，输入的数太少会直接爆炸。</p>
<img src="/2023/03/14/09-43-26/WX20230315-164848@2x.png" class>
<p>这道题的密码应该是6位数字，输入的内容保存在后面的传参里，地址在<code>rsi</code>寄存器里，根据phase_2的调用之前，<code>rsi</code>寄存器里存的是<code>rsp</code>的地址，所以可以判断出，读取函数把输入的数字存到了<code>rsp</code>内的地址里。</p>
</li>
<li><p>根据返回到phase_2函数的下一条指令<code>cmpl   $0x1,(%rsp)</code><br><code>cmpl</code>：代表双字，四个字节，用<code>x/6dw $rsp</code>：打印输入的内容看一下是不是自己输入的；根据跳转条件可以看出，第一个密码是1，</p>
</li>
<li><p>第一个密码正确后，依次看后面的指令:<br><code>lea    0x4(%rsp),%rbx</code>：下四个字节，第二个密码地址放到<code>rbx</code>里。<br><code>lea    0x18(%rsp),%rbp</code>：第24字节地址放到<code>rbx</code>里。<code>相当于for循环结束条件</code><br><code>mov    -0x4(%rbx),%eax</code>：第一个密码1存到<code>eax</code>。<br><code>add    %eax,%eax</code>：<code>eax</code>+<code>eax</code>。<code>乘2</code><br><code>cmp    %eax,(%rbx)</code>：对比第二个密码是不是2。<br><code>add    $0x4,%rbx</code>：<code>rbx</code>指向下四个字节，第三个密码。<br><code>cmp    %rbp,%rbx</code>：对比是不是到了结束末尾，否则继续进行循环。<br>所以这六个数字密码是一个1开头的等比数列，公比为2。</p>
</li>
</ol>
<p>答案:</p>
<p><code>1 2 4 8 16 32</code></p>
<h2 id="phase-3"><a href="#phase-3" class="headerlink" title="phase_3"></a>phase_3</h2><p>思路：</p>
<p>第三个和第二个相同,查看输入的是两个%d,存在phase3的栈里面,不一样的地方是,这道题目的答案有7种,因为第一个密码小于7,所有一共有8组答案,分别是0,207,   1-0x137,2-0x2c3,3-0x100,4-0x185,5-0xce, 6-0x2aa, 7-0x147.<br>做法：</p>
<ol>
<li><p><code>break phase_3</code> ：打断点</p>
</li>
<li><p><code>disas</code>：显示汇编指令</p>
<img src="/2023/03/14/09-43-26/WX20230316-103622@2x.png" class>
<p>找到<code>sscanf</code>函数，和之前一样打断点查看输入格式。</p>
</li>
<li><p><code>x/1sb $rsi</code>：这道题的密码应该是两个数</p>
<img src="/2023/03/14/09-43-26/WX20230316-103921@2x.png" class>
</li>
<li><p>基本步骤和上面一样，看一下密码对比的循环指令：<br><code>cmpl   $0x7,0x8(%rsp)</code>：对比第一个数字和7的大小<br><code>ja     0x400fad &lt;phase_3+106&gt;</code>：无符号大于就跳转，跳转之后调用了爆炸函数，说明第一个数字小于等于7并且大于等于0<code>负数当无符号数时第一位总是1，一定比8大</code>。<br><code>mov    0x8(%rsp),%eax</code>：把第一个数字放到<code>eax</code>里。<br><code>jmpq   *0x402470(,%rax,8)</code>：跳转到<code>*(0x402470 + (%rax * 8))</code><br>打印一下从<code>0x402470</code>这里开始存的是什么：<code>x/8xg 0x402470</code>或者<code>x/64xb 0x402470</code>：这里要注意数据是按照大端法存储的。</p>
<img src="/2023/03/14/09-43-26/WX20230316-110646@2x.png" class>
<p>可以看出对应跳转关系，跳转后执行的指令为：<code>mov    $0xcf,%eax</code>，根据不同的第一位数字得到的为：<code>0--0xcf</code> <code>1--0x137</code> <code>2--0x2c3</code> <code>3--0x100</code> <code>4--0x185</code> <code>5--0xce</code> <code>6--0x2aa</code> <code>7--0x147</code></p>
</li>
</ol>
<p>答案:</p>
<p><code>0 207</code></p>
<h2 id="phase-4"><a href="#phase-4" class="headerlink" title="phase_4"></a>phase_4</h2><p>思路：</p>
<p>第四个主要难点在func4函数里有一个单操作数的sar指令，之前一直以为移位的数存在寄存器cl里，后来才发现这是默认移位1，所有这个题的密码第一位是在用二分法在(0-14)中找值，但是由于函数返回值是0才不会爆炸，往右半部分查找时，返回值不为0,所以密码只能在左半部分，分别是7，3，1，0，四种答案，第二个密码是在phase_4函数里，对比的数字是0，所以第二个密码是0。<br>做法：</p>
<ol>
<li><p><code>break phase_4</code> ：打断点</p>
</li>
<li><p><code>disas</code>：显示汇编指令</p>
<img src="/2023/03/14/09-43-26/WX20230316-112833@2x.png" class>
<p>找到<code>sscanf</code>函数，和之前一样打断点查看输入格式。这道题和上一道的<code>rsi</code>内容一样，说明都是两个数。</p>
</li>
<li><p><code>cmpl   $0xe,0x8(%rsp)</code>：<br><code>jbe    0x40103a &lt;phase_4+46&gt;</code>：小于等于跳转，说明第一个密码小于等于14。<br><code>mov    $0xe,%edx</code>：14存到<code>rdx</code>第三个参数。<br><code>mov    $0x0,%esi</code>：0存到<code>rsi</code>第二个参数。<br><code>mov    0x8(%rsp),%edi</code>：第一个密码存到<code>rdi</code>第一个参数。<br><code>callq  0x400fce &lt;func4&gt;</code>：调用func4函数。</p>
</li>
<li><p><code>disas</code>：打断点反汇编，区间(a,b)</p>
<img src="/2023/03/14/09-43-26/WX20230316-113725@2x.png" class>
<p><code>mov    %edx,%eax</code>：第三个参数14存到<code>rax</code><br><code>sub    %esi,%eax</code>：<code>rax</code>-<code>rsi</code><br><code>mov    %eax,%ecx</code>：把两个参数的差存到<code>rcx</code> 记为m<br><code>shr    $0x1f,%ecx</code>：<code>ecx</code>逻辑右移31位，<code>ecx</code>是32位的，移动31位，只剩下了符号位。<br><code>add    %ecx,%eax</code>：<code>eax</code>等于0+差。<code>如果b-a=-1，ecx=1，eax+1=0</code><br><code>sar    %eax</code>：<code>eax</code>右移一位，算术右移，等于7，m<br><code>lea    (%rax,%rsi,1),%ecx</code>：加载有效地址，效果是<code>rcx</code>里存入了a+m<br><code>cmp    %edi,%ecx</code>比较中间值是否等于第一个数字。<br>如果大于：<code>lea    -0x1(%rcx),%edx</code>b等于m-1。递归函数，左边区间查找是结束递归后<code>rax</code>乘2。<br>小于等于：<br><code>mov    $0x0,%eax</code>：<code>eax</code>等于0<br><code>cmp    %edi,%ecx</code>：对比中间值和第一个数字<br><code>jge    0x401007 &lt;func4+57&gt;</code>：大于等于跳转，本来是小于等于，相等于等于跳转，所以结束标志是第一个参数等于中间值m。返回值<code>rax</code>为0。<br><code>0x1(%rcx),%esi</code>：小于时，a = m+ 1，继续递归，小于时结束递归后执行的是<code>0x1(%rax,%rax,1),%eax</code>，不为0。</p>
</li>
<li><p>根据<code>func4</code>返回后<code>test   %eax,%eax</code>，如果不相等就爆炸，所以eax只能等于0，所以第一个数字一定是在二分查找的左边区间，所以可能值为7，3，1，0。</p>
</li>
<li><p><code>cmpl   $0x0,0xc(%rsp)</code>：相等时函数结束，说明第二个密码是固定值0。</p>
</li>
</ol>
<p>答案:</p>
<p><code>0 0</code></p>
<h2 id="phase-5"><a href="#phase-5" class="headerlink" title="phase_5"></a>phase_5</h2><p>思路：</p>
<p>第五个比较有趣,还是和12一样计算字符串长度,知道密码应该是一个6位长度字符串，计算字符串是不是相等函数的参数显示待匹配的字符串是”flyers”，编译的时候用了canary，栈指针每次都不同，这个对解密应该是没影响，主要是输入的字符串，每一个字节只保留了低四位,然后加上一个地址，将内存中的值送到了rsp里，那么这个地址应该是一个数组，打印一下这个地址开始的15个字节，是一串字符maduiersnfotvbyl，需要我们从里面找出密码，位置分别是9，15，14，5，6，7。所以输入的字符串的ASCII编码后四位应该就是这个值，答案比较多，找<br>了一个’a’-‘z’中的答案。<br>做法：</p>
<ol>
<li><p><code>break phase_5</code> ：打断点</p>
</li>
<li><p><code>disas</code>：显示汇编指令</p>
<img src="/2023/03/14/09-43-26/WX20230316-132301@2x.png" class>
<p>根据字符串对比函数的下一条<code>cmp    $0x6,%eax</code>，可以看出这道题密码长度为6。</p>
</li>
<li><p>分析跳转后的代码</p>
<img src="/2023/03/14/09-43-26/WX20230316-160027@2x.png" class>
<p><code>movzbl (%rbx,%rax,1),%ecx</code>：<code>ecx</code>内存的是<code>rbx</code>的第<code>rax</code>个字节做零扩展到双字，如图是<code>0x69</code>；<code>rax</code>相当于是for循环里的i。<br><code>mov    %cl,(%rsp)</code>：把<code>rcx</code>的低八位送入栈顶指针指向的地址。<br><code>mov    (%rsp),%rdx</code>：把值继续送到<code>rdx</code>。<br><code>and    $0xf,%edx</code>：只保留<code>rdx</code>的低四位。<br><code>movzbl 0x4024b0(%rdx),%edx</code>：把<code>rdx</code>的值加上<code>0x4024b0</code>，新地址的内容存到<code>edx</code>里。<br><code>mov    %dl,0x10(%rsp,%rax,1)</code>：把<code>rdx</code>的低八位存入<code>rax+rsp+0x10</code>的位置。<br><code>add    $0x1,%rax</code>：i自加<br><code>cmp    $0x6,%rax</code>：循环结束判断</p>
</li>
<li><p>后续指令把所有的输入字符改变之后保存在了<code>rsp + 0x10</code>指向的地址里，然后调用<code>strings_not_equal</code>：我们可以根据查看传参，看一下标准答案是什么。</p>
<img src="/2023/03/14/09-43-26/WX20230316-161808@2x.png" class>
<p>可以看到密码是字符串<code>flyers</code>，对应的字节是<code>0x66 0x6c 0x79 0x65 0x72 0x73</code>。</p>
</li>
<li><p>我们在打印一下for循环里跳转的地址<code>0x4024b0</code>保存的是什么?</p>
<img src="/2023/03/14/09-43-26/WX20230316-162400@2x.png" class>
<p>由于偏移量总是低四位，范围是0-15，所以字符串只需要关注<br><code>maduiersnfotvbyl</code>前面这些，需要我们从里面找出密码，位置分别是9，15，14，5，6，7。所以输入的字符串的ASCII编码后四位应该就是这个值，答案比较多，找了一个’a’-‘z’中的答案。</p>
</li>
</ol>
<p>答案:</p>
<p><code>ionefg</code></p>
<h2 id="phase-6"><a href="#phase-6" class="headerlink" title="phase_6"></a>phase_6</h2><p>思路：</p>
<p>第六个是一个链表，节点的前八个字节中低位是数据，高位是输入的密码，高位的8个字节是下一跳地址，根据输入的6个数必须是1-6，切两两不相同，还进行了一次处理，每一位密码x，变成了y = 7-x。</p>
<p>然后是链表的操作，每一个密码的节点对应的是(y - 1) * 8 + 结点首地址， 分配好结点后进行结点连接。<br>每一位密码的下一跳就是下一位密码对应的结点地址，最后一位结点的下一跳是0；要求是必要让结点中的数据，满足前一个结点大于等于后一个结点，<br>而所有结点的数据分别是：<br>(1):332<br>(2):168<br>(3):924<br>(4):691<br>(5):477<br>(6):443<br>所以数据最大的结点应该在最前面，序号应该是3 4 5 6 1 2。<br>而输入的密码应该是7 - y：   4 3 2 1 6 5<br>做法：</p>
<ol>
<li><p><code>break phase_6</code> ：打断点</p>
</li>
<li><p><code>disas</code>：显示汇编指令</p>
<img src="/2023/03/14/09-43-26/WX20230316-163103@2x.png" class>
<img src="/2023/03/14/09-43-26/WX20230316-163118@2x.png" class>
<img src="/2023/03/14/09-43-26/WX20230316-163134@2x.png" class>
</li>
<li><p>根据指令，这道题是读取了六个数字，我们直接看后面的处理。输入的数字保存在<code>rsp</code>指向的地址里。每个数字存储在四个字节里<br><code>mov    %rsp,%r14</code>：把输入的数组起始地址送到<code>r14</code><br><code>mov    $0x0,%r12d</code>：<code>r12d</code>赋值0，记为<code>i</code><br><code>mov    %r13,%rbp</code>：<code>rbp</code>保存栈顶指针，数组起始地址<br><code>mov    0x0(%r13),%eax</code>：<code>rax</code>等于输入的第一个值<br><code>sub    $0x1,%eax</code>：自减1<br><code>cmp    $0x5,%eax</code>：判断<code>rax</code>是不是等于5<br><code>jbe    0x401128 &lt;phase_6+52&gt;</code>：无符号小于等于才不会爆炸，所以输入的数字自减前必须是<code>1-6</code>。<br><code>add    $0x1,%r12d</code>：<code>i</code>+1<br><code>cmp    $0x6,%r12d</code>：循环结束标志，相等跳转<br>不相等：<br><code>mov    %r12d,%ebx</code>：把<code>i</code>送到<code>rbx</code>， 记为<code>j = i + 1</code>。<br><code>movslq %ebx,%rax</code>：符号扩展送到<code>rax</code>，等于<code>j</code><br><code>mov    (%rsp,%rax,4),%eax</code>：输入的第<code>j</code>个数 送到 <code>rax</code><br><code>cmp    %eax,0x0(%rbp)</code>：对比<code>rax</code>和栈顶指针指向的数是否相同，相等就爆炸，数字不能重复。<br><code>add    $0x1,%ebx</code>：<code>j++</code><br><code>cmp    $0x5,%ebx</code>：循环跳出条件，后面的每一个数不能和第一个数相等。<br><code>add    $0x4,%r13</code>：循环重复，<code>i</code>指向下一个数<br>所有的数范围是<code>1-6</code>，且不重复时。</p>
</li>
<li><p><code>lea    0x18(%rsp),%rsi</code>：把最后一个数的地址送到<code>rsi</code><br><code>mov    %r14,%rax</code>：数组的第一个地址送到<code>rax</code><br><code>mov    $0x7,%ecx</code>：<code>rcx</code>等于7<br><code>mov    %ecx,%edx</code>：<code>rdx</code> = 7<br><code>sub    (%rax),%edx</code>：<code>rdx</code> = 7 - <code>*rax</code>，数组的第一个数<br><code>mov    %edx,(%rax)</code>：覆盖这个数<br><code>add    $0x4,%rax</code>：数组下一个<br><code>cmp    %rsi,%rax</code>：跳出条件<br>这段代码把所有数组的数，全部用7减，保存差</p>
</li>
<li><p><code>mov    $0x0,%esi</code>：<code>rsi</code> = 0，记为<code>i</code><br><code>mov    (%rsp,%rsi,1),%ecx</code>：<code>rcx</code>保存<code>rsp</code>的第<code>i</code>个字节地址的数字，记为<code>x</code>，注意数字在4的倍数字节处<br><code>cmp    $0x1,%ecx</code>：<code>x</code> == 1，1进行这个if判断<br>如果x==1：<br><code>mov    $0x6032d0,%edx</code>：<code>rdx</code>赋值，这里要注意应该就是密码所在地<br>如果x&gt;1：<br><code>mov    $0x1,%eax</code>：<code>rax</code> = 1<br><code>mov    $0x6032d0,%edx</code>:<br><code>mov    0x8(%rdx),%rdx</code>：<code>rdx</code> =  （<code>rdx</code> + 8地址保存的数据），就是<code>rdx</code>地址指向的节点的next。<br><code>add    $0x1,%eax</code>：<code>rax</code> + 1<br><code>cmp    %ecx,%eax</code>：对比<code>x</code>和<code>rax</code><br>不相等时<code>rax</code>一直加1，最终<code>rdx</code>指向第x个节点地址<br>if判断结束：<br><code>mov    %rdx,0x20(%rsp,%rsi,2)</code>：把<code>rdx</code>数送入<code>rsp</code> + 2 <em> i + 0x20，这个地址，相当于把数组整体换位置，后面是偏移量。<br><code>add    $0x4,%rsi</code>：<code>i</code> = <code>i</code> + 4<br><code>cmp    $0x18,%rsi</code>：结束条件<br>这段循环把数组里的数，全部向后偏移保存了一个数组，这个数组中0，1对应的是固定值<code>$0x6032d0</code>，大于1的值都是`$0x6032d0 + 8 </em> (x - 1)`。新数组的每一个数长度是8字节，数据是一串地址。</p>
<img src="/2023/03/14/09-43-26/WX20230317-124521@2x.png" class>
<p>可以看出在<code>rsp</code>后0x20地址处，是六个地址。</p>
</li>
<li><p><code>mov    0x20(%rsp),%rbx</code>：<code>rbx</code>等于新建数组的第一个数，记为<code>i</code><br><code>lea    0x28(%rsp),%rax</code>：<code>rax</code>是新数组第二个数的地址，记为<code>&amp;j</code><br><code>lea    0x50(%rsp),%rsi</code>：<code>rsi</code>数组最大地址<br><code>mov    %rbx,%rcx</code>：<code>rcx</code>存第一个数，记为<code>i</code><br>循环：<br><code>mov    (%rax),%rdx</code>：<code>rdx</code>：存下个数，记为<code>j</code><br><code>mov    %rdx,0x8(%rcx)</code>：把<code>j</code>放到<code>i</code>后8个字节处<br><code>add    $0x8,%rax</code>：<code>&amp;j = &amp;j + 8</code>，后移八个字节<br><code>cmp    %rsi,%rax</code>：对比有没有到了数组末尾<br>没有到最后一个地址：<br><code>mov    %rdx,%rcx</code>：<code>rcx</code> = <code>rdx</code>，<code>i</code>后移8个字节<br>继续上面的循环<br>如果到了最后一个地址：<br><code>movq   $0x0,0x8(%rdx)</code>：把0存到最后一个节点的后8字节处，尾结点的next=0。<br>上面把所有链表连接起来了。链接方式是按照输入的六个数字进行连接，按照1-6的顺序，数字越大，地址越大。每个数字代表一个地址，从1-6链接起来。</p>
</li>
<li><p>下面进行的是爆炸判断：<br><code>mov    $0x5,%ebp</code>：<code>rbp</code> = 5<br>循环：<br><code>mov    0x8(%rbx),%rax</code>：<code>rax</code>=第一个节点的next<br><code>mov    (%rax),%eax</code>：<code>rax</code>=地址保存的数<br><code>cmp    %eax,(%rbx)</code>：<em>next对比*</em>this<br>小于就爆炸：<br>大于等于：<br><code>mov    0x8(%rbx),%rbx</code>：this向后移动8位<br><code>sub    $0x1,%ebp</code>：<code>rbp</code>减1<br>不等于0时，循环继续<br>上面这段循环如果链表的数据表示的地址内存的数是递增的就爆炸了。所以就是要把链表调整为递减的链表，通过输入的1-6。</p>
</li>
<li><p>打印看一下这个<code>$0x6032d0</code>地址处保存的数据是什么？<br>链接之前的链表：</p>

<p>重新链接之后的链表：</p>

<p>根据爆炸判断，这道题就是让链表排序成递减。</p>
</li>
</ol>
<p>答案:</p>
<p><code>4 3 2 1 6 5</code></p>
<h2 id="secret-phase"><a href="#secret-phase" class="headerlink" title="secret_phase"></a>secret_phase</h2><p>重新开始写解题笔记的时候，按照小土刀的博客搭建了hexo博客，根据他的文章格式写出了这篇解题笔记，在他的文章里发现还有一道彩蛋题目，藏在了<code>phase_defused</code>函数里。这个不仔细读汇编代码很难发现，因为一般不会看拆弹成功的函数。现在自己把这个彩蛋也做一下。</p>
<p>思路：</p>
<p>做法：</p>
<ol>
<li><p>在函数<code>phase_defused</code>处打断点。</p>
<img src="/2023/03/14/09-43-26/WX20230317-143025@2x.png" class>
<p>发现这里有一个<code>secret_phase</code>函数，对这个函数打断点发现不会进入。<br>在汇编代码里找一下函数入口。</p>
</li>
<li><p><code>cmpl   $0x6,0x202181(%rip)        # 0x603760 &lt;num_input_strings&gt;</code>：对比当前的关卡数，不等于6时跳转，等于6时，可能会进入彩蛋。</p>
</li>
<li><p><code>mov    $0x402619,%esi</code>和<code>mov    $0x603870,%edi</code>，根据这两个参数，应该是调用的sscanf函数，看一下输入参数是什么？</p>
<img src="/2023/03/14/09-43-26/WX20230317-145028@2x.png" class>
<p>可以看出密码是两个数字一个字符串。</p>
</li>
<li><p><code>mov    $0x402622,%esi</code>：打印看一下这里存的是什么？</p>
<img src="/2023/03/14/09-43-26/WX20230317-150241@2x.png" class>
<p>可以确定第三个密码是<code>DrEvil</code></p>
</li>
<li><p><code>mov    $0x4024f8,%edi</code>：<br><code>mov    $0x402520,%edi</code>：打印看一下这里是什么？</p>
<img src="/2023/03/14/09-43-26/WX20230317-150454@2x.png" class>
</li>
<li><p>修改一下我们的<code>psol.txt</code>，最后一行加上<code>0 0 DrEvil</code>，重新调试进入<code>secret_phase</code>看一下汇编代码，代码没有进入，说明输入不在最后，继续看一下汇编代码。</p>
</li>
<li><p>根据sscanf的第一个参数地址在<code>0x603870</code>，我们就找一下之前输入的数据哪一个存放在这周围，在<code>bomb.s</code>文件里搜一下最近的是<code>read_line</code>函数里的<code>0x603780</code>。<br>看一下附件的代码：<br><code>lea    (%rax,%rax,4),%rsi</code>：<br><code>shl    $0x4,%rsi</code>：<br><code>add    $0x603780,%rsi</code>：<br><code>rax</code>放的是题目序号，乘5，左移4位，加上<code>0x603780</code>后等于我要的值，<code>rax</code>应该等于3。也就是说第三道题目解出来之后，第四道题目的输入缓冲区在<code>0x603870</code>。把输入文件改一下。</p>
</li>
<li><p>成功进入彩蛋函数，现在开始解这个彩蛋。<br>这里要把彩蛋的输入放在第六道题的下一行，彩蛋的入口在第四道题的输入后面。<br><code>disas</code>：先看一下汇编代码</p>
<img src="/2023/03/14/09-43-26/WX20230317-163059@2x.png" class>
<p><code>mov    $0xa,%edx</code>：<code>rdx</code>=10<br><code>mov    $0x0,%esi</code>：<code>rsi</code>=0<br><code>mov    %rax,%rdi</code>：<code>rdi</code> = read_line的返回值。就是我们输入的字符串地址。<br><code>callq  0x400bd0 &lt;strtol@plt&gt;</code>：调用函数，找开头的十进制数转换为long int，存在<code>rax</code>里，结尾的下一个地址放在<code>rsi</code>里。<br><code>mov    %rax,%rbx</code>：<code>rbx</code>=数字<br><code>lea    -0x1(%rax),%eax</code>：<code>rax</code>-1存到<code>rax</code>里<br><code>cmp    $0x3e8,%eax</code>：对比如果，0x3e8是1000<br>如果大于，爆炸：<br>如果小于等于：<br><code>mov    %ebx,%esi</code>：<code>rsi</code>=<code>rbx</code><br><code>mov    $0x6030f0,%edi</code>：<code>rdi</code>赋值，这里应该是密码所在地<br><code>callq  0x401204 &lt;fun7&gt;</code>：跳转到fun7函数</p>
</li>
<li><p>在fun7函数里打断点，看下汇编代码</p>
<img src="/2023/03/14/09-43-26/WX20230317-174533@2x.png" class>
<p><code>test   %rdi,%rdi</code>：test<code>rdi</code><br>如果等于0就跳转：<br><code>mov    $0xffffffff,%eax</code>：赋值32位全1，返回<br>不等于0：<br><code>mov    (%rdi),%edx</code>：把<code>rdi</code>地址保存的数据<code>$</code>给<code>rdx</code><br><code>cmp    %esi,%edx</code>：对比<code>rdx</code>和我们输入的密码<br><code>jle    0x401220 &lt;fun7+28&gt;</code>：<br>如果<code>rdx</code>&lt;=<code>rsi</code>：<br><code>mov    $0x0,%eax</code>：赋值0<br><code>cmp    %esi,%edx</code>：继续对比<br>如果<code>rdx</code>==<code>rsi</code>：返回0。根据彩蛋的汇编代码，可以看出返回值等于2才可以拆弹成功。<br>如果<code>rdx</code>&lt;<code>rsi</code>：<br><code>mov    0x10(%rdi),%rdi</code>：把<code>rdi</code>指向的地址+0x10，取数据送到<code>rdi</code><br>递归fun7。<br><code>lea    0x1(%rax,%rax,1),%eax</code>：<code>rax</code>左移一位再加1。<br>如果<code>rdx</code> &gt; <code>rsi</code>：<br><code>mov    0x8(%rdi),%rdi</code>：把<code>rdi</code>指向的地址+0x8，取数据送到<code>rdi</code><br>递归fun7。<br><code>add    %eax,%eax</code>：<code>rax</code>左移一位</p>
</li>
<li><p>根据上面的分析。递归的返回值一共四种<br>（1）：返回32位1<br>（2）：左移一位<br>（3）：0<br>（4）：左移一位加1<br>要想拆掉炸弹，需要返回的是2。<br>返回值顺序是：左移一位&lt;左移一位加1&lt;返回0<br>先从内层递归找条件：fun(x，y)<br>返回0：<code>rdi</code>指向的数 == y<br>左移一位加1：<code>rdi</code>指向的数 &lt; y，这里x+16字节<br>左移一位：<code>rdi</code>指向的数 &gt; y，这里x+8字节<br>递归顺序为：<br>fun(x，y) -&gt; fun(x + 8，y) -&gt; fun(x+16，y)</p>
</li>
<li><p>打印一下链表存的数据是哪些？</p>
<img src="/2023/03/14/09-43-26/WX20230317-174533@2x.png" class>
<img src="/2023/03/14/09-43-26/WX20230317-174534@2x.png" class>
<p>根据搜索路径可以看出，需要输入的密码应该是22。</p>
</li>
<li><p>测试一下</p>
<img src="/2023/03/14/09-43-26/WX20230317-174535@2x.png" class>
<p>彩蛋拆除成功！！</p>
</li>
</ol>
<p>答案：<code>22</code>+任意字符串</p>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
        <tag>shell</tag>
        <tag>gdb</tag>
        <tag>objdump</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux和SSH相关命令</title>
    <url>/2023/03/11/15-22-24/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="fcc81e67a5f114190e7654d76179bbb55ee6638dbe3e4cbf8bdd164daeb07bd0">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>ssh</tag>
        <tag>mac</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP 实验I Data Lab</title>
    <url>/2023/03/09/11-54-50/</url>
    <content><![CDATA[<p>更新历史</p>
<ul>
<li>23.03.10：初稿</li>
<li></li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2023/03/15/10-59-10/" title="csapp 笔记汇总">CSAPP - 笔记汇总</a></li>
<li><a href="/2023/03/09/11-54-50/" title="CSAPP 实验I Data Lab">I Data Lab - 位操作，数据表示</a></li>
<li><a href="/2023/03/14/09-43-26/" title="CSAPP 实验II Bomb Lab">II Bomb Lab - 汇编，栈帧与 gdb</a></li>
<li><a href="/2023/03/18/15-54-47/" title="CSAPP 实验III Attack Lab">III Attack Lab - 漏洞是如何被攻击的</a></li>
<li><a href="/2024/03/09/16-34-34/" title="CSAPP 实验IV CacheLab">IV Cache Lab - 实现一个缓存系统</a></li>
<li><a href="/2024/03/14/12-34-42/" title="CSAPP 实验V ShellLab">V Shell Lab - 实现一个Shell</a>
</li>
</ul>
<h1 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h1><p>DataLab实验，需要我们完善函数，条件是使用限定的操作符，并且操作符的数量也有限制，“=”不做数量限制。</p>
<h1 id="相关内容"><a href="#相关内容" class="headerlink" title="相关内容"></a>相关内容</h1><ol>
<li><p>自己定义的int整型的赋值范围是0到0xFF，8bits</p>
</li>
<li><p>机器是32位</p>
</li>
<li><p>只能使用一元操作符<code>!</code> <code>~</code>，二元操作符<code>&amp;</code> <code>|</code> <code>+</code> <code>&lt;&lt;</code> <code>&gt;&gt;</code> </p>
</li>
<li><p>不可以调用其他函数</p>
</li>
<li><p>测试命令</p>
<blockquote>
<p><code>make clean</code>：清除生成，每次修改<code>bits.c</code>文件都要清除重新生成<code>make</code>：生成文件<br><code>./btest -f funname -1 v</code>：测试单独的函数，带参数 <code>-T 100</code>：修改限制时间<br><code>./dlc bits.c</code> ：测试编码是否符合规则,符合规则无输出，参数<code>-e</code>，输出所有信息<br><code>./ishow 22</code>：输出int值的二进制，数值<br><code>./fshow 22</code>：输出float的二进制，数值，已经非规范值</p>
</blockquote>
</li>
</ol>
<h1 id="题目及解法"><a href="#题目及解法" class="headerlink" title="题目及解法"></a>题目及解法</h1><h2 id="bitXor"><a href="#bitXor" class="headerlink" title="bitXor"></a>bitXor</h2><ul>
<li><p>题目要求：返回两个int的异或值</p>
</li>
<li><p>允许操作：<code>~ &amp;</code></p>
</li>
<li><p>操作数量：14</p>
</li>
<li><p>分值：1</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">bitXor</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> ~(~(~x &amp; y) &amp; ~(x &amp; ~y));  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="tmin"><a href="#tmin" class="headerlink" title="tmin"></a>tmin</h2><ul>
<li><p>题目要求：求补码的最小值</p>
</li>
<li><p>允许操作：<code>! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</code></p>
</li>
<li><p>操作数量：4</p>
</li>
<li><p>分值：1</p>
</li>
</ul>
<p>32位机器码的补码最小值是最高位为1，其他位0，所以用1左移31位形成。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">tmin</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">  <span class="type">int</span> x = <span class="number">1</span>;</span><br><span class="line">  x = x &lt;&lt; <span class="number">31</span>;</span><br><span class="line">  <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="isTmax"><a href="#isTmax" class="headerlink" title="isTmax"></a>isTmax</h2><ul>
<li><p>题目要求：如果x是补码最大值返回1，否则返回0</p>
</li>
<li><p>允许操作：<code>! ~ &amp; ^ | +</code></p>
</li>
<li><p>操作数量：10</p>
</li>
<li><p>分值：1</p>
</li>
</ul>
<p>补码的最大值是最高位0，其他位1。加1后是最小值，Tmin和Tmax每位都相反，Tmin^Tmax是全1，可以使用这个值进行返回。需要注意补码的特殊值Tmin，Tmax，还需要注意-1，32位全1 <code>1111</code>，因为-1 ^ 0 = 1，需要排除这种情况</p>
<ul>
<li><p>flag_2 表示x是否等于 -1 </p>
<ol>
<li><p>如果x==-1， y=0，y^0 == 0   flag_2为0</p>
</li>
<li><p>如果x!=-1，y!=0， y^0 != 0，flag_2不为0，flag的值是0或1，所以需要取非两次，把非零值转换为1</p>
</li>
</ol>
</li>
<li><p>flag_1 表示(x+1)^x，有两种情况结果为全1，x是<code>Tmax</code>和<code>-1</code>，我们只需要全为1的情况，所以先取反~，换成全0，再取非!， 转为1。</p>
</li>
<li><p>返回值为两个标志相与，用flag_2排除掉flag_1中-1的这种情况，剩下的就是Tmax</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">isTmax</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">  <span class="type">int</span> y = x + <span class="number">1</span>;  <span class="comment">//Tmin</span></span><br><span class="line">  <span class="type">int</span> flag_1 = !(~(x ^ y)); </span><br><span class="line">  <span class="type">int</span> flag_2= !!(y ^ <span class="number">0</span>);</span><br><span class="line">  <span class="keyword">return</span> flag_1 &amp; flag_2; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="allOddBits"><a href="#allOddBits" class="headerlink" title="allOddBits"></a>allOddBits</h2><ul>
<li><p>题目要求：如果每一个偶数位都是1返回1，否则返回0</p>
</li>
<li><p>允许操作：<code>! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</code></p>
</li>
<li><p>操作数量：12</p>
</li>
<li><p>分值：2</p>
</li>
</ul>
<p>先从32比特中只保留所有偶数位的值，其他位赋值0，再和<code>0xAAAAAAAA</code>作对比相等返回1，因为不能用等号，所以用异或^代替判断，如果相等全为0，取非!值为1，否则为0。</p>
<ul>
<li><p>移位和加法让变量y等于0xAAAAAAAA。</p>
</li>
<li><p>x与y相与，只留下x的偶数位的值</p>
</li>
<li><p>用异或判断x与y是不是相等</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">allOddBits</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">  <span class="type">int</span> y = <span class="number">0xAA</span>; </span><br><span class="line">  <span class="comment">//+优先级大于 &lt;&lt; </span></span><br><span class="line">  y  = y + (y &lt;&lt; <span class="number">8</span>);</span><br><span class="line">  y = y + (y &lt;&lt; <span class="number">16</span>);</span><br><span class="line">  <span class="comment">//只留下x的偶数位值</span></span><br><span class="line">  x = x &amp; y;</span><br><span class="line">  <span class="keyword">return</span> !(y ^ x) ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="negate"><a href="#negate" class="headerlink" title="negate"></a>negate</h2><ul>
<li><p>题目要求：返回-x</p>
</li>
<li><p>允许操作：<code>! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</code></p>
</li>
<li><p>操作数量：5</p>
</li>
<li><p>分值：2</p>
</li>
</ul>
<p>这个比较简单，根据二进制补码x与-x就是按位取反再加1。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">negate</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> ~x + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="isAsciiDigit"><a href="#isAsciiDigit" class="headerlink" title="isAsciiDigit"></a>isAsciiDigit</h2><ul>
<li><p>题目要求：判断x是不是ASCII码的0-9， 相当于判断x的值是不是0x30到0x39。</p>
</li>
<li><p>允许操作：<code>! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</code></p>
</li>
<li><p>操作数量：15</p>
</li>
<li><p>分值：3</p>
</li>
</ul>
<p>先观察0-9的补码形式</p>
<p>0x30 <code>110000</code>  </p>
<p>0x37 <code>110111</code>  </p>
<p>0x38 <code>111000</code>  </p>
<p>0x39 <code>111001</code></p>
<p>首先需要满足除后6位外，高位全为0，满足第一个条件后0-7满足前三位为<code>110</code>，剩下的直接对比是不是等于8或9。<code>不能使用等号所以用异或判断相等</code></p>
<ul>
<li><p>flag_1 右移6位后，如果高位全为0，就是0，取非!为1，不然就是0</p>
</li>
<li><p>flag_2 右移3位后，满足第一个条件的除了低3位其他全为0，与<code>110</code>比较相等就是0到7中的值</p>
</li>
<li><p>flag_3， flag_4，直接判断是不是等于8，9。</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">isAsciiDigit</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">  <span class="type">int</span> y = x &amp; <span class="number">0x3f</span>; <span class="comment">//截取x的后6位</span></span><br><span class="line">  <span class="type">int</span> flag_1 = !(x &gt;&gt; <span class="number">6</span>); <span class="comment">//右移六位后,取非为1   x的高位全是0  符合条件</span></span><br><span class="line">  <span class="type">int</span> flag_2 = !((x &gt;&gt; <span class="number">3</span>) ^ <span class="number">0x6</span>);  <span class="comment">//满足第一个条件后,右移3位   异或110   如果为0  就是属于&#x27;0&#x27; 到&#x27;7&#x27;</span></span><br><span class="line">  <span class="type">int</span> flag_3 = !(y ^ <span class="number">0x38</span>);  <span class="comment">//满足第一个条件后   是&#x27;8&#x27;</span></span><br><span class="line">  <span class="type">int</span> flag_4 = !(y ^ <span class="number">0x39</span>);   <span class="comment">//满足第一个条件后   是&#x27;9&#x27;</span></span><br><span class="line">  <span class="keyword">return</span> flag_1 &amp; (flag_2 | flag_3 | flag_4);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="conditional"><a href="#conditional" class="headerlink" title="conditional"></a>conditional</h2><ul>
<li><p>题目要求：x不为0，返回y， x等于0，返回z。</p>
</li>
<li><p>允许操作：<code>! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</code></p>
</li>
<li><p>操作数量：16</p>
</li>
<li><p>分值：3</p>
</li>
</ul>
<p>先找返回值，x等于0，return (x &amp; y) ^ (~x &amp; z)，x不等于0的时候，结果很多，这时候把x处理一下变为全1，返回值就相等了。</p>
<ul>
<li><p>对于x不等于0的情况，换全1比较麻烦，对x取非!，把x简化成0或1。</p>
</li>
<li><p>把x扩展到全1和全0，用移位加也能做出来而且满足操作数量的限制，继续做后面的题目<code>logicalNeg</code>想到了一个快速扩展的方法，0或1，取反加1，就是全0或者全1。</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">conditional</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> z)</span> &#123;</span><br><span class="line">  x = !x;   <span class="comment">//取非 把x变为 0 1两种    把他们扩展到32位</span></span><br><span class="line">  x = ~x + <span class="number">1</span>;  <span class="comment">//01快速扩展到全0全1 </span></span><br><span class="line"> <span class="comment">// x = (x &lt;&lt; 1) + x;</span></span><br><span class="line"> <span class="comment">// x = (x &lt;&lt; 2) + x;</span></span><br><span class="line"> <span class="comment">// x = (x &lt;&lt; 4) + x;</span></span><br><span class="line"> <span class="comment">// x = (x &lt;&lt; 8) + x;</span></span><br><span class="line"> <span class="comment">// x = (x &lt;&lt; 16) + x;</span></span><br><span class="line">  <span class="keyword">return</span> ((~x) &amp; y) ^ (x &amp; z);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="isLessOrEqual"><a href="#isLessOrEqual" class="headerlink" title="isLessOrEqual"></a>isLessOrEqual</h2><ul>
<li><p>题目要求：x&lt;=y，返回1，否则返回0。</p>
</li>
<li><p>允许操作：<code>! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</code></p>
</li>
<li><p>操作数量：24</p>
</li>
<li><p>分值：3</p>
</li>
</ul>
<p>找出所以返回1的情况，一共两种情况，相等，小于。相等的时候比较好做，用异或判断相等，小于的情况比较复杂。</p>
<ul>
<li><p>等于：flag_3表示相等的情况  异或判断</p>
</li>
<li><p>小于：有两种情况 <code>需要注意减法同号一点不会溢出</code></p>
<ol>
<li><p>flag_5：x负数，y正数。</p>
</li>
<li><p>flag_4：x，y同号，x &lt; y，转换为 y-x&gt;0。</p>
</li>
</ol>
</li>
<li><p>用flag_5 表示x负数，y正数。   <code>用右移31位看符号看判断正负</code></p>
</li>
<li><p>用flag_6 表示x正数，y负数。</p>
</li>
<li><p>用sum=y - x，flag_4 表示符号。</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">isLessOrEqual</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span> &#123;</span><br><span class="line">  <span class="type">int</span> nagete_x = ~x + <span class="number">1</span>;</span><br><span class="line">  <span class="type">int</span> sum = y + nagete_x; </span><br><span class="line">  <span class="type">int</span> flag_3 = !(x ^ y);</span><br><span class="line">  <span class="type">int</span> flag_5 = (x &gt;&gt; <span class="number">31</span>) &amp; !(y &gt;&gt; <span class="number">31</span>);   </span><br><span class="line">  <span class="type">int</span> flag_6 = !(x &gt;&gt; <span class="number">31</span>) &amp; (y &gt;&gt; <span class="number">31</span>);   </span><br><span class="line">  <span class="type">int</span> flag_4 = !(sum &gt;&gt; <span class="number">31</span>) &amp; !flag_5 &amp; !flag_6;   <span class="comment">//xy同号  减法不会溢出  减法有效</span></span><br><span class="line">  <span class="keyword">return</span> flag_3 | flag_4 | flag_5;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="logicalNeg"><a href="#logicalNeg" class="headerlink" title="logicalNeg"></a>logicalNeg</h2><ul>
<li><p>题目要求：实现取非!的功能</p>
</li>
<li><p>允许操作：<code>~ &amp; ^ | + &lt;&lt; &gt;&gt;</code></p>
</li>
<li><p>操作数量：12</p>
</li>
<li><p>分值：4</p>
</li>
</ul>
<p>思路：取非结果只有0和1，把全0或全1加1，就是1或0，所以就需要把0转换为全1，非0转换为全0，然后加1，就是返回值。默认返回0，找出返回1的情况，也就是全1。<code>只有0和Tmin的负值和自己符号相同，其他的符号都相反</code></p>
<ul>
<li><p>取负值，对符号位扩展，然后自己的符号位和负值符号位相加，</p>
<ol>
<li><p>0和Tmin，结果为0。</p>
</li>
<li><p>其他结果为全1。</p>
</li>
</ol>
</li>
<li><p>flag_1：~sum，0和Tmin结果为全1， 其他为全0。</p>
</li>
<li><p>flag_2：~Tmin 01111 ~0全1</p>
</li>
<li><p>flag_3：flag_1 中排除到flag_2的Tmin情况</p>
<ol>
<li><p>Tmin <code>0111</code>  0为<code>1111</code>  其余为<code>0000</code></p>
</li>
<li><p>取反，只保留符号位， Tmin <code>1111</code> 0为<code>0000</code> 其余为<code>1111</code></p>
</li>
<li><p>加1后，0为<code>0001</code>， 其余都统一为<code>0000</code></p>
</li>
</ol>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">logicalNeg</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">  <span class="type">int</span> neg_x = ~x;</span><br><span class="line">  <span class="type">int</span> y = neg_x + <span class="number">1</span>;</span><br><span class="line">  <span class="type">int</span> hight_x = x &gt;&gt; <span class="number">31</span>;</span><br><span class="line">  <span class="type">int</span> hight_y = y &gt;&gt; <span class="number">31</span>;</span><br><span class="line">  <span class="type">int</span> flag_1 = ~(hight_x + hight_y);</span><br><span class="line">  <span class="type">int</span> flag_2 = neg_x;</span><br><span class="line">  <span class="type">int</span> flag_3 = (~(flag_1 &amp; flag_2) &gt;&gt; <span class="number">31</span>) + <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">return</span> flag_3;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="howManyBits"><a href="#howManyBits" class="headerlink" title="howManyBits"></a>howManyBits</h2><ul>
<li><p>题目要求：返回表示x的补码最短长度 <code>注意-1，用一位</code></p>
</li>
<li><p>允许操作：<code>! ~ &amp; ^ | + &lt;&lt; &gt;&gt;</code></p>
</li>
<li><p>操作数量：90</p>
</li>
<li><p>分值：4</p>
</li>
</ul>
<p>思路：用上面做的函数 conditional 做两个if功能， 一次返回分半查找的剩余一半函数 ，一次计算分半如果选择前一半，bit数要加上一半的长度，选择后面长度加0不做处理。</p>
<blockquote>
<p>用0xffff0000 留下x的前16位,如果不等于0 说明位数大于16 等于0 说明位数小于16 在后16位内再进行分半查找</p>
</blockquote>
<p>正负数的处理<code>重点</code><br>都处理成正数，方便右移添0。加上符号位就是有效位数。</p>
<ul>
<li><p>正数：找出最高位的1。</p>
</li>
<li><p>负数：找出最高位的0，<code>Tmin也符合</code></p>
<blockquote>
<p>负数高位连续的1都是无用数据，只需要找到高位连续的1后的第一个0在哪里，做法是取反，如<code>1110010</code>，有用的数据是<code>10010</code>，全部取反后<code>0001101</code>， 有效位数是4位，加上符号位。</p>
</blockquote>
</li>
<li><p>flag_nage： x是负数 值为1</p>
</li>
<li><p>重点代码，</p>
<ol>
<li><p>if函数功能</p>
<blockquote>
<p><code>x = (extend_f_n &amp; (~x)) ^ ((~extend_f_n) &amp; x);</code> ：如果标记为全1，选择的~x， 如果为全0，选择的x。异或的两边有一个一直为全0，结果一定是另一个。</p>
</blockquote>
</li>
<li><p>一个二分法</p>
<blockquote>
<p><code>x_32 = x &amp; bit_32;</code> ：取x的前16位<br><code>x_32 = !x_32;</code>：取非，如果前16位全0，值就是1，否则是0<br><code>x_32 = ~x_32 + 1;</code> 前16位全0，扩充到全1，否则扩充到全0<br><code>flag_32 = ~x_32;</code> 这里是为了减少一个op值，重复使用代码<br><code>x_16 = (flag_32 &amp; (x &gt;&gt; 16)) ^ ((x_32 &amp; (x &amp; (~bit_32))));</code><br>x_32如果全0，说明前16位，有1，那就把x右移16位，覆盖掉后面，继续对这16位进行二分法。<code>这里不用考虑右移补1，因为都是正数最高位都是0</code>。并且总数加16。<br>如果全1，说明前16位都是0，要从后16位开始二分法，不用移位，x_16要等于x的后16位。总数加0。<br><code>sum = sum + (flag_32 &amp; 16);</code>：</p>
</blockquote>
</li>
</ol>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">howManyBits</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">  <span class="type">int</span> x_32, x_16, x_8, x_4, x_2;  <span class="comment">//分别表示x的不同比特</span></span><br><span class="line">  <span class="type">int</span> bit_8 = <span class="number">0xff</span> &lt;&lt; <span class="number">8</span>;</span><br><span class="line">  <span class="type">int</span> bit_32 = (bit_8 + <span class="number">0xff</span>) &lt;&lt; <span class="number">16</span>;   </span><br><span class="line">  <span class="type">int</span> bit_16 = bit_8;</span><br><span class="line">  <span class="type">int</span> flag_32,flag_16,flag_8,flag_4,flag_2;  <span class="comment">//表示 ~x_32  重使用一次 减少 ops数量</span></span><br><span class="line">  <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> flag_nage = !!(x &gt;&gt; <span class="number">31</span>);     <span class="comment">//  x是负数   值为1</span></span><br><span class="line">  <span class="comment">//通过负0和Tmin 都是自己   ^之后为0  排除x是0的情况    当x是Tmin 是值为1</span></span><br><span class="line">  <span class="comment">//int flag_Tmin = !(x ^ (~x + 1)) &amp; x; </span></span><br><span class="line">  <span class="type">int</span> flag_Tmin = !(x &lt;&lt; <span class="number">1</span>) &amp; x;   <span class="comment">//Tmin 左移一位 去掉符号位后是全0   比上面的方法减少op</span></span><br><span class="line">  <span class="type">int</span> extend_f_T = ~flag_Tmin + <span class="number">1</span>;      <span class="comment">//扩展flag_Tmin</span></span><br><span class="line">  <span class="type">int</span> extend_f_n = ~flag_nage + <span class="number">1</span>;      <span class="comment">//扩展flag_nage</span></span><br><span class="line">  sum = sum + (flag_Tmin &amp; <span class="number">32</span>);</span><br><span class="line">  x = (extend_f_T &amp; <span class="number">0</span>) ^ ((~extend_f_T) &amp; x);  <span class="comment">// flag_Tmin 是1时  x赋值0   是0是  x不变</span></span><br><span class="line"></span><br><span class="line">  x = (extend_f_n &amp; (~x)) ^ ((~extend_f_n) &amp; x);  <span class="comment">// flag_nage 是1时  x赋值~x   是0是  x不变</span></span><br><span class="line"></span><br><span class="line">  bit_8 = <span class="number">0xf0</span>; </span><br><span class="line">  x_32 = x &amp; bit_32;</span><br><span class="line">  x_32 = !x_32;</span><br><span class="line">  x_32 = ~x_32 + <span class="number">1</span>;</span><br><span class="line">  flag_32 = ~x_32;</span><br><span class="line">  x_16 = (flag_32 &amp; (x &gt;&gt; <span class="number">16</span>)) ^ ((x_32 &amp; (x &amp; (~bit_32))));</span><br><span class="line">  sum = sum + (flag_32 &amp; <span class="number">16</span>);</span><br><span class="line"></span><br><span class="line">  x = x_16;   <span class="comment">//改变x</span></span><br><span class="line">  x_16 = x &amp; bit_16;</span><br><span class="line">  x_16 = !x_16;</span><br><span class="line">  x_16 = ~x_16 + <span class="number">1</span>;</span><br><span class="line">  flag_16 = ~x_16;</span><br><span class="line">  x_8 = (flag_16 &amp; (x &gt;&gt; <span class="number">8</span>)) ^ ((x_16 &amp; (x &amp; <span class="number">0xff</span>)));</span><br><span class="line">  sum = sum + (flag_16 &amp; <span class="number">8</span>);</span><br><span class="line"></span><br><span class="line">  x = x_8;   <span class="comment">//改变x</span></span><br><span class="line">  x_8 = x &amp; bit_8;</span><br><span class="line">  x_8 = !x_8;</span><br><span class="line">  x_8 = ~x_8 + <span class="number">1</span>;</span><br><span class="line">  flag_8 = ~x_8;</span><br><span class="line">  x_4 = (flag_8 &amp; (x &gt;&gt; <span class="number">4</span>)) ^ ((x_8 &amp; (x &amp; (<span class="number">0xf</span>))));   <span class="comment">//~bit_8  改为0xf  省一个op</span></span><br><span class="line">  sum = sum + (flag_8 &amp; <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">  x = x_4;   <span class="comment">//改变x</span></span><br><span class="line">  x_4 = x &amp; <span class="number">12</span>;</span><br><span class="line">  x_4 = !x_4;</span><br><span class="line">  x_4 = ~x_4 + <span class="number">1</span>;</span><br><span class="line">  flag_4 = ~x_4;</span><br><span class="line">  x_2 = (flag_4 &amp; (x &gt;&gt; <span class="number">2</span>)) ^ ((x_4 &amp; (x &amp; <span class="number">3</span>)));</span><br><span class="line">  sum = sum + (flag_4 &amp; <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">  x = x_2;</span><br><span class="line">  x_2 = x_2 &gt;&gt; <span class="number">1</span>;</span><br><span class="line">  x_2 = !x_2;</span><br><span class="line">  x_2 = ~x_2 + <span class="number">1</span>;</span><br><span class="line">  flag_2 = ~x_2;</span><br><span class="line">  x = (flag_2 &amp; (x &gt;&gt; <span class="number">1</span>)) ^ ((x_2 &amp; (x &amp; <span class="number">1</span>)));</span><br><span class="line">  sum = sum + (flag_2 &amp; <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  sum = sum + (x &amp; <span class="number">1</span>);  <span class="comment">//如果x是0  或者处理之后的Tmin  剩余位全是0    不加最后一个比特位   其他情况加上1</span></span><br><span class="line"></span><br><span class="line">  sum = sum + (!flag_Tmin &amp; <span class="number">1</span>);   <span class="comment">//非Tmin  加上符号位    0符号位就是0  一个bit表示</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="floatScale2"><a href="#floatScale2" class="headerlink" title="floatScale2"></a>floatScale2</h2><ul>
<li><p>题目要求：无符号数保存的float值，返回乘2</p>
</li>
<li><p>允许操作：<code>所有的int，无符号操作符，if，while</code></p>
</li>
<li><p>操作数量：30</p>
</li>
<li><p>分值：4</p>
</li>
</ul>
<p>思路：</p>
<ol>
<li><p>0返回0</p>
</li>
<li><p>exp全1： 为无穷 直接返回</p>
</li>
<li><p>exp全0： <code>非规格化值：小数部分没有隐藏位</code></p>
<blockquote>
<p>f不是全0 直接加左移一位 最后补上符号。<code>非规格化到规格化的过渡</code><br>f是全0 不做处理 返回原值 ，就是第一种情况</p>
</blockquote>
</li>
<li><p>exp 不全1全0 直接加1 相当于乘2</p>
</li>
</ol>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="title function_">floatScale2</span><span class="params">(<span class="type">unsigned</span> uf)</span> &#123;</span><br><span class="line">  <span class="type">unsigned</span> bit_exp;</span><br><span class="line">  <span class="type">unsigned</span> bit_f;  <span class="comment">//小数部分</span></span><br><span class="line">  <span class="type">unsigned</span> flag_s;  <span class="comment">//符号位</span></span><br><span class="line">  <span class="keyword">if</span>(uf == <span class="number">0</span>)&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  bit_f = uf &amp; (<span class="number">0x007fffff</span>);</span><br><span class="line">  bit_exp = uf &amp; (<span class="number">0x7f800000</span>);   <span class="comment">//取出指数部分</span></span><br><span class="line">  flag_s = uf &gt;&gt; <span class="number">31</span>;</span><br><span class="line">  <span class="keyword">if</span>(bit_exp == <span class="number">0x7f800000</span>)&#123;</span><br><span class="line">    <span class="keyword">return</span> uf;</span><br><span class="line">  &#125;</span><br><span class="line">  bit_exp = bit_exp &gt;&gt; <span class="number">23</span>;       <span class="comment">//exp的值</span></span><br><span class="line">  <span class="keyword">if</span>(bit_exp != <span class="number">0</span>)&#123;</span><br><span class="line">    bit_exp = bit_exp + <span class="number">1</span>;</span><br><span class="line">    bit_exp = bit_exp &lt;&lt; <span class="number">23</span>;</span><br><span class="line">    uf = uf &amp; ~(<span class="number">0x7f800000</span>);</span><br><span class="line">    uf = uf ^ bit_exp;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(bit_exp == <span class="number">0</span> &amp;&amp; bit_f != <span class="number">0</span>)&#123;</span><br><span class="line">      uf = uf &lt;&lt; <span class="number">1</span>;</span><br><span class="line">      <span class="keyword">if</span>(flag_s == <span class="number">1</span>)&#123;</span><br><span class="line">        uf = uf | (flag_s &lt;&lt; <span class="number">31</span>);  <span class="comment">//补上符号位</span></span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> uf;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="floatFloat2Int"><a href="#floatFloat2Int" class="headerlink" title="floatFloat2Int"></a>floatFloat2Int</h2><ul>
<li><p>题目要求：float转int值</p>
</li>
<li><p>允许操作：<code>所有的int，无符号操作符，if，while</code></p>
</li>
<li><p>操作数量：30</p>
</li>
<li><p>分值：4</p>
</li>
</ul>
<p>思路：<code>确定指数的范围</code></p>
<ol>
<li><p>exp &lt; 127 指数小于0时，是小数，返回0。</p>
</li>
<li><p>exp &gt; 158 指数大于31 越界</p>
</li>
<li><p>exp 全0 返回0</p>
</li>
<li><p>exp 全1 返回0x80000000u</p>
</li>
<li><p>exp 其他情况下 对f部分补上隐藏位1 再右移exp-127位，最后补上符号位</p>
</li>
</ol>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">floatFloat2Int</span><span class="params">(<span class="type">unsigned</span> uf)</span> &#123;</span><br><span class="line">  <span class="type">unsigned</span> bit_exp;</span><br><span class="line">  <span class="type">unsigned</span> bit_f;  <span class="comment">//小数部分</span></span><br><span class="line">  <span class="type">unsigned</span> flag_s;  <span class="comment">//符号位</span></span><br><span class="line">  <span class="type">unsigned</span> E;      <span class="comment">//指数</span></span><br><span class="line">  <span class="type">int</span> F2Int;</span><br><span class="line">  bit_f = uf &amp; (<span class="number">0x007fffff</span>);   <span class="comment">//取出小数部分</span></span><br><span class="line">  bit_f = bit_f | (<span class="number">0x00800000</span>);  <span class="comment">//补上前面隐藏1</span></span><br><span class="line">  bit_exp = uf &amp; (<span class="number">0x7f800000</span>);   <span class="comment">//取出指数部分</span></span><br><span class="line">  bit_exp = bit_exp &gt;&gt; <span class="number">23</span>;</span><br><span class="line">  flag_s = uf &gt;&gt; <span class="number">31</span>;</span><br><span class="line">  <span class="comment">//printf(&quot;s %u %x  exp  %u %x   f  %u  %x\n&quot;, flag_s, flag_s, bit_exp, bit_exp, bit_f, bit_f);</span></span><br><span class="line">  <span class="keyword">if</span>(bit_exp == <span class="number">0xff</span> || bit_exp &gt; <span class="number">158</span>)&#123;  <span class="comment">//无穷   或者NaN   指数越界  </span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0x80000000</span>u;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(bit_exp &lt; <span class="number">127</span> || bit_exp  == <span class="number">0</span>)&#123;  <span class="comment">//指数是负数   非规格数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;   </span><br><span class="line">  E = bit_exp - <span class="number">127</span>;</span><br><span class="line">  bit_f = bit_f &gt;&gt; (<span class="number">23</span> - E);  <span class="comment">//右移</span></span><br><span class="line">  <span class="comment">//printf(&quot;E %u %x  bit_f   %u %x \n&quot;, E, E, bit_f, bit_f);</span></span><br><span class="line">  F2Int = bit_f;   </span><br><span class="line">  <span class="keyword">if</span>(flag_s == <span class="number">1</span>)&#123;</span><br><span class="line">    F2Int = -F2Int;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> F2Int;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="floatPower2"><a href="#floatPower2" class="headerlink" title="floatPower2"></a>floatPower2</h2><ul>
<li><p>题目要求：返回float值 2的x次方，x为32位int</p>
</li>
<li><p>允许操作：<code>所有的int，无符号操作符，if，while</code></p>
</li>
<li><p>操作数量：30</p>
</li>
<li><p>分值：4</p>
</li>
</ul>
<p>思路：<code>确定指数的范围</code> x为-127到128之间。</p>
<p>时间超时 10s内跑不完所有测试数据 可能是电脑问题</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="title function_">floatPower2</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">  <span class="type">unsigned</span> bit_exp;</span><br><span class="line">  <span class="type">unsigned</span> bit_float;  <span class="comment">//float值的bit位</span></span><br><span class="line">  <span class="type">unsigned</span> INF = <span class="number">0x7f800000</span>;</span><br><span class="line">  <span class="keyword">if</span>(x &lt; <span class="number">-127</span>)&#123;  <span class="comment">//值太小</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;   </span><br><span class="line">  <span class="keyword">if</span>(x &gt; <span class="number">128</span>)&#123;  <span class="comment">//太大  返回+INF</span></span><br><span class="line">    <span class="keyword">return</span> INF;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//printf(&quot;x    %d\n&quot;, x);</span></span><br><span class="line">  bit_exp = x + <span class="number">127</span>;</span><br><span class="line">  bit_exp = bit_exp &lt;&lt; <span class="number">23</span>;</span><br><span class="line">  <span class="comment">//printf(&quot;bit_float  %u  %x\n&quot;, bit_float, bit_float);</span></span><br><span class="line">  <span class="keyword">return</span> bit_exp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>csapp</category>
      </categories>
      <tags>
        <tag>csapp</tag>
      </tags>
  </entry>
  <entry>
    <title>git笔记</title>
    <url>/2023/03/08/14-59-55/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">
  <script id="hbeData" type="hbeData" data-hmacdigest="26745236fcb5652ab112c58424b00376b288303a3c5d5eeb4338860c51b08a23">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>算法模板</title>
    <url>/2023/03/08/14-45-11/</url>
    <content><![CDATA[<h1 id="特殊函数"><a href="#特殊函数" class="headerlink" title="特殊函数"></a>特殊函数</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//万能头文件</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span> </span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;functional&gt;</span></span></span><br><span class="line"><span class="built_in">sort</span>(a,a+n,<span class="built_in">less</span>&lt;<span class="type">int</span>&gt;());<span class="comment">//内置类型的由小到大排序</span></span><br><span class="line"><span class="built_in">greater</span>&lt;<span class="type">int</span>&gt;();</span><br><span class="line"><span class="comment">//作用相当于 bool cmd();</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bitset&gt;</span></span></span><br><span class="line">  string s;</span><br><span class="line">  s = <span class="built_in">bitset</span>&lt;<span class="number">8</span>&gt;(a[i]).<span class="built_in">to_string</span>();     <span class="comment">//  转换字符串</span></span><br><span class="line">  <span class="built_in">to_ulong</span>()    <span class="comment">// 转换为unsigned long类型</span></span><br><span class="line">  <span class="built_in">to_ullong</span>();　<span class="comment">// 转换成unsigned long long类型</span></span><br><span class="line">  <span class="built_in">set</span>(x, v);    <span class="comment">// 没有参数时 将全部位设为1</span></span><br><span class="line">                <span class="comment">// 有一个x， 将x下标处设为1</span></span><br><span class="line">                <span class="comment">// 两个参数，将x处设为v值</span></span><br><span class="line">  <span class="built_in">reset</span>(x);    <span class="comment">// 置0   无参全部置，有参置下标</span></span><br><span class="line">  <span class="built_in">flip</span>(x);     <span class="comment">// 取反，无参全部取，有参取下标处</span></span><br><span class="line">  <span class="built_in">text</span>(x);     <span class="comment">// 检测下标处是不是1 返回true 0返回false</span></span><br><span class="line">  <span class="built_in">any</span>();       <span class="comment">// 检查是否有1</span></span><br><span class="line">  <span class="built_in">none</span>();      <span class="comment">// 检查是否没有1</span></span><br><span class="line">  <span class="built_in">all</span>();       <span class="comment">// 检查是否全为1</span></span><br><span class="line">  <span class="built_in">count</span>();     <span class="comment">// 求1的个数</span></span><br><span class="line">  <span class="built_in">size</span>();      <span class="comment">// 求位数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;typeinfo&gt;</span></span></span><br><span class="line"><span class="built_in">typeid</span>(x).<span class="built_in">name</span>();   <span class="comment">// 获取x的类型</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//输出重定向</span></span><br><span class="line"><span class="built_in">freopen</span>(<span class="string">&quot;input.txt&quot;</span>, <span class="string">&quot;r&quot;</span>, stdin);</span><br><span class="line"><span class="built_in">freopen</span>(<span class="string">&quot;output.txt&quot;</span>, <span class="string">&quot;w&quot;</span>, stdout);</span><br><span class="line"><span class="comment">//控制台输入数据，如何输入文件结束标志</span></span><br><span class="line"><span class="comment">//**在Windows下，输入完毕后先按Enter键，再按Ctrl＋Z键，最后再按Enter键</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//加快输入速度的两种方法</span></span><br><span class="line">ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);   cin &gt;&gt; x;</span><br><span class="line"><span class="built_in">scanf</span>()；</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//类型转换</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sstream&gt;</span></span></span><br><span class="line">ss.<span class="built_in">clear</span>();  <span class="comment">// 清空</span></span><br><span class="line">ss &lt;&lt; s;</span><br><span class="line">ss &gt;&gt; a;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cctype&gt;</span></span></span><br><span class="line"><span class="built_in">isdigit</span>(s[i]);  <span class="comment">// 若参数c为阿拉伯数字0~9 字符，则返回非0值，否则返回0。</span></span><br><span class="line"><span class="type">int</span> n = <span class="built_in">stoi</span>(s);   <span class="comment">//  把s转化为int型 </span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line">cout &lt;&lt; <span class="built_in">setw</span>(<span class="number">2</span>);   <span class="comment">// 设置位数</span></span><br><span class="line"><span class="built_in">setprecision</span>(n)     <span class="comment">//设置显示小数精度为n位</span></span><br><span class="line">cout &lt;&lt; <span class="built_in">stefill</span>(c);  <span class="comment">// 设置填充字符</span></span><br><span class="line"><span class="built_in">setiosflags</span>(ios::scientific)   <span class="comment">//指数表示</span></span><br><span class="line"><span class="built_in">setiosflags</span>(ios::left)     <span class="comment">//左对齐</span></span><br><span class="line"><span class="built_in">setiosflags</span>(ios::right)    <span class="comment">//右对齐</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">include &lt;string&gt;</span><br><span class="line"><span class="built_in">substr</span>(<span class="type">int</span> index, <span class="type">int</span> mum); <span class="comment">// 返回一个子字符串 </span></span><br><span class="line"><span class="comment">//std::string::npos是一个常数，它等于size_type类型可以表示的最大值</span></span><br><span class="line"><span class="comment">//用在 s.find()  返回值</span></span><br></pre></td></tr></table></figure>
<h1 id="算法模板"><a href="#算法模板" class="headerlink" title="算法模板"></a>算法模板</h1><h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>算法比赛</tag>
        <tag>c++</tag>
        <tag>算法模板</tag>
      </tags>
  </entry>
  <entry>
    <title>macOS使用ssh登陆WSL2的ubuntu</title>
    <url>/2023/03/07/14-58-00/</url>
    <content><![CDATA[<h1 id="ssh-登录wsl-ubuntu"><a href="#ssh-登录wsl-ubuntu" class="headerlink" title="ssh 登录wsl ubuntu"></a>ssh 登录wsl ubuntu</h1><h2 id="固定wsl2的ip地址"><a href="#固定wsl2的ip地址" class="headerlink" title="固定wsl2的ip地址"></a>固定wsl2的ip地址</h2><p>wsl2的ip地址每次都会改变，所以一直要重新设置防火墙端口，通过脚本，每次管理员运行脚本固定ip，就不需要重复设置防火墙端口。</p>
<ul>
<li>优化之后固定wsl2 IP:192.168.120.181,运行脚本,转发端口,设置防火墙,自动命令</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@echo off</span><br><span class="line">setlocal enabledelayedexpansion</span><br><span class="line"></span><br><span class="line">::先停掉wsl实例</span><br><span class="line">wsl --shutdown</span><br><span class="line">::重新拉起，启动ssh服务</span><br><span class="line">wsl -u root -d Ubuntu-20.04 service ssh start</span><br><span class="line">:: 固定IP</span><br><span class="line">wsl -u root ip addr | findstr &quot;192.168.120.181&quot; &gt; nul</span><br><span class="line">if !errorlevel! equ 0 (</span><br><span class="line">   echo wsl ip has set</span><br><span class="line">) else (</span><br><span class="line">   wsl -u root ip addr add 192.168.120.181/24 broadcast 192.168.120.0 dev eth0 label eth0:1</span><br><span class="line">   echo set wsl ip success: 192.168.120.181</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">::windows作为wsl的宿主，在wsl的固定IP的同一网段也给安排另外一个IP</span><br><span class="line">ipconfig | findstr &quot;192.168.120.100&quot; &gt; nul</span><br><span class="line">if !errorlevel! equ 0 (</span><br><span class="line">    echo windows ip has set</span><br><span class="line">) else (</span><br><span class="line">    netsh interface ip add address &quot;vEthernet (WSL)&quot; 192.168.120.100 255.255.255.0</span><br><span class="line">    echo set windows ip success: 192.168.120.100</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">::windows的2222端口转发到ubuntu</span><br><span class="line">netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=2222 connectaddress=192.168.120.181 connectport=2222</span><br><span class="line">::防火墙开启入站规则</span><br><span class="line">::netsh advfirewall firewall add rule name=WSL2 dir=in action=allow protocol=TCP localport=2222</span><br><span class="line"></span><br><span class="line">pause</span><br></pre></td></tr></table></figure>
<ul>
<li><p>管理员模式运行脚本后直接连接</p>
</li>
<li><p>连接 ip为<code>windows的地址</code></p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh name@ip -p2222</span><br></pre></td></tr></table></figure>
<h2 id="ssh服务器命令"><a href="#ssh服务器命令" class="headerlink" title="ssh服务器命令"></a>ssh服务器命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo service ssh restart</span><br><span class="line">sudo service ssh start</span><br><span class="line">sudo service ssh stop</span><br><span class="line">sudo service ssh status</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="wsl命令"><a href="#wsl命令" class="headerlink" title="wsl命令"></a>wsl命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wsl -- hostname -I    #显示wsl的ip</span><br><span class="line">wsl -l -v             #列出当前linux版本状态</span><br><span class="line">wsl --shutdown        #关闭linux</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>ssh</tag>
        <tag>ubuntu</tag>
        <tag>wsl2</tag>
        <tag>mac</tag>
      </tags>
  </entry>
</search>

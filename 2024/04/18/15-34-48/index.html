<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"pkunicor.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="吴恩达机器学习笔记week1--引言，单变量线性回归">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记一 单变量线性回归">
<meta property="og:url" content="https://pkunicor.github.io/2024/04/18/15-34-48/index.html">
<meta property="og:site_name" content="且听风吟">
<meta property="og:description" content="吴恩达机器学习笔记week1--引言，单变量线性回归">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180105_194839.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180105_201639.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180105_212048.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180105_224648.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_085915.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_091307.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_090904.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/0f38a99c8ceb8aa5b90a5f12136fdf43.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_092119.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/db48c81304317847870d486ba5bb2015.jpg">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_101659.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_184926.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_190944.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_191023.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_191956.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180106_203726.png">
<meta property="og:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/24e9420f16fdd758ccb7097788f879e7.png">
<meta property="article:published_time" content="2024-04-18T07:34:48.000Z">
<meta property="article:modified_time" content="2024-04-22T05:50:06.792Z">
<meta property="article:author" content="yao">
<meta property="article:tag" content="机器学习, 深度学习, 吴恩达">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pkunicor.github.io/2024/04/18/15-34-48/20180105_194839.png">


<link rel="canonical" href="https://pkunicor.github.io/2024/04/18/15-34-48/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://pkunicor.github.io/2024/04/18/15-34-48/","path":"2024/04/18/15-34-48/","title":"机器学习笔记一 单变量线性回归"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习笔记一 单变量线性回归 | 且听风吟</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">且听风吟</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">轻舟过万重,青山依旧在</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%B3%BB%E5%88%97"><span class="nav-number">1.</span> <span class="nav-text">系列</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%BC%95%E8%A8%80%EF%BC%88Introduction%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">1 引言（Introduction）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Welcome"><span class="nav-number">2.1.</span> <span class="nav-text">1.1 Welcome</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88What-is-Machine-Learning%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">1.2 什么是机器学习（What is Machine Learning）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Supervised-Learning%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">1.3 监督学习（Supervised Learning）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Unsupervised-Learning%EF%BC%89"><span class="nav-number">2.4.</span> <span class="nav-text">1.4 无监督学习（Unsupervised Learning）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88Linear-Regression-with-One-Variable%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">2 单变量线性回归（Linear Regression with One Variable）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA%EF%BC%88Model-Representation%EF%BC%89"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 模型表示（Model Representation）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88Cost-Function%EF%BC%89"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 代价函数（Cost Function）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A31%EF%BC%88Cost-Function-Intuition-I%EF%BC%89"><span class="nav-number">3.3.</span> <span class="nav-text">2.3 代价函数 - 直观理解1（Cost Function - Intuition I）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A32%EF%BC%88Cost-Function-Intuition-II%EF%BC%89"><span class="nav-number">3.4.</span> <span class="nav-text">2.4 代价函数 - 直观理解2（Cost Function - Intuition II）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Gradient-Descent%EF%BC%89"><span class="nav-number">3.5.</span> <span class="nav-text">2.5 梯度下降（Gradient Descent）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%EF%BC%88Gradient-Descent-Intuition%EF%BC%89"><span class="nav-number">3.6.</span> <span class="nav-text">2.6 梯度下降直观理解（Gradient Descent Intuition）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Gradient-Descent-For-Linear-Regression%EF%BC%89"><span class="nav-number">3.7.</span> <span class="nav-text">2.7 线性回归中的梯度下降（Gradient Descent For Linear Regression）</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yao"
      src="/images/logo.png">
  <p class="site-author-name" itemprop="name">yao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">87</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/PKunicor" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PKunicor" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://pkunicor.github.io/2024/04/18/15-34-48/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.png">
      <meta itemprop="name" content="yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="且听风吟">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习笔记一 单变量线性回归 | 且听风吟">
      <meta itemprop="description" content="吴恩达机器学习笔记week1--引言，单变量线性回归">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习笔记一 单变量线性回归
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-18 15:34:48" itemprop="dateCreated datePublished" datetime="2024-04-18T15:34:48+08:00">2024-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-22 13:50:06" itemprop="dateModified" datetime="2024-04-22T13:50:06+08:00">2024-04-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>



        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
            <div class="post-description">吴恩达机器学习笔记week1--引言，单变量线性回归</div>
	<hr>
        <p>更新历史</p>
<ul>
<li>24.04.18：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2024/04/18/15-40-53/" title="机器笔记汇总">吴恩达机器学习 - 笔记汇总</a>
</li>
</ul>
<h1 id="1-引言（Introduction）"><a href="#1-引言（Introduction）" class="headerlink" title="1 引言（Introduction）"></a>1 引言（Introduction）</h1><h2 id="1-1-Welcome"><a href="#1-1-Welcome" class="headerlink" title="1.1 Welcome"></a>1.1 Welcome</h2><p>随着互联网数据不断累积，硬件不断升级迭代，在这个信息爆炸的时代，机器学习已被应用在各行各业中，可谓无处不在。</p>
<h2 id="1-2-什么是机器学习（What-is-Machine-Learning）"><a href="#1-2-什么是机器学习（What-is-Machine-Learning）" class="headerlink" title="1.2 什么是机器学习（What is Machine Learning）"></a>1.2 什么是机器学习（What is Machine Learning）</h2><ol>
<li><p>机器学习定义<br> 这里主要有两种定义：</p>
<ul>
<li><p>Arthur Samuel (1959). Machine Learning:<br>Field of study that gives computers the ability to learn without being explicitly programmed.<br>这个定义有点不正式但提出的时间最早，来自于一个懂得计算机编程的下棋菜鸟。他编写了一个程序，但没有显式地编程每一步该怎么走，而是让计算机自己和自己对弈，并不断地计算布局的好坏，来判断什么情况下获胜的概率高，从而积累经验，好似学习，最后，这个计算机程序成为了一个比他自己还厉害的棋手。</p>
</li>
<li><p>Tom Mitchell (1998) Well-posed Learning Problem:<br>A computer program is said to learn from experience E with respect to some <strong>task T</strong> and some <strong>performance measure P</strong>, if its performance on T, as measured by P, improves with <strong>experience E</strong>.<br>Tom Mitchell 的定义更为现代和正式。在过滤垃圾邮件这个例子中，电子邮件系统会根据用户对电子邮件的标记（是/不是垃圾邮件）不断学习，从而提升过滤垃圾邮件的准确率，定义中的三个字母分别代表：</p>
<ul>
<li>T(Task): 过滤垃圾邮件任务。</li>
<li>P(Performance): 电子邮件系统过滤垃圾邮件的准确率。</li>
<li>E(Experience): 用户对电子邮件的标记。<blockquote>
<p>类似监督学习</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li>机器学习算法<br>主要有两种机器学习的算法分类<ol>
<li>监督学习</li>
<li>无监督学习<br>还有一些算法也属于机器学习领域，诸如：</li>
</ol>
<ul>
<li>半监督学习: 介于监督学习于无监督学习之间</li>
<li>推荐算法: </li>
<li>强化学习: 通过观察来学习如何做出动作，每个动作都会对环境有所影响，而环境的反馈又可以引导该学习算法。</li>
<li>迁移学习: 某个领域应用到另个领域</li>
</ul>
</li>
</ol>
<h2 id="1-3-监督学习（Supervised-Learning）"><a href="#1-3-监督学习（Supervised-Learning）" class="headerlink" title="1.3 监督学习（Supervised Learning）"></a>1.3 监督学习（Supervised Learning）</h2><p>监督学习，即为教计算机如何去完成预测任务（有反馈），预先给一定数据量的输入<strong>和对应的结果</strong>即训练集，建模拟合，最后让计算机预测未知数据的结果。<br>监督学习一般有两种：</p>
<ol>
<li><p>回归问题（Regression）<br>回归问题即为预测一系列的<strong>连续值</strong>。<br>在房屋价格预测的例子中，给出了一系列的房屋面积数据，根据这些数据来预测任意面积的房屋价格。给出照片-年龄数据集，预测给定照片的年龄。</p>

</li>
<li><p>分类问题（Classification）<br>分类问题即为预测一系列的<strong>离散值</strong>。<br>即根据数据预测被预测对象属于哪个分类。<br>视频中举了癌症肿瘤这个例子，针对诊断结果，分别分类为良性或恶性。还例如垃圾邮件分类问题，也同样属于监督学习中的分类问题。</p>
<img src="/2024/04/18/15-34-48/20180105_194839.png" class="">
</li>
</ol>
<p>视频中提到<strong>支持向量机</strong>这个算法，旨在解决当特征量很大的时候（特征即如癌症例子中的肿块大小，颜色，气味等各种特征），计算机内存一定会不够用的情况。<strong>支持向量机能让计算机处理无限多个特征。</strong></p>
<h2 id="1-4-无监督学习（Unsupervised-Learning）"><a href="#1-4-无监督学习（Unsupervised-Learning）" class="headerlink" title="1.4 无监督学习（Unsupervised Learning）"></a>1.4 无监督学习（Unsupervised Learning）</h2><p>相对于监督学习，训练集不会有人为标注的结果（无反馈），我们<strong>不会给出</strong>结果或<strong>无法得知</strong>训练集的结果是什么样，而是单纯由计算机通过无监督学习算法自行分析，从而“得出结果”。计算机可能会把特定的数据集归为几个不同的簇，故叫做聚类算法。</p>
<p>无监督学习一般分为两种：</p>
<ol>
<li>聚类（Clustering）<ul>
<li>新闻聚合</li>
<li>DNA 个体聚类</li>
<li>天文数据分析</li>
<li>市场细分</li>
<li>社交网络分析</li>
</ul>
</li>
<li>非聚类（Non-clustering）<ul>
<li>鸡尾酒问题</li>
</ul>
</li>
</ol>
<p><strong>新闻聚合</strong></p>
<p>在例如谷歌新闻这样的网站中，每天后台都会收集成千上万的新闻，然后将这些新闻分组成一个个的新闻专题，这样一个又一个聚类，就是应用了无监督学习的结果。</p>
<p><strong>鸡尾酒问题</strong></p>
 <img src="/2024/04/18/15-34-48/20180105_201639.png" class="">
<p>在鸡尾酒会上，大家说话声音彼此重叠，几乎很难分辨出面前的人说了什么。我们很难对于这个问题进行数据标注，而这里的通过机器学习的无监督学习算法，就可以将说话者的声音同背景音乐分离出来。</p>
<h1 id="2-单变量线性回归（Linear-Regression-with-One-Variable）"><a href="#2-单变量线性回归（Linear-Regression-with-One-Variable）" class="headerlink" title="2 单变量线性回归（Linear Regression with One Variable）"></a>2 单变量线性回归（Linear Regression with One Variable）</h1><h2 id="2-1-模型表示（Model-Representation）"><a href="#2-1-模型表示（Model-Representation）" class="headerlink" title="2.1 模型表示（Model Representation）"></a>2.1 模型表示（Model Representation）</h2><ol>
<li>房价预测训练集</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>Size in $feet^2$ ($x$)</th>
<th>Price ($) in 1000’s($y$)</th>
</tr>
</thead>
<tbody>
<tr>
<td>2104</td>
<td>460</td>
</tr>
<tr>
<td>1416</td>
<td>232</td>
</tr>
<tr>
<td>1534</td>
<td>315</td>
</tr>
<tr>
<td>852</td>
<td>178</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
<p>房价预测训练集中，同时给出了输入 $x$ 和输出结果 $y$，即给出了人为标注的<strong>“正确结果”</strong>，且预测的量是连续的，属于监督学习中的回归问题。</p>
<ol>
<li><p><strong>问题解决模型</strong></p>
<img src="/2024/04/18/15-34-48/20180105_212048.png" class="">
</li>
</ol>
<p>其中 $h$ 代表结果函数，也称为<strong>假设（hypothesis）</strong> 。假设函数根据输入（房屋的面积），给出预测结果输出（房屋的价格），即是一个 $X\to Y$ 的映射。</p>
<p>$h_\theta(x)=\theta_0+\theta_1x$，为解决房价问题的一种可行表达式。</p>
<blockquote>
<p>$x$: 特征/输入变量。</p>
</blockquote>
<p>上式中，$\theta$ 为参数，$\theta$ 的变化才决定了输出结果，不同以往，这里的 $x$ 被我们<strong>视作已知</strong>（不论是数据集还是预测时的输入），所以怎样解得 $\theta$ 以更好地拟合数据，成了求解该问题的最终问题。</p>
<p>单变量，即只有一个特征（如例子中房屋的面积这个特征）。</p>
<h2 id="2-2-代价函数（Cost-Function）"><a href="#2-2-代价函数（Cost-Function）" class="headerlink" title="2.2 代价函数（Cost Function）"></a>2.2 代价函数（Cost Function）</h2><blockquote>
<p>李航《统计学习方法》一书中，损失函数与代价函数两者为<strong>同一概念</strong>，未作细分区别，全书没有和《深度学习》一书一样混用，而是统一使用<strong>损失函数</strong>来指代这类类似概念。</p>
<p>吴恩达（Andrew Ng）老师在其公开课中对两者做了细分。</p>
<p><strong>损失函数</strong>（Loss/Error Function）: 计算<strong>单个</strong>样本的误差。</p>
<p><strong>代价函数</strong>（Cost Function）: 计算整个训练集<strong>所有损失函数之和的平均值</strong></p>
</blockquote>
<p>我们的目的在于求解预测结果 $h$ 最接近于实际结果 $y$ 时 $\theta$ 的取值，则问题可表达为<strong>求解 $\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})$ 的最小值</strong>。</p>
<blockquote>
<p>$m$: 训练集中的样本总数</p>
<p>$y$: 目标变量/输出变量</p>
<p>$\left(x, y\right)$: 训练集中的实例</p>
<p>$\left(x^{\left(i\right)},y^{\left(i\right)}\right)$: 训练集中的第 $i$ 个样本实例</p>
</blockquote>
<img src="/2024/04/18/15-34-48/20180105_224648.png" class="">
<p>上图展示了当 $\theta$ 取不同值时，$h_\theta\left(x\right)$ 对数据集的拟合情况，蓝色虚线部分代表<strong>建模误差</strong>。</p>
<p>为了求解最小值，引入代价函数（Cost Function）概念，用于度量建模误差。考虑到要计算最小值，应用二次函数对求和式建模，即应用统计学中的平方损失函数（最小二乘法）：</p>
<p>$$<br>J(\theta_0, \theta_1)= \dfrac{ 1 }{ 2m } \displaystyle \sum_ {i=1}^m\left(\hat{y}_{i}-y_{i} \right)^2=\dfrac{1}{2m}\displaystyle\sum_{i=1}^m\left(h_\theta(x_{i})-y_{i}\right)^2<br>$$</p>
<blockquote>
<p>$\hat{y}$: $y$ 的预测值</p>
<p>系数 $\frac{1}{2}$ 存在与否都不会影响结果，这里是为了在应用梯度下降时便于求解，平方的导数会抵消掉 $\frac{1}{2}$ 。</p>
</blockquote>
<p>讨论到这里，我们的问题就转化成了<strong>求解 $J\left( \theta_0, \theta_1  \right)$ 的最小值</strong>。</p>
<h2 id="2-3-代价函数-直观理解1（Cost-Function-Intuition-I）"><a href="#2-3-代价函数-直观理解1（Cost-Function-Intuition-I）" class="headerlink" title="2.3 代价函数 - 直观理解1（Cost Function - Intuition I）"></a>2.3 代价函数 - 直观理解1（Cost Function - Intuition I）</h2><p>根据上节视频，列出如下定义：</p>
<ul>
<li>假设函数（Hypothesis）: $h_\theta(x)=\theta_0+\theta_1x$</li>
<li>参数（Parameters）: $\theta_0, \theta_1$</li>
<li>代价函数（Cost Function）: $J\left( \theta_0, \theta_1  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ { {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)}^{2} } }$</li>
<li>目标（Goal）: $\underset{\theta_0, \theta_1}{\text{minimize} } J \left(\theta_0, \theta_1 \right)$</li>
</ul>
<p>为了直观理解代价函数到底是在做什么，先假设 $\theta_0 = 0$，并假设训练集有三个数据，分别为$\left(1, 1\right), \left(2, 2\right), \left(3, 3\right)$，这样在平面坐标系中绘制出 $h_\theta\left(x\right)$ ，并分析 $J\left(\theta_0, \theta_1\right)$ 的变化。</p>
<img src="/2024/04/18/15-34-48/20180106_085915.png" class="">
<p>右图 $J\left(\theta_0, \theta_1\right)$ 随着 $\theta_1$ 的变化而变化，可见<strong>当 $\theta_1 = 1$ 时，$J\left(\theta_0, \theta_1 \right) = 0$，取得最小值，</strong>对应于左图青色直线，即函数 $h$ 拟合程度最好的情况。</p>
<h2 id="2-4-代价函数-直观理解2（Cost-Function-Intuition-II）"><a href="#2-4-代价函数-直观理解2（Cost-Function-Intuition-II）" class="headerlink" title="2.4 代价函数 - 直观理解2（Cost Function - Intuition II）"></a>2.4 代价函数 - 直观理解2（Cost Function - Intuition II）</h2><p>给定数据集：<br><img src="/2024/04/18/15-34-48/20180106_091307.png" class=""></p>
<p>参数在 $\theta_0$ 不恒为 $0$ 时代价函数 $J\left(\theta\right)$ 关于 $\theta_0, \theta_1$ 的3-D图像，图像中的高度为代价函数的值。</p>
<img src="/2024/04/18/15-34-48/20180106_090904.png" class="">
<p>由于3-D图形不便于标注，所以将3-D图形转换为<strong>轮廓图（contour plot）</strong>，下面用轮廓图（下图中的右图）来作直观理解，其中相同颜色的一个圈代表着同一高度（同一 $J\left(\theta\right)$ 值）。</p>
<p>$\theta_0 = 360, \theta_1 =0$ 时：</p>
<img src="/2024/04/18/15-34-48/0f38a99c8ceb8aa5b90a5f12136fdf43.png" class="">
<p>大概在 $\theta_0 = 0.12, \theta_1 =250$ 时：</p>
<img src="/2024/04/18/15-34-48/20180106_092119.png" class="">
<p>上图中最中心的点（红点），近乎为图像中的最低点，也即代价函数的最小值，此时对应 $h_\theta\left(x\right)$ 对数据的拟合情况如左图所示。</p>
<h2 id="2-5-梯度下降（Gradient-Descent）"><a href="#2-5-梯度下降（Gradient-Descent）" class="headerlink" title="2.5 梯度下降（Gradient Descent）"></a>2.5 梯度下降（Gradient Descent）</h2><p>在特征量很大的情况下，即便是借用计算机来生成图像，人工的方法也很难读出 $J\left(\theta\right)$ 的最小值，并且大多数情况无法进行可视化，故引入<strong>梯度下降（Gradient Descent）方法，让计算机自动找出最小化代价函数时对应的 $\theta$ 值。</strong></p>
<p>梯度下降背后的思想是：开始时，我们随机选择一个参数组合$\left( {\theta_{0} },{\theta_{1} },……,{\theta_{n} } \right)$即起始点，计算代价函数，然后寻找下一个能使得代价函数下降最多的参数组合。不断迭代，直到找到一个<strong>局部最小值（local minimum）</strong>，由于下降的情况只考虑当前参数组合周围的情况，所以无法确定当前的局部最小值是否就是<strong>全局最小值（global minimum）</strong>，不同的初始参数组合，可能会产生不同的局部最小值。</p>
<p>下图根据不同的起始点，产生了两个不同的局部最小值。</p>
<img src="/2024/04/18/15-34-48/db48c81304317847870d486ba5bb2015.jpg" class="">
<p>视频中举了下山的例子，即我们在山顶上的某个位置，为了下山，就不断地看一下周围<strong>下一步往哪走</strong>下山比较快，然后就<strong>迈出那一步</strong>，一直重复，直到我们到达山下的某一处<strong>陆地</strong>。</p>
<p>梯度下降公式：</p>
<p>$$<br>\begin{split}<br>&amp; \text{Repeat until convergence:} \; \lbrace \\<br>&amp;{ {\theta }_{j} }:={ {\theta }_{j} }-\alpha \frac{\partial }{\partial { {\theta }_{j} } }J\left( {\theta_{0} },{\theta_{1} }  \right) \\<br>\rbrace<br>\end{split}<br>$$</p>
<blockquote>
<p>“-”来源于：<strong>正值向左移动减少，负值向右移动增加</strong><br>${\theta }_{j}$: 第 $j$ 个特征参数</p>
<p>”:=“: 赋值操作符</p>
<p>$\alpha$: 学习速率（learning rate）, $\alpha &gt; 0$</p>
<p>$\frac{\partial }{\partial { {\theta }_{j} } }J\left( \theta_0, \theta_j  \right)$: $J\left( \theta_0, \theta_j \right)$ 的偏导</p>
</blockquote>
<p>公式中，学习速率决定了参数值变化的速率即”<strong>走多少距离</strong>“，而偏导这部分决定了下降的方向即”<strong>下一步往哪里</strong>“走（当然实际上的走多少距离是由偏导值给出的，学习速率起到调整后决定的作用），收敛处的局部最小值又叫做极小值，即”<strong>陆地</strong>“。</p>
<img src="/2024/04/18/15-34-48/20180106_101659.png" class="">
<p>注意，在计算时要<strong>批量更新 $\theta$ 值</strong>，即如上图中的左图所示，否则结果上会有所出入，原因不做细究。</p>
<h2 id="2-6-梯度下降直观理解（Gradient-Descent-Intuition）"><a href="#2-6-梯度下降直观理解（Gradient-Descent-Intuition）" class="headerlink" title="2.6 梯度下降直观理解（Gradient Descent Intuition）"></a>2.6 梯度下降直观理解（Gradient Descent Intuition）</h2><p>该节探讨 $\theta_1$ 的梯度下降更新过程，即 $\theta_1 := \theta_1 - \alpha\frac{d}{d\theta_1}J\left(\theta_1\right)$。</p>
<img src="/2024/04/18/15-34-48/20180106_184926.png" class="">
<p>直线的斜率，表示了函数 $J\left(\theta\right)$ 在初始点处有<strong>正斜率</strong>，也就是说它有<strong>正导数</strong>，会<strong>向左边移动</strong>。不断重复，直到收敛。</p>
<p>初始 $\theta$ 值（初始点）是任意的。</p>
<p>对于学习速率 $\alpha$，需要选取一个合适的值才能使得梯度下降算法运行良好。</p>
<ul>
<li><p>学习速率过小图示：</p>
<img src="/2024/04/18/15-34-48/20180106_190944.png" class="">
<p>收敛的太慢，需要更多次的迭代。</p>
</li>
<li><p>学习速率过大图示：</p>
<img src="/2024/04/18/15-34-48/20180106_191023.png" class="">
<p>可能越过最低点，甚至导致无法收敛。</p>
</li>
</ul>
<p><strong>学习速率只需选定即可</strong>，不需要动态改变，随着斜率越来越接近于0，代价函数的变化幅度会越来越小，直到收敛到局部极小值。</p>
<p>代价函数随着迭代的进行，变化的幅度越来越小。</p>
<img src="/2024/04/18/15-34-48/20180106_191956.png" class="">
<h2 id="2-7-线性回归中的梯度下降（Gradient-Descent-For-Linear-Regression）"><a href="#2-7-线性回归中的梯度下降（Gradient-Descent-For-Linear-Regression）" class="headerlink" title="2.7 线性回归中的梯度下降（Gradient Descent For Linear Regression）"></a>2.7 线性回归中的梯度下降（Gradient Descent For Linear Regression）</h2><p>线性回归模型</p>
<ul>
<li>$h_\theta(x)=\theta_0+\theta_1x$</li>
<li>$J\left( \theta_0, \theta_1  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{ { {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)}^{2} } }$</li>
</ul>
<p>梯度下降算法<br>$$<br>\begin{split}<br>  &amp; \text{Repeat until convergence:} \; \lbrace \\<br>  &amp;{ {\theta }_{j} }:={ {\theta }_{j} }-\alpha \frac{\partial }{\partial { {\theta }_{j} } }J\left( {\theta_{0} },{\theta_{1} }  \right) \\<br>  \rbrace<br>  \end{split}<br>$$</p>
<p>直接将线性回归模型公式代入梯度下降公式可得出公式</p>
<img src="/2024/04/18/15-34-48/20180106_203726.png" class="">
<p>当 $j = 0, j = 1$ 时，<strong>线性回归中代价函数求导的推导过程（：</strong></p>
<p>$$<br>\begin{split}<br>\frac{\partial}{\partial\theta_j} J(\theta_1, \theta_1)&amp;=\frac{\partial}{\partial\theta_j} \left(\frac{1}{2m}\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)}^{2} } \right)\\<br>&amp;=\left(\frac{1}{2m}*2\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} } \right)*\frac{\partial}{\partial\theta_j}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} } \\<br>&amp;=\left(\frac{1}{m}\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} } \right)*\frac{\partial}{\partial\theta_j} {\left( \theta_0 + \theta_1{x_1^{(i)} }-{ {y}^{(i)} } \right)}<br>\end{split}<br>$$</p>
<p>所以当 $j = 0$ 时：</p>
<p>$$<br>\frac{\partial}{\partial\theta_0} J(\theta)=\frac{1}{m}\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} }<br>$$</p>
<p>所以当 $j = 1$ 时：</p>
<p>$$<br>\frac{\partial}{\partial\theta_1} J(\theta)=\frac{1}{m}\sum\limits_{i=1}^{m}{ {\left( { {h}_{\theta } }\left( { {x}^{(i)} } \right)-{ {y}^{(i)} } \right)} } *x_1^{(i)}<br>$$</p>
<p>上文中所提到的梯度下降，都为批量梯度下降（Batch Gradient Descent），即每次计算都使用<strong>所有</strong>的数据集 $\left(\sum\limits_{i=1}^{m}\right)$ 更新。</p>
<p>由于线性回归函数呈现<strong>碗状</strong>，且<strong>只有一个</strong>全局的最优值，所以函数<strong>一定总会</strong>收敛到全局最小值（学习速率不可过大）。同时，函数 $J$ 被称为<strong>凸二次函数</strong>，而线性回归函数求解最小值问题属于<strong>凸函数优化问题</strong>。</p>
<img src="/2024/04/18/15-34-48/24e9420f16fdd758ccb7097788f879e7.png" class="">
<p>另外，使用循环求解，代码较为冗余，后面会讲到如何使用<strong>向量化（Vectorization）</strong>来简化代码并优化计算，使梯度下降运行的更快更好。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>yao
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://pkunicor.github.io/2024/04/18/15-34-48/" title="机器学习笔记一 单变量线性回归">https://pkunicor.github.io/2024/04/18/15-34-48/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/" rel="tag"># 机器学习, 深度学习, 吴恩达</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/04/12-45-10/" rel="prev" title="天梯赛刷题记录">
                  <i class="fa fa-chevron-left"></i> 天梯赛刷题记录
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/04/18/15-40-53/" rel="next" title="27-GooLeNet">
                  27-GooLeNet <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"PKunicor","repo":"PKunicor.github.io","client_id":"2efe0e153686e5d9e67d","client_secret":"c84e031b6ac86ce366a6b61640a4b1a8e01d05e0","admin_user":"PKunicor","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"1a5f07cfaff09093cdf293f493b36d26"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>

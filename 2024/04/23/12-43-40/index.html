<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zzhenyao.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="吴恩达机器学习笔记week5--神经网络的学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记五 神经网络的学习">
<meta property="og:url" content="https://zzhenyao.github.io/2024/04/23/12-43-40/index.html">
<meta property="og:site_name" content="且听风吟">
<meta property="og:description" content="吴恩达机器学习笔记week5--神经网络的学习">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-04-23T04:43:40.000Z">
<meta property="article:modified_time" content="2024-04-23T05:08:49.089Z">
<meta property="article:author" content="yao">
<meta property="article:tag" content="机器学习, 深度学习, 吴恩达">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zzhenyao.github.io/2024/04/23/12-43-40/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://zzhenyao.github.io/2024/04/23/12-43-40/","path":"2024/04/23/12-43-40/","title":"机器学习笔记五 神经网络的学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习笔记五 神经网络的学习 | 且听风吟</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">且听风吟</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">轻舟过万重,青山依旧在</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%B3%BB%E5%88%97"><span class="nav-number">1.</span> <span class="nav-text">系列</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#9-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%AD%A6%E4%B9%A0%EF%BC%88Neural-Networks-Learning%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">9 神经网络: 学习（Neural Networks: Learning）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#9-1-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88Cost-Function%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">9.1 代价函数（Cost Function）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%EF%BC%88Backpropagation-Algorithm%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">9.2 反向传播算法（Backpropagation Algorithm）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-3-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Backpropagation-Intuition%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">9.3 直观理解反向传播（Backpropagation Intuition）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-4-%E5%AE%9E%E7%8E%B0%E6%B3%A8%E6%84%8F%E7%82%B9-%E5%8F%82%E6%95%B0%E5%B1%95%E5%BC%80%EF%BC%88Implementation-Note-Unrolling-Parameters%EF%BC%89"><span class="nav-number">2.4.</span> <span class="nav-text">9.4 实现注意点: 参数展开（Implementation Note: Unrolling Parameters）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-5-%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%EF%BC%88Gradient-Checking%EF%BC%89"><span class="nav-number">2.5.</span> <span class="nav-text">9.5 梯度检验（Gradient Checking）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-6-%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%88Random-Initialization%EF%BC%89"><span class="nav-number">2.6.</span> <span class="nav-text">9.6 随机初始化（Random Initialization）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-7-%E7%BB%BC%E5%90%88%E8%B5%B7%E6%9D%A5%EF%BC%88Putting-It-Together%EF%BC%89"><span class="nav-number">2.7.</span> <span class="nav-text">9.7 综合起来（Putting It Together）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-8-%E8%87%AA%E4%B8%BB%E9%A9%BE%E9%A9%B6%EF%BC%88Autonomous-Driving%EF%BC%89"><span class="nav-number">2.8.</span> <span class="nav-text">9.8 自主驾驶（Autonomous Driving）</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yao"
      src="/images/logo.png">
  <p class="site-author-name" itemprop="name">yao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">144</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zzhenyao" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzhenyao" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zzhenyao.github.io/2024/04/23/12-43-40/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.png">
      <meta itemprop="name" content="yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="且听风吟">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="机器学习笔记五 神经网络的学习 | 且听风吟">
      <meta itemprop="description" content="吴恩达机器学习笔记week5--神经网络的学习">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习笔记五 神经网络的学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-04-23 12:43:40 / 修改时间：13:08:49" itemprop="dateCreated datePublished" datetime="2024-04-23T12:43:40+08:00">2024-04-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>



        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
            <div class="post-description">吴恩达机器学习笔记week5--神经网络的学习</div>
	<hr>
        <p>更新历史</p>
<ul>
<li>24.04.23：初稿</li>
</ul>
<h1 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h1><ul>
<li><a href="/2024/04/18/15-40-33/" title="机器笔记汇总">吴恩达机器学习 - 笔记汇总</a>
</li>
</ul>
<h1 id="9-神经网络-学习（Neural-Networks-Learning）"><a href="#9-神经网络-学习（Neural-Networks-Learning）" class="headerlink" title="9 神经网络: 学习（Neural Networks: Learning）"></a>9 神经网络: 学习（Neural Networks: Learning）</h1><h2 id="9-1-代价函数（Cost-Function）"><a href="#9-1-代价函数（Cost-Function）" class="headerlink" title="9.1 代价函数（Cost Function）"></a>9.1 代价函数（Cost Function）</h2><p>神经网络的分类问题有两种：</p>
<ul>
<li><p>二元分类问题（0/1分类）</p>
<p>只有一个输出单元（$K=1$）</p>
</li>
<li><p>多元（$K$）分类问题</p>
<p>输出单元不止一个（$K\gt1$）</p>
</li>
</ul>
<p>神经网络的代价函数公式：</p>
<p>$h_\Theta(x) = a^{(L)} = g(\Theta^{(L-1)}a^{(L-1)}) = g(z^{(L)})$</p>
<p>$$ \begin{split} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1} } ( \Theta_{j,i}^{(l)})^2\end{split}$$</p>
<blockquote>
<p>$L$: 神经网络的总层数</p>
<p>$s_l$: 第 $l$ 层激活单元的数量（不包含偏置单元）</p>
<p>$h_\Theta(x)_k$: 分为第 $k$ 个分类($k^{th}$)的概率 $P(y=k | x ; \Theta) $</p>
<p>$K$: 输出层的输出单元数量，即类数 - 1</p>
<p>$y_k^{(i)}$: 第 $i$ 个训练样本的第 $k$ 个分量值</p>
<p>$y$: $K$ 维向量</p>
</blockquote>
<p>对照下逻辑回归中的代价函数：</p>
<p>$$<br>J(\theta) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2<br>$$</p>
<p>在神经网络的代价函数中，</p>
<ul>
<li>左边的变化实际上是为了求解 $K$ 分类问题，即公式会对每个样本特征都运行 $K$ 次，并依次给出分为第 $k$ 类的概率，$h_\Theta(x)\in \mathbb{R}^{K}, y \in \mathbb{R}^{K}$。</li>
<li>右边的正则化项比较容易理解，每一层有多维矩阵 $\Theta^{(l)}\in \mathbb{R}^{(s_l + 1)\times s_{l+1} }$，从左到右看这个三次求和式 $\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l+1} }$ ，就是对每一层间的多维矩权重 $\Theta^{(l)}$ ，依次平方后求取其除了偏置权重部分的和值，并循环累加即得结果。</li>
</ul>
<blockquote>
<p>$\mathbb{R}^{m}$: 即 $m$ 维向量</p>
<p>$\mathbb{R}^{m\times n}$: 即 $m \times n$ 维矩阵</p>
</blockquote>
<p>再次可见，神经网络背后的思想是和逻辑回归一样的，但由于计算复杂，实际上神经网络的代价函数 $J(\Theta)$ 是一个非凸（non-convex）函数。</p>
<h2 id="9-2-反向传播算法（Backpropagation-Algorithm）"><a href="#9-2-反向传播算法（Backpropagation-Algorithm）" class="headerlink" title="9.2 反向传播算法（Backpropagation Algorithm）"></a>9.2 反向传播算法（Backpropagation Algorithm）</h2><p>类似于回归模型中的梯度下降算法，为了求解神经网络最优化问题，我们也要计算 $\frac{\partial}{\partial\Theta}J(\Theta)$，以此 $\underset{\Theta}{\text{minimize} }J(\Theta)$ 。</p>
<p>在神经网络中，代价函数看上去虽然不复杂，但要注意到其中 $h_\Theta(x)$ 的求取实际上是由前向传播算法求得，即需从输入层开始，根据每层间的权重矩阵 $\Theta$ 依次计算激活单元的值 $a$。 在最优化代价函数时，我们必然也需要最优化每一层的权重矩阵，再次强调一下，<strong>算法最优化的是权重，而不是输入</strong>。</p>

<p><strong>反向传播算法</strong>用于计算每一层权重矩阵的偏导 $\frac{\partial}{\partial\Theta}J(\Theta)$，算法实际上是对代价函数求导的拆解。</p>
<ol>
<li><p>对于给定训练集 $\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$ ，初始化每层间的误差和矩阵 $\Delta$，即令所有的 $\Delta^{(l)}_{i,j}=0$，使得每个 $\Delta^{(l)}$ 为一个全零矩阵。</p>
</li>
<li><p>接下来遍历所有样本实例，对于每一个样本实例，有下列步骤：</p>
<ol>
<li><p>运行前向传播算法，得到初始预测 $a^{(L)}=h_\Theta(x)$ 。</p>
</li>
<li><p>运行反向传播算法，从输出层开始计算每一层预测的<strong>误差（error）</strong>，以此来求取偏导。</p>

<p>输出层的误差即为预测与训练集结果的之间的差值：$\delta^{(L)} = a^{(L)} - y$，</p>
<p>对于隐藏层中每一层的误差，都通过上一层的误差来计算：</p>
<p>$\delta^{(l)} = (\Theta^{(l)})^T\delta^{(l+1)} .*\ \frac{\partial a^{(l)} }{\partial z^{(l)} }\; \; \; \; \;  \text{for }l := L-1, L-2,\dots,2.$</p>
<p>隐藏层中，$a^{(l)}$ 即为增加偏置单元后的 $g(z^{(l)})$，$a^{(l)}$ 与 $\Theta^{(l)}$ 维度匹配，得以完成矩阵运算。</p>
<p>即对于隐藏层，有 $a^{(l)} = (g(z^{(l)})$ 添加偏置单元 $a^{(l)}_0 = 1)$</p>
<p>解得 $\frac{\partial}{\partial z^{(l)} }g(z^{(l)})=g’(z^{(l)})=g(z^{(l)}) .* \ (1-g(z^{(l)}))$，</p>
<p>则有 $\delta^{(l)} = (\Theta^{(l)})^T\delta^{(l+1)} .<em>\ a^{(l)} .</em>\ (1-a^{(l)}), \ \ a^{(l)}_0 = 1$。</p>
<blockquote>
<p>$\delta^{(l)}$ 求导前的公式不同于视频内容，经核实为视频内容错误。推导请阅下节。</p>
</blockquote>
<p>根据以上公式计算依次每一层的误差 $\delta^{(L)}, \delta^{(L-1)},\dots,\delta^{(2)}$。</p>
</li>
<li><p>依次求解并累加误差 $\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$，向量化实现即 $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$</p>
</li>
</ol>
</li>
<li><p>遍历全部样本实例，求解完 $\Delta$ 后，最后则求得偏导 $\frac \partial {\partial \Theta_{i,j}^{(l)} } J(\Theta)=D_{i,j}^{(l)}$</p>
<ul>
<li>$D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)$, if $j\neq0$,</li>
<li>$D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}$, if $j=0$.（对应于偏置单元）</li>
</ul>
</li>
</ol>
<blockquote>
<p>$\delta^{(l)}$: 第 $l$ 层的误差向量</p>
<p>$\delta^{(l)}_i$: 第 $l$ 层的第 $i$ 个激活单元的误差</p>
<p>$\Delta^{(l)}_{i,j}$: 从第 $l$ 层的第 $j$ 个单元映射到第 $l+1$ 层的第 $i$ 个单元的权重代价的偏导（所有样本实例之和）</p>
<p>$D^{(l)}_{i,j}$: $\Delta^{(l)}_{i,j}$ 的样本均值与正则化项之和</p>
<p>注：无需计算 $\delta^{(1)}$，因为输入没有误差。</p>
</blockquote>
<p>这就是反向传播算法，即从输出层开始不断<strong>向前迭代</strong>，根据<strong>上一层</strong>的误差依次计算当前层的误差，以求得代价函数的偏导。</p>
<blockquote>
<p>应用反向传播（BP）算法的神经网络被称为 BP 网络，也称前馈网络（向前反馈）。</p>
</blockquote>
<p>《机器学习》一书中提到的 BP 网络强大之处：</p>
<blockquote>
<p>任何布尔函数都可由两层神经网络准确表达，但所需的中间单元的数量随输入呈指数级增长;</p>
<p>任何连续函数都可由两层神经网络以任意精度逼近;</p>
<p>任何函数都可由三层神经网络以任意程度逼近。</p>
</blockquote>
<h2 id="9-3-直观理解反向传播（Backpropagation-Intuition）"><a href="#9-3-直观理解反向传播（Backpropagation-Intuition）" class="headerlink" title="9.3 直观理解反向传播（Backpropagation Intuition）"></a>9.3 直观理解反向传播（Backpropagation Intuition）</h2><p>这节给出了反向传播算法中误差的数学意义：</p>
<p>$cost(t) =y^{(t)} \ \log (h_\Theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\Theta(x^{(t)}))$</p>
<p>$\delta_j^{(l)} = \dfrac{\partial}{\partial z_j^{(l)} } cost(t)$</p>
<p>视频内容实际在上文都涉及到了，上节也做了解释：</p>
<blockquote>
<p>反向传播算法，即从输出层开始不断<strong>向前迭代</strong>，根据<strong>上一层</strong>的误差依次计算当前层的误差，以求得代价函数的偏导。</p>
</blockquote>
<p>前文提到输入层没有偏差，所以没有 $\delta^{(1)}$，同样的，偏置单元的值始终为 1，也没有误差，故一般会选择<strong>忽略偏置单元项的误差</strong>。</p>
<p><strong>神经网络中代价函数求导的推导过程</strong>：</p>
<p>代价函数无正则化项时：</p>
<p>$\begin{split} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \left[y^{(i)} \log ((h_\Theta (x^{(i)}))) + (1 - y^{(i)})\log (1 - (h_\Theta(x^{(i)})))\right] \end{split}$</p>
<p>再次的，为了方便起见，这里假设样本只有一个，则有：</p>
<p>$\begin{split} J(\Theta) = -\left[y \log ((h_\Theta (x))) + (1 - y)\log (1 - (h_\Theta(x)))\right] \end{split}$</p>
<p>忆及 $h_\Theta(x) = a^{(L)} = g(z^{(L)})$，$g(z) = \frac{1}{1+e^{(-z)} }$，代入后整理后可得：</p>
<p>$J(\Theta) ={y}\log \left( 1+{ {e}^{-z^{(L)} }} \right)+\left( 1-{y} \right)\log \left( 1+{ {e}^{z^{(L)} }} \right)$</p>

<p>再次为了便于计算，我们用到如上图这个三层（输入层一般不计数）神经网络。</p>
<p>忆及 $z^{(l)} = \Theta^{(l-1)}a^{(l-1)}$，我们有 $h_\Theta(x)=a^{(4)}= g(z^{(4)})=g(\Theta^{(3)}a^{(3)})$</p>
<p>观察考虑各变量与 $\Theta^{(3)}$ 之间的关系，有 $J(\Theta) \rightarrow  a^{(4)}\rightarrow z^{(4)}\rightarrow \Theta^{(3)}$</p>
<p>要计算 $J(\Theta)$ 的偏导，就要按照关系不断往前看，每一次回头看，就称为一次反向传播。</p>
<p>把回头看的关系说的“微积分一点”，那就是 $\Theta^{(3)}$ 的微小改变会引起 $z^{(4)}$ 的改变， $z^{(4)}$ 的微小改变会引起 $a^{(4)}$ 的改变，$a^{(4)}$ 的微小改变又会引起 $ J(\Theta)$ 的改变，关系方向也可以反过来写：$\Theta^{(3)} \rightarrow z^{(4)} \rightarrow a^{(4)} \rightarrow J(\Theta) $。</p>
<p>令 $\delta^{(l)} = \frac{\partial}{\partial z^{(l)} } J(\Theta)$，则有 $J(\Theta)$ 关于 $\Theta^{(3)}$ 的偏导：</p>
<p>$\frac{\partial}{\partial\Theta^{(3)} } J(\Theta) = \frac{\partial J(\Theta)}{\partial z^{(4)} }   \frac{\partial z^{(4)} }{\partial\Theta^{(3)} } = \delta^{(4)}\frac{\partial z^{(4)} }{\partial\Theta^{(3)} }$</p>
<p>再次忆及 $z^{(l)} = \Theta^{(l-1)}a^{(l-1)}$，则 $\frac{\partial z^{(4)} }{\partial\Theta^{(3)} } = a^{(3)}$</p>
<p>则对于输出层，我们证得 $\frac{\partial}{\partial\Theta^{(3)} } J(\Theta) =  a^{(3)}\delta^{(4)}$。</p>
<p>再次忆及 $g(z) = \frac{1}{1+e^{-z} }$，$a^{(L)}=g(z^{(L)})$</p>
<p>$\delta^{(4)}=\frac{\partial}{\partial z^{(4)} }J(\Theta)={ {y} }\frac{-e^{-z^{(4)} }}{1+e^{-z^{(4)} }}+\left( 1-{ {y} } \right)\frac{ {e^{z^{(4)} }} }{1+e^{z^{(4)} }} = g(z^{(4)}) - y = a^{(4)}-y$</p>
<p>即证得 $\delta^{(4)} = a^{(4)}-y$</p>
<p>对于任意的输出层 $L$ 及 $\Theta^{(L-1)}$，有 $J(\Theta) \rightarrow  a^{(L)}\rightarrow z^{(L)}\rightarrow \Theta^{(L-1)}$ 关系不变，故证得：<br>$$<br>\frac{\partial}{\partial\Theta^{(L-1)} } J(\Theta) =  a^{(L-1)}\delta^{(L)}, \ \ \delta^{(L)} = a^{(L)}-y<br>$$<br>好了，接下来来看一下 $J(\Theta)$ 关于 $\Theta^{(2)}$ 的偏导</p>
<p>仍然观察考虑各变量与 $\Theta^{(2)}$ 之间的关系，有 $J(\Theta)\rightarrow a^{(4)} \rightarrow z^{(4)} \rightarrow    a^{(3)} \rightarrow z^{(3)} \rightarrow\Theta^{(2)}$ </p>
<p>$\frac{\partial}{\partial \Theta^{(2)} }J(\Theta) = \frac{\partial J(\Theta)}{\partial z^{(3)} } \frac{\partial z^{(3)} }{\partial \Theta^{(2)} }=\delta^{(3)} \frac{\partial z^{(3)} }{\partial \Theta^{(2)} }=  a^{(2)}\delta^{(3)}$</p>
<p>$\delta^{(3)} = \frac{\partial}{\partial z^{(3)} }J(\Theta) =\frac{\partial J(\Theta)}{\partial z^{(4)} } \frac{\partial z^{(4)} }{\partial a^{(3)} }\frac{\partial a^{(3)} }{\partial z^{(3)} } = \delta^{(4)}\frac{\partial z^{(4)} }{\partial a^{(3)} }\frac{\partial a^{(3)} }{\partial z^{(3)} }$</p>
<p>易求得 $\frac{\partial z^{(4)} }{\partial a^{(3)} }=\Theta^{(3)}$</p>
<p>$g’(z) =\frac{e^{-z} }{(1+e^{-z})^2}=\frac{(1+e^{-z})-1}{(1+e^{-z})^2}=\frac{1}{1+e^{-z} }-\frac{1}{(1+e^{-z})^2}=g(z)(1-g(z))$</p>
<p>即 $g’(z^{(l)})=g(z^{(l)}) .* \ (1-g(z^{(l)}))$</p>
<p>有 $a^{(l)} = (g(z^{(l)})$ 添加偏置单元 $a^{(l)}_0 = 1)$，则 $\frac{\partial a^{(3)} }{\partial z^{(3)} }=a^{(3)} .*\ (1-a^{(3)})$，</p>
<blockquote>
<p>证明时为先求导后添加偏置单元，与前向传播算法顺序一致，实际实现时，求导和添加偏置单元的顺序可作调换，由于一般选择忽略偏置单元的误差，所以并不影响结果。</p>
</blockquote>
<p>即证得 $\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}.*(a^{(3)})’=(\Theta^{(3)})^T\delta^{(4)}.*\ a^{(3)} .*\ (1-a^{(3)})$</p>
<p>对于任意的隐藏层 $l + 1$ 及权重矩阵 $\Theta^{(l)}$，有 $J(\Theta)\rightarrow a^{(L)} \rightarrow z^{(L)} \rightarrow \dots \rightarrow a^{(l+1)} \rightarrow z^{(l+1)} \rightarrow\Theta^{(l)}$ 关系不变，故证得：</p>
<p>$$<br>\frac{\partial}{\partial\Theta^{(l)} } J(\Theta) =  a^{(l)}\delta^{(l+1)}, \ \ \delta^{(l)} = (\Theta^{(l)})^T\delta^{(l+1)}.*\ a^{(l)} .*\ (1-a^{(l)})\; \; \; \; \;  \text{for }l := L-1, L-2,\dots,2.<br>$$</p>
<p>再添回为了计算方便去掉的 $\frac{1}{m}$ 和正则化项（时刻记住偏置单元不正则化）等，即可得上节中 $J(\Theta)$ 的偏导。</p>
<h2 id="9-4-实现注意点-参数展开（Implementation-Note-Unrolling-Parameters）"><a href="#9-4-实现注意点-参数展开（Implementation-Note-Unrolling-Parameters）" class="headerlink" title="9.4 实现注意点: 参数展开（Implementation Note: Unrolling Parameters）"></a>9.4 实现注意点: 参数展开（Implementation Note: Unrolling Parameters）</h2><p>在 Octave/Matlab 中，如果要使用类似于 <code>fminunc</code> 等高级最优化函数，其函数参数、函数返回值等都为且只为向量，而由于神经网络中的权重是多维矩阵，所以需要用到参数展开这个技巧。</p>
<p>说白了，这个技巧就是把多个矩阵转换为一个长长的向量，便于传入函数，之后再根据矩阵维度，转回矩阵即可。</p>
<p>Octave 代码：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">% 多个矩阵展开为一个向量</span></span><br><span class="line">Theta1 = <span class="built_in">ones</span>(<span class="number">11</span>, <span class="number">10</span>);    <span class="comment">% 创建维度为 11 * 10 的矩阵</span></span><br><span class="line">Theta2 = <span class="built_in">ones</span>(<span class="number">2</span>, <span class="number">4</span>) * <span class="number">2</span>;  <span class="comment">% 创建维度为 2 * 4 的矩阵</span></span><br><span class="line">ThetaVec = [Theta1(:); Theta2(:)]; <span class="comment">% 将上面两个矩阵展开为向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 从一个向量重构还原回多个矩阵</span></span><br><span class="line">Theta1 = <span class="built_in">reshape</span>(ThetaVec(<span class="number">1</span>:<span class="number">110</span>), <span class="number">11</span>, <span class="number">10</span>)</span><br><span class="line">Theta2 = <span class="built_in">reshape</span>(ThetaVec(<span class="number">111</span>:<span class="number">118</span>), <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">% Theta2 = reshape(ThetaVec(111:(111 + 2 * 4) - 1), 2, 4)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>reshape(A,m,n)</code>: 将向量 A 重构为 m * n 维矩阵。</p>
</blockquote>
<h2 id="9-5-梯度检验（Gradient-Checking）"><a href="#9-5-梯度检验（Gradient-Checking）" class="headerlink" title="9.5 梯度检验（Gradient Checking）"></a>9.5 梯度检验（Gradient Checking）</h2><p>由于神经网络模型中的反向传播算法较为复杂，在小细节非常容易出错，从而无法得到最优解，故引入梯度检验。</p>
<p>梯度检验采用数值估算（Numerical estimation）梯度的方法，被用于验证反向传播算法的正确性。</p>

<p>把视 $\Theta$ 为一个实数，数值估算梯度的原理如上图所示，即有 $\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}$</p>
<p>其中，$\epsilon$ 为极小值，由于太小时容易出现数值运算问题，一般取 $10^{-4}$。</p>
<p>对于矩阵 $\Theta$，有 $\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$</p>
<p>Octave 代码：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">epsilon = <span class="number">1e-4</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n,</span><br><span class="line">  thetaPlus = theta;</span><br><span class="line">  thetaPlus(<span class="built_in">i</span>) += epsilon;</span><br><span class="line">  thetaMinus = theta;</span><br><span class="line">  thetaMinus(<span class="built_in">i</span>) -= epsilon;</span><br><span class="line">  gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus))/(<span class="number">2</span>*epsilon);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>在得出 gradApprox 梯度向量后，将其同之前计算的偏导 $D$ 比较，如果相等或很接近，即说明算法没有问题。</p>
<p>在确认算法<strong>没有问题后</strong>（一般只需运行一次），由于数值估计的梯度检验效率很低，所以一定要<strong>禁用它</strong>。</p>
<h2 id="9-6-随机初始化（Random-Initialization）"><a href="#9-6-随机初始化（Random-Initialization）" class="headerlink" title="9.6 随机初始化（Random Initialization）"></a>9.6 随机初始化（Random Initialization）</h2><p>逻辑回归中，初始参数向量全为 0 没什么问题，在神经网络中，情况就不一样了。</p>
<p>初始权重如果全为 0，忆及 $z^{(l)} = \Theta^{(l-1)}a^{(l-1)}$，则隐藏层除了偏置单元，都为 0，而每个单元求导的值也都一样，这就相当于是在不断<strong>重复计算同一结果</strong>，也就是算着算着，一堆特征在每一层都变成只有一个特征（虽然有很多单元，但值都相等），这样，神经网络的性能和效果都会大打折扣，故需要随机初始化初始权重。</p>
<p>随机初始化权重矩阵也为实现细节之一，用于打破对称性（Symmetry Breaking），使得 $\Theta^{(l)}_{ij} \in [-\epsilon,\epsilon]$ 。</p>
<p>Octave 代码：</p>
<p>当然，初始权重的波动也不能太大，一般限定在极小值 $\epsilon$ 范围内，即 $\Theta^{(l)}_{i,j} \in [-\epsilon, \epsilon]$。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">If the dimensions of Theta1 is <span class="number">10</span>x11, Theta2 is <span class="number">10</span>x11 and Theta3 is <span class="number">1</span>x11.</span><br><span class="line"></span><br><span class="line">Theta1 = <span class="built_in">rand</span>(<span class="number">10</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta2 = <span class="built_in">rand</span>(<span class="number">10</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta3 = <span class="built_in">rand</span>(<span class="number">1</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>rand(m,n)</code>: 返回一个在区间 $(0,1)$ 内均匀分布的随机矩阵。</p>
<p>$\epsilon$: 和梯度下降中的 $\epsilon$ 没有联系，这里只是一个任意实数，给定了权重矩阵初始化值的范围。</p>
</blockquote>
<h2 id="9-7-综合起来（Putting-It-Together）"><a href="#9-7-综合起来（Putting-It-Together）" class="headerlink" title="9.7 综合起来（Putting It Together）"></a>9.7 综合起来（Putting It Together）</h2><p>一般来说，应用神经网络有如下步骤：</p>
<ol>
<li><p>神经网络的建模（后续补充）</p>
<ul>
<li>选取特征，确定特征向量 $x$ 的维度，即输入单元的数量。</li>
<li>鉴别分类，确定预测向量 $h_\Theta(x)$ 的维度，即输出单元的数量。</li>
<li>确定隐藏层有几层以及每层隐藏层有多少个隐藏单元。</li>
</ul>
<blockquote>
<p>默认情况下，隐藏层至少要有一层，也可以有多层，层数越多一般意味着效果越好，计算量越大。</p>
</blockquote>
</li>
<li><p>训练神经网络</p>
<ol>
<li><p>随机初始化初始权重矩阵</p>
</li>
<li><p>应用前向传播算法计算初始预测</p>
</li>
<li><p>计算代价函数 $J(\Theta)$ 的值</p>
</li>
<li><p>应用后向传播宣发计算 $J(\Theta)$ 的偏导数</p>
</li>
<li><p>使用梯度检验检查算法的正确性，别忘了用完就禁用它</p>
</li>
<li><p>丢给最优化函数最小化代价函数</p>
<blockquote>
<p>由于神经网络的代价函数非凸，最优化时不一定会收敛在全局最小值处，高级最优化函数能确保收敛在某个<strong>局部</strong>最小值处。</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h2 id="9-8-自主驾驶（Autonomous-Driving）"><a href="#9-8-自主驾驶（Autonomous-Driving）" class="headerlink" title="9.8 自主驾驶（Autonomous Driving）"></a>9.8 自主驾驶（Autonomous Driving）</h2>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>yao
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://zzhenyao.github.io/2024/04/23/12-43-40/" title="机器学习笔记五 神经网络的学习">https://zzhenyao.github.io/2024/04/23/12-43-40/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/" rel="tag"># 机器学习, 深度学习, 吴恩达</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/23/11-06-12/" rel="prev" title="72-优化算法">
                  <i class="fa fa-chevron-left"></i> 72-优化算法
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/04/24/12-18-34/" rel="next" title="动手学深度学习笔记汇总">
                  动手学深度学习笔记汇总 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.0/mermaid.min.js","integrity":"sha256-3JloMMI/ZQx6ryuhhZTsQJQmGAkXeni6PkshX7UUO2s="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"PKunicor","repo":"PKunicor.github.io","client_id":"2efe0e153686e5d9e67d","client_secret":"c84e031b6ac86ce366a6b61640a4b1a8e01d05e0","admin_user":"PKunicor","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"029ba9d46c0d5032baef3aca62da8fa9"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zzhenyao.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="STGCN是一种时空图卷积网络,通过将图神经网络扩展到时空图模型。设计一种用于动作识别的骨架序列的通用表示，称为时空图卷积神经网络。">
<meta property="og:type" content="article">
<meta property="og:title" content="ST-GCN 时空图卷积神经网路">
<meta property="og:url" content="https://zzhenyao.github.io/2024/09/13/10-54-10/index.html">
<meta property="og:site_name" content="且听风吟">
<meta property="og:description" content="STGCN是一种时空图卷积网络,通过将图神经网络扩展到时空图模型。设计一种用于动作识别的骨架序列的通用表示，称为时空图卷积神经网络。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/31983eeeb54690c58ff568866b3d2f13.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/c837aa026e5755538d5406dd26bdb6d2.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/image-20250227134832573.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/2cd7d888fa671be10bd482949b831e2d.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1727574536215.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1727574561853.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1727575199340.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1727575217843.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1727575622534.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1727576201509.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1727576424168.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1726535732803.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1726535781943.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1726535863661.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1726538726736.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1728000528016.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1728022424193.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1728352320911.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/image-20250303145211970.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1726539633950.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/1_8MwnBuSizz7-eUhrQcri5g.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/1_ZxJrjNHXUCk0uhzvd5E97w.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1727597036491.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1727597188885.png">
<meta property="og:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/QQ_1726562068335.png">
<meta property="article:published_time" content="2024-09-13T02:54:10.000Z">
<meta property="article:modified_time" content="2025-05-10T04:59:02.784Z">
<meta property="article:author" content="yao">
<meta property="article:tag" content="深度学习, 计算机视觉">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zzhenyao.github.io/2024/09/13/10-54-10/31983eeeb54690c58ff568866b3d2f13.png">


<link rel="canonical" href="https://zzhenyao.github.io/2024/09/13/10-54-10/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://zzhenyao.github.io/2024/09/13/10-54-10/","path":"2024/09/13/10-54-10/","title":"ST-GCN 时空图卷积神经网路"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ST-GCN 时空图卷积神经网路 | 且听风吟</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">且听风吟</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">轻舟过万重,青山依旧在</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B3%BB%E5%88%97"><span class="nav-number">1.</span> <span class="nav-text">系列</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0"><span class="nav-number"></span> <span class="nav-text">论文笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9"><span class="nav-number">2.</span> <span class="nav-text">主要内容</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ST-GCN%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E5%8A%A8%E6%80%81%E9%AA%A8%E9%AA%BC%E5%BB%BA%E6%A8%A1%E9%80%9A%E7%94%A8%E5%85%AC%E5%BC%8F"><span class="nav-number">2.1.</span> <span class="nav-text">ST-GCN：基于图的动态骨骼建模通用公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ST-GCN-%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%BB%84%E4%BB%B6"><span class="nav-number">2.2.</span> <span class="nav-text">ST-GCN 模型中的组件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A9%BA%E9%97%B4%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.3.</span> <span class="nav-text">空间图卷积神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="nav-number">2.4.</span> <span class="nav-text">分区策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E5%AD%A6%E4%B9%A0%E8%BE%B9%E7%BC%98%E9%87%8D%E8%A6%81%E6%80%A7%E5%8A%A0%E6%9D%83"><span class="nav-number">2.5.</span> <span class="nav-text">可学习边缘重要性加权</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-ST-GCN"><span class="nav-number">3.</span> <span class="nav-text">实现 ST-GCN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E7%9A%84%E8%A7%A3%E6%9E%90"><span class="nav-number">3.1.</span> <span class="nav-text">公式的解析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%AE%AD%E7%BB%83"><span class="nav-number">3.2.</span> <span class="nav-text">网络架构与训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">3.3.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AF%84%E4%BC%B0%E6%A0%87%E5%87%86"><span class="nav-number">3.4.</span> <span class="nav-text">数据集与评估标准</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3"><span class="nav-number"></span> <span class="nav-text">论文理解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A9%BA%E9%97%B4%E7%89%B9%E5%BE%81%E7%9A%84%E6%8F%90%E5%8F%96"><span class="nav-number">1.</span> <span class="nav-text">空间特征的提取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E7%89%B9%E5%BE%81%E7%9A%84%E6%8F%90%E5%8F%96"><span class="nav-number">2.</span> <span class="nav-text">时间特征的提取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">整体网络架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%B1%E5%9B%A0%E6%96%AF%E5%9D%A6%E6%B1%82%E5%92%8C"><span class="nav-number">4.</span> <span class="nav-text">爱因斯坦求和</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ST-GCN%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">5.</span> <span class="nav-text">ST-GCN的缺点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0"><span class="nav-number"></span> <span class="nav-text">代码复现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">源码结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="nav-number">2.</span> <span class="nav-text">环境安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#net-utils-graph-py"><span class="nav-number">3.</span> <span class="nav-text">net&#x2F;utils&#x2F;graph.py</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4"><span class="nav-number">4.</span> <span class="nav-text">相关命令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96%E6%BC%94%E7%A4%BA"><span class="nav-number">4.1.</span> <span class="nav-text">特征可视化演示</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">5.</span> <span class="nav-text">数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#kinetics-skeleton"><span class="nav-number">5.1.</span> <span class="nav-text">kinetics-skeleton</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E9%93%BE%E6%8E%A5"><span class="nav-number"></span> <span class="nav-text">相关链接</span></a></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yao"
      src="/images/logo.png">
  <p class="site-author-name" itemprop="name">yao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">144</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zzhenyao" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzhenyao" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zzhenyao.github.io/2024/09/13/10-54-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.png">
      <meta itemprop="name" content="yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="且听风吟">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ST-GCN 时空图卷积神经网路 | 且听风吟">
      <meta itemprop="description" content="STGCN是一种时空图卷积网络,通过将图神经网络扩展到时空图模型。设计一种用于动作识别的骨架序列的通用表示，称为时空图卷积神经网络。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ST-GCN 时空图卷积神经网路
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-09-13 10:54:10" itemprop="dateCreated datePublished" datetime="2024-09-13T10:54:10+08:00">2024-09-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-05-10 12:59:02" itemprop="dateModified" datetime="2025-05-10T12:59:02+08:00">2025-05-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>



        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
            <div class="post-description">STGCN是一种时空图卷积网络,通过将图神经网络扩展到时空图模型。设计一种用于动作识别的骨架序列的通用表示，称为时空图卷积神经网络。</div>
	<hr>
        <h2 id="系列"><a href="#系列" class="headerlink" title="系列"></a>系列</h2><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">AQA论文汇总</a>
<p>论文名称：Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</p>
<h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><img src="/2024/09/13/10-54-10/31983eeeb54690c58ff568866b3d2f13.png" class>
<p>在这里插入图片描述</p>
<ul>
<li>对视频进行姿态估计，在骨架序列上构造时空图；</li>
<li>在输入数据上应用多层时空图卷积(ST-GCN)，逐步在图上生成更高层次的特征图；</li>
<li>然后由标准 Softmax 分类器将其分类到相应的动作类别。</li>
<li>整个模型采用反向传播的端到端方式训练。</li>
</ul>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>传统的骨骼建模方法依赖于手工制作的部件或遍历规则，表达能力有限和泛化困难，模型很难推广到其他应用。<br>这项工作提出了动态骨架模型。</p>
<h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><ul>
<li>提出了 ST-GCN，这是一种基于图的动态骨骼建模通用公式，这是第一个将基于图的神经网络应用于该任务。</li>
<li>针对骨架建模的具体要求，提出了 ST-GCN 中卷积核的设计原则。</li>
<li>在基于骨骼的动作识别的两个大规模数据集上，与之前使用手工制作部件或遍历规则的方法相比，所提出的模型获得了更好的性能，在手工设计方面的工作量大大减少。ST-GCN 的代码和模型是公开的。</li>
</ul>
<h3 id="ST-GCN：基于图的动态骨骼建模通用公式"><a href="#ST-GCN：基于图的动态骨骼建模通用公式" class="headerlink" title="ST-GCN：基于图的动态骨骼建模通用公式"></a>ST-GCN：基于图的动态骨骼建模通用公式</h3><img src="/2024/09/13/10-54-10/c837aa026e5755538d5406dd26bdb6d2.png" class>
<p>分别为两种边：时间边，空间边。<br>ST-GCN 的层次特性消除了手工制作的部件分配或遍历规则的需要。</p>
<blockquote>
<p>传统的骨架模型方法中，根据预定义的部件分配和遍历规则对骨架进行建模。<br>具体规则：<br>1.人体骨架的构成方式和连接顺序。<br>2.节点对应的身体部位、编号和名称等信息。<br>3.骨架节点的搜索顺序和遍历方式。</p>
</blockquote>
<p><strong>STGCN可以动态学习现有数据的关系规则</strong></p>
<p>ST-GCN 在进行时空图卷积之前，仍然需要对骨架节点进行部件分配和遍历规则的设计。可以根据不同任务，设定不同规则。</p>
<h3 id="ST-GCN-模型中的组件"><a href="#ST-GCN-模型中的组件" class="headerlink" title="ST-GCN 模型中的组件"></a>ST-GCN 模型中的组件</h3><p>在具有 N 个关节和 T 个框架（帧）的骨架序列上构造了无向时空图 $G=(V,E)$</p>
<ul>
<li>节点集$V=\{v_{ti}\left|t=1, \ldots, T;i=1, \ldots, N\right\}$包括骨架序列中的所有关节,节点 $F(v_{ti}\mathrm{~})$ 上的特征向量由帧 $t$ 上第 $i$ 个关节的坐标向量和估计置信度组成。</li>
<li>边集 E:<br>骨架内连接，记为 $E_S=\{v_{ti} v_{tj} |(i,j)\in H\},$，其中 $H$ 为自然连接的人体关节集合<br>帧间边：$E_F=\{v_{ti}v_{(t+1)i}\}$</li>
</ul>
<h3 id="空间图卷积神经网络"><a href="#空间图卷积神经网络" class="headerlink" title="空间图卷积神经网络"></a>空间图卷积神经网络</h3><h4 id="单帧的图CNN模型"><a href="#单帧的图CNN模型" class="headerlink" title="单帧的图CNN模型"></a>单帧的图CNN模型</h4><p>在时间 $\tau$ 的单个帧上，将有 $N$ 个关节节点 $V_t$​，以及骨架边 $E_S\left(\tau\right)=\{v_{ti} v_{tj} |t=\tau , (i,j)\in H\}$。</p>
<p>给定核尺寸为 $K×K$ 的卷积算子，以及通道数量为 $c$ 的输入特征映射 $f_{in}$​。在空间位置 $x$ 处，单个通道的输出值可以写成：</p>
<p>$\displaystyle f_{out}\left(x\right)=\sum_{h=1}^K\sum_{w=1}^Kf_{in}\left(\mathbf{p}(\mathbf{x},h,w)\right)\cdotp\mathbf{w}(h,w)$</p>
<p><img src="/2024/09/13/10-54-10/image-20250227134832573.png" alt="image-20250227134832573"></p>
<p>抽样函数 $\mathbf{p}$：$Z^2\times Z^2\to Z^2$ 列举了空间位置 $x$ 的邻居的位置。在图像卷积，它也可以表示为 $\mathbf{p}(\mathbf{x},h,w)=\mathbf{x}+\mathbf{p}^{\prime}(h,w)$。权函数 $\mathbf{w}$:$Z^2\to\mathbb{R}^c$ 提供了一个 $c$ 维实空间中的权向量，用于计算与采样的 $c$ 维输入特征向量的内积。注意，权函数与输入位置 $x$ 无关。因此，在输入图像滤波器权重到处都是共享的。</p>
<blockquote>
<p>$\mathbf{p}^{\prime}(h,w)$：映射一个方向访问函数，类似象棋中的[0,-1],[1,0];</p>
<p>抽样函数 $\mathbf{p}$： 就是一个编码的定位函数，映射$x$位置附近的卷积核内对应的位置。</p>
</blockquote>
<p>从二维抽象到三维：<br>通过将上述公式扩展到输入特征映射位于空间图 $V_t$​ 上的情况，来定义图上的卷积操作。即特征映射 $f_{in}^t$​：$V_t​→R^c$ 在图的每个节点上都有一个向量。扩展的下一步是重新定义抽样函数 p 和权重函数 w。</p>
<h4 id="采样函数"><a href="#采样函数" class="headerlink" title="采样函数"></a>采样函数</h4><p>在图像上，采样函数 $\mathbf{p}(h,w)$ 是在相邻像素点关于中心位置 $x$ 上定义的。在图上，我们同样可以在节点 $v_{ti}$​ 的邻居集 $B(v_{ti})=\{v_{tj}|d(v_{tj},v_{ti})\leq D\}$ 上定义采样函数。这里 $d(v_{tj},v_{ti}\mathrm{~})$ 表示从 $v_{tj}$​ 到 $v_{ti}$​ 的任何路径的最小长度。因此，抽样函数 $\mathbf{p}:B(v_{ti})\to V$ 可以写成：<br>$\mathbf{p}(v_{ti},v_{tj})=v_{tj}$</p>
<blockquote>
<p>注意这里的 $v_{tj}$ 是节点 $v_{ti}$ 的邻居集 $B(v_{ti})$ 中的节点，即当 $D=1$ 时，采样函数 $\mathbf{p}$ 取的是邻接点。</p>
</blockquote>
<h4 id="权重函数"><a href="#权重函数" class="headerlink" title="权重函数"></a>权重函数</h4><p>主要是如何定义图的顺序，保证权重可以按顺序对邻接点作用。</p>
<p>将一个关节点 $v_{ti}$ 的邻居集 $B(v_{ti})$ 划分为固定数量的 $K$ 个子集，其中每个子集都有一个数字标签，从 $0$ 到 $K-1$。</p>
<p>映射 $l_{ti}: B(v_{ti} )\to\{0, \ldots, K-1\}$，<br>权重函数 $\mathbf{w}(v_{ti},v_{tj}):B(v_{ti})\to R^c$ 可以通过索引一个 $(c,K)$ 维张量或<br>$\mathbf{w}(v_{ti},v_{tj})=\mathbf{w}^{\prime}(l_{ti}\left(v_{tj}\right))$<br>来实现。</p>
<blockquote>
<p>即相同标签会使用同一个权重，保持权重函数不变，拓展到每个节点。</p>
<p>c：指通道数</p>
</blockquote>
<h4 id="空间图卷积"><a href="#空间图卷积" class="headerlink" title="空间图卷积"></a>空间图卷积</h4><p>有了改进的抽样函数和权函数，我们现在用图卷积重写公式(1)如下：<br>$\displaystyle f_{out}\left(v_{ti}\right)=\sum_{v_{tj}\in B(v_{ti})}\frac1{Z_{ti}\left(v_{tj}\right)}f_{in}\left(\mathbf{p}(v_{ti},v_{tj})\right)\cdotp\mathbf{w}(v_{ti},v_{tj})$<br>其中归一项 $Z_{ti}\left(v_{tj}\right.)=\left|v_{tk}\left|l_{ti}\left(v_{tk}\right.\right)=l_{ti}\left(v_{tj}\right.\right)|$ 等于相应子集的基数。为了平衡不同子集对输出的贡献。得到<br>$\displaystyle f_{out}\left(v_{ti}\right)=\sum_{v_{tj}\in B\left(v_{ti}\right)}\frac1{Z_{ti}\left(v_{tj}\right)}f_{in}\left(v_{tj}\right)\cdotp\mathbf{w}(l_{ti}\left(v_{tj}\right))$</p>
<h4 id="时空建模"><a href="#时空建模" class="headerlink" title="时空建模"></a>时空建模</h4><p>需要重新定义相邻位置的关系，增加时间相邻点。</p>
<blockquote>
<p>将2D图像增加时间维度，转换为3D图像，应用到图卷积神经网络。</p>
</blockquote>
<p>$\displaystyle B(v_{ti})=\{v_{qj}\left|d(v_{tj},v_{ti})\leq K,\left|q-t\right|\leq\left\lfloor\Gamma/2\right\rfloor\right\}$<br>该公式定义了每个节点的邻域 $B(v_{ti})$ 包括空间距离不超过 $K $个单位的所有节点和时间距离不超过 $\lfloor\Gamma/2\rfloor$ 个单位的所有节点。其中，参数 $\Gamma$ 控制时间上要包含在邻域图中的范围，因此可以称为时间内核大小。</p>
<p>重新修改映射标签：$\displaystyle l_{ST}\left(v_{qj}\right)=l_{ti}\left(v_{tj}\right)+\left(q-t+\left\lfloor\Gamma/2\right\rfloor\right)\times K$<br>其中 $l_{ti}\left(v_{tj}\right)$ 是在一个单帧 $t$ 上关节 $i$ 的邻接点 $j$ 的标签映射，而 $(q-t+\lfloor\Gamma/2\rfloor)\times K$ 用于在时间维度上对标签进行编码。</p>
<blockquote>
<p>对时间的前后正负，转移到正数轴上，乘$K$，将所有的数据转移到一维数组内的顺序。可以通过每一个下标，定位到第几个节点的第几个时间帧内。</p>
<p>注：时间顺序(-1,0,1)，变换映射到(0,1,2)的三组K维标签上。</p>
</blockquote>
<h3 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h3><p>标签映射：将相邻的接点映射到标签内。<br>标签划分：如何确定接点的属于哪个标签。</p>
<img src="/2024/09/13/10-54-10/2cd7d888fa671be10bd482949b831e2d.png" class>
<p>上图是构造卷积运算的划分策略。从左到右：</p>
<p>（a）输入骨架示例帧。身体关节用蓝点绘制。D=1 的过滤器的接受域用红色虚线圈绘制。</p>
<p>（b）单标签分区策略，即一个邻域内的所有节点都有相同的标签(绿色)。</p>
<p>（c）距离分区策略，这两个子集是距离为0的根节点本身(绿色)和距离为1的其他相邻点(蓝色)。</p>
<p>（d）空间结构分区策略，节点根据与骨架重心(黑色十字)的距离分为：根节点(绿色)的距离最近，向心节点的距离较短(蓝色)，而离心节点的距离较长(黄色)。</p>
<p>相同标签的子集，使用相同的权重。</p>
<p>Uni-labeling 单标签分区策略</p>
<p>所有节点一个标签，只有一种权重。</p>
<p>istance partitioning 距离分区策略</p>
<p>$D$ = 1,的情况下，只有两种权重，分为两个子集。</p>
<p>Spatial configuration partitioning 空间结构分区策略</p>
<p><strong>将节点集分为三个子集：</strong>  只有一种连接划分，没有节点划分<br>1)根节点；距离中心节点最近的节点</p>
<p>2)近节点：距离跟第二近的节点</p>
<p>3)否则远连接。其他节点</p>
<p><strong>将节点之间的连接分为三个子集：</strong><br>1)根连接；如果两个节点与中心节点跳距相同，视为根连接<br>具有对称性</p>
<p>2)近连接：一个节点比另一个节点距离中心节点更近<br>j&gt;i,则$adj[j][i]$属于近连接，看终点的距离 </p>
<p>3)否则远连接。</p>
<p>$\begin{aligned}<br>l_{ti}\left(v_{tj}\right)=<br>\begin{cases}<br>0, &amp; r_j=r_i \\<br>1, &amp; r_j &lt; r_i \\<br>2, &amp; r_j &gt; r_i<br>\end{cases}<br>\end{aligned}$</p>
<p>其中 $r_i$​ 是训练集中所有帧中从重心到关节 $i$ 的平均距离。</p>
<blockquote>
<p>会不会出现三种标签，只有两种：如端节点</p>
</blockquote>
<h3 id="可学习边缘重要性加权"><a href="#可学习边缘重要性加权" class="headerlink" title="可学习边缘重要性加权"></a>可学习边缘重要性加权</h3><p>我们在时空图卷积的每一层上都加上一个可学习的掩码 $M$。<br>掩码将根据 $E_s$​ 中每个空间图边的学习重要性权重，将节点的特征贡献扩展到其邻近节点。</p>
<p><strong>作者认为未来改进方向</strong></p>
<ul>
<li>也可以使用一个依赖于数据的注意力图，来提升空间特征的贡献扩展效果。</li>
</ul>
<h2 id="实现-ST-GCN"><a href="#实现-ST-GCN" class="headerlink" title="实现 ST-GCN"></a>实现 ST-GCN</h2><p>我们采用类似于(Kipf和Welling 2017)中的图卷积实现。单帧内关节的体内连接由表示自连接的邻接矩阵 $A$ 和单位矩阵 $I$ 表示。<br>在单帧情况下，采用第一个分区策略的 ST-GCN 可以用以下公式实现<br> $\mathbf{f}_{out}=\mathbf{\Lambda}^{ {-\frac{1}{2}} }(\mathbf{A}+\mathbf{I})\mathbf{\Lambda}^{ {-\frac{1}{2}} }\mathbf{f}_{in}\mathbf{W}$<br>解释：</p>
<ul>
<li>$\mathbf{f}_{in}$: 输入特征矩阵,每行对应一个节点的特征向量,是一个行向量，具有多个特征。</li>
<li>$\mathbf{\Lambda}$: 度矩阵</li>
<li>$\mathbf{\Lambda}^{ {-\frac{1}{2}} }$: 平方根的倒数，用于归一化</li>
<li>$\mathbf{A}$: 邻接矩阵, 行$i$是出发点，列$j$是连接点</li>
<li>$\mathbf{I}$: 单位矩阵，表示自环</li>
</ul>
<h3 id="公式的解析"><a href="#公式的解析" class="headerlink" title="公式的解析"></a>公式的解析</h3><p>(此公式来源于GCN中，ST-GCN没有做修改)</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727574536215.png" alt=" "></p>
<p><img src="/2024/09/13/10-54-10/QQ_1727574561853.png" alt=" "></p>
<p><img src="/2024/09/13/10-54-10/QQ_1727575199340.png" alt=" "></p>
<p>$D^{-1}$：代表了节点的度，用于平衡节点贡献。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727575217843.png" alt=" "></p>
<blockquote>
<p>用公式法展开算一遍，就能算出上面的公式。</p>
</blockquote>
<p><img src="/2024/09/13/10-54-10/QQ_1727575622534.png" alt=" "></p>
<p>$\begin{aligned}<br>\bar{\boldsymbol{x}}_i &amp;= \sum_{j=1}^n \tilde{a}_{i,j} \boldsymbol{x}_j  \\<br>&amp;= \sum_{j \in \text{Neigh}(i)} \tilde{a}_{i,j} \boldsymbol{x}_j  \\<br>&amp;= \sum_{j \in \text{Neigh}(i)} \frac{1}{\sqrt{d_{i,i} d_{j,j} } } \boldsymbol{x}_j<br>\end{aligned}$</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727576201509.png" alt=" "></p>
<blockquote>
<p>$W$:控制下一个layer的输入维度。做特征维度变换。</p>
<p>$A$:做特征聚合。</p>
</blockquote>
<p><img src="/2024/09/13/10-54-10/QQ_1727576424168.png" alt=" "></p>
<p>上图中：初始输入$x_i$，经过矩阵$A$，转变为下一层的输入向量。数学表示为：</p>
<p>$\begin{aligned}<br>\boldsymbol{H}_1 &amp;:= f_{\boldsymbol{W}_1}(\boldsymbol{X}, \boldsymbol{A}) \\ \boldsymbol{H}_2 &amp;:= f_{\boldsymbol{W}_2}(\boldsymbol{H}_1, \boldsymbol{A}) \\ \boldsymbol{H}_3 &amp;:= f_{\boldsymbol{W}_3}(\boldsymbol{H}_2, \boldsymbol{A})<br>\end{aligned}$</p>
<p>公式计算过程</p>
<ul>
<li><p>自环矩阵：$\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}$</p>
</li>
<li><p>对称归一化：$\tilde{\mathbf{A}}_{\mathrm{norm}}=\Lambda^{-\frac12}\tilde{\mathbf{A}}\Lambda^{-\frac12}$</p>
<blockquote>
<p>为什么不采均值归一化，高度节点在聚合过程中，与低度节点具有不同的作用，削弱高度节点的影响，防止多重传播信息爆炸。</p>
</blockquote>
<p>保持了数值的稳定性，防止特征信息在传播中过度缩放。<br>过程：</p>
<blockquote>
<p>左行右列：<br>左乘：是i行出发节点</p>
<p>右乘：是j列到达节点</p>
</blockquote>
<ol>
<li>先右乘$\Lambda^{-\frac12}$：对每个达到节点的邻接节点进行缩放,高度节点，影响被缩小了。</li>
<li>再左乘$\Lambda^{-\frac12}$：对出发节点的邻接节点进行缩放，保持传播平衡，避免单向影响。即添加了信息回传的平衡。</li>
</ol>
</li>
<li><p>输入特征与邻接矩阵传播: $\tilde{\mathbf{A}}_{\mathrm{norm}}\mathbf{f}_{\mathrm{in}}$每个节点将从它的邻居节点中接收特征并进行聚合。<br>过程：</p>
<p>$\begin{aligned}<br>\mathbf{f}_{\mathrm{in}}=\begin{bmatrix}\mathbf{f}_1 \\<br>\mathbf{f}_2 \\<br>\vdots \\<br>\mathbf{f}_N\end{bmatrix}\in\mathbb{R}^{N\times F_{\mathrm{in} } }<br>\end{aligned}$</p>
</li>
</ul>
<ul>
<li>特征传播公式：$\mathbf{f}_{\mathrm{prop}}=\mathbf{Af}_{\mathrm{in}}$</li>
<li>对于每个节点$i$，它的新特征$\displaystyle \mathbf{f}_{i,\mathrm{prop}}$，可以表示为：$\displaystyle \mathbf{f}_{i,\mathrm{prop}}=\sum_{j\in\mathcal{N}(i)}A_{ij}\mathbf{f}_j$<br>其中，$\mathcal{N}(i)$: 表示$i$的所有邻接节点</li>
<li>节点$i$的新特征向量是其所有邻居节点特征向量的加权和</li>
</ul>
<ul>
<li>假设$N$个节点，$d_{in}$维的输入特征: $\tilde{\mathbf{A}}_{\mathrm{norm}}f_{\mathrm{in}}$<br>表示将邻居节点的信息通过归一化邻接矩阵传播给每个节点。每个节点的新特征是根据其相邻节点的特征进行加权求和</li>
</ul>
<blockquote>
<p>使用自连接矩阵，是为了用邻接矩阵算度</p>
</blockquote>
<p> 其中$\Lambda^{ii}=\sum_j(A^{ij}+I^{ij})$，度矩阵。$\mathbf{W}$多个通道叠加的权重。<br>实际中在时空的情况下，输入维度为$(C,V,T)$的特征张量。</p>
<p>在有多个子集的分区中，将邻接矩阵分解为几个矩阵：例如距离划分策略中，$\mathbf{A}_0=\mathbf{I}$$   ，\mathbf{A}_1=\mathbf{A}$ 表达式带入得到：<br>$\displaystyle \mathbf{f}_{out}=\sum_j\boldsymbol{\Lambda}_j^{-\frac12}\mathbf{A}_j\boldsymbol{\Lambda}_j^{-\frac12}\mathbf{f}_{in}\mathbf{W}_j$<br>将$\displaystyle \Lambda_j^{ii}=\sum_k(A_j^{ik})+\alpha $，$\alpha = 0.001$,避免度矩阵出现空行。</p>
<p>科学系边缘重要性加权的实现：使用一个可学习掩码权重矩阵$\text{M}$，与矩阵进行一个对应位置相乘的算法。用$(\mathbf{A}+\mathbf{I})\otimes\mathbf{M}$代替$(\mathbf{A}+\mathbf{I})$，用$\mathbf{A}_j\otimes\mathbf{M}$代替$\mathbf{A}_j$。$\text{M}$被初始化为全1的矩阵。</p>
<h3 id="网络架构与训练"><a href="#网络架构与训练" class="headerlink" title="网络架构与训练"></a>网络架构与训练</h3><p>由于 ST-GCN 在不同节点上共享权重，因此在不同节点上保持输入数据的比例一致是很重要的。首先将输入骨架提供给批处理规范化层来规范化数据。</p>
<p>ST-GCN 模型由9层时空图卷积算子(ST-GCN 单元)组成。前三层有64个输出通道，接着三层有128个输出通道，最后三层有256个输出通道。这些层有9个时间内核大小。<strong>Resnet 机制应用于每个 ST-GCN 单元</strong>。</p>
<p>在每个 ST-GCN 单元后，我们<strong>以0.5概率随机剔除特征，以避免过拟合</strong>。第4和第7时序卷积层的步长设为2作为池化层。然后对得到的张量进行全局池化，得到每个序列的256维特征向量。最后，我们将它们输入 <strong>SoftMax</strong> 分类器。使用随机梯度下降学习模型，学习率为0.01。我们在每10个 epoch 之后将学习率衰减0.1。</p>
<p>为了避免过拟合，我们在动力学数据集上训练时执行<strong>两种增强</strong>来替换掉层(Kay et al 2017)。</p>
<p>首先，为了模拟相机运动，我们对所有帧的骨架序列执行随机仿射变换。特别是从第一帧到最后一帧，我们选取了几个固定的角度、平移和比例因子作为候选因子，然后随机采样三个因子的两个组合来生成仿射变换。这种转换被插入到中间帧中，以产生一种效果，就好像我们在回放过程中平滑地移动视点一样。我们把这种增强称为随机移动。</p>
<p>其次，我们在训练中从原始骨架序列中随机抽取片段，并在测试中使用所有帧。网络顶部的全局池使网络能够处理不确定长度的输入序列。</p>
<blockquote>
<p>不确定长度的输入序列，长视频？</p>
</blockquote>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>两个数据集：Kinetics 人类动作数据集(Kinetics) (Kay et al 2017)，NTURGB+D (Shahroudy et al 2016)。</p>
<p>首先对动力学数据集进行详细的<strong>消融研究</strong>，以检查所提出的模型组件对识别性能的贡献。</p>
<p>为了验证我们在无约束设置中获得的经验是否具有普遍性，我们对 NTURGB+D 上的约束设置进行了实验，并将 ST-GCN 与其他最先进的方法进行了比较。所有实验均在 PyTorch 深度学习框架上进行，并配有8个 TITANX 图形处理器。</p>
<h3 id="数据集与评估标准"><a href="#数据集与评估标准" class="headerlink" title="数据集与评估标准"></a>数据集与评估标准</h3><p><strong>Kinetics-skeleton数据集</strong></p>
<p>该数据集包含从YouTube检索的大约300000个视频片段。且划分240000个视频片段为训练集、20000个视频片段为验证集。这些视频涵盖了多达400个人类动作类，从日常活动、体育场景到复杂的互动动作。每个视频片段大约10秒。该数据集仅提供没有骨架数据的原始视频片段。</p>
<p>为了获得骨架数据，作者首先将所有视频的分辨率调整为$340×256$，并将帧速率转换为$30 FPS$。然后，作者使用OpenPose姿态估计工具来估计视频片段中的每个帧上的18个关节的位置。OpenPose给出了像素坐标系中的$2D$坐标$(X，Y)$和18个人体关节的置信度得分。因此，作者使用一个元组($X，Y，C)$来表示每个关节，一个骨架帧被记录为一个18元组的数组。对于多人情况，作者在每个片段中选择平均联合置信度最高的2个人。</p>
<h1 id="论文理解"><a href="#论文理解" class="headerlink" title="论文理解"></a>论文理解</h1><p>ST-GCN 这篇论文算是 GCN 在骨骼行为识别里面的开山之作了，他提供了一种新的思路和实现方式，解决了以前方法的局限性并取得了较好的效果。虽然他只是2018年发表的，但是这篇论文给了很详细的代码，2019年发表在 CVPR 上的 AS-GCN 和 2s-AGCN 都是在该代码的基础上改进的。</p>
<h2 id="空间特征的提取"><a href="#空间特征的提取" class="headerlink" title="空间特征的提取"></a>空间特征的提取</h2><h4 id="1-图卷积"><a href="#1-图卷积" class="headerlink" title="1. 图卷积"></a>1. 图卷积</h4><p>特征提取器，输入结点特征和图结构，输出结点最终的特征表达。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1726535732803.png" alt></p>
<p>主要的框架就是这样，图卷积相比于图像卷积，只是多左乘了一个邻接矩阵 <strong>A</strong>，后面就是一些细节。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1726535781943.png" alt></p>
<h4 id="2-感受野"><a href="#2-感受野" class="headerlink" title="2. 感受野"></a>2. 感受野</h4><p>一般图像二维卷积最小的卷积核就是 3×3 的卷积核，感受野就是一个中心点和周围八个元素共九个元素的组合。</p>
<p>这里和CNN相似，定义离中心点距离 <em>D</em>=1 ，也就是<strong>与中心点直接相连的点</strong>为一个卷积核的<strong>感受野</strong>。如图（a）所示：</p>
<p><img src="/2024/09/13/10-54-10/QQ_1726535863661.png" alt></p>
<h4 id="3-卷积核"><a href="#3-卷积核" class="headerlink" title="3. 卷积核"></a>3. 卷积核</h4><p>对应论文中的分区策略。选择的空间结构分区策略</p>
<p>选择合适的感受野和卷积核之后就能够像 CNN 那样<strong>一个点一个点的卷积计算</strong>了，<strong>卷积的过程就是提取特征的过程</strong>。</p>
<h4 id="4-注意力机制"><a href="#4-注意力机制" class="headerlink" title="4. 注意力机制"></a>4. 注意力机制</h4><p>就是增加的可学习权重掩码。</p>
<h2 id="时间特征的提取"><a href="#时间特征的提取" class="headerlink" title="时间特征的提取"></a>时间特征的提取</h2><p><img src="/2024/09/13/10-54-10/QQ_1726538726736.png" alt=" "></p>
<h2 id="整体网络架构"><a href="#整体网络架构" class="headerlink" title="整体网络架构"></a>整体网络架构</h2><p><img src="/2024/09/13/10-54-10/QQ_1728000528016.png" alt=" "></p>
<p>输出格式：</p>
<p>每个通道的数据为(T,V), 代表300帧的18个节点。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1728022424193.png" alt=" "></p>
<p>卷积：</p>
<p>空间卷积通过1x1卷积，时间卷积9x1，<code>指的是时间和空间维度，不是通道维度</code></p>
<h5 id="空间卷积：对所有时间和空间做1x1卷积。矩阵乘法会转置，图中没有转置"><a href="#空间卷积：对所有时间和空间做1x1卷积。矩阵乘法会转置，图中没有转置" class="headerlink" title="空间卷积：对所有时间和空间做1x1卷积。矩阵乘法会转置，图中没有转置"></a>空间卷积：对所有时间和空间做1x1卷积。<code>矩阵乘法会转置，图中没有转置</code></h5><blockquote>
<p>二维卷积，一个组卷积核，输出一个值。</p>
</blockquote>
<p><img src="/2024/09/13/10-54-10/QQ_1728352320911.png" alt=" "></p>
<p><img src="/2024/09/13/10-54-10/image-20250303145211970.png" alt="image-20250303145211970"></p>
<p>一层GCN中，输出的特征维度为$f_{in}*3$，3来源于子集划分。一共3组卷积核，一组卷积核有$C$个$(1,1,C)$的卷积核。</p>
<blockquote>
<p>所有子集特征叠加，不会出问题吗？ 不应该保留三种子集的特征吗？</p>
<p>不会出问题：三组卷积核处理不同的子集，最后叠加，不属于这个子集的，没有被处理。</p>
</blockquote>
<h5 id="时间卷积："><a href="#时间卷积：" class="headerlink" title="时间卷积："></a>时间卷积：</h5><p><img src="/2024/09/13/10-54-10/QQ_1726539633950.png" alt=" "></p>
<p><img src="/2024/09/13/10-54-10/1_8MwnBuSizz7-eUhrQcri5g.png" alt=" "><br>左边是向前传递函数，右边是ST-GCN网络。</p>
<ul>
<li>N：批量大小。</li>
<li>C：原始节点特征，即（坐标-X/坐标-Y/置信度）三元组。</li>
<li>T：时间步长。</li>
<li>V：图中的节点数。</li>
<li>M： 一个帧的数据记录中的骨骼数量。</li>
</ul>
<p>代码解释：</p>
<p><img src="/2024/09/13/10-54-10/1_ZxJrjNHXUCk0uhzvd5E97w.png" alt=" "></p>
<p>代码的实现是，先进行传递到下层，再进行信息聚合。</p>
<h2 id="爱因斯坦求和"><a href="#爱因斯坦求和" class="headerlink" title="爱因斯坦求和"></a>爱因斯坦求和</h2><p>口诀：</p>
<p>外部重复做乘积 ,</p>
<p>内部重复把数取 ,</p>
<p>从有到无要求和 ,</p>
<p>重复默认要丢弃.</p>
<blockquote>
<p>$\text{result}[n,c,t,w]=\sum_{k=1}^K\sum_{v=1}^Vx[n,k,c,t,v]\cdot A[k,v,w]$</p>
<p>对A进行广播，扩充$n$维，公式变为：</p>
<p>$\text{result}[c,t,w]=\sum_{k=1}^K\sum_{v=1}^Vx[k,c,t,v]\cdot A[k,v,w]$</p>
<p>$\text{result}[c,t,w]=\sum_{k=1}^K(\sum_{v=1}^Vx[k,c,t,v]\cdot A[k,v,w])$</p>
<p>对于内部：广播A，变成：$(\sum_{v=1}^Vx[k,c,t,v]\cdot A[k,c,v,w])$</p>
<p>即变为：$\text{result}^1[k,c,t,w]=(\sum_{v=1}^Vx[k,c,t,v]\cdot A[k,c,v,w])$</p>
<p>表示为第k，c维下的第$t$帧的$w$节点的特征，等于第$t$帧所有节点特征乘邻接矩阵$A$的第$w$列向量。(等于聚合了w节点的其他相邻节点特征)</p>
<p>$\text{result}[c,t,w]=\sum_{k=1}^K(\text{result}^1[k,c,t,w])$</p>
<p>表示对新的特征向量进行k重叠加。（把GCN中的输出通道拉长3倍，又消掉了）</p>
</blockquote>
<ul>
<li>$AX_1$ 与 $X_2A$ 的的区别：$A:<v,w>$,$X_1:<t,v,c>$$X_2:<c,t,v>$</c,t,v></t,v,c></v,w></li>
</ul>
<p>与输入对第t帧卷积不同，爱因斯坦求和约定，是对第c通道的第t帧进行的。</p>
<blockquote>
<p>也就是说公式计算的是正方体的上截面，而代码计算的是前截面。所以计算方式不同！！</p>
<p>左行右列：$X_2$的行向量是t向量，列向量是v向量，所以对列操作，聚合不同帧t下的节点。</p>
</blockquote>
<p>只是完成了公式中一个邻接矩阵聚合。</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727597036491.png" alt=" "></p>
<p>在对节点进行空间卷积之后，它转向<em>对每个节点分别</em>进行时间卷积。我们注意到这里的元组<code>kernel=(temporal_ker, 1)</code>，其中<code>temporal_ker</code>是硬编码的<code>9</code>（9个时间步骤）。</p>
<p>主训练循环在文件“recognition.py”  中定义，其内容很典型。<em>虽然论文中没有讨论</em>，但实现利用了<em>每个 ST-GCN 块中的</em>残差连接（又名跳过连接）来处理梯度退化。由于输入输出通道不同，连接实际上是一个微型神经网络（而不是恒等连接）：</p>
<p><img src="/2024/09/13/10-54-10/QQ_1727597188885.png" alt=" "></p>
<p>在 ST-GCN 块中，为了防止梯度爆炸并实现更稳定的收敛，在非线性层之前和之后使用 BatchNorm，这是经常建议的。DropOut 层也有助于减少过度拟合 。但代码使用 Dropout 的默认设置（意思是：概率<code>0.5</code>），而卷积层的推荐值为<code>0.1</code>或<code>0.2</code>。</p>
<h2 id="ST-GCN的缺点"><a href="#ST-GCN的缺点" class="headerlink" title="ST-GCN的缺点"></a>ST-GCN的缺点</h2><p>在上述ST-GCN网络模型的表述中，不难发现，虽然其拥有许多优点，但是ST-GCN的缺点也十分明显：</p>
<ul>
<li><p>ST-GCN中的人体骨架图是人为手工定义的，限制后面的网络学习到骨架图中的必要关联与新建连接，例如：在识别如“鼓掌”，“阅读”等与人为手工定义的人体骨架图空间上相距甚远的双手间的关系至关重要的动作时，网络很难学习到其中的关联，也很难对该原本不存在的边进行新建以建立连接，很大程度上降低了动作识别的精度；</p>
</li>
<li><p>除此之外，ST-GCN中的网络结构是分层结构，每一层都代表了不同层次的语义信息，然而作者所采用的是一个全局固定的网络拓扑结构图，导致该网络无法灵活的对各层中的语义进行建模，且由于固定的全局网络拓扑结构，其感受野野不够灵活，也无法提取多尺度的信息；</p>
</li>
<li><p>ST-GCN计算复杂度过高，且由于图卷积各节点之间的信息相互聚合，传统dropout对其无效，导致过拟合，根据此项工作，许多科研人员在此基础上做了大量的相关工作，由于手工定义图的局限性，2s-AGCN与MS-AAGCN引入了多种自适应的拓扑图，而对于分层结构，为方便对各层语义进行建模，SGCN提出了一个通道融合模块 (CAMM)；最后由于高计算复杂度shiftGCN将shift操作引入图卷积代替卷积的特征融合操作，除此之外DC-GCN_ADG提出了DC-GCN（DeCoupling Graph Convolutional Network）图卷积网络，可以在不增加计算量的同时增强图卷积的表达能力，并提出了ADG（attention-guided DropGraph）取代原始dropout，解决了图卷积过拟合的问题。</p>
</li>
</ul>
<h1 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h1><h2 id="源码结构"><a href="#源码结构" class="headerlink" title="源码结构"></a>源码结构</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config/              包含不同模型训练的配置文件</span><br><span class="line">data/                用于数据加载，处理骨架数据集。</span><br><span class="line">feeder/              数据预处理模块，读取并规范化骨架数据。</span><br><span class="line">models/              训练好的模型参数</span><br><span class="line">net/                包含ST-GCN网络的具体实现和其他模型文件</span><br><span class="line">--utils/            utilities实用工具包含一些通用函数，类或模块</span><br><span class="line">----graph.py        图的定义，节点分集</span><br><span class="line">----tgcn.py          时间图卷积网络</span><br><span class="line">----__init__.py      导入utils包</span><br><span class="line">----st_gcn.py     </span><br><span class="line">----st_gcn_twostream.py</span><br><span class="line">processor/          初始化相关模块</span><br><span class="line">----io.py            模型加载</span><br><span class="line">----processor.py    重载模型加载</span><br><span class="line">----recognition.py  识别模块</span><br><span class="line">resource/            辅助工具，如骨架数据可视化。</span><br><span class="line">tools/              训练、测试和评估的脚本。</span><br><span class="line">torchlight/  </span><br><span class="line">main.py              项目的主入口</span><br><span class="line">requirements.txt    项目所需软件包</span><br></pre></td></tr></table></figure>
<h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><p>需要先安装torchlight包</p>
<p>修改安装路径的方式：</p>
<ul>
<li><del>指定安装目录</del>：<br>pip安装的版本有问题，需要进入torchlight文件夹，运行stepup.py安装，与pip安装不同，setup.py将文件安装到了<code>/usr/lib/python3.8/site-packages/</code>,<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python setup.py install --prefix=/usr/local</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>进入torchlight文件夹，运行<code>pip install .</code>，可以安装成功</strong></p>
<p>远程同步排除文件夹：<code>./data</code>, <code>./work_dir</code></p>
<h2 id="net-utils-graph-py"><a href="#net-utils-graph-py" class="headerlink" title="net/utils/graph.py"></a>net/utils/graph.py</h2><p>图的定义函数：</p>
<p><img src="/2024/09/13/10-54-10/QQ_1726562068335.png" alt=" "></p>
<p>上图表示，openpose关节点邻接顺序。</p>
<p>矩阵的幂运算：<code>np.linalg.matrix_power(A, d)</code>:表示邻接矩阵的d跳可达关系。</p>
<h2 id="相关命令"><a href="#相关命令" class="headerlink" title="相关命令"></a>相关命令</h2><ul>
<li>测试命令：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python main.py recognition --phase test --work_dir ./work_dir/recognition/kinetics_skeleton --config ./config/kinetics.yaml --weights ./models/model.pth --test_batch_size 64 --device 0 --save_result True</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python main.py recognition -c config/st_gcn/kinetics-skeleton/test.yaml</span><br></pre></td></tr></table></figure>
<ul>
<li>训练命令</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python main.py recognition -c config/st_gcn/kinetics-skeleton/train.yaml</span><br></pre></td></tr></table></figure>
<p>测试命令<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python main.py recognition -c config/st_gcn/kinetics-skeleton/test.yaml --weights ./work_dir/recognition/kinetics_skeleton/ST_GCN/epoch50_model.pt</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>训练相关</p>
<p>train.yaml</p>
<ul>
<li><p>feeder 通常是指数据加载器（Data Feeder）</p>
</li>
<li><p>修改config文件夹内的，train.yaml，设置显卡编号为单显卡。</p>
</li>
<li>减少batch_size = 64</li>
</ul>
<h3 id="特征可视化演示"><a href="#特征可视化演示" class="headerlink" title="特征可视化演示"></a>特征可视化演示</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">python main.py demo_offline --video ./resource/media/skateboarding.mp4 --openpose /root/share/openpose/build</span><br><span class="line"></span><br><span class="line">python main.py demo_offline --video video_1.mp4 --openpose /root/share/openpose/build</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">特征可视化：</span><br><span class="line">python main.py feature_visualization  --video video_1.mp4 --openpose /root/share/openpose/build</span><br><span class="line">python main.py feature_visualization  --video video_2_1.mp4 --openpose /root/share/openpose/build</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">python main.py get_pose</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">torch源码</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># nn/conv.py</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"> def __init__(</span><br><span class="line">        self,</span><br><span class="line">        in_channels: int,</span><br><span class="line">        out_channels: int,</span><br><span class="line">        kernel_size: _size_2_t,</span><br><span class="line">        stride: _size_2_t = 1,</span><br><span class="line">        padding: Union[str, _size_2_t] = 0,</span><br><span class="line">        dilation: _size_2_t = 1,</span><br><span class="line">        groups: int = 1,</span><br><span class="line">        bias: bool = True,</span><br><span class="line">        padding_mode: str = &#x27;zeros&#x27;,  # TODO: refine this type</span><br><span class="line">        device=None,</span><br><span class="line">        dtype=None</span><br><span class="line">    ) -&gt; None:</span><br><span class="line">        factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125;</span><br><span class="line">        kernel_size_ = _pair(kernel_size)</span><br><span class="line">        stride_ = _pair(stride)</span><br><span class="line">        padding_ = padding if isinstance(padding, str) else _pair(padding)</span><br><span class="line">        dilation_ = _pair(dilation)</span><br><span class="line">        super().__init__(</span><br><span class="line">            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,</span><br><span class="line">            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)</span><br><span class="line">       </span><br></pre></td></tr></table></figure>
<p>参数解释：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">1.  in_channels: 输入通道数。这个参数定义输入数据的通道数量。例如，对于 RGB 图像，输入通道数为 3。</span><br><span class="line">2.  out_channels: 输出通道数。这个定义卷积操作后输出的通道数量。</span><br><span class="line">3.  kernel_size: _size_2_t: 卷积核的大小，表示在每个维度上卷积核的尺寸。_size_2_t 是一个表示二维尺寸的类型（例如 (height, width)），你可以输入一个单个整数或者一个二元组。</span><br><span class="line">4.  stride: _size_2_t = 1: 卷积步长，决定卷积核在输入上滑动的步幅。步长可以是单个整数或者二元组，默认为 1。</span><br><span class="line">5.  padding: Union[str, _size_2_t] = 0: 填充，控制输入在边界处的填充大小。可以是一个字符串（如 &#x27;same&#x27; 表示输出与输入大小相同，自动计算填充），或者一个二元组指定各维度的填充大小。</span><br><span class="line">6.  dilation: _size_2_t = 1: 膨胀率，表示在卷积核内插入的空洞数，默认为 1，表示没有膨胀。</span><br><span class="line">7.  groups: int = 1: 分组卷积的参数。groups 值为 1 表示标准卷积，值大于 1 时表示使用分组卷积，即将输入通道分为多个组，分别进行卷积。</span><br><span class="line">8.  bias: bool = True: 是否在卷积操作中使用偏置项。默认为 True，表示在卷积结果中加上一个偏置。</span><br><span class="line">9.  padding_mode: str = &#x27;zeros&#x27;: 填充的模式，默认为 &#x27;zeros&#x27;，表示在边界填充零。其他填充方式还包括 &#x27;reflect&#x27;、&#x27;replicate&#x27; 等。</span><br><span class="line">10.  device=None 和 dtype=None: 这两个参数用于指定设备（如 GPU 或 CPU）和数据类型（如 torch.float32）。</span><br></pre></td></tr></table></figure>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="kinetics-skeleton"><a href="#kinetics-skeleton" class="headerlink" title="kinetics-skeleton"></a>kinetics-skeleton</h3><p>是作者自己提取的数据，做了归一化0-1。<br>在转npy文件的时候继续做了预处理，转为[-0.5,0.5]，置信度没有变。</p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1801.07455.pdf">论文地址</a><br><a target="_blank" rel="noopener" href="https://github.com/yysijie/st-gcn">论文代码</a></p>
<p>论文解读博客：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/672346603">爱因斯坦求和</a></p>
<p><a target="_blank" rel="noopener" href="https://thachngoctran.medium.com/spatial-temporal-graph-convolutional-networks-st-gcn-explained-bf926c811330">时空图卷积网络（ST-GCN）——详解</a></p>
<p><a target="_blank" rel="noopener" href="https://mbernste.github.io/posts/gcn/">图卷积神经网络</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xiaoyuting999/article/details/130039164?spm=1001.2014.3001.5506">一杯水果茶！-ST-GCN 论文解读</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36756312/article/details/121351721?spm=1001.2014.3001.5506">Relissc_Cao-AAAI2018||ST-GCN：Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/409348916">yyxy-清晰图解，一图看懂图卷积GCN、时空图卷积ST-GCN</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>yao
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://zzhenyao.github.io/2024/09/13/10-54-10/" title="ST-GCN 时空图卷积神经网路">https://zzhenyao.github.io/2024/09/13/10-54-10/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"># 深度学习, 计算机视觉</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/09/13/09-59-13/" rel="prev" title="GNN 图神经网络">
                  <i class="fa fa-chevron-left"></i> GNN 图神经网络
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/09/13/14-56-06/" rel="next" title="深度学习训练中的技巧">
                  深度学习训练中的技巧 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.0/mermaid.min.js","integrity":"sha256-3JloMMI/ZQx6ryuhhZTsQJQmGAkXeni6PkshX7UUO2s="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"PKunicor","repo":"PKunicor.github.io","client_id":"2efe0e153686e5d9e67d","client_secret":"c84e031b6ac86ce366a6b61640a4b1a8e01d05e0","admin_user":"PKunicor","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"ae86932e9c9d90610353f6fd415891f9"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>

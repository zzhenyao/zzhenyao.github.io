<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zzhenyao.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="系统梳理了动作质量评估（Action Quality Assessment, AQA）领域自 1995 年至今的主要方法演进脉络，涵盖从早期的计算机视觉手工特征阶段，到传统机器学习框架，再到深度学习时代的时空建模、多模态融合与连续学习（CAQA）等关键技术路线。">
<meta property="og:type" content="article">
<meta property="og:title" content="AQA（动作质量评估）方法发展时间线">
<meta property="og:url" content="https://zzhenyao.github.io/2025/11/18/13-26-24/index.html">
<meta property="og:site_name" content="且听风吟">
<meta property="og:description" content="系统梳理了动作质量评估（Action Quality Assessment, AQA）领域自 1995 年至今的主要方法演进脉络，涵盖从早期的计算机视觉手工特征阶段，到传统机器学习框架，再到深度学习时代的时空建模、多模态融合与连续学习（CAQA）等关键技术路线。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-11-18T05:26:24.000Z">
<meta property="article:modified_time" content="2025-12-19T04:16:24.207Z">
<meta property="article:author" content="yao">
<meta property="article:tag" content="AQA, 深度学习">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zzhenyao.github.io/2025/11/18/13-26-24/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://zzhenyao.github.io/2025/11/18/13-26-24/","path":"2025/11/18/13-26-24/","title":"AQA（动作质量评估）方法发展时间线"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>AQA（动作质量评估）方法发展时间线 | 且听风吟</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">且听风吟</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">轻舟过万重,青山依旧在</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B1%87%E6%80%BB"><span class="nav-number">1.</span> <span class="nav-text">汇总</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AQA%EF%BC%88%E5%8A%A8%E4%BD%9C%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0%EF%BC%89%E6%96%B9%E6%B3%95%E5%8F%91%E5%B1%95%E6%97%B6%E9%97%B4%E7%BA%BF"><span class="nav-number">2.</span> <span class="nav-text">AQA（动作质量评估）方法发展时间线</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%951%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E6%96%B9%E6%B3%95"><span class="nav-number">2.0.1.</span> <span class="nav-text">方法1：计算机视觉基础方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%952%EF%BC%9AL-SVR%EF%BC%88%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">2.0.2.</span> <span class="nav-text">方法2：L-SVR（线性支持向量回归）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%953%EF%BC%9AJIGSAWS%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8EHMM-SDL"><span class="nav-number">2.0.3.</span> <span class="nav-text">方法3：JIGSAWS数据集与HMM-SDL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%954%EF%BC%9A%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95%E4%B8%8E%E6%97%B6%E9%A2%91%E5%9F%9F%E7%89%B9%E5%BE%81"><span class="nav-number">2.0.4.</span> <span class="nav-text">方法4：分类方法与时频域特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%955%EF%BC%9ALearning-to-Score-Olympic-Events"><span class="nav-number">2.0.5.</span> <span class="nav-text">方法5：Learning to Score Olympic Events</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%956%EF%BC%9AC3D%E7%B3%BB%E5%88%97%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AA%81%E7%A0%B4%EF%BC%89"><span class="nav-number">2.0.6.</span> <span class="nav-text">方法6：C3D系列（深度学习突破）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%957%EF%BC%9ALSTM-GM%EF%BC%88%E5%AF%B9%E6%AF%94%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="nav-number">2.0.7.</span> <span class="nav-text">方法7：LSTM-GM（对比排序学习）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%958%EF%BC%9AScoringNet%EF%BC%88%E5%85%B3%E9%94%AE%E7%89%87%E6%AE%B5%E4%B8%8E%E6%8E%92%E5%BA%8F%E6%8D%9F%E5%A4%B1%EF%BC%89"><span class="nav-number">2.0.8.</span> <span class="nav-text">方法8：ScoringNet（关键片段与排序损失）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%959%EF%BC%9A2S-CNN%EF%BC%88%E6%97%B6%E7%A9%BA%E5%88%86%E5%89%B2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.9.</span> <span class="nav-text">方法9：2S-CNN（时空分割卷积网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9510%EF%BC%9AS3D%EF%BC%88%E5%A0%86%E5%8F%A03D%E5%9B%9E%E5%BD%92%E5%99%A8%EF%BC%89"><span class="nav-number">2.0.10.</span> <span class="nav-text">方法10：S3D（堆叠3D回归器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9511%EF%BC%9AC3D-AVG-MTL%EF%BC%88%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%EF%BC%89"><span class="nav-number">2.0.11.</span> <span class="nav-text">方法11：C3D-AVG-MTL（多任务学习框架）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9512%EF%BC%9ARAA%EF%BC%88%E6%8E%92%E5%90%8D%E6%84%9F%E7%9F%A5%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%89"><span class="nav-number">2.0.12.</span> <span class="nav-text">方法12：RAA（排名感知注意力）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9513%EF%BC%9AJR-GCN%EF%BC%88%E5%85%B3%E8%8A%82%E5%85%B3%E7%B3%BB%E5%9B%BE%E5%8D%B7%E7%A7%AF%EF%BC%89"><span class="nav-number">2.0.13.</span> <span class="nav-text">方法13：JR-GCN（关节关系图卷积）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9514%EF%BC%9AUSDL%EF%BC%88%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%84%9F%E7%9F%A5%E8%AF%84%E5%88%86%E5%88%86%E5%B8%83%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="nav-number">2.0.14.</span> <span class="nav-text">方法14：USDL（不确定性感知评分分布学习）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9516%EF%BC%9AACTION-Net%EF%BC%88%E5%8A%A8%E4%BD%9C-%E9%9D%99%E6%80%81%E4%B8%8A%E4%B8%8B%E6%96%87%E6%B3%A8%E6%84%8F%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.15.</span> <span class="nav-text">方法16：ACTION-Net（动作-静态上下文注意网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9517%EF%BC%9AAIM%EF%BC%88%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%85%B3%E7%B3%BB%E5%BB%BA%E6%A8%A1%EF%BC%89"><span class="nav-number">2.0.16.</span> <span class="nav-text">方法17：AIM（非对称关系建模）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9518%EF%BC%9AMS-LSTM"><span class="nav-number">2.0.17.</span> <span class="nav-text">方法18：MS-LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9518%EF%BC%9ACoRe%EF%BC%88%E5%AF%B9%E6%AF%94%E5%9B%9E%E5%BD%92%E6%A1%86%E6%9E%B6%EF%BC%89"><span class="nav-number">2.0.18.</span> <span class="nav-text">方法18：CoRe（对比回归框架）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9519%EF%BC%9ASportCap%EF%BC%88%E8%BF%90%E5%8A%A8%E6%8D%95%E6%8D%89%E4%B8%8E%E7%BB%86%E7%B2%92%E5%BA%A6%E7%90%86%E8%A7%A3%EF%BC%89"><span class="nav-number">2.0.19.</span> <span class="nav-text">方法19：SportCap（运动捕捉与细粒度理解）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9520%EF%BC%9AST-GCN%EF%BC%88%E6%97%B6%E7%A9%BA%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.20.</span> <span class="nav-text">方法20：ST-GCN（时空图卷积网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9521%EF%BC%9AEAGLE-Eye%EF%BC%88%E6%9E%81%E7%AB%AF%E5%A7%BF%E6%80%81%E5%8A%A8%E4%BD%9C%E8%AF%84%E5%88%86%E5%99%A8%EF%BC%89"><span class="nav-number">2.0.21.</span> <span class="nav-text">方法21：EAGLE-Eye（极端姿态动作评分器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9522%EF%BC%9ARGR%EF%BC%88%E5%8F%82%E8%80%83%E5%AF%BC%E5%90%91%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">2.0.22.</span> <span class="nav-text">方法22：RGR（参考导向回归）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9523%EF%BC%9ATSA-Net%EF%BC%88%E7%AE%A1%E8%87%AA%E6%B3%A8%E6%84%8F%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.23.</span> <span class="nav-text">方法23：TSA-Net（管自注意网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9524%EF%BC%9ATAL%EF%BC%88%E6%97%B6%E9%97%B4%E6%B3%A8%E6%84%8F%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="nav-number">2.0.24.</span> <span class="nav-text">方法24：TAL（时间注意学习）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9525%EF%BC%9AFineDiving%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8ETSA%EF%BC%88%E6%97%B6%E9%97%B4%E5%88%86%E5%89%B2%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%89"><span class="nav-number">2.0.25.</span> <span class="nav-text">方法25：FineDiving数据集与TSA（时间分割注意力）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9526%EF%BC%9AGDLT%EF%BC%88%E7%AD%89%E7%BA%A7%E8%A7%A3%E8%80%A6Likert-Transformer%EF%BC%89"><span class="nav-number">2.0.26.</span> <span class="nav-text">方法26：GDLT（等级解耦Likert Transformer）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9527%EF%BC%9ATPT%EF%BC%88%E6%97%B6%E9%97%B4%E8%A7%A3%E6%9E%90Transformer%EF%BC%89"><span class="nav-number">2.0.27.</span> <span class="nav-text">方法27：TPT（时间解析Transformer）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9528%EF%BC%9AI3D-Transformer"><span class="nav-number">2.0.28.</span> <span class="nav-text">方法28：I3D-Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9529%EF%BC%9AI3D-TA%EF%BC%88%E6%97%B6%E9%97%B4%E6%84%9F%E7%9F%A5%E6%B3%A8%E6%84%8F%EF%BC%89"><span class="nav-number">2.0.29.</span> <span class="nav-text">方法29：I3D-TA（时间感知注意）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9530%EF%BC%9AUD-AQA%EF%BC%88%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E9%A9%B1%E5%8A%A8%E7%9A%84AQA%EF%BC%89"><span class="nav-number">2.0.30.</span> <span class="nav-text">方法30：UD-AQA（不确定性驱动的AQA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9531%EF%BC%9APCLN%EF%BC%88%E9%85%8D%E5%AF%B9%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.31.</span> <span class="nav-text">方法31：PCLN（配对对比学习网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9532%EF%BC%9AMD%E4%B8%8ECVCSPC%EF%BC%88%E8%BF%90%E5%8A%A8%E8%A7%A3%E7%BC%A0%E7%BB%95%EF%BC%89"><span class="nav-number">2.0.32.</span> <span class="nav-text">方法32：MD与CVCSPC（运动解缠绕）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%EF%BC%9AAdaptive-Net%EF%BC%88%E8%87%AA%E9%80%82%E5%BA%94%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.33.</span> <span class="nav-text">方法：Adaptive Net（自适应网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9533%EF%BC%9AFSPN%EF%BC%88%E7%BB%86%E7%B2%92%E5%BA%A6%E6%97%B6%E7%A9%BA%E8%A7%A3%E6%9E%90%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.34.</span> <span class="nav-text">方法33：FSPN（细粒度时空解析网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9534%EF%BC%9AMSRM%E4%B8%8ESSRMM%EF%BC%88%E5%A4%9A%E9%98%B6%E6%AE%B5%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9D%97%EF%BC%89"><span class="nav-number">2.0.35.</span> <span class="nav-text">方法34：MSRM与SSRMM（多阶段回归模块）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9535%EF%BC%9ATECN%EF%BC%88%E9%AB%98%E6%96%AF%E5%BC%95%E5%AF%BC%E5%B8%A7%E5%BA%8F%E5%88%97%E7%BC%96%E7%A0%81%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.36.</span> <span class="nav-text">方法35：TECN（高斯引导帧序列编码网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9536%EF%BC%9ADAE%EF%BC%88%E5%88%86%E5%B8%83%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%89"><span class="nav-number">2.0.37.</span> <span class="nav-text">方法36：DAE（分布自编码器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9537%EF%BC%9ALUSD-Net%EF%BC%88%E5%AE%9A%E4%BD%8D%E8%BE%85%E5%8A%A9%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E8%AF%84%E5%88%86%E8%A7%A3%E7%BC%A0%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.38.</span> <span class="nav-text">方法37：LUSD-Net（定位辅助不确定性评分解缠网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9538%EF%BC%9AASTRM%EF%BC%88%E8%B7%A8%E9%98%B6%E6%AE%B5%E6%97%B6%E9%97%B4%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9D%97%EF%BC%89"><span class="nav-number">2.0.39.</span> <span class="nav-text">方法38：ASTRM（跨阶段时间推理模块）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9539%EF%BC%9ARG-AQA%EF%BC%88%E5%9B%9E%E6%94%BE%E5%BC%95%E5%AF%BC%E7%9A%84%E5%8A%A8%E4%BD%9C%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0%EF%BC%89"><span class="nav-number">2.0.40.</span> <span class="nav-text">方法39：RG-AQA（回放引导的动作质量评估）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9540%EF%BC%9APSL%EF%BC%88%E4%BC%AA%E5%AD%90%E8%AF%84%E5%88%86%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="nav-number">2.0.41.</span> <span class="nav-text">方法40：PSL（伪子评分学习）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9541%EF%BC%9AGOAT%EF%BC%88%E7%BE%A4%E4%BD%93%E6%84%9F%E7%9F%A5%E6%B3%A8%E6%84%8F%EF%BC%89"><span class="nav-number">2.0.42.</span> <span class="nav-text">方法41：GOAT（群体感知注意）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9542%EF%BC%9AHGCN%EF%BC%88%E5%88%86%E5%B1%82%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.43.</span> <span class="nav-text">方法42：HGCN（分层图卷积网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9543%EF%BC%9AMS-GCN%EF%BC%88%E5%A4%9A%E9%AA%A8%E6%9E%B6%E7%BB%93%E6%9E%84GCN%EF%BC%89"><span class="nav-number">2.0.44.</span> <span class="nav-text">方法43：MS-GCN（多骨架结构GCN）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9544%EF%BC%9AMLP-Mixer%EF%BC%88%E9%95%BF%E6%9C%9F%E8%BF%90%E5%8A%A8%E9%9F%B3%E9%A2%91-%E8%A7%86%E8%A7%89%E5%BB%BA%E6%A8%A1%EF%BC%89"><span class="nav-number">2.0.45.</span> <span class="nav-text">方法44：MLP-Mixer（长期运动音频-视觉建模）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9545%EF%BC%9AAdaST%EF%BC%88%E8%87%AA%E9%80%82%E5%BA%94%E9%98%B6%E6%AE%B5%E6%84%9F%E7%9F%A5%E8%AF%84%E4%BC%B0%E6%8A%80%E8%83%BD%E8%BD%AC%E7%A7%BB%EF%BC%89"><span class="nav-number">2.0.46.</span> <span class="nav-text">方法45：AdaST（自适应阶段感知评估技能转移）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9546%EF%BC%9ASGN%EF%BC%88%E8%AF%AD%E4%B9%89%E5%BC%95%E5%AF%BC%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.47.</span> <span class="nav-text">方法46：SGN（语义引导网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9547%EF%BC%9AMCoRe%EF%BC%88%E5%A4%9A%E9%98%B6%E6%AE%B5%E5%AF%B9%E6%AF%94%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">2.0.48.</span> <span class="nav-text">方法47：MCoRe（多阶段对比回归）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9548%EF%BC%9AT2CR%EF%BC%88%E5%8F%8C%E8%B7%AF%E5%BE%84%E7%9B%AE%E6%A0%87%E6%84%9F%E7%9F%A5%E5%AF%B9%E6%AF%94%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">2.0.49.</span> <span class="nav-text">方法48：T2CR（双路径目标感知对比回归）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9549%EF%BC%9ASSPR%EF%BC%88%E8%AF%AD%E4%B9%89%E5%BA%8F%E5%88%97%E6%80%A7%E8%83%BD%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">2.0.50.</span> <span class="nav-text">方法49：SSPR（语义序列性能回归）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9550%EF%BC%9AFineParser%EF%BC%88%E7%BB%86%E7%B2%92%E5%BA%A6%E6%97%B6%E7%A9%BA%E5%8A%A8%E4%BD%9C%E8%A7%A3%E6%9E%90%E5%99%A8%EF%BC%89"><span class="nav-number">2.0.51.</span> <span class="nav-text">方法50：FineParser（细粒度时空动作解析器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9551%EF%BC%9ASTSA%EF%BC%88%E6%97%B6%E7%A9%BA%E5%88%86%E5%89%B2%E6%B3%A8%E6%84%8F%EF%BC%89"><span class="nav-number">2.0.52.</span> <span class="nav-text">方法51：STSA（时空分割注意）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9552%EF%BC%9ARhythmer%EF%BC%88%E8%8A%82%E5%A5%8F%E6%84%9F%E7%9F%A5Transformer%EF%BC%89"><span class="nav-number">2.0.53.</span> <span class="nav-text">方法52：Rhythmer（节奏感知Transformer）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9553%EF%BC%9APAMFN%EF%BC%88%E6%B8%90%E8%BF%9B%E8%87%AA%E9%80%82%E5%BA%94%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.54.</span> <span class="nav-text">方法53：PAMFN（渐进自适应多模态融合网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9554%EF%BC%9ANAE-AQA%EF%BC%88%E5%8F%99%E8%BF%B0%E6%80%A7%E5%8A%A8%E4%BD%9C%E8%AF%84%E4%BC%B0%EF%BC%89"><span class="nav-number">2.0.55.</span> <span class="nav-text">方法54：NAE-AQA（叙述性动作评估）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9555%EF%BC%9AVATP-Net%EF%BC%88%E8%A7%86%E8%A7%89-%E8%AF%AD%E4%B9%89%E5%AF%B9%E9%BD%90%E6%97%B6%E9%97%B4%E8%A7%A3%E6%9E%90%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.56.</span> <span class="nav-text">方法55：VATP-Net（视觉-语义对齐时间解析网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9556%EF%BC%9A2M-AF%EF%BC%88%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%84%E4%BC%B0%E6%A1%86%E6%9E%B6%EF%BC%89"><span class="nav-number">2.0.57.</span> <span class="nav-text">方法56：2M-AF（多模态评估框架）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9557%EF%BC%9AEGCN%EF%BC%88%E9%9B%86%E6%88%90%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.58.</span> <span class="nav-text">方法57：EGCN（集成图卷积网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9558%EF%BC%9AEK-GCN%EF%BC%88%E4%B8%93%E5%AE%B6%E7%9F%A5%E8%AF%86%E5%BC%95%E5%AF%BC%E7%9A%84%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.59.</span> <span class="nav-text">方法58：EK-GCN（专家知识引导的图卷积网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9559%EF%BC%9ADNLA%EF%BC%88%E5%88%A4%E5%88%AB%E6%80%A7%E9%9D%9E%E5%B1%80%E9%83%A8%E6%B3%A8%E6%84%8F%EF%BC%89"><span class="nav-number">2.0.60.</span> <span class="nav-text">方法59：DNLA（判别性非局部注意）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9560%EF%BC%9ANS-AQA%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%AC%A6%E5%8F%B7AQA%EF%BC%89"><span class="nav-number">2.0.61.</span> <span class="nav-text">方法60：NS-AQA（神经符号AQA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9561%EF%BC%9ARICA2%EF%BC%88%E8%A7%84%E5%88%99%E7%9F%A5%E6%83%85%E6%A0%A1%E5%87%86%E8%AF%84%E4%BC%B0%EF%BC%89"><span class="nav-number">2.0.62.</span> <span class="nav-text">方法61：RICA2（规则知情校准评估）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9562%EF%BC%9ADuRA%EF%BC%88%E5%8F%8C%E5%8F%82%E8%80%83%E8%BE%85%E5%8A%A9%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.63.</span> <span class="nav-text">方法62：DuRA（双参考辅助网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9563%EF%BC%9ASAP-Net%EF%BC%88%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%90%E5%8A%A8%E4%BD%9C%E8%A7%A3%E6%9E%90%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="nav-number">2.0.64.</span> <span class="nav-text">方法63：SAP-Net（自监督子动作解析网络）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9564%EF%BC%9ATRS%EF%BC%88%E6%95%99%E5%B8%88-%E5%8F%82%E8%80%83-%E5%AD%A6%E7%94%9F%E6%9E%B6%E6%9E%84%EF%BC%89"><span class="nav-number">2.0.65.</span> <span class="nav-text">方法64：TRS（教师-参考-学生架构）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9565%EF%BC%9APECoP%EF%BC%88%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E8%BF%9E%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%89"><span class="nav-number">2.0.66.</span> <span class="nav-text">方法65：PECoP（参数高效连续预训练）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9566%EF%BC%9AContinual-AQA%EF%BC%88%E8%BF%9E%E7%BB%AD%E5%AD%A6%E4%B9%A0AQA%EF%BC%89"><span class="nav-number">2.0.67.</span> <span class="nav-text">方法66：Continual-AQA（连续学习AQA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9567%EF%BC%9AMAGR%EF%BC%88%E6%B5%81%E5%BD%A2%E5%AF%B9%E9%BD%90%E5%9B%BE%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%89"><span class="nav-number">2.0.68.</span> <span class="nav-text">方法67：MAGR（流形对齐图正则化）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9568%EF%BC%9ACoFInAl%EF%BC%88%E7%B2%97%E7%BB%86%E6%8C%87%E4%BB%A4%E5%AF%B9%E9%BD%90%EF%BC%89"><span class="nav-number">2.0.69.</span> <span class="nav-text">方法68：CoFInAl（粗细指令对齐）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9569%EF%BC%9AZEAL%EF%BC%88%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%A4%96%E7%A7%91%E6%8A%80%E8%83%BD%E8%AF%84%E4%BC%B0%EF%BC%89"><span class="nav-number">2.0.70.</span> <span class="nav-text">方法69：ZEAL（零样本外科技能评估）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9570%EF%BC%9ALucidAction%EF%BC%88%E5%88%86%E5%B1%82%E5%A4%9A%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%89"><span class="nav-number">2.0.71.</span> <span class="nav-text">方法70：LucidAction（分层多模型数据集）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9571%EF%BC%9AGAIA%EF%BC%88AI%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%91%E7%9A%84AQA%EF%BC%89"><span class="nav-number">2.0.72.</span> <span class="nav-text">方法71：GAIA（AI生成视频的AQA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9572%EF%BC%9ACLN%EF%BC%88%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%BA%B7%E5%A4%8D%E8%AF%84%E4%BC%B0%EF%BC%89"><span class="nav-number">2.0.73.</span> <span class="nav-text">方法72：CLN（对比学习康复评估）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9573%EF%BC%9ADPFL-FS%EF%BC%88%E9%AA%A8%E6%9E%B6%E6%B7%B1%E5%BA%A6%E5%A7%BF%E6%80%81%E7%89%B9%E5%BE%81-%E8%8A%B1%E6%A0%B7%E6%BB%91%E5%86%B0%EF%BC%89"><span class="nav-number">2.0.74.</span> <span class="nav-text">方法73：DPFL-FS（骨架深度姿态特征-花样滑冰）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9574%EF%BC%9ASSTDT%EF%BC%88%E9%AA%A8%E6%9E%B6%E6%97%B6%E7%A9%BA%E8%A7%A3%E8%80%A6Transformer%EF%BC%89"><span class="nav-number">2.0.75.</span> <span class="nav-text">方法74：SSTDT（骨架时空解耦Transformer）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9575%EF%BC%9AVL-AKL%EF%BC%88%E8%AF%AD%E4%B9%89%E6%84%9F%E7%9F%A5%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="nav-number">2.0.76.</span> <span class="nav-text">方法75：VL-AKL（语义感知视觉语言知识学习）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9576%EF%BC%9AHP-MSCR%EF%BC%88%E5%B1%82%E7%BA%A7%E5%A7%BF%E6%80%81%E5%BC%95%E5%AF%BC%E7%9A%84%E5%A4%9A%E9%98%B6%E6%AE%B5%E5%AF%B9%E6%AF%94%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">2.0.77.</span> <span class="nav-text">方法76：HP-MSCR（层级姿态引导的多阶段对比回归）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9577%EF%BC%9APHI%EF%BC%88%E6%B8%90%E8%BF%9B%E5%B1%82%E7%BA%A7%E6%8C%87%E4%BB%A4%EF%BC%89"><span class="nav-number">2.0.78.</span> <span class="nav-text">方法77：PHI（渐进层级指令）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9578%EF%BC%9AHC-FGAQA%EF%BC%88%E4%BB%A5%E4%BA%BA%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E7%BB%86%E7%B2%92%E5%BA%A6AQA%EF%BC%89"><span class="nav-number">2.0.79.</span> <span class="nav-text">方法78：HC-FGAQA（以人为中心的细粒度AQA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9580%EF%BC%9AB2S%EF%BC%88%E4%BB%8E%E8%8A%82%E6%8B%8D%E5%88%B0%E8%AF%84%E5%88%86%EF%BC%89"><span class="nav-number">2.0.80.</span> <span class="nav-text">方法80：B2S（从节拍到评分）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9581%EF%BC%9AFineCausal%EF%BC%88%E5%8F%AF%E8%A7%A3%E9%87%8A%E5%9B%A0%E6%9E%9C%E7%BB%86%E7%B2%92%E5%BA%A6AQA%EF%BC%89"><span class="nav-number">2.0.81.</span> <span class="nav-text">方法81：FineCausal（可解释因果细粒度AQA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9582%EF%BC%9ATS-MambaPyramid%EF%BC%88%E4%B8%A4%E6%B5%81Mamba%E9%87%91%E5%AD%97%E5%A1%94%EF%BC%89"><span class="nav-number">2.0.82.</span> <span class="nav-text">方法82：TS-MambaPyramid（两流Mamba金字塔）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9583%EF%BC%9ADanceFix%EF%BC%88%E7%BE%A4%E8%88%9E%E6%95%B4%E9%BD%90%E5%BA%A6%E8%AF%84%E4%BC%B0%EF%BC%89"><span class="nav-number">2.0.83.</span> <span class="nav-text">方法83：DanceFix（群舞整齐度评估）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9584%EF%BC%9AMLAVL%EF%BC%88%E8%AF%AD%E8%A8%80%E5%BC%95%E5%AF%BC%E8%A7%86%E5%90%AC%E8%AF%84%E4%BC%B0%EF%BC%89"><span class="nav-number">2.0.84.</span> <span class="nav-text">方法84：MLAVL（语言引导视听评估）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9585%EF%BC%9AQGVL%EF%BC%88%E8%B4%A8%E9%87%8F%E5%BC%95%E5%AF%BC%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%EF%BC%89"><span class="nav-number">2.0.85.</span> <span class="nav-text">方法85：QGVL（质量引导视觉语言）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9586%EF%BC%9ASBS%EF%BC%88%E5%B0%BA%E5%BA%A6%E5%8C%96%E8%83%8C%E6%99%AF%E7%BD%AE%E6%8D%A2%EF%BC%89"><span class="nav-number">2.0.86.</span> <span class="nav-text">方法86：SBS（尺度化背景置换）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9587%EF%BC%9AASGTN%EF%BC%88%E8%87%AA%E9%80%82%E5%BA%94%E6%97%B6%E7%A9%BA%E5%9B%BETransformer%EF%BC%89"><span class="nav-number">2.0.87.</span> <span class="nav-text">方法87：ASGTN（自适应时空图Transformer）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9588%EF%BC%9AMB-AQA%EF%BC%88%E5%A4%9A%E5%88%86%E6%94%AF%E7%BB%BC%E5%90%88%E5%BB%BA%E6%A8%A1%EF%BC%89"><span class="nav-number">2.0.88.</span> <span class="nav-text">方法88：MB-AQA（多分支综合建模）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9589%EF%BC%9AAQA%E7%BB%BC%E8%BF%B0%EF%BC%88%E6%96%B9%E6%B3%95%E4%B8%8E%E5%9F%BA%E5%87%86%EF%BC%89"><span class="nav-number">2.0.89.</span> <span class="nav-text">方法89：AQA综述（方法与基准）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9590%EF%BC%9AAQA%E7%B3%BB%E7%BB%9F%E6%80%A7%E7%BB%BC%E8%BF%B0%EF%BC%88ESWA%EF%BC%89"><span class="nav-number">2.0.90.</span> <span class="nav-text">方法90：AQA系统性综述（ESWA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9591%EF%BC%9AAQA%E5%8D%81%E5%B9%B4%E5%9B%9E%E9%A1%BE%EF%BC%88%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0%EF%BC%89"><span class="nav-number">2.0.91.</span> <span class="nav-text">方法91：AQA十年回顾（系统综述）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%9592%EF%BC%9AResFNN%EF%BC%88%E6%AE%8B%E5%B7%AE%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9CAQA%EF%BC%89"><span class="nav-number">2.0.92.</span> <span class="nav-text">方法92：ResFNN（残差前馈网络AQA）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B6%85%E5%9B%BE"><span class="nav-number">3.</span> <span class="nav-text">超图</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Hypergraph-Neural-Networks"><span class="nav-number">3.1.</span> <span class="nav-text">Hypergraph Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HGNN"><span class="nav-number">3.2.</span> <span class="nav-text">HGNN+</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HypergraphBasedMultiViewAction"><span class="nav-number">3.2.1.</span> <span class="nav-text">HypergraphBasedMultiViewAction</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adaptive-Hyper-Graph"><span class="nav-number">3.3.</span> <span class="nav-text">Adaptive Hyper-Graph</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Hyper-YOLO"><span class="nav-number">3.3.1.</span> <span class="nav-text">Hyper-YOLO</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GeneratingHypergraphBasedHighOrder"><span class="nav-number">3.4.</span> <span class="nav-text">GeneratingHypergraphBasedHighOrder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Selective-HCN"><span class="nav-number">3.5.</span> <span class="nav-text">Selective-HCN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB"><span class="nav-number">4.</span> <span class="nav-text">动作识别</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2s-AGCN"><span class="nav-number">4.1.</span> <span class="nav-text">2s-AGCN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#InfoGCN"><span class="nav-number">4.2.</span> <span class="nav-text">InfoGCN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DS-STGCN"><span class="nav-number">4.3.</span> <span class="nav-text">DS-STGCN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C"><span class="nav-number">5.</span> <span class="nav-text">经典网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM"><span class="nav-number">5.1.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer"><span class="nav-number">5.2.</span> <span class="nav-text">Transformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mamba"><span class="nav-number">5.3.</span> <span class="nav-text">Mamba</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Openpose"><span class="nav-number">5.4.</span> <span class="nav-text">Openpose</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inception"><span class="nav-number">5.5.</span> <span class="nav-text">inception</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">5.6.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E6%BC%94%E5%8C%96"><span class="nav-number">5.6.1.</span> <span class="nav-text">1. 特征提取演化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E8%8C%83%E5%BC%8F%E6%BC%94%E5%8C%96"><span class="nav-number">5.6.2.</span> <span class="nav-text">2. 模型范式演化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%A8%A1%E6%80%81%E8%8C%83%E5%9B%B4%E6%89%A9%E5%B1%95"><span class="nav-number">5.6.3.</span> <span class="nav-text">3. 模态范围扩展</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%90%86%E8%AE%BA%E6%B7%B1%E5%BA%A6%E6%8F%90%E5%8D%87"><span class="nav-number">5.6.4.</span> <span class="nav-text">4. 理论深度提升</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E6%89%A9%E5%B1%95"><span class="nav-number">5.6.5.</span> <span class="nav-text">5. 应用场景扩展</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E7%BB%86%E7%B2%92%E5%BA%A6%E7%A8%8B%E5%BA%A6%E6%8F%90%E5%8D%87"><span class="nav-number">5.6.6.</span> <span class="nav-text">6. 细粒度程度提升</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E8%B7%A8%E5%9F%9F%E8%83%BD%E5%8A%9B%E8%BF%9B%E6%AD%A5"><span class="nav-number">5.6.7.</span> <span class="nav-text">7. 跨域能力进步</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yao"
      src="/images/logo.png">
  <p class="site-author-name" itemprop="name">yao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zzhenyao" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzhenyao" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zzhenyao.github.io/2025/11/18/13-26-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.png">
      <meta itemprop="name" content="yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="且听风吟">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="AQA（动作质量评估）方法发展时间线 | 且听风吟">
      <meta itemprop="description" content="系统梳理了动作质量评估（Action Quality Assessment, AQA）领域自 1995 年至今的主要方法演进脉络，涵盖从早期的计算机视觉手工特征阶段，到传统机器学习框架，再到深度学习时代的时空建模、多模态融合与连续学习（CAQA）等关键技术路线。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AQA（动作质量评估）方法发展时间线
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-11-18 13:26:24" itemprop="dateCreated datePublished" datetime="2025-11-18T13:26:24+08:00">2025-11-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-12-19 12:16:24" itemprop="dateModified" datetime="2025-12-19T12:16:24+08:00">2025-12-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AQA/" itemprop="url" rel="index"><span itemprop="name">AQA</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>



        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
            <div class="post-description">系统梳理了动作质量评估（Action Quality Assessment, AQA）领域自 1995 年至今的主要方法演进脉络，涵盖从早期的计算机视觉手工特征阶段，到传统机器学习框架，再到深度学习时代的时空建模、多模态融合与连续学习（CAQA）等关键技术路线。</div>
	<hr>
        <h1 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h1><ul>
<li><a href="/2025/02/20/14-13-19/" title="AQA,AR论文汇总">部分论文阅读笔记</a></li>
<li><a href="/2025/08/06/15-40-59/" title="骨架动作识别的backbone 和姿态估计方法">动作识别Backbone与姿态估计算法汇总</a>
</li>
</ul>
<h1 id="AQA（动作质量评估）方法发展时间线"><a href="#AQA（动作质量评估）方法发展时间线" class="headerlink" title="AQA（动作质量评估）方法发展时间线"></a>AQA（动作质量评估）方法发展时间线</h1><h3 id="方法1：计算机视觉基础方法"><a href="#方法1：计算机视觉基础方法" class="headerlink" title="方法1：计算机视觉基础方法"></a>方法1：计算机视觉基础方法</h3><p><strong>方法名称：</strong> Computer Vision-Based Action Quality Tracking<a href="zotero://select/library/items/E6G4K6YN">📚</a></p>
<p><strong>论文标题：</strong> Automated video assessment of human performance</p>
<p><strong>核心技术：</strong> 光学字符识别(OCR)与手工特征提取</p>
<p><strong>使用数据集：</strong> 无公开数据集（早期探索）</p>
<p><strong>性能指标：</strong> 定性评估，无量化指标</p>
<p><strong>主要贡献：</strong> 首次将计算机视觉技术应用于AQA领域</p>
<p><strong>应用背景：</strong> 初期探索，主要关注理论可行性与手工设计特征</p>
<p><strong>优缺点：</strong> ✅ 开创性工作；❌ 特征提取方式原始，泛化能力极弱</p>
<p><strong>演变与进步：</strong> 为后续有监督/无监督学习框架奠定基础</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">@article&#123;Gordon-AutomatedVideoAssessment-,</span><br><span class="line">  title = &#123;Automated Video Assessment of Human Performance&#125;,</span><br><span class="line">  author = &#123;Gordon, Andrew S&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="方法2：L-SVR（线性支持向量回归）"><a href="#方法2：L-SVR（线性支持向量回归）" class="headerlink" title="方法2：L-SVR（线性支持向量回归）"></a>方法2：L-SVR（线性支持向量回归）</h3><p><strong>方法名称：</strong> L-SVR <a href="zotero://select/library/items/H8E33VWV">📚</a></p>
<p><strong>论文标题：</strong> Assessing the quality of actions</p>
<p><strong>核心技术：</strong> HOG（方向梯度直方图）+ MBH（运动边界直方图）+ 线性SVR</p>
<p><strong>数据集：</strong> MIT-Dive, MIT-Skate</p>
<p><strong>主要贡献：</strong> 开创性地建立了第一个AQA数据集和有监督回归框架，引入了Spearman秩相关系数(SRC)作为标准评估指标，定义了AQA任务的基本问题设置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Pirsiavash-AssessingQualityActions-2014,</span><br><span class="line">  title = &#123;Assessing the Quality of Actions&#125;,</span><br><span class="line">  booktitle = &#123;ECCV&#125;,</span><br><span class="line">  author = &#123;Pirsiavash, Hamed and Vondrick, Carl and Torralba, Antonio&#125;,</span><br><span class="line">  year = 2014,</span><br><span class="line">  pages = &#123;556--571&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">H. Pirsiavash, C. Vondrick, A. Torralba, &quot;Assessing the Quality of Actions,&quot; in Computer Vision – ECCV 2014, 2014, pp. 556–571.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法3：JIGSAWS数据集与HMM-SDL"><a href="#方法3：JIGSAWS数据集与HMM-SDL" class="headerlink" title="方法3：JIGSAWS数据集与HMM-SDL"></a>方法3：JIGSAWS数据集与HMM-SDL</h3><p><strong>方法名称：</strong>  HMM-SDL<a href="zotero://select/library/items/WPMPMMPL">📚</a></p>
<p><strong>论文标题：</strong> JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS)</p>
<p><strong>核心技术：</strong> 运动捕捉(MoCap)系统 + Hidden Markov Model + 稀疏字典学习</p>
<p><strong>数据集：</strong> JIGSAWS（手术技能评估）</p>
<p><strong>实验环境：</strong> 真实手术室环境，采用标准化的缝合/打结/打线外科技能</p>
<p><strong>主要贡献：</strong> 建立了首个外科手术技能评估数据集</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Gao-JHUISIGestureSkill-2014,</span><br><span class="line">  title=&#123;JHU-ISI Gesture and Skill Assessment Working Set ( JIGSAWS ) : A Surgical Activity Dataset for Human Motion Modeling&#125;,</span><br><span class="line">  author=&#123;Gao, Yixin and Vedula, S. Swaroop and Reiley, Carol E. and Ahmidi, Narges and Varadarajan, Balakrishnan and Lin, Henry C. and Tao, Lingling and Zappella, Luca and B&#123;\&#x27;e&#125;jar, Benjam&#123;\&#x27;i&#125;n and Yuh, David D. and Chen, Chi Chiung Grace and Vidal, Ren&#123;\&#x27;e&#125; and Khudanpur, Sanjeev and Hager, Gregory&#125;,</span><br><span class="line">  journal = &#123;Medical Image Computing and Computer-Assisted Intervention&#125;,</span><br><span class="line">  year=&#123;2014&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Gao, Y., et al. &quot;JHU-ISI Gesture and Skill Assessment Working Set ( JIGSAWS ) : A Surgical Activity Dataset for Human Motion Modeling,&quot; in Medical Image Computing and Computer-Assisted Intervention, 2014.</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法4：分类方法与时频域特征"><a href="#方法4：分类方法与时频域特征" class="headerlink" title="方法4：分类方法与时频域特征"></a>方法4：分类方法与时频域特征</h3><p><strong>方法名称：</strong> Classification-Based AQA (时频域特征分析)<a href="zotero://select/library/items/E6ESLL67">📚</a> , <a href="zotero://select/library/items/S34ZUNBS">📚</a></p>
<p><strong>论文标题：</strong> Video based assessment of OSATs using sequential motion textures; Automated Video-Based Assessment of Surgical Skills for Training and Evaluation in Medical Schools</p>
<p><strong>核心技术：</strong> Dense Trajectory + LBP-TOP（时空纹理）+ SVM分类</p>
<p><strong>数据集：</strong> OSATS</p>
<p><strong>性能：</strong> 分类准确率 = 0.75-0.80</p>
<p><strong>主要贡献：</strong> 将AQA问题转化为分类任务，引入时间序列和频率域特征表示，证明了基于动作轨迹的手工特征的有效性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Sharma-VideoBasedAssessment-,</span><br><span class="line">  title = &#123;Video Based Assessment of OSATS Using Sequential Motion Textures&#125;,</span><br><span class="line">  author = &#123;Sharma, Yachna and Bettadapura, Vinay and Plotz, Thomas and Hammerla, Nils and Mellor, Sebastian and McNaney, Roisin and Olivier, Patrick and Deshmukh, Sandeep and McCaskie, Andrew and Essa, Irfan&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@article&#123;Zia-AutomatedVideobasedAssessment-2016,</span><br><span class="line">  title = &#123;Automated Video-Based Assessment of Surgical Skills for Training and Evaluation in Medical Schools&#125;,</span><br><span class="line">  author = &#123;Zia, Aneeq and Sharma, Yachna and Bettadapura, Vinay and Sarin, Eric L. and Ploetz, Thomas and Clements, Mark A. and Essa, Irfan&#125;,</span><br><span class="line">  year = 2016,</span><br><span class="line">  journal = &#123;International Journal of Computer Assisted Radiology and Surgery&#125;,</span><br><span class="line">  volume = &#123;11&#125;,</span><br><span class="line">  number = &#123;9&#125;,</span><br><span class="line">  pages = &#123;1623--1636&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法5：Learning-to-Score-Olympic-Events"><a href="#方法5：Learning-to-Score-Olympic-Events" class="headerlink" title="方法5：Learning to Score Olympic Events"></a>方法5：Learning to Score Olympic Events</h3><p><strong>方法名称：</strong> Deep Learning-Based Regression for Olympic Events Scoring <a href="zotero://select/library/items/V9SG4UQ5">📚</a></p>
<p><strong>论文标题：</strong> Learning to Score Olympic Events</p>
<p><strong>核心技术：</strong> C3D +LSTM + 手工设计的全连接回归层 + L2回归损失</p>
<p><strong>数据集：</strong> UNLV-Dive , MIT-Dive</p>
<p><strong>主要贡献：</strong> 首次系统地将深度学习特征（VGG预训练权重）应用于奥运项目评分，证明了深层特征对浅层特征的优越性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Parmar-LearningScoreOlympic-2017,</span><br><span class="line">  title = &#123;Learning to Score Olympic Events&#125;,</span><br><span class="line">  booktitle = &#123;2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)&#125;,</span><br><span class="line">  author = &#123;Parmar, Paritosh and Morris, Brendan Tran&#125;,</span><br><span class="line">  year = 2017,</span><br><span class="line">  pages = &#123;76--84&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">P. Parmar, B. Morris, &quot;Learning to Score Olympic Events,&quot; in 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2017, pp. 76–84.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法6：C3D系列（深度学习突破）"><a href="#方法6：C3D系列（深度学习突破）" class="headerlink" title="方法6：C3D系列（深度学习突破）"></a>方法6：C3D系列（深度学习突破）</h3><p><strong>方法名称：</strong> C3D-SVR, C3D-LSTM, C3D-LSTM-SVR, <a href="zotero://select/library/items/Z6KV7ERX">📚</a></p>
<p><strong>论文标题：</strong> Action quality assessment across multiple actions</p>
<p><strong>核心技术：</strong> C3D特征提取 + SVR/LSTM回归 + 多任务联合学习</p>
<p><strong>数据集：</strong> AQA-7</p>
<p><strong>主要贡献：</strong> 首次在AQA中引入3D卷积神经网络进行端到端时空特征学习，开启AQA的深度学习新时代。相比2D特征，C3D显著提升了时序建模能力</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Parmar-ActionQualityAssessment-2019,</span><br><span class="line">  title = &#123;Action Quality Assessment Across Multiple Actions&#125;,</span><br><span class="line">  booktitle = &#123;2019 IEEE Winter Conference on Applications of Computer Vision (WACV)&#125;,</span><br><span class="line">  author = &#123;Parmar, Paritosh and Morris, Brendan&#125;,</span><br><span class="line">  year = 2019,</span><br><span class="line">  pages = &#123;1468--1476&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法7：LSTM-GM（对比排序学习）"><a href="#方法7：LSTM-GM（对比排序学习）" class="headerlink" title="方法7：LSTM-GM（对比排序学习）"></a>方法7：LSTM-GM（对比排序学习）</h3><p><strong>方法名称：</strong> LSTM-Gaussian Mixture (LSTM-GM) <a href="zotero://select/library/items/3AMM9QNM">📚</a></p>
<p><strong>论文标题：</strong> Am I a Baller? Basketball Performance Assessment from First-Person Videos</p>
<p><strong>核心技术：</strong> AlexNet/VGG特征提取 + LSTM序列建模 + 高斯混合模型 + 排序损失</p>
<p><strong>数据集：</strong> FP-Basketball</p>
<p><strong>主要贡献：</strong> 首个基于对比/排序学习的AQA方法，，验证了排序信号对AQA的有效性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Bertasius-AmBallerBasketball-2017,</span><br><span class="line">  title = &#123;Am I a Baller? Basketball Performance Assessment from First-Person Videos&#125;,</span><br><span class="line">  booktitle = &#123;2017 IEEE International Conference on Computer Vision (ICCV)&#125;,</span><br><span class="line">  author = &#123;Bertasius, Gedas and Park, Hyun Soo and Yu, Stella X. and Shi, Jianbo&#125;,</span><br><span class="line">  year = 2017,</span><br><span class="line">  pages = &#123;2196--2204&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法8：ScoringNet（关键片段与排序损失）"><a href="#方法8：ScoringNet（关键片段与排序损失）" class="headerlink" title="方法8：ScoringNet（关键片段与排序损失）"></a>方法8：ScoringNet（关键片段与排序损失）</h3><p><strong>方法名称：</strong> ScoringNet <a href="zotero://select/library/items/4NXRYVAX">📚</a></p>
<p><strong>论文标题：</strong> Scoring Net: Learning Key Fragment for Action Quality Assessment with Ranking Loss in Skilled Sports</p>
<p><strong>核心技术：</strong> CNN特征提取 + 关键片段检测模块 +Bi-LSTM+ 排序损失 + 回归器</p>
<p><strong>数据集：</strong> Mit-Dive, UNLV-Diving,UNLV-Vault</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">@incollection&#123;Li-ScoringNetLearningKey-2019,</span><br><span class="line">  title = &#123;ScoringNet: Learning Key Fragment for Action Quality Assessment with Ranking Loss in Skilled Sports&#125;,</span><br><span class="line">  booktitle = &#123;Computer Vision -- ACCV 2018&#125;,</span><br><span class="line">  author = &#123;Li, Yongjun and Chai, Xiujuan and Chen, Xilin&#125;,</span><br><span class="line">  year = 2019,</span><br><span class="line">  volume = &#123;11366&#125;,</span><br><span class="line">  pages = &#123;149--164&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法9：2S-CNN（时空分割卷积网络）"><a href="#方法9：2S-CNN（时空分割卷积网络）" class="headerlink" title="方法9：2S-CNN（时空分割卷积网络）"></a>方法9：2S-CNN（时空分割卷积网络）</h3><p><strong>方法名称：</strong> 2S-CNN (Temporal and Spatial Segment Networks) <a href="zotero://select/library/items/L2732NQP">📚</a></p>
<p><strong>论文标题：</strong> Who’s Better? Who’s Best? Pairwise Deep Ranking for Skill Determination</p>
<p><strong>核心技术：</strong> 两流网络(RGB + 光流) + 时间分割 + 配对Siamese网络 + 排序损失</p>
<p><strong>数据集：</strong> EPIC-Skills, JIGSAWS</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Doughty-WhosBetterWhos-2018,</span><br><span class="line">  author=&#123;Doughty, Hazel and Damen, Dima and Mayol-Cuevas, Walterio&#125;,</span><br><span class="line">  booktitle=&#123;CVPR&#125;, </span><br><span class="line">  title=&#123;Who&#x27;s Better? Who&#x27;s Best? Pairwise Deep Ranking for Skill Determination&#125;, </span><br><span class="line">  year=&#123;2018&#125;,</span><br><span class="line">  pages=&#123;6057-6066&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">H. Doughty, D. Damen, W. Mayol-Cuevas, &quot;Who&#x27;s Better? Who&#x27;s Best? Pairwise Deep Ranking for Skill Determination,&quot; in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 6057-6066.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法10：S3D（堆叠3D回归器）"><a href="#方法10：S3D（堆叠3D回归器）" class="headerlink" title="方法10：S3D（堆叠3D回归器）"></a>方法10：S3D（堆叠3D回归器）</h3><p><strong>方法名称：</strong> S3D (Stacking Segmental P3D) <a href="zotero://select/library/items/8T6AJNP2">📚</a></p>
<p><strong>论文标题：</strong> S3D: Stacking Segmental P3D for Action Quality Assessment</p>
<p><strong>核心技术：</strong> P3D卷积 + 视频分段 + 阶段特征融合 + 多阶段回归器堆叠</p>
<p><strong>数据集：</strong> UNLV-Dive</p>
<p><strong>主要贡献：</strong> 首个分段感知特征提取方法，将视频分割为不同动作阶段（准备、起跳、翻腾、入水）进行特征提取与阶段感知回归</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Xiang-S3DStackingSegmental-2018,</span><br><span class="line">  title = &#123;S3D: Stacking Segmental P3D for Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;2018 25th IEEE International Conference on Image Processing (ICIP)&#125;,</span><br><span class="line">  author = &#123;Xiang, Xiang and Tian, Ye and Reiter, Austin and Hager, Gregory D. and Tran, Trac D.&#125;,</span><br><span class="line">  year = 2018,</span><br><span class="line">  pages = &#123;928--932&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="方法11：C3D-AVG-MTL（多任务学习框架）"><a href="#方法11：C3D-AVG-MTL（多任务学习框架）" class="headerlink" title="方法11：C3D-AVG-MTL（多任务学习框架）"></a>方法11：C3D-AVG-MTL（多任务学习框架）</h3><p><strong>方法名称：</strong> C3D-AVG-MTL (Multi-Task Learning with Text) <a href="zotero://select/library/items/A8VEZ6AP">📚</a></p>
<p><strong>论文标题：</strong> What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment</p>
<p><strong>核心技术：</strong> C3D视频特征 + LSTM序列建模 + 多任务学习(性能评分+描述生成) + 注意机制</p>
<p><strong>数据集：</strong> MTL-AQA</p>
<p><strong>主要贡献：</strong> 首个多任务学习框架，引入文本模态（性能描述）进行AQA。证明了跨模态学习对AQA的增强效果。发布MTL-AQA成为后续研究基准</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Parmar-WhatHowWell-2019,</span><br><span class="line">  title = &#123;What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&#125;,</span><br><span class="line">  author = &#123;Parmar, Paritosh and Morris, Brendan Tran&#125;,</span><br><span class="line">  year = 2019,</span><br><span class="line">  pages = &#123;304--313&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法12：RAA（排名感知注意力）"><a href="#方法12：RAA（排名感知注意力）" class="headerlink" title="方法12：RAA（排名感知注意力）"></a>方法12：RAA（排名感知注意力）</h3><p><strong>方法名称：</strong> RAA (Rank-Aware Attention Network) <a href="zotero://select/library/items/SZKLRJKL">📚</a></p>
<p><strong>论文标题：</strong> The Pros and Cons: Rank-Aware Temporal Attention for Skill Determination in Long Videos</p>
<p><strong>核心技术：</strong> 两流网络(RGB+光流) + 时间注意机制 + 双重注意(优点/缺点) + 排序/分类头</p>
<p><strong>数据集：</strong> EPIC-Skill, BEST</p>
<p><strong>主要贡献：</strong> 提出排名感知注意力模型，采用双重注意机制分别关注动作的性能优点与缺点。创新地将正面/负面特征分离</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Doughty-ProsConsRankAware-2019,</span><br><span class="line">  title = &#123;The Pros and Cons: Rank-Aware Temporal Attention for Skill Determination in Long Videos&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  author = &#123;Doughty, Hazel and &#123;Mayol-Cuevas&#125;, Walterio and Damen, Dima&#125;,</span><br><span class="line">  year = 2019,</span><br><span class="line">  pages = &#123;7854--7863&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">H. Doughty, W. Mayol-Cuevas, D. Damen, &quot;The Pros and Cons: Rank-Aware Temporal Attention for Skill Determination in Long Videos,&quot; in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7854–7863.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法13：JR-GCN（关节关系图卷积）"><a href="#方法13：JR-GCN（关节关系图卷积）" class="headerlink" title="方法13：JR-GCN（关节关系图卷积）"></a>方法13：JR-GCN（关节关系图卷积）</h3><p><strong>方法名称：</strong> JR-GCN <a href="zotero://select/library/items/QPWNMVSV">📚</a></p>
<p><strong>论文标题：</strong> Action Assessment by Joint Relation Graphs</p>
<p><strong>核心技术</strong>：关节关系图构建 + 图卷积网络(GCN)，骨架只用于把关节的RGB块裁剪出来，还是使用RGB的关节进行图建模。</p>
<p><strong>数据集：</strong> AQA-7</p>
<p><strong>主要贡献：</strong> 将关节间的空间关系显式建模为图结构，相比CNN对背景更鲁棒</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Pan-ActionAssessmentJoint-2019,</span><br><span class="line">  author=&#123;Pan, Jia-Hui and Gao, Jibin and Zheng, Wei-Shi&#125;,</span><br><span class="line">  booktitle=&#123;ICCV&#125;, </span><br><span class="line">  title=&#123;Action Assessment by Joint Relation Graphs&#125;, </span><br><span class="line">  year=&#123;2019&#125;,</span><br><span class="line">  pages=&#123;6330-6339&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">J. Pan, J. Gao, W. Zheng, &quot;Action Assessment by Joint Relation Graphs,&quot; in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 6330-6339.</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法14：USDL（不确定性感知评分分布学习）"><a href="#方法14：USDL（不确定性感知评分分布学习）" class="headerlink" title="方法14：USDL（不确定性感知评分分布学习）"></a>方法14：USDL（不确定性感知评分分布学习）</h3><p><strong>方法名称：</strong> USDL <a href="zotero://select/library/items/W64Z5QZT">📚</a></p>
<p><strong>论文标题：</strong> Uncertainty-Aware Score Distribution Learning for Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D特征提取 + LSTM序列建模 + 高斯分布参数预测(均值+方差) + 不确定性感知损失</p>
<p><strong>数据集：</strong> JIGSAWS, AQA-7, MTL-AQA</p>
<p><strong>主要贡献：</strong> 首次将不确定性建模引入AQA，使用高斯分布建模评分的多裁判变异性。相比确定性输出，分布预测更能捕捉标注噪声与评分歧义</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Tang-UncertaintyAwareScoreDistribution-2020,</span><br><span class="line">  title = &#123;Uncertainty-Aware Score Distribution Learning for Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&#125;,</span><br><span class="line">  author = &#123;Tang, Yansong and Ni, Zanlin and Zhou, Jiahuan and Zhang, Danyang and Lu, Jiwen and Wu, Ying and Zhou, Jie&#125;,</span><br><span class="line">  year = 2020,</span><br><span class="line">  pages = &#123;9836--9845&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法16：ACTION-Net（动作-静态上下文注意网络）"><a href="#方法16：ACTION-Net（动作-静态上下文注意网络）" class="headerlink" title="方法16：ACTION-Net（动作-静态上下文注意网络）"></a>方法16：ACTION-Net（动作-静态上下文注意网络）</h3><p><strong>方法名称：</strong> ACTION-Net <a href="zotero://select/library/items/C2YPH342">📚</a></p>
<p><strong>论文标题：</strong> Hybrid Dynamic-Static Context-Aware Attention Network for Action Assessment in Long Videos</p>
<p><strong>核心技术：</strong> I3D(动作) + ResNet-50(静态上下文) + 上下文注意机制 ，基于图的全局建模</p>
<p><strong>数据集：</strong> MIT-Skate, RG(新数据集)</p>
<p><strong>主要贡献：</strong> 结合运动特征(I3D)和静态上下文(环境、器械)，特征提取和注意融合</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Zeng-HybridDynamicstaticContextaware-2020,</span><br><span class="line">  title = &#123;Hybrid Dynamic-Static Context-Aware Attention Network for Action Assessment in Long Videos&#125;,</span><br><span class="line">  booktitle = &#123;ACM MM&#125;,</span><br><span class="line">  author = &#123;Zeng, Ling-An and Hong, Fa-Ting and Zheng, Wei-Shi and Yu, Qi-Zhi and Zeng, Wei and Wang, Yao-Wei and Lai, Jian-Huang&#125;,</span><br><span class="line">  year = 2020,</span><br><span class="line">  pages = &#123;2526--2534&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Zeng, L., et al, &quot;Hybrid Dynamic-Static Context-Aware Attention Network for Action Assessment in Long Videos,&quot; in Proceedings of the 28th ACM International Conference on Multimedia, 2020, pp. 2526–2534.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法17：AIM（非对称关系建模）"><a href="#方法17：AIM（非对称关系建模）" class="headerlink" title="方法17：AIM（非对称关系建模）"></a>方法17：AIM（非对称关系建模）</h3><p><strong>方法名称：</strong> AIM <a href="zotero://select/library/items/9FF55ESX">📚</a></p>
<p><strong>论文标题：</strong> An Asymmetric Modeling for Action Assessment</p>
<p><strong>核心技术：</strong> 骨架特征提取 + 非对称角色建模(主/辅) + 互动关系编码 + GCN + 回归头，Skeleton, LSTM全局建模</p>
<p><strong>数据集：</strong> JIGSAWS, AQA-7, TASD-2</p>
<p><strong>主要贡献：</strong> 首次在AQA中显式建模多智能体间的非对称关系。针对双人/多人协作场景，区分主要参与者与辅助参与者的角色</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Gao-AsymmetricModelingAction-2020,</span><br><span class="line">  title = &#123;An Asymmetric Modeling for Action Assessment&#125;,</span><br><span class="line">  booktitle = &#123;ECCV&#125;,</span><br><span class="line">  author = &#123;Gao, Jibin and Zheng, Wei-Shi and Pan, Jia-Hui and Gao, Chengying and Wang, Yaowei and Zeng, Wei and Lai, Jianhuang&#125;,</span><br><span class="line">  year = 2020,</span><br><span class="line">  pages = &#123;222--238&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法18：MS-LSTM"><a href="#方法18：MS-LSTM" class="headerlink" title="方法18：MS-LSTM"></a>方法18：MS-LSTM</h3><p><strong>方法名称：</strong> MS-LSTM <a href="zotero://select/library/items/9VGQMRT7">📚</a></p>
<p><strong>论文标题：</strong> Learning to Score Figure Skating Sport Videos</p>
<p><strong>核心技术：</strong> 多尺度LSTM，全局依赖</p>
<p><strong>数据集：</strong> Fis-V(新数据集)，MIT-skate</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Xu-LearningScoreFigure-2020,</span><br><span class="line">  title = &#123;Learning to Score Figure Skating Sport Videos&#125;,</span><br><span class="line">  author = &#123;Xu, Chengming and Fu, Yanwei and Zhang, Bing and Chen, Zitian and Jiang, Yu-Gang and Xue, Xiangyang&#125;,</span><br><span class="line">  year = 2020,</span><br><span class="line">  journal = &#123;IEEE Transactions on Circuits and Systems for Video Technology&#125;,</span><br><span class="line">  volume = &#123;30&#125;,</span><br><span class="line">  number = &#123;12&#125;,</span><br><span class="line">  pages = &#123;4578--4590&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Xu, C., et al. &quot;Learning to Score Figure Skating Sport Videos,&quot; in IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 12, pp. 4578–4590, 2020.</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法18：CoRe（对比回归框架）"><a href="#方法18：CoRe（对比回归框架）" class="headerlink" title="方法18：CoRe（对比回归框架）"></a>方法18：CoRe（对比回归框架）</h3><p><strong>方法名称：</strong> CoRe <a href="zotero://select/library/items/K9IYUISD">📚</a></p>
<p><strong>论文标题：</strong> Group-Aware Contrastive Regression for Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D特征提取 + 参考视频池 + 对比回归损失 + 相对评分预测</p>
<p><strong>数据集：</strong> JIGSAWS, AQA-7, MTL-AQA</p>
<p><strong>主要贡献：</strong> 将评分任务转化为相对评分学习，采用参考视频进行对比。相比绝对回归，相对评分学习对数据缩放更鲁棒</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Yu-GroupawareContrastiveRegression-2021,</span><br><span class="line">  title = &#123;Group-Aware Contrastive Regression for Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;ICCV&#125;,</span><br><span class="line">  author = &#123;Yu, Xumin and Rao, Yongming and Zhao, Wenliang and Lu, Jiwen and Zhou, Jie&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  pages = &#123;7899--7908&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Yu, X., et al, &quot;Group-Aware Contrastive Regression for Action Quality Assessment,&quot; in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 7899–7908.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法19：SportCap（运动捕捉与细粒度理解）"><a href="#方法19：SportCap（运动捕捉与细粒度理解）" class="headerlink" title="方法19：SportCap（运动捕捉与细粒度理解）"></a>方法19：SportCap（运动捕捉与细粒度理解）</h3><p><strong>方法名称：</strong> SportCap <a href="zotero://select/library/items/8CV9PELD">📚</a></p>
<p><strong>论文标题：</strong> SportCap: Monocular 3D Human Motion Capture and Fine-Grained Understanding in Challenging Sports Videos</p>
<p><strong>核心技术：</strong> 单目3D姿态估计 + 骨架特征提取 + 时空图卷积 + 细粒度评分模块</p>
<p><strong>数据集：</strong> SMART(新建数据集)(来自FineGym)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Chen-SportsCapMonocular3D-2021,</span><br><span class="line">  title = &#123;SportsCap: Monocular 3D Human Motion Capture and Fine-Grained Understanding in Challenging Sports Videos&#125;,</span><br><span class="line">  author = &#123;Chen, Xin and Pang, Anqi and Yang, Wei and Ma, Yuexin and Xu, Lan and Yu, Jingyi&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  journal = &#123;International Journal of Computer Vision&#125;,</span><br><span class="line">  volume = &#123;129&#125;,</span><br><span class="line">  number = &#123;10&#125;,</span><br><span class="line">  pages = &#123;2846--2864&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法20：ST-GCN（时空图卷积网络）"><a href="#方法20：ST-GCN（时空图卷积网络）" class="headerlink" title="方法20：ST-GCN（时空图卷积网络）"></a>方法20：ST-GCN（时空图卷积网络）</h3><p><strong>方法名称：</strong> ST-GCN <a href="zotero://select/library/items/LS3C3LLB">📚</a></p>
<p><strong>论文标题：</strong> Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</p>
<p><strong>核心技术：</strong> ST-GCN卷积</p>
<p><strong>主要贡献：</strong> 建模骨架时空关系</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Yan-SpatialTemporalGraph-2018,</span><br><span class="line">author = &#123;Yan, Sijie and Xiong, Yuanjun and Lin, Dahua&#125;,</span><br><span class="line">title = &#123;Spatial temporal graph convolutional networks for skeleton-based action recognition&#125;,</span><br><span class="line">  booktitle = &#123;AAAI Conference on Artificial Intelligence (AAAI)&#125;,</span><br><span class="line">  year      = &#123;2018&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">S. Yan, Y. Xiong, D. Lin, &quot;Spatial temporal graph convolutional networks for skeleton-based action recognition,&quot; in AAAI Conference on Artificial Intelligence (AAAI), 2018.</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法21：EAGLE-Eye（极端姿态动作评分器）"><a href="#方法21：EAGLE-Eye（极端姿态动作评分器）" class="headerlink" title="方法21：EAGLE-Eye（极端姿态动作评分器）"></a>方法21：EAGLE-Eye（极端姿态动作评分器）</h3><p><strong>方法名称：</strong> EAGLE-Eye <a href="zotero://select/library/items/CE4C9TUM">📚</a></p>
<p><strong>论文标题：</strong> EAGLE-Eye: Extreme-Pose Action Grader Using Detail Bird’s-Eye View</p>
<p><strong>核心技术：</strong> 俯视图变换 + 多尺度时间卷积 + 交互建模 + 回归头</p>
<p><strong>数据集：</strong> MIT-Skate, AQA-7</p>
<p><strong>主要贡献：</strong> 采用俯视图视角增强对骨架动态的感知，捕捉骨架点动态和交互</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Nekoui-EAGLEEyeExtremeposeAction-2021,</span><br><span class="line">  title = &#123;EAGLE-Eye: Extreme-Pose Action Grader Using detaiL Bird&#x27;s-Eye View&#125;,</span><br><span class="line">  booktitle = &#123;WACV&#125;,</span><br><span class="line">  author = &#123;Nekoui, Mahdiar and Tito Cruz, Fidel Omar and Cheng, Li&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  pages = &#123;394--402&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法22：RGR（参考导向回归）"><a href="#方法22：RGR（参考导向回归）" class="headerlink" title="方法22：RGR（参考导向回归）"></a>方法22：RGR（参考导向回归）</h3><p><strong>方法名称：</strong> RGR  <a href="zotero://select/library/items/L7A36FLN">📚</a></p>
<p><strong>论文标题：</strong> Action Quality Assessment Using Siamese Network-Based Deep Metric Learning</p>
<p><strong>核心技术：</strong> I3D特征提取 + Siamese孪生网络 + 度量学习 + 相似度评分</p>
<p><strong>数据集：</strong> UNLV-Dive, MTL-AQA</p>
<p><strong>主要贡献：</strong> 使用Siamese网络将评分任务转化为相似度评分，通过对比参考样本进行相对评分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Jain-ActionQualityAssessment-2021,</span><br><span class="line">  title = &#123;Action Quality Assessment Using Siamese Network-Based Deep Metric Learning&#125;,</span><br><span class="line">  author = &#123;Jain, Hiteshi and Harit, Gaurav and Sharma, Avinash&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  journal = &#123;IEEE Trans. Circuits Syst. Video Technol.&#125;,</span><br><span class="line">  volume = &#123;31&#125;,</span><br><span class="line">  number = &#123;6&#125;,</span><br><span class="line">  pages = &#123;2260--2273&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">H. Jain, G. Harit, A. Sharma. &quot;Action Quality Assessment Using Siamese Network-Based Deep Metric Learning,&quot; in IEEE Trans. Circuits Syst. Video Technol., vol. 31, no. 6, pp. 2260–2273, 2021.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法23：TSA-Net（管自注意网络）"><a href="#方法23：TSA-Net（管自注意网络）" class="headerlink" title="方法23：TSA-Net（管自注意网络）"></a>方法23：TSA-Net（管自注意网络）</h3><p><strong>方法名称：</strong> TSA-Net <a href="zotero://select/library/items/KZC5BU2M">📚</a></p>
<p><strong>论文标题：</strong> TSA-Net: Tube Self-Attention Network for Action Quality Assessment</p>
<p><strong>核心技术：</strong> 目标检测与跟踪 + I3D特征提取 + 自注意机制 + RPN+管追踪 + 回归头</p>
<p><strong>数据集：</strong> AQA-7, MTL-AQA</p>
<p><strong>主要贡献：</strong> 结合目标跟踪和自注意机制进行动作评估，使用管状区域提高关注度</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Wang-TSANetTubeSelfAttention-2021,</span><br><span class="line">  title = &#123;TSA-Net: Tube Self-Attention Network for Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;Proceedings of the 29th ACM International Conference on Multimedia&#125;,</span><br><span class="line">  author = &#123;Wang, Shunli and Yang, Dingkang and Zhai, Peng and Chen, Chixiao and Zhang, Lihua&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  pages = &#123;4902--4910&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法24：TAL（时间注意学习）"><a href="#方法24：TAL（时间注意学习）" class="headerlink" title="方法24：TAL（时间注意学习）"></a>方法24：TAL（时间注意学习）</h3><p><strong>方法名称：</strong> TAL <a href="zotero://select/library/items/S2S3N4D9">📚</a></p>
<p><strong>论文标题：</strong> Temporal Attention Learning for Action Quality Assessment in Sports Video</p>
<p><strong>核心技术：</strong> 视频分段 + CNN特征提取 + 动态时间注意权重学习 + 加权回归</p>
<p><strong>数据集：</strong> UNLV-Dive, UNLV-Vault</p>
<p><strong>主要贡献：</strong> 动态学习不同动作阶段的注意力权重，自适应关注关键时间段</p>
<p><strong>应用背景：</strong> 不同阶段重要性差异大的动作</p>
<p><strong>优缺点：</strong> ✅ 时间建模直观，性能稳定；❌ 对长序与跨阶段关系仍有限制</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Lei-TemporalAttentionLearning-2021,</span><br><span class="line">  title = &#123;Temporal Attention Learning for Action Quality Assessment in Sports Video&#125;,</span><br><span class="line">  author = &#123;Lei, Qing and Zhang, Hongbo and Du, Jixiang&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  journal = &#123;Signal, Image and Video Processing&#125;,</span><br><span class="line">  volume = &#123;15&#125;,</span><br><span class="line">  number = &#123;7&#125;,</span><br><span class="line">  pages = &#123;1575--1583&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法25：FineDiving数据集与TSA（时间分割注意力）"><a href="#方法25：FineDiving数据集与TSA（时间分割注意力）" class="headerlink" title="方法25：FineDiving数据集与TSA（时间分割注意力）"></a>方法25：FineDiving数据集与TSA（时间分割注意力）</h3><p><strong>方法名称：</strong> TSA <a href="zotero://select/library/items/RHSQG6LX">📚</a></p>
<p><strong>论文标题：</strong> FineDiving: A Fine-Grained Dataset for Procedure-Aware Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D特征提取 + 时间分割注意力模块 + 子动作检测 + 多任务学习(评分+分割)</p>
<p><strong>数据集：</strong> FineDiving</p>
<p><strong>主要贡献：</strong> 提出首个细粒度子动作标注数据集(FineDiving)，引入时间分割注意力模块用于子过程识别。推动AQA向细粒度、可解释方向发展</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Xu-FineDivingFinegrainedDataset-2022,</span><br><span class="line">  title = &#123;FineDiving: A Fine-Grained Dataset for Procedure-Aware Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  author = &#123;Xu, Jinglin and Rao, Yongming and Yu, Xumin and Chen, Guangyi and Zhou, Jie and Lu, Jiwen&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  pages = &#123;2939--2948&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Xu, J., et al, &quot;FineDiving: A Fine-Grained Dataset for Procedure-Aware Action Quality Assessment,&quot; in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 2939–2948.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法26：GDLT（等级解耦Likert-Transformer）"><a href="#方法26：GDLT（等级解耦Likert-Transformer）" class="headerlink" title="方法26：GDLT（等级解耦Likert Transformer）"></a>方法26：GDLT（等级解耦Likert Transformer）</h3><p><strong>方法名称：</strong> GDLT <a href="zotero://select/library/items/BYRPA3W6">📚</a></p>
<p><strong>论文标题：</strong> Likert Scoring with Grade Decoupling for Long-Term Action Assessment</p>
<p><strong>核心技术：</strong> Transformer编码器 + 等级原型学习 + Likert分级机制 + 强度解码器 + 等级解耦损失，transformer全局建模</p>
<p><strong>数据集：</strong> Fis-V, RG</p>
<p><strong>主要贡献：</strong> 将长期动作评分分解为等级与强度组合，引入 Likert 分级+”等级解耦”思想。通过等级原型与解码器实现”量化分+响应强度”合成最终分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Xu-LikertScoringGrade-2022,</span><br><span class="line">  title = &#123;Likert Scoring with Grade Decoupling for Long-Term Action Assessment&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  author = &#123;Xu, Angchi and Zeng, Ling-An and Zheng, Wei-Shi&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  pages = &#123;3222--3231&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法27：TPT（时间解析Transformer）"><a href="#方法27：TPT（时间解析Transformer）" class="headerlink" title="方法27：TPT（时间解析Transformer）"></a>方法27：TPT（时间解析Transformer）</h3><p><strong>方法名称：</strong> TPT <a href="zotero://select/library/items/E6E4WULV">📚</a></p>
<p><strong>论文标题：</strong> Action Quality Assessment with Temporal Parsing Transformer</p>
<p><strong>核心技术：</strong> I3D特征提取 + 可学习查询 + Transformer编码器 + 特征解析模块 + 排序+稀疏性损失 + 回归头</p>
<p><strong>数据集：</strong> JIGSAWS, AQA-7, MTL-AQA</p>
<p><strong>主要贡献：</strong> 使用学习查询解耦整体视频特征为局部时间特征，实现可解析的动作过程识别。引入排序和稀疏性损失增强约束</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Bai-ActionQualityAssessment-2022,</span><br><span class="line">  title = &#123;Action Quality Assessment with Temporal Parsing Transformer&#125;,</span><br><span class="line">  booktitle = &#123;ECCV&#125;,</span><br><span class="line">  author = &#123;Bai, Yang and Zhou, Desen and Zhang, Songyang and Wang, Jian and Ding, Errui and Guan, Yu and Long, Yang and Wang, Jingdong&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  pages = &#123;422--438&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Bai, Y., et al, &quot;Action Quality Assessment with Temporal Parsing Transformer,&quot; in Computer Vision - ECCV 2022, 2022, pp. 422–438.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法28：I3D-Transformer"><a href="#方法28：I3D-Transformer" class="headerlink" title="方法28：I3D-Transformer"></a>方法28：I3D-Transformer</h3><p><strong>方法名称：</strong> I3D-Transformer <a href="zotero://select/library/items/8L24CQQB">📚</a></p>
<p><strong>论文标题：</strong> Action Quality Assessment Using Transformers</p>
<p><strong>核心技术：</strong> I3D特征提取 + 多种Transformer变体(ViT, BERT-style, 自定义) + 回归头</p>
<p><strong>数据集：</strong> MTL-AQA</p>
<p><strong>主要贡献：</strong> 系统探索不同Transformer架构用于AQA，比较各种变体的效果</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Iyer-ActionQualityAssessment-2022,</span><br><span class="line">  title = &#123;Action Quality Assessment Using Transformers&#125;,</span><br><span class="line">  author = &#123;Iyer, Abhay and Alali, Mohammad and Bodala, Hemanth and Vaidya, Sunit&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  number = &#123;arXiv:2207.12318&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法29：I3D-TA（时间感知注意）"><a href="#方法29：I3D-TA（时间感知注意）" class="headerlink" title="方法29：I3D-TA（时间感知注意）"></a>方法29：I3D-TA（时间感知注意）</h3><p><strong>方法名称：</strong> I3D-TA <a href="zotero://select/library/items/GAHLQTSM">📚</a></p>
<p><strong>论文标题：</strong> Learning Time-Aware Features for Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D特征提取 + 时间感知注意模块 + 关键片段提取 + 加权回归</p>
<p><strong>数据集：</strong> MTL-AQA</p>
<p><strong>主要贡献：</strong> 采用时间感知注意机制学习关键片段，动态调整不同时刻的注意权重</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhang-LearningTimeawareFeatures-2022,</span><br><span class="line">  title = &#123;Learning Time-Aware Features for Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Zhang, Yu and Xiong, Wei and Mi, Siya&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  journal = &#123;Pattern Recognition Letters&#125;,</span><br><span class="line">  volume = &#123;158&#125;,</span><br><span class="line">  pages = &#123;104--110&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法30：UD-AQA（不确定性驱动的AQA）"><a href="#方法30：UD-AQA（不确定性驱动的AQA）" class="headerlink" title="方法30：UD-AQA（不确定性驱动的AQA）"></a>方法30：UD-AQA（不确定性驱动的AQA）</h3><p><strong>方法名称：</strong> UD-AQA  <a href="zotero://select/library/items/7D6JYDU4">📚</a></p>
<p><strong>论文标题：</strong> Uncertainty-Driven Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D特征提取 + 条件变分自编码器(CVAE) + 隐变量采样 + 高斯分布建模</p>
<p><strong>数据集：</strong> JIGSAWS, MTL-AQA, FineDiving</p>
<p><strong>主要贡献：</strong> 采用条件变分自编码器(CVAE)建模评分的内在模糊性，允许从分布中采样多个可能的评分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Zhou-UncertaintyDrivenActionQuality-2022,</span><br><span class="line">  title = &#123;Uncertainty-Driven Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Zhou, Caixia and Huang, Yaping&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  number = &#123;arXiv:2207.14513&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法31：PCLN（配对对比学习网络）"><a href="#方法31：PCLN（配对对比学习网络）" class="headerlink" title="方法31：PCLN（配对对比学习网络）"></a>方法31：PCLN（配对对比学习网络）</h3><p><strong>方法名称：</strong> PCLN <a href="zotero://select/library/items/QMVE5RMD">📚</a></p>
<p><strong>论文标题：</strong> Pairwise Contrastive Learning Network for Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D特征提取 + 样本对采集 + 对比损失 + 排序损失 + 回归损失</p>
<p><strong>数据集：</strong> AQA-7, MTL-AQA</p>
<p><strong>主要贡献：</strong> 结合配对学习和回归学习进行对比AQA，使用动态样本对生成策略</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@incollection&#123;Li-PairwiseContrastiveLearning-2022,</span><br><span class="line">  title = &#123;Pairwise Contrastive Learning Network for Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;Computer Vision -- ECCV 2022&#125;,</span><br><span class="line">  author = &#123;Li, Mingzhe and Zhang, Hong-Bo and Lei, Qing and Fan, Zongwen and Liu, Jinghua and Du, Ji-Xiang&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  volume = &#123;13664&#125;,</span><br><span class="line">  pages = &#123;457--473&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法32：MD与CVCSPC（运动解缠绕）"><a href="#方法32：MD与CVCSPC（运动解缠绕）" class="headerlink" title="方法32：MD与CVCSPC（运动解缠绕）"></a>方法32：MD与CVCSPC（运动解缠绕）</h3><p><strong>方法名称：</strong> S^4 AQA <a href="zotero://select/library/items/HQF9GFGF">📚</a></p>
<p><strong>论文标题：</strong> Semi-Supervised Action Quality Assessment with Self-Supervised Segment Feature Recovery</p>
<p><strong>核心技术：</strong> I3D特征提取 + 运动解缠绕模块 + 自监督恢复损失 + 对比一致性正则化，无标签测试结果。</p>
<p><strong>数据集：</strong> MTL-AQA, RG, JIGSAWS</p>
<p><strong>主要贡献：</strong> 将错误运动从全局运动中分离，采用自监督方法进行半监督AQA。运动解缠绕思想</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhang-SemiSupervisedActionQuality-2022,</span><br><span class="line">  title = &#123;Semi-Supervised Action Quality Assessment With Self-Supervised Segment Feature Recovery&#125;,</span><br><span class="line">  author = &#123;Zhang, Shao-Jie and Pan, Jia-Hui and Gao, Jibin and Zheng, Wei-Shi&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  journal = &#123;IEEE Transactions on Circuits and Systems for Video Technology&#125;,</span><br><span class="line">  volume = &#123;32&#125;,</span><br><span class="line">  number = &#123;9&#125;,</span><br><span class="line">  pages = &#123;6017--6028&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法：Adaptive-Net（自适应网络）"><a href="#方法：Adaptive-Net（自适应网络）" class="headerlink" title="方法：Adaptive Net（自适应网络）"></a>方法：Adaptive Net（自适应网络）</h3><p><strong>方法名称：</strong> Adaptive Net <a href="zotero://select/library/items/X2NJPU4I">📚</a></p>
<p><strong>论文标题：</strong> Adaptive Action Assessment</p>
<p><strong>核心技术：</strong> 神经架构搜索(NAS) + 适配模块 + 领域自适应 + 跨域AQA框架</p>
<p><strong>数据集：</strong> JIGSAWS, AQA-7, EPIC-Skill, BEST</p>
<p><strong>主要贡献：</strong> 目标检测，然后动作评估，自动网络架构搜索(NAS)进行跨域泛化，自动学习最优的领域适配机制</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Pan-AdaptiveActionAssessment-2022,</span><br><span class="line">  title = &#123;Adaptive Action Assessment&#125;,</span><br><span class="line">  author = &#123;Pan, Jia-Hui and Gao, Jibin and Zheng, Wei-Shi&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  journal = &#123;IEEE Transactions on Pattern Analysis and Machine Intelligence&#125;,</span><br><span class="line">  volume = &#123;44&#125;,</span><br><span class="line">  number = &#123;12&#125;,</span><br><span class="line">  pages = &#123;8779--8795&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法33：FSPN（细粒度时空解析网络）"><a href="#方法33：FSPN（细粒度时空解析网络）" class="headerlink" title="方法33：FSPN（细粒度时空解析网络）"></a>方法33：FSPN（细粒度时空解析网络）</h3><p><strong>方法名称：</strong> FSPN <a href="zotero://select/library/items/GVX3DHDH">📚</a></p>
<p><strong>论文标题：</strong> Fine-Grained Spatio-Temporal Parsing Network for Action Quality Assessment</p>
<p><strong>核心技术：</strong> 时空特征提取 + 细粒度子动作检测 + 多尺度Transformer模块 + 层次化评分</p>
<p><strong>数据集：</strong> AQA-7, MTL-AQA, FineDiving</p>
<p><strong>主要贡献：</strong> 以演员为中心，提取细粒度子动作特征，采用多尺度时空Transformer模块进行分层处理</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Gedamu-FineGrainedSpatioTemporalParsing-2023,</span><br><span class="line">  title = &#123;Fine-Grained Spatio-Temporal Parsing Network for Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Gedamu, Kumie and Ji, Yanli and Yang, Yang and Shen, Heng Tao&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;IEEE TRANSACTIONS ON IMAGE PROCESSING&#125;,</span><br><span class="line">  volume = &#123;32&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法34：MSRM与SSRMM（多阶段回归模块）"><a href="#方法34：MSRM与SSRMM（多阶段回归模块）" class="headerlink" title="方法34：MSRM与SSRMM（多阶段回归模块）"></a>方法34：MSRM与SSRMM（多阶段回归模块）</h3><p><strong>方法名称：</strong> MSRM/SSRMM <a href="zotero://select/library/items/ZXBRWTS6">📚</a></p>
<p><strong>论文标题：</strong> Learning and Fusing Multiple Hidden Substages for Action Quality Assessment</p>
<p><strong>核心技术：</strong> CNN特征提取 + 5阶段分割 + 多个回归器 + 阶段特定训练策略</p>
<p><strong>数据集：</strong> UNLV-Dive</p>
<p><strong>主要贡献：</strong> 将视频分为5个阶段(助跑、起跳、翻腾、转身、入水)进行回归，提出不同场景的训练策略</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Dong-LearningFusingMultiple-2021,</span><br><span class="line">  title = &#123;Learning and Fusing Multiple Hidden Substages for Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Dong, Li-Jia and Zhang, Hong-Bo and Shi, Qinghongya and Lei, Qing and Du, Ji-Xiang and Gao, Shangce&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  journal = &#123;Knowledge-Based Systems&#125;,</span><br><span class="line">  volume = &#123;229&#125;,</span><br><span class="line">  pages = &#123;107388&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法35：TECN（高斯引导帧序列编码网络）"><a href="#方法35：TECN（高斯引导帧序列编码网络）" class="headerlink" title="方法35：TECN（高斯引导帧序列编码网络）"></a>方法35：TECN（高斯引导帧序列编码网络）</h3><p><strong>方法名称：</strong> TECN  <a href="zotero://select/library/items/RECHPGSF">📚</a></p>
<p><strong>论文标题：</strong> Gaussian Guided Frame Sequence Encoder Network for Action Quality Assessment</p>
<p><strong>核心技术：</strong> 帧序列编码 + 高斯损失函数 + 概率拟合 + 回归头</p>
<p><strong>数据集：</strong> AQA-7, MTL-AQA</p>
<p><strong>主要贡献：</strong> 使用高斯损失函数最大化预测评分与分布的拟合概率，增强概率建模</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Li-GaussianGuidedFrame-2023,</span><br><span class="line">  title = &#123;Gaussian Guided Frame Sequence Encoder Network for Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Li, Ming-Zhe and Zhang, Hong-Bo and Dong, Li-Jia and Lei, Qing and Du, Ji-Xiang&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;Complex \&amp; Intelligent Systems&#125;,</span><br><span class="line">  volume = &#123;9&#125;,</span><br><span class="line">  number = &#123;2&#125;,</span><br><span class="line">  pages = &#123;1963--1974&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法36：DAE（分布自编码器）"><a href="#方法36：DAE（分布自编码器）" class="headerlink" title="方法36：DAE（分布自编码器）"></a>方法36：DAE（分布自编码器）</h3><p><strong>方法名称：</strong> DAE <a href="zotero://select/library/items/I4QN2633">📚</a></p>
<p><strong>论文标题：</strong> Auto-Encoding Score Distribution Regression for Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D特征提取 + VAE编码器 + 评分分布参数预测 + 重构损失</p>
<p><strong>数据集：</strong> JIGSAWS, AQA-7, MTL-AQA</p>
<p><strong>主要贡献：</strong> 使用VAE技术将视频特征编码为评分分布，实现隐空间的分布建模</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhang-AutoencodingScoreDistribution-2024,</span><br><span class="line">  title = &#123;Auto-Encoding Score Distribution Regression for Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Zhang, Boyu and Chen, Jiayuan and Xu, Yinfei and Zhang, Hui and Yang, Xu and Geng, Xin&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;Neural Computing and Applications&#125;,</span><br><span class="line">  volume = &#123;36&#125;,</span><br><span class="line">  number = &#123;2&#125;,</span><br><span class="line">  pages = &#123;929--942&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Zhang, B., et al. &quot;Auto-Encoding Score Distribution Regression for Action Quality Assessment,&quot; in Neural Comput. Appl., vol. 36, no. 2, pp. 929–942, 2024.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法37：LUSD-Net（定位辅助不确定性评分解缠网络）"><a href="#方法37：LUSD-Net（定位辅助不确定性评分解缠网络）" class="headerlink" title="方法37：LUSD-Net（定位辅助不确定性评分解缠网络）"></a>方法37：LUSD-Net（定位辅助不确定性评分解缠网络）</h3><p><strong>方法名称：</strong> LUSD-Net <a href="zotero://select/library/items/TCXEFQT9">📚</a></p>
<p><strong>论文标题：</strong> Localization-Assisted Uncertainty Score Disentanglement Network for Action Quality Assessment</p>
<p><strong>核心技术：</strong> 动作定位模块 + 不确定性分解(PCS+TES) + 高斯分布建模，transformer全局建模</p>
<p><strong>数据集：</strong> Fis-V, FineFS（新数据集）</p>
<p><strong>主要贡献：</strong> 通过不确定性分解模块解耦过程一致性评分(PCS)和技术执行评分(TES)，技术子动作定位模块，引入定位辅助的不确定性解耦</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Ji-LocalizationassistedUncertaintyScore-2023,</span><br><span class="line">  title = &#123;Localization-Assisted Uncertainty Score Disentanglement Network for Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;ACM MM&#125;,</span><br><span class="line">  author = &#123;Ji, Yanli and Ye, Lingfeng and Huang, Huili and Mao, Lijing and Zhou, Yang and Gao, Lingling&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  pages = &#123;8590--8597&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Ji, Y., et al, &quot;Localization-Assisted Uncertainty Score Disentanglement Network for Action Quality Assessment,&quot; in Proceedings of the 31st ACM International Conference on Multimedia, 2023, pp. 8590–8597.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法38：ASTRM（跨阶段时间推理模块）"><a href="#方法38：ASTRM（跨阶段时间推理模块）" class="headerlink" title="方法38：ASTRM（跨阶段时间推理模块）"></a>方法38：ASTRM（跨阶段时间推理模块）</h3><p><strong>方法名称：</strong> ASTRM <a href="zotero://select/library/items/8J6TK6BS">📚</a></p>
<p><strong>论文标题：</strong> Improving Action Quality Assessment with Across-Staged Temporal Reasoning on Imbalanced Data</p>
<p><strong>核心技术：</strong> I3D特征提取 + 子动作分割 + 跨阶段时间推理模块 + 对不均衡数据的处理</p>
<p><strong>数据集：</strong> FineDiving</p>
<p><strong>主要贡献：</strong> 采用交叉阶段时间推理学习阶段间的时间关系，处理样本不均衡问题</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Lian-ImprovingActionQuality-2023,</span><br><span class="line">  title = &#123;Improving Action Quality Assessment with Across-Staged Temporal Reasoning on Imbalanced Data&#125;,</span><br><span class="line">  author = &#123;Lian, Pu-Xiang and Shao, Zhi-Gang&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;Applied Intelligence&#125;,</span><br><span class="line">  volume = &#123;53&#125;,</span><br><span class="line">  number = &#123;24&#125;,</span><br><span class="line">  pages = &#123;30443--30454&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法39：RG-AQA（回放引导的动作质量评估）"><a href="#方法39：RG-AQA（回放引导的动作质量评估）" class="headerlink" title="方法39：RG-AQA（回放引导的动作质量评估）"></a>方法39：RG-AQA（回放引导的动作质量评估）</h3><p><strong>方法名称：</strong> RG-AQA <a href="zotero://select/library/items/NIZWHAR8">📚</a></p>
<p><strong>论文标题：</strong> A Figure Skating Jumping Dataset for Replay-Guided Action Quality Assessment</p>
<p><strong>核心技术：</strong> 三流网络(现场+回放+示范) + 对比学习 + 三角形相似度 + 回归头，transformer全局建模</p>
<p><strong>数据集：</strong> RFJS(新建数据集)</p>
<p><strong>主要贡献：</strong> 首次利用现场/回放/示范视频的三流对比学习进行AQA</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Liu-FigureSkatingJumping-2023,</span><br><span class="line">  title = &#123;A Figure Skating Jumping Dataset for Replay-Guided Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;Proceedings of the 31st ACM International Conference on Multimedia&#125;,</span><br><span class="line">  author = &#123;Liu, Yanchao and Cheng, Xina and Ikenaga, Takeshi&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  pages = &#123;2437--2445&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法40：PSL（伪子评分学习）"><a href="#方法40：PSL（伪子评分学习）" class="headerlink" title="方法40：PSL（伪子评分学习）"></a>方法40：PSL（伪子评分学习）</h3><p><strong>方法名称：</strong> PSL <a href="zotero://select/library/items/4YRMGH59">📚</a></p>
<p><strong>论文标题：</strong> Label-Reconstruction-Based Pseudo-Subscore Learning for Action Quality Assessment in Sporting Events</p>
<p><strong>核心技术：</strong> 总体评分 + 伪子评分生成 + 多任务辅助学习 + 迭代精化</p>
<p><strong>数据集：</strong> UNLV-Dive</p>
<p><strong>主要贡献：</strong> 使用总体评分作为训练标签，为子阶段生成伪评分标签，减少标注成本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhang-LabelreconstructionbasedPseudosubscoreLearning-2023,</span><br><span class="line">  title = &#123;Label-Reconstruction-Based Pseudo-Subscore Learning for Action Quality Assessment in Sporting Events&#125;,</span><br><span class="line">  author = &#123;Zhang, Hong-Bo and Dong, Li-Jia and Lei, Qing and Yang, Li-Jie and Du, Ji-Xiang&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;Applied Intelligence&#125;,</span><br><span class="line">  volume = &#123;53&#125;,</span><br><span class="line">  number = &#123;9&#125;,</span><br><span class="line">  pages = &#123;10053--10067&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法41：GOAT（群体感知注意）"><a href="#方法41：GOAT（群体感知注意）" class="headerlink" title="方法41：GOAT（群体感知注意）"></a>方法41：GOAT（群体感知注意）</h3><p><strong>方法名称：</strong> GOAT <a href="zotero://select/library/items/57BAN795">📚</a></p>
<p><strong>论文标题：</strong> Logo: A Long-Form Video Dataset for Group Action Quality Assessment</p>
<p><strong>核心技术：</strong> 群体检测与跟踪 + GCN提取空间群体特征 + 时间特征融合 + 关键片段注意，transformer全局建模</p>
<p><strong>数据集：</strong> LOGO(新数据集)</p>
<p><strong>主要贡献：</strong> 首个长形式群体动作数据集(LOGO)，使用GCN提取空间群体特征进行评估</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Zhang-LOGOLongFormVideo-2023,</span><br><span class="line">  title = &#123;LOGO: A Long-Form Video Dataset for Group Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  author = &#123;Zhang, Shiyi and Dai, Wenxun and Wang, Sujia and Shen, Xiangwei and Lu, Jiwen and Zhou, Jie and Tang, Yansong&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  pages = &#123;2405--2414&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法42：HGCN（分层图卷积网络）"><a href="#方法42：HGCN（分层图卷积网络）" class="headerlink" title="方法42：HGCN（分层图卷积网络）"></a>方法42：HGCN（分层图卷积网络）</h3><p><strong>方法名称：</strong> HGCN <a href="zotero://select/library/items/PZ24VZ3A">📚</a></p>
<p><strong>论文标题：</strong> Hierarchical Graph Convolutional Networks for Action Quality Assessment</p>
<p><strong>核心技术：</strong>  时序图分层构建 + 多层GCN + 时序LSTM + 回归头，<strong>显式图结构下的全局建模</strong></p>
<p><strong>数据集：</strong> JIGSAWS AQA-7, MTL-AQA</p>
<p><strong>主要贡献：</strong> 消除片段内的语义混淆，构造有意义的场景捕捉局部动作动态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhou-HierarchicalGraphConvolutional-2023,</span><br><span class="line">  title = &#123;Hierarchical Graph Convolutional Networks for Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Zhou, Kanglei and Ma, Yue and Shum, Hubert P. H. and Liang, Xiaohui&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;IEEE Transactions on Circuits and Systems for Video Technology&#125;,</span><br><span class="line">  volume = &#123;33&#125;,</span><br><span class="line">  number = &#123;12&#125;,</span><br><span class="line">  pages = &#123;7749--7763&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Zhou, K., et al. &quot;Hierarchical Graph Convolutional Networks for Action Quality Assessment,&quot; in IEEE Transactions on Circuits and Systems for Video Technology, vol. 33, no. 12, pp. 7749–7763, 2023.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法43：MS-GCN（多骨架结构GCN）"><a href="#方法43：MS-GCN（多骨架结构GCN）" class="headerlink" title="方法43：MS-GCN（多骨架结构GCN）"></a>方法43：MS-GCN（多骨架结构GCN）</h3><p><strong>方法名称：</strong> MS-GCN <a href="zotero://select/library/items/Z4RBJW4I">📚</a></p>
<p><strong>论文标题：</strong> Multi-Skeleton Structures Graph Convolutional Network for Action Quality Assessment in Long Videos</p>
<p><strong>核心技术：</strong> 骨架提取 + 三种图结构(自连接、部分内连接、部分间连接) + 并行GCN + 特征融合 + 回归， transformer全局建模，Skeleton</p>
<p><strong>数据集：</strong> RG, MIT-skate</p>
<p><strong>主要贡献：</strong> 构造三种骨架图进行特征提取，充分利用不同连接模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Lei-MultiskeletonStructuresGraph-2023,</span><br><span class="line">  title = &#123;Multi-Skeleton Structures Graph Convolutional Network for Action Quality Assessment in Long Videos&#125;,</span><br><span class="line">  author = &#123;Lei, Qing and Li, Huiying and Zhang, Hongbo and Du, Jixiang and Gao, Shangce&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;Applied Intelligence&#125;,</span><br><span class="line">  volume = &#123;53&#125;,</span><br><span class="line">  number = &#123;19&#125;,</span><br><span class="line">  pages = &#123;21692--21705&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Lei, Q., et al. &quot;Multi-Skeleton Structures Graph Convolutional Network for Action Quality Assessment in Long Videos,&quot; in Applied Intelligence, vol. 53, no. 19, pp. 21692–21705, 2023.</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法44：MLP-Mixer（长期运动音频-视觉建模）"><a href="#方法44：MLP-Mixer（长期运动音频-视觉建模）" class="headerlink" title="方法44：MLP-Mixer（长期运动音频-视觉建模）"></a>方法44：MLP-Mixer（长期运动音频-视觉建模）</h3><p><strong>方法名称：</strong> MLP-Mixer <a href="zotero://select/library/items/ZY3XKV4S">📚</a></p>
<p><strong>论文标题：</strong> Skating-Mixer: Long-Term Sport Audio-Visual Modeling with MLPs</p>
<p><strong>核心技术：</strong> RGB，Radio，特征提取 + MLP-Mixer架构 + 记忆机制 + 长期依赖建模，一种类似LSTM，RNN的记忆时序模型，<strong>递归式顺序建模，局部建模</strong></p>
<p><strong>数据集：</strong> Fis-V, FS1000</p>
<p><strong>主要贡献：</strong> 提取音视频特征，结合MLP-Mixer与记忆机制处理长期依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Xia-SkatingMixerLongTermSport-2023,</span><br><span class="line">  title = &#123;Skating-Mixer: Long-Term Sport Audio-Visual Modeling with MLPs&#125;,</span><br><span class="line">  author = &#123;Xia, Jingfei and Zhuge, Mingchen and Geng, Tiantian and Fan, Shun and Wei, Yuantai and He, Zhenyu and Zheng, Feng&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;AAAI&#125;,</span><br><span class="line">  volume = &#123;37&#125;,</span><br><span class="line">  number = &#123;3&#125;,</span><br><span class="line">  pages = &#123;2901--2909&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法45：AdaST（自适应阶段感知评估技能转移）"><a href="#方法45：AdaST（自适应阶段感知评估技能转移）" class="headerlink" title="方法45：AdaST（自适应阶段感知评估技能转移）"></a>方法45：AdaST（自适应阶段感知评估技能转移）</h3><p><strong>方法名称：</strong> AdaST <a href="zotero://select/library/items/5NCXK24S">📚</a></p>
<p><strong>论文标题：</strong> Adaptive Stage-Aware Assessment Skill Transfer for Skill Determination</p>
<p><strong>核心技术：</strong> 源动作评估网络 + 自适应阶段对齐 + 领域自适应 + 迁移学习框架</p>
<p><strong>数据集：</strong> AQA-7, EPIC-Skill, BEST</p>
<p><strong>主要贡献：</strong> 自适应源动作搜索，将源动作评估技能转移到目标动作的相应阶段</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhang-AdaptiveStageAwareAssessment-2024,</span><br><span class="line">  title = &#123;Adaptive Stage-Aware Assessment Skill Transfer for Skill Determination&#125;,</span><br><span class="line">  author = &#123;Zhang, Shao-Jie and Pan, Jia-Hui and Gao, Jibin and Zheng, Wei-Shi&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;IEEE Transactions on Multimedia&#125;,</span><br><span class="line">  volume = &#123;26&#125;,</span><br><span class="line">  pages = &#123;4061--4072&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Zhang, S., et al. &quot;Adaptive Stage-Aware Assessment Skill Transfer for Skill Determination,&quot; in IEEE Transactions on Multimedia, vol. 26, pp. 4061–4072, 2024.</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法46：SGN（语义引导网络）"><a href="#方法46：SGN（语义引导网络）" class="headerlink" title="方法46：SGN（语义引导网络）"></a>方法46：SGN（语义引导网络）</h3><p><strong>方法名称：</strong> SGN <a href="zotero://select/library/items/CSSCQTDJ">📚</a></p>
<p><strong>论文标题：</strong> Learning Semantics-Guided Representations for Scoring Figure Skating</p>
<p><strong>核心技术：</strong> 教师-学生架构 + 可学习原子查询 + 语义引导注意 + 文本特征融合，RGB，Text，transformer全局建模</p>
<p><strong>数据集：</strong> MTL-AQA, Fis-V, FS1000，OlympicFS</p>
<p><strong>主要贡献：</strong> 教师-学生架构，使用学习的原子查询和注意机制适应文本语义知识</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Du-LearningSemanticsGuidedRepresentations-2024,</span><br><span class="line">  title = &#123;Learning Semantics-Guided Representations for Scoring Figure Skating&#125;,</span><br><span class="line">  author = &#123;Du, Zexing and He, Di and Wang, Xue and Wang, Qing&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;IEEE Transactions on Multimedia&#125;,</span><br><span class="line">  volume = &#123;26&#125;,</span><br><span class="line">  pages = &#123;4987--4997&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法47：MCoRe（多阶段对比回归）"><a href="#方法47：MCoRe（多阶段对比回归）" class="headerlink" title="方法47：MCoRe（多阶段对比回归）"></a>方法47：MCoRe（多阶段对比回归）</h3><p><strong>方法名称：</strong> MCoRe <a href="zotero://select/library/items/DP3TFVE2">📚</a></p>
<p><strong>论文标题：</strong> Multi-Stage Contrastive Regression for Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D特征提取 + 子动作分割 + 阶段内对比损失 + 多阶段回归器</p>
<p><strong>数据集：</strong> FineDiving</p>
<p><strong>主要贡献：</strong> 在不同阶段内建立正负样本对，增强阶段分割准确性与评分精细度</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;An-MultiStageContrastiveRegression-2024,</span><br><span class="line">  title = &#123;Multi-Stage Contrastive Regression for Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024&#125;,</span><br><span class="line">  author = &#123;An, Qi and Qi, Mengshi and Ma, Huadong&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;4110--4114&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法48：T2CR（双路径目标感知对比回归）"><a href="#方法48：T2CR（双路径目标感知对比回归）" class="headerlink" title="方法48：T2CR（双路径目标感知对比回归）"></a>方法48：T2CR（双路径目标感知对比回归）</h3><p><strong>方法名称：</strong> T2CR <a href="zotero://select/library/items/IGLKTCPD">📚</a></p>
<p><strong>论文标题：</strong> Two-Path Target-Aware Contrastive Regression for Action Quality Assessment</p>
<p><strong>核心技术：</strong> 直接回归路径(绝对评分) + 对比回归路径(相对评分) + 目标感知特征融合 + 双路径损失</p>
<p><strong>数据集：</strong> JIGSAWS, AQA-7, MTL-AQA, FineDiving</p>
<p><strong>主要贡献：</strong> 融合直接回归和对比回归，结合全局和局部特征，双路径互补</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Ke-TwopathTargetawareContrastive-2024,</span><br><span class="line">  title = &#123;Two-Path Target-Aware Contrastive Regression for Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Ke, Xiao and Xu, Huangbiao and Lin, Xiaofeng and Guo, Wenzhong&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;Information Sciences&#125;,</span><br><span class="line">  volume = &#123;664&#125;,</span><br><span class="line">  pages = &#123;120347&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Ke, X., et al. &quot;Two-Path Target-Aware Contrastive Regression for Action Quality Assessment,&quot; in Information Sciences, vol. 664, pp. 120347, 2024.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法49：SSPR（语义序列性能回归）"><a href="#方法49：SSPR（语义序列性能回归）" class="headerlink" title="方法49：SSPR（语义序列性能回归）"></a>方法49：SSPR（语义序列性能回归）</h3><p><strong>方法名称：</strong> SSPR <a href="zotero://select/library/items/679DEMQZ">📚</a></p>
<p><strong>论文标题：</strong> Assessing Action Quality with Semantic-Sequence Performance Regression and Densely Distributed Sample Weighting</p>
<p><strong>核心技术：</strong> 分割模块+MS-TCN</p>
<p><strong>数据集：</strong> UNLV-Dive, AQA-7</p>
<p><strong>主要贡献：</strong> 提取语义特征进行分段，采用密集加权策略增强样本利用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Huang-AssessingActionQuality-2024,</span><br><span class="line">  title = &#123;Assessing Action Quality with Semantic-Sequence Performance Regression and Densely Distributed Sample Weighting&#125;,</span><br><span class="line">  author = &#123;Huang, Feng and Li, Jianjun&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;Applied Intelligence&#125;,</span><br><span class="line">  volume = &#123;54&#125;,</span><br><span class="line">  number = &#123;4&#125;,</span><br><span class="line">  pages = &#123;3245--3259&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法50：FineParser（细粒度时空动作解析器）"><a href="#方法50：FineParser（细粒度时空动作解析器）" class="headerlink" title="方法50：FineParser（细粒度时空动作解析器）"></a>方法50：FineParser（细粒度时空动作解析器）</h3><p><strong>方法名称：</strong> FineParser <a href="zotero://select/library/items/DBY5TWPF">📚</a></p>
<p><strong>论文标题：</strong> FineParser: A Fine-Grained Spatio-Temporal Action Parser for Human-Centric Action Quality Assessment</p>
<p><strong>核心技术：</strong> 四模块设计：(1)空间动作解析(SAP-检测关键身体部位), (2)时间动作解析(TAP-分割子动作), (3)静态视觉编码(SVE-背景特征), (4)细粒度对比回归(FCR)</p>
<p><strong>数据集：</strong> MTL-AQA, FineDiving, FineDiving-HM</p>
<p><strong>主要贡献：</strong> 四模块设计实现精细时空解析，显著提升可解释性与性能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Xu-FineParserFineGrainedSpatioTemporal-2024,</span><br><span class="line">  title = &#123;FineParser: A Fine-Grained Spatio-Temporal Action Parser for Human-Centric Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  author = &#123;Xu, Jinglin and Yin, Sibo and Zhao, Guohao and Wang, Zishuo and Peng, Yuxin&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;14628--14637&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法51：STSA（时空分割注意）"><a href="#方法51：STSA（时空分割注意）" class="headerlink" title="方法51：STSA（时空分割注意）"></a>方法51：STSA（时空分割注意）</h3><p><strong>方法名称：</strong> STSA <a href="zotero://select/library/items/226XHFUT">📚</a></p>
<p><strong>论文标题：</strong> Procedure-Aware Action Quality Assessment: Datasets and Performance Evaluation</p>
<p><strong>核心技术：</strong> 时间分割(TSA) + 空间动作注意(SMA) + 多尺度融合，对比学习</p>
<p><strong>数据集：</strong> FineDiving</p>
<p><strong>主要贡献：</strong> 在TSA基础上加入空间动作注意模块(SMA)，更好地区分前景和背景</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Xu-ProcedureAwareActionQuality-2024,</span><br><span class="line">  title = &#123;Procedure-Aware Action Quality Assessment: Datasets and Performance Evaluation&#125;,</span><br><span class="line">  author = &#123;Xu, Jinglin and Rao, Yongming and Zhou, Jie and Lu, Jiwen&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;International Journal of Computer Vision&#125;,</span><br><span class="line">  volume = &#123;132&#125;,</span><br><span class="line">  number = &#123;12&#125;,</span><br><span class="line">  pages = &#123;6069--6090&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法52：Rhythmer（节奏感知Transformer）"><a href="#方法52：Rhythmer（节奏感知Transformer）" class="headerlink" title="方法52：Rhythmer（节奏感知Transformer）"></a>方法52：Rhythmer（节奏感知Transformer）</h3><p><strong>方法名称：</strong> Rhythmer <a href="zotero://select/library/items/Q2F58496">📚</a></p>
<p><strong>论文标题：</strong> Ranking-Based Skill Assessment with Rhythm-Aware Transformer</p>
<p><strong>核心技术：</strong> 节奏特征提取 + 自适应持续时间建模 + Transformer编码器 + 排序/分类头</p>
<p><strong>数据集：</strong> EPIC-Skill, ROSMA, HeiChole</p>
<p><strong>主要贡献：</strong> 自适应挖掘任务持续时间相关的节奏模式，动态调整注意力焦点</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Luo-RhythmerRankingBasedSkill-2025,</span><br><span class="line">  title = &#123;Rhythmer: Ranking-Based Skill Assessment With Rhythm-Aware Transformer&#125;,</span><br><span class="line">  author = &#123;Luo, Zhuang and Xiao, Yang and Yang, Feng and Zhou, Joey Tianyi and Fang, Zhiwen&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;IEEE Transactions on Circuits and Systems for Video Technology&#125;,</span><br><span class="line">  volume = &#123;35&#125;,</span><br><span class="line">  number = &#123;1&#125;,</span><br><span class="line">  pages = &#123;259--272&#125;,</span><br><span class="line">  doi = &#123;10.1109/TCSVT.2024.3459938&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Luo, Z., et al. &quot;Rhythmer: Ranking-Based Skill Assessment With Rhythm-Aware Transformer,&quot; in IEEE Transactions on Circuits and Systems for Video Technology, vol. 35, no. 1, pp. 259–272, 2025.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法53：PAMFN（渐进自适应多模态融合网络）"><a href="#方法53：PAMFN（渐进自适应多模态融合网络）" class="headerlink" title="方法53：PAMFN（渐进自适应多模态融合网络）"></a>方法53：PAMFN（渐进自适应多模态融合网络）</h3><p><strong>方法名称：</strong> PAMFN <a href="zotero://select/library/items/84MMVAKK">📚</a></p>
<p><strong>论文标题：</strong> Multimodal Action Quality Assessment</p>
<p><strong>核心技术：</strong> RGB(I3D) + Flow(ConvLSTM) +Radio(VGGish) + 渐进融合模块 + 自适应权重学习，一种局部建模的时序金字塔TCN，</p>
<p><strong>数据集：</strong> Fis-V, RG</p>
<p><strong>主要贡献：</strong> RGB、光流、音频多模态逐步融合，自适应选择最优融合策略</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zeng-MultimodalActionQuality-2024,</span><br><span class="line">  title = &#123;Multimodal Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Zeng, Ling-An and Zheng, Wei-Shi&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;IEEE Transactions on Image Processing&#125;,</span><br><span class="line">  volume = &#123;33&#125;,</span><br><span class="line">  pages = &#123;1600--1613&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">L. Zeng, W. Zheng. &quot;Multimodal Action Quality Assessment,&quot; in IEEE Transactions on Image Processing, vol. 33, pp. 1600–1613, 2024.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法54：NAE-AQA（叙述性动作评估）"><a href="#方法54：NAE-AQA（叙述性动作评估）" class="headerlink" title="方法54：NAE-AQA（叙述性动作评估）"></a>方法54：NAE-AQA（叙述性动作评估）</h3><p><strong>方法名称：</strong> NAE-AQA <a href="zotero://select/library/items/NPJKZ8LF">📚</a></p>
<p><strong>论文标题：</strong> Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</p>
<p><strong>核心技术：</strong> CLIP视觉编码器 + 文本编码器 + Prompt工程 + 视频-文本匹配 + 自然语言生成，RGB，Text， transformer全局建模</p>
<p><strong>数据集：</strong> MTL-AQA, FineGym</p>
<p><strong>主要贡献：</strong> 将评分回归转化为视频-文本匹配任务，生成自然语言描述，实现可解释的评分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Zhang-NarrativeActionEvaluation-2024,</span><br><span class="line">  title = &#123;Narrative Action Evaluation with Prompt-Guided Multimodal Interaction&#125;,</span><br><span class="line">  booktitle = &#123;IEEE/CVF Conference on Computer Vision and Pattern Recognition&#125;,</span><br><span class="line">  author = &#123;Zhang, Shiyi and Bai, Sule and Chen, Guangyi and Chen, Lei and Lu, Jiwen and Wang, Junle and Tang, Yansong&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;18430--18439&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法55：VATP-Net（视觉-语义对齐时间解析网络）"><a href="#方法55：VATP-Net（视觉-语义对齐时间解析网络）" class="headerlink" title="方法55：VATP-Net（视觉-语义对齐时间解析网络）"></a>方法55：VATP-Net（视觉-语义对齐时间解析网络）</h3><p><strong>方法名称：</strong> VATP-Net <a href="zotero://select/library/items/D2H3IQQL">📚</a></p>
<p><strong>论文标题：</strong> Visual-Semantic Alignment Temporal Parsing for Action Quality Assessment</p>
<p><strong>核心技术：</strong> 自监督时间解析模块(SUTPM) + 多模态交互模块(MMI) + 视觉-文本对齐损失，RGB，Text，transformer时序建模</p>
<p><strong>数据集：</strong> MTL-AQA, Fis-V, RG, FineFS</p>
<p><strong>主要贡献：</strong> 自监督时间解析模块与多模态交互模块联合学习，实现视觉与语义对齐</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Gedamu-VisualsemanticAlignmentTemporal-2025,</span><br><span class="line">  author=&#123;Gedamu, Kumie and Ji, Yanli and Yang, Yang and Shao, Jie and Tao Shen, Heng&#125;,</span><br><span class="line">  journal=&#123;IEEE Transactions on Circuits and Systems for Video Technology&#125;, </span><br><span class="line">  title=&#123;Visual-Semantic Alignment Temporal Parsing for Action Quality Assessment&#125;, </span><br><span class="line">  year=&#123;2025&#125;,</span><br><span class="line">  volume=&#123;35&#125;,</span><br><span class="line">  number=&#123;3&#125;,</span><br><span class="line">  pages=&#123;2436-2449&#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Gedamu, K., et al. &quot;Visual-Semantic Alignment Temporal Parsing for Action Quality Assessment,&quot; in IEEE Transactions on Circuits and Systems for Video Technology, vol. 35, no. 3, pp. 2436-2449, 2025.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法56：2M-AF（多模态评估框架）"><a href="#方法56：2M-AF（多模态评估框架）" class="headerlink" title="方法56：2M-AF（多模态评估框架）"></a>方法56：2M-AF（多模态评估框架）</h3><p><strong>方法名称：</strong> 2M-AF <a href="zotero://select/library/items/JTYBT7RQ">📚</a></p>
<p><strong>论文标题：</strong> A Strong Multi-Modality Framework for Human Action Quality Assessment with Self-Supervised Representation Learning</p>
<p><strong>核心技术：</strong> RGB(I3D)+Skeleton(GCN) + 自监督掩码编码GCN + 多模态融合 + 对比学习，骨架网络基于CTR-GCN，TCN用的多尺度的</p>
<p><strong>数据集：</strong> UNLV-Diving, AQA-7, MMFS-63</p>
<p><strong>主要贡献：</strong> RGB+骨架多模态融合，结合自监督掩码编码GCN增强表示学习</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Ding-2MAFStrongMultiModality-2024,</span><br><span class="line">  title = &#123;2M-AF: A Strong Multi-Modality Framework For Human Action Quality Assessment with Self-Supervised Representation Learning&#125;,</span><br><span class="line">  booktitle = &#123;ACM MM&#125;,</span><br><span class="line">  author = &#123;Ding, Yuning and Zhang, Sifan and Shenglan, Liu and Zhang, Jinrong and Chen, Wenyue and Haifei, Duan and Dong, Bingcheng and Sun, Tao&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;1564--1572&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Ding, Y., et al, &quot;2M-AF: A Strong Multi-Modality Framework For Human Action Quality Assessment with Self-Supervised Representation Learning,&quot; in Proceedings of the 32nd ACM International Conference on Multimedia, 2024, pp. 1564–1572.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法57：EGCN（集成图卷积网络）"><a href="#方法57：EGCN（集成图卷积网络）" class="headerlink" title="方法57：EGCN（集成图卷积网络）"></a>方法57：EGCN（集成图卷积网络）</h3><p><strong>方法名称：</strong> EGCN<a href="zotero://select/library/items/DX9572JU">📚</a></p>
<p><strong>论文标题：</strong> EGCN: An Ensemble-Based Learning Framework for Exploring Graph Convolutional Network Variations</p>
<p><strong>核心技术：</strong> 骨架提取 + 位置和方向信息融合 + 多个GCN变体集成 + 加权融合，基于2T-GCN，也是一种解耦式STGCN。</p>
<p><strong>数据集：</strong> UI-PRDM, KIMORE</p>
<p><strong>主要贡献：</strong> 结合位置和方向信息的集成学习，多个GCN的融合策略提升康复评估</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Yu-EGCNEnsemblebasedLearning-2022,</span><br><span class="line">  title = &#123;EGCN: An Ensemble-Based Learning Framework for Exploring Effective Skeleton-Based Rehabilitation Exercise Assessment&#125;,</span><br><span class="line">  booktitle = &#123;IJCAI&#125;,</span><br><span class="line">  author = &#123;Yu, Bruce X.B. and Liu, Yan and Zhang, Xiang and Chen, Gong and Chan, Keith C.C.&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  pages = &#123;3681--3687&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Yu, B., et al, &quot;EGCN: An Ensemble-Based Learning Framework for Exploring Effective Skeleton-Based Rehabilitation Exercise Assessment,&quot; in Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, 2022, pp. 3681–3687.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法58：EK-GCN（专家知识引导的图卷积网络）"><a href="#方法58：EK-GCN（专家知识引导的图卷积网络）" class="headerlink" title="方法58：EK-GCN（专家知识引导的图卷积网络）"></a>方法58：EK-GCN（专家知识引导的图卷积网络）</h3><p><strong>方法名称：</strong> EK-GCN<a href="zotero://select/library/items/6C59QUZB">📚</a></p>
<p><strong>论文标题：</strong> An Expert-Knowledge-Based Graph Convolutional Network for Skeleton-Based Physical Rehabilitation Exercises Assessment</p>
<p><strong>核心技术：</strong> 骨架提取 + 专家知识驱动的加权图构建 + 关键关节强调 + GCN卷积 + 康复评分预测，Skeleton，GCN+TCN的解耦结构，利用transformer建立全局依赖</p>
<p><strong>数据集：</strong> KIMORE(康复)</p>
<p><strong>主要贡献：</strong> 加权图结构由专家知识指导关键关节和运动特征的选择</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;He-ExpertKnowledgeBasedGraphConvolutional-2024,</span><br><span class="line">  title = &#123;An Expert-Knowledge-Based Graph Convolutional Network for Skeleton- Based Physical Rehabilitation Exercises Assessment&#125;,</span><br><span class="line">  author = &#123;He, Tian and Chen, Yang and Wang, Ling and Cheng, Hong&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;IEEE Transactions on Neural Systems and Rehabilitation Engineering&#125;,</span><br><span class="line">  volume = &#123;32&#125;,</span><br><span class="line">  pages = &#123;1916--1925&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法59：DNLA（判别性非局部注意）"><a href="#方法59：DNLA（判别性非局部注意）" class="headerlink" title="方法59：DNLA（判别性非局部注意）"></a>方法59：DNLA（判别性非局部注意）</h3><p><strong>方法名称：</strong> DNLA <a href="zotero://select/library/items/PWTW8WGZ">📚</a></p>
<p><strong>论文标题：</strong> Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics</p>
<p><strong>核心技术：</strong> 骨架与视频多模态特征 + 稀疏特征提取 + 判别性非局部注意 + 动作质量评分，STGCN建模骨架，transformer建模长时序依赖，Skeleton，RGB</p>
<p><strong>数据集：</strong> AGF-Olympics(新数据集)</p>
<p><strong>主要贡献：</strong> 从骨架和视频中提取稀疏特征，抑制多余信息提升判别性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zahan-LearningSparseTemporal-2024,</span><br><span class="line">  title = &#123;Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics&#125;,</span><br><span class="line">  author = &#123;Zahan, Sania and Mubashar Hassan, Ghulam and Mian, Ajmal&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;IEEE Transactions on Instrumentation and Measurement&#125;,</span><br><span class="line">  volume = &#123;73&#125;,</span><br><span class="line">  pages = &#123;1--11&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法60：NS-AQA（神经符号AQA）"><a href="#方法60：NS-AQA（神经符号AQA）" class="headerlink" title="方法60：NS-AQA（神经符号AQA）"></a>方法60：NS-AQA（神经符号AQA）</h3><p><strong>方法名称：</strong> NS-AQA <a href="zotero://select/library/items/I459GXXP">📚</a></p>
<p><strong>论文标题：</strong> Hierarchical Neurosymbolic Approach for Comprehensive and Explainable Action Quality Assessment</p>
<p><strong>核心技术：</strong> 神经网络(I3D特征提取) + 符号推理(规则库、本体) + 分层解释模块 + 综合报告生成</p>
<p><strong>数据集：</strong> FineDiving</p>
<p><strong>主要贡献：</strong> 结合神经网络与符号推理，提供可解释的详细AQA报告与改进建议</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Okamoto-HierarchicalNeuroSymbolicApproach-2024,</span><br><span class="line">  title = &#123;Hierarchical NeuroSymbolic Approach for Comprehensive and Explainable Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)&#125;,</span><br><span class="line">  author = &#123;Okamoto, Lauren and Parmar, Paritosh&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;3204--3213&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法61：RICA2（规则知情校准评估）"><a href="#方法61：RICA2（规则知情校准评估）" class="headerlink" title="方法61：RICA2（规则知情校准评估）"></a>方法61：RICA2（规则知情校准评估）</h3><p><strong>方法名称：</strong> RICA2 <a href="zotero://select/library/items/BJ5XFDQ7">📚</a></p>
<p><strong>论文标题：</strong> RICA2: Rubric-Informed, Calibrated Assessment of Actions</p>
<p><strong>核心技术：</strong> DAG表示(动作步骤+评分标准) + 图神经网络(GNN) + 概率嵌入 + 校准机制</p>
<p><strong>数据集：</strong> JIGSAWS, MTL-AQA, FineDiving</p>
<p><strong>主要贡献：</strong> 将动作步骤和评分标准表示为有向无环图(DAG)，使用GNN生成概率嵌入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Majeedi-RICA2RubricInformedCalibrated-2024,</span><br><span class="line">  title = &#123;RICA2: Rubric-Informed, Calibrated Assessment of Actions&#125;,</span><br><span class="line">  booktitle = &#123;ECCV&#125;,</span><br><span class="line">  author = &#123;Majeedi, Abrar and Gajjala, Viswanatha Reddy and GNVV, Satya Sai Srinath Namburi and Li, Yin&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;143--161&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法62：DuRA（双参考辅助网络）"><a href="#方法62：DuRA（双参考辅助网络）" class="headerlink" title="方法62：DuRA（双参考辅助网络）"></a>方法62：DuRA（双参考辅助网络）</h3><p><strong>方法名称：</strong> DuRA <a href="zotero://select/library/items/BZ9CPRS2">📚</a></p>
<p><strong>论文标题：</strong> Dual-Referenced Assistive Network for Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D特征提取 + 语义级等级原型库 + 个体级参考样本池 + 双参考对比机制</p>
<p><strong>数据集：</strong> AQA-7, MTL-AQA</p>
<p><strong>主要贡献：</strong> 利用语义级等级原型和个体级参考样本增强细节关注，过滤无关信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Huang-DualreferencedAssistiveNetwork-2025,</span><br><span class="line">  title = &#123;Dual-Referenced Assistive Network for Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Huang, Keyi and Tian, Yi and Yu, Chen and Huang, Yaping&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;Neurocomputing&#125;,</span><br><span class="line">  volume = &#123;614&#125;,</span><br><span class="line">  pages = &#123;128786&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法63：SAP-Net（自监督子动作解析网络）"><a href="#方法63：SAP-Net（自监督子动作解析网络）" class="headerlink" title="方法63：SAP-Net（自监督子动作解析网络）"></a>方法63：SAP-Net（自监督子动作解析网络）</h3><p><strong>方法名称：</strong> SAP-Net  <a href="zotero://select/library/items/DH7WUES7">📚</a></p>
<p><strong>论文标题：</strong> Self-Supervised Subaction Parsing Network for Semi-Supervised Action Quality Assessment</p>
<p><strong>核心技术：</strong> 教师-学生网络 + 伪标签生成 + 一致性正则化 + 自监督子动作解析，无标签的实验结果</p>
<p><strong>数据集：</strong> MTL-AQA, FineDiving, RG, FineFS</p>
<p><strong>主要贡献：</strong> 教师-学生网络，利用伪标签进行一致性正则化，支持半监督学习，利用剪辑演员为中心，减少背景的干扰</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Gedamu-SelfsupervisedSubactionParsing-2024,</span><br><span class="line">  title = &#123;Self-Supervised Subaction Parsing Network for Semi-Supervised Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Gedamu, Kumie and Ji, Yanli and Yang, Yang and Shao, Jie and Shen, Heng Tao&#125;,</span><br><span class="line">  journal = &#123;IEEE Transactions on Image Processing&#125;,</span><br><span class="line">  year=&#123;2024&#125;,</span><br><span class="line">  volume=&#123;33&#125;,</span><br><span class="line">  number=&#123;&#125;,</span><br><span class="line">  pages=&#123;6057-6070&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Gedamu, K., et al. &quot;Self-Supervised Subaction Parsing Network for Semi-Supervised Action Quality Assessment,&quot; in IEEE Transactions on Image Processing, pp. 6057-6070, 2024.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法64：TRS（教师-参考-学生架构）"><a href="#方法64：TRS（教师-参考-学生架构）" class="headerlink" title="方法64：TRS（教师-参考-学生架构）"></a>方法64：TRS（教师-参考-学生架构）</h3><p><strong>方法名称：</strong> TRS <a href="zotero://select/library/items/5T7P5T93">📚</a></p>
<p><strong>论文标题：</strong> Semi-Supervised Teacher-Reference-Student Architecture for Action Quality Assessment</p>
<p><strong>核心技术：</strong> 教师网络(生成伪标签) + 参考网络(提供补充监督) + 学生网络(目标模型) + 知识蒸馏， 主要是无标签的实验结果</p>
<p><strong>数据集：</strong> JIGSAWS , MTL-AQA, RG</p>
<p><strong>主要贡献：</strong> 半监督框架，教师网络生成伪标签，参考网络提供补充监督</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Yun-SemisupervisedTeacher-2024,</span><br><span class="line">  title = &#123;Semi-Supervised Teacher-Reference-Student Architecture for Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;ECCV&#125;,</span><br><span class="line">  author = &#123;Yun, Wulian and Qi, Mengshi and Peng, Fei and Ma, Huadong&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;161--178&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法65：PECoP（参数高效连续预训练）"><a href="#方法65：PECoP（参数高效连续预训练）" class="headerlink" title="方法65：PECoP（参数高效连续预训练）"></a>方法65：PECoP（参数高效连续预训练）</h3><p><strong>方法名称：</strong> PECoP <a href="zotero://select/library/items/X7XH84R9">📚</a></p>
<p><strong>论文标题：</strong> PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment</p>
<p><strong>核心技术：</strong> I3D主干 + 轻量级3D-Adapter注入 + 域特定自监督预训练 + 参数高效微调，短视频通过I3D建模时序信息</p>
<p><strong>数据集：</strong> JIGSAWS, MTL-AQA, FineDiving</p>
<p><strong>主要贡献：</strong> 轻量级Adapter实现参数高效的连续预训练，通过域特定自监督任务增强</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Dadashzadeh-PECoPParameterEfficient-2024,</span><br><span class="line">  title = &#123;PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;WACV&#125;,</span><br><span class="line">  author = &#123;Dadashzadeh, Amirhossein and Duan, Shuchao and Whone, Alan and Mirmehdi, Majid&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;42--52&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法66：Continual-AQA（连续学习AQA）"><a href="#方法66：Continual-AQA（连续学习AQA）" class="headerlink" title="方法66：Continual-AQA（连续学习AQA）"></a>方法66：Continual-AQA（连续学习AQA）</h3><p><strong>方法名称：</strong> Continual-AQA <a href="zotero://select/library/items/JMDBZY7G">📚</a></p>
<p><strong>论文标题：</strong> Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling</p>
<p><strong>核心技术：</strong> 连续学习框架 + FSCAR模块(代表样本抽取+增强) + AGSG模块(新旧任务知识组合) + 特征分布建模，</p>
<p><strong>数据集：</strong> AQA-7, BEST, MTL-AQA</p>
<p><strong>主要贡献：</strong> FSCAR模块提取代表样本并增强特征/评分；AGSG模块组合新旧任务知识，防止灾难性遗忘</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">@article&#123;Li-ContinualActionAssessment-2024,</span><br><span class="line">  title = &#123;Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling&#125;,</span><br><span class="line">  author = &#123;Li, Yuan-Ming and Zeng, Ling-An and Meng, Jing-Ke and Zheng, Wei-Shi&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;IEEE Transactions on Circuits and Systems for Video Technology&#125;,</span><br><span class="line">  volume = &#123;34&#125;,</span><br><span class="line">  number = &#123;10&#125;,</span><br><span class="line">  pages = &#123;9112--9124&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Li, Y., et al. &quot;Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling,&quot; in IEEE Transactions on Circuits and Systems for Video Technology, vol. 34, no. 10, pp. 9112–9124, 2024.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法67：MAGR（流形对齐图正则化）"><a href="#方法67：MAGR（流形对齐图正则化）" class="headerlink" title="方法67：MAGR（流形对齐图正则化）"></a>方法67：MAGR（流形对齐图正则化）</h3><p><strong>方法名称：</strong> MAGR <a href="zotero://select/library/items/29T9CXZ2">📚</a></p>
<p><strong>论文标题：</strong> MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment</p>
<p><strong>核心技术：</strong> 流形学习 + 特征流形对齐 + 图正则化 + 特征分布一致性约束，持续学习</p>
<p><strong>数据集：</strong> MTL-AQA, UNLV-Dive, FineDiving, JDM-MSA</p>
<p><strong>主要贡献：</strong> 对齐旧特征与动态变化的特征流形，图正则化维持特征分布一致性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Zhou-MAGRManifoldAlignedGraph-2024,</span><br><span class="line">  title = &#123;MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;ECCV&#125;,</span><br><span class="line">  author = &#123;Zhou, Kanglei and Wang, Liyuan and Zhang, Xingxing and Shum, Hubert P. H. and Li, Frederick W. B. and Li, Jianguo and Liang, Xiaohui&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;375--392&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Zhou, K., et al, &quot;MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment,&quot; in Computer Vision – ECCV 2024, 2024, pp. 375–392.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法68：CoFInAl（粗细指令对齐）"><a href="#方法68：CoFInAl（粗细指令对齐）" class="headerlink" title="方法68：CoFInAl（粗细指令对齐）"></a>方法68：CoFInAl（粗细指令对齐）</h3><p><strong>方法名称：</strong> CoFInAl <a href="zotero://select/library/items/HWM79QV2">📚</a></p>
<p><strong>论文标题：</strong> CoFInAl: Enhancing Action Quality Assessment with Coarse-to-Fine Instruction Alignment</p>
<p><strong>核心技术：</strong> 粗粒度评级模块(CRPM) + 精细评分模块(FSRM) + 指令引导的特征提取 + 两阶段评分</p>
<p><strong>数据集：</strong> Fis-V, RG</p>
<p><strong>主要贡献：</strong> 模仿评委评分过程，先进行粗粒度评级，再进行精细评分，两阶段互补</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Zhou-CoFInAlEnhancingAction-2024,</span><br><span class="line">  title = &#123;CoFInAl: Enhancing Action Quality Assessment with Coarse-to-Fine Instruction Alignment&#125;,</span><br><span class="line">  booktitle = &#123;IJCAI&#125;,</span><br><span class="line">  author = &#123;Zhou, Kanglei and Li, Junlin and Cai, Ruizhi and Wang, Liyuan and Zhang, Xingxing and Liang, Xiaohui&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  pages = &#123;1771--1779&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Zhou, K., et al, &quot;CoFInAl: Enhancing Action Quality Assessment with Coarse-to-Fine Instruction Alignment,&quot; in Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence, 2024, pp. 1771–1779.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法69：ZEAL（零样本外科技能评估）"><a href="#方法69：ZEAL（零样本外科技能评估）" class="headerlink" title="方法69：ZEAL（零样本外科技能评估）"></a>方法69：ZEAL（零样本外科技能评估）</h3><p><strong>方法名称：</strong> ZEAL <a href="zotero://select/library/items/95P5T46F">📚</a></p>
<p><strong>论文标题：</strong> ZEAL: Surgical Skill Assessment with Zero-Shot Tool Inference Using Unified Foundation Model</p>
<p><strong>核心技术：</strong> 统一基础模型(如SAM) + 工具分割与推理 + 零样本转移 + 手术技能评估</p>
<p><strong>应用背景：</strong> 外科手术技能评估</p>
<p><strong>主要贡献：</strong> 利用统一基础模型进行工具分割和手术技能评估，实现零样本泛化</p>
<p><strong>优缺点：</strong> ✅ 零样本泛化能力强；❌ 依赖基础模型性能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Kondo-ZEALSurgicalSkill-2024,</span><br><span class="line">  title = &#123;ZEAL: Surgical Skill Assessment with Zero-Shot Tool Inference Using Unified Foundation Model&#125;,</span><br><span class="line">  author = &#123;Kondo, Satoshi&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  number = &#123;arXiv:2407.02738&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法70：LucidAction（分层多模型数据集）"><a href="#方法70：LucidAction（分层多模型数据集）" class="headerlink" title="方法70：LucidAction（分层多模型数据集）"></a>方法70：LucidAction（分层多模型数据集）</h3><p><strong>方法名称：</strong> LucidAction <a href="zotero://select/library/items/BGHYJJXR">📚</a></p>
<p><strong>论文标题：</strong> LucidAction: A Hierarchical and Multi-Model Dataset for Comprehensive Action Quality Assessment</p>
<p><strong>数据集特性：</strong> 多视角RGB视频、2D骨架序列、3D骨架序列、分层注释(整体评分、阶段评分、关键点评分)</p>
<p><strong>数据集规模：</strong> 1000+ 视频，覆盖多个运动项目</p>
<p><strong>主要贡献：</strong> 发布多视角、多模态、分层标注的AQA数据集，支持综合AQA分析与多任务学习</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Dong-LucidActionHierarchicalMultimodel-2024,</span><br><span class="line">  title=&#123;LucidAction: A hierarchical and multi-model dataset for comprehensive action quality assessment&#125;,</span><br><span class="line">  author=&#123;Dong, Linfeng and others&#125;,</span><br><span class="line">  booktitle = &#123;NeurIPS&#125;,</span><br><span class="line">  volume = &#123;37&#125;,</span><br><span class="line">  year = &#123;2024&#125;,</span><br><span class="line">  pages = &#123;96468--96482&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法71：GAIA（AI生成视频的AQA）"><a href="#方法71：GAIA（AI生成视频的AQA）" class="headerlink" title="方法71：GAIA（AI生成视频的AQA）"></a>方法71：GAIA（AI生成视频的AQA）</h3><p><strong>方法名称：</strong> GAIA <a href="zotero://select/library/items/Q3U6TZVH">📚</a></p>
<p><strong>论文标题：</strong> GAIA: Rethinking Action Quality Assessment for AI-Generated Videos</p>
<p><strong>核心技术：</strong> 生成视频检测 + 运动一致性评估 + 物理可行性检验 + AI生成视频的质量评分</p>
<p><strong>数据集：</strong> GAIA(新建 - AI生成视频AQA数据集)</p>
<p><strong>主要贡献：</strong> 首个AI生成视频的AQA数据集和评估方法，应对生成视频质量评估新挑战</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Chen-GAIARethinkingAction-2024,</span><br><span class="line">  title = &#123;GAIA: Rethinking Action Quality Assessment for AI-Generated Videos&#125;,</span><br><span class="line">  booktitle = &#123;NeurIPS&#125;,</span><br><span class="line">  author = &#123;Chen, Zijian and Sun, Wei and Tian, Yuan and Jia, Jun and Zhang, Zicheng and Wang, Jiarui and Huang, Ru and Min, Xiongkuo and Zhai, Guangtao and Zhang, Wen-Jun&#125;,</span><br><span class="line">  year = 2024</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法72：CLN（对比学习康复评估）"><a href="#方法72：CLN（对比学习康复评估）" class="headerlink" title="方法72：CLN（对比学习康复评估）"></a>方法72：CLN（对比学习康复评估）</h3><p> 方法名称： <a href="zotero://select/library/items/UEJ24GER">📚</a><br> 论文标题： A Contrastive Learning Network for Performance Metric and Assessment of Physical Rehabilitation Exercises<br> 核心技术： 对比学习框架 + 绩效度量学习 + 多样本特征对齐，Skeleton<br> 数据集： KIMORE,UI-PRMD<br> 主要贡献： 面向康复场景构建对比学习网络，联合学习动作表现度量与评分，实现对异常/偏差动作的区分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Yao-ContrastiveLearningNetwork-2023,</span><br><span class="line">  title = &#123;A Contrastive Learning Network for Performance Metric and Assessment of Physical Rehabilitation Exercises&#125;,</span><br><span class="line">  author = &#123;Yao, Long and Lei, Qing and Zhang, Hongbo and Du, Jixiang and Gao, Shangce&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;IEEE Transactions on Neural Systems and Rehabilitation Engineering&#125;,</span><br><span class="line">  volume = &#123;31&#125;,</span><br><span class="line">  pages = &#123;3790--3802&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法73：DPFL-FS（骨架深度姿态特征-花样滑冰）"><a href="#方法73：DPFL-FS（骨架深度姿态特征-花样滑冰）" class="headerlink" title="方法73：DPFL-FS（骨架深度姿态特征-花样滑冰）"></a>方法73：DPFL-FS（骨架深度姿态特征-花样滑冰）</h3><p> 方法名称： ST-GCN+ <a href="zotero://select/library/items/C2CV8B4U">📚</a><br> 论文标题： Skeleton-Based Deep Pose Feature Learning for Action Quality Assessment on Figure Skating Videos<br> 核心技术： 基于骨架的深度姿态特征学习 + 时序聚合，Skeleton<br> 数据集： MIT-Skate, Fis-V<br> 主要贡献： 以骨架特征为核心，弱化背景/外观干扰，提升对花样滑冰细粒度姿态与节奏的建模，长序列建模</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Li-SkeletonbasedDeepPose-2022,</span><br><span class="line">  title = &#123;Skeleton-Based Deep Pose Feature Learning for Action Quality Assessment on Figure Skating Videos&#125;,</span><br><span class="line">  author = &#123;Li, Huiying and Lei, Qing and Zhang, Hongbo and Du, Jixiang and Gao, Shangce&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  journal = &#123;Journal of Visual Communication and Image Representation&#125;,</span><br><span class="line">  volume = &#123;89&#125;,</span><br><span class="line">  pages = &#123;103625&#125;,</span><br><span class="line">  doi = &#123;10.1016/j.jvcir.2022.103625&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Li, H., et al. &quot;Skeleton-Based Deep Pose Feature Learning for Action Quality Assessment on Figure Skating Videos,&quot; in Journal of Visual Communication and Image Representation, vol. 89, pp. 103625, 2022.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法74：SSTDT（骨架时空解耦Transformer）"><a href="#方法74：SSTDT（骨架时空解耦Transformer）" class="headerlink" title="方法74：SSTDT（骨架时空解耦Transformer）"></a>方法74：SSTDT（骨架时空解耦Transformer）</h3><p> 方法名称：SSTDT <a href="zotero://select/library/items/H5BIU25A">📚</a><br> 论文标题： Skeletal Spatio-Temporal Decoupling Transformer for Long-Duration Action Quality Assessment<br> 核心技术： 时空解耦式Transformer + 长时序建模 + 骨架表示，Skeleton<br> 数据集： MIT-Skate, Fis-V, RG<br> 主要贡献： 将时空因素解耦建模，缓解长视频中的依赖稀释与信息混杂</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Lei-SkeletalSpatiotemporalDecoupling-2025,</span><br><span class="line">  title = &#123;Skeletal Spatio-Temporal Decoupling Transformer for Long-Duration Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Lei, Qing and Yao, Long and Zhang, Hongbo and Du, Jixiang&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;Knowledge-Based Systems&#125;,</span><br><span class="line">  volume = &#123;330&#125;,</span><br><span class="line">  pages = &#123;114672&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Lei, Q., et al. &quot;Skeletal Spatio-Temporal Decoupling Transformer for Long-Duration Action Quality Assessment,&quot; in Knowledge-Based Systems, vol. 330, pp. 114672, 2025.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法75：VL-AKL（语义感知视觉语言知识学习）"><a href="#方法75：VL-AKL（语义感知视觉语言知识学习）" class="headerlink" title="方法75：VL-AKL（语义感知视觉语言知识学习）"></a>方法75：VL-AKL（语义感知视觉语言知识学习）</h3><p> 方法名称： VL-AKL <a href="zotero://select/library/items/BRGKCIE4">📚</a><br> 论文标题： Vision-Language Action Knowledge Learning for Semantic-Aware Action Quality Assessment<br> 核心技术： 视觉-语言知识学习 + 语义对齐 + 指令/文本引导特征，RGB，Text<br> 数据集： FineDiving, MTL-AQA, JIGSAWS, Fis-V<br> 主要贡献： 将动作语义知识注入AQA，利用文本先验指导特征抽取与评分，transformer全局依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Xu-VisionLanguageActionKnowledge-2025,</span><br><span class="line">  title = &#123;Vision-Language Action Knowledge Learning for Semantic-Aware Action Quality Assessment&#125;,</span><br><span class="line">  booktitle = &#123;ECCV&#125;,</span><br><span class="line">  author = &#123;Xu, Huangbiao and Ke, Xiao and Li, Yuezhou and Xu, Rui and Wu, Huanqi and Lin, Xiaofeng and Guo, Wenzhong&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  pages = &#123;423--440&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法76：HP-MSCR（层级姿态引导的多阶段对比回归）"><a href="#方法76：HP-MSCR（层级姿态引导的多阶段对比回归）" class="headerlink" title="方法76：HP-MSCR（层级姿态引导的多阶段对比回归）"></a>方法76：HP-MSCR（层级姿态引导的多阶段对比回归）</h3><p> 方法名称： HP-MSCR <a href="zotero://select/library/items/LPAB3YK4">📚</a><br> 论文标题： Action Quality Assessment via Hierarchical Pose-Guided Multi-Stage Contrastive Regression<br> 核心技术： 层级姿态引导 + 多阶段对比学习 + 回归评分， Skeleton，RGB<br> 数据集：FineDiving, MTL-AQA<br> 主要贡献： 将层级姿态线索逐步注入对比式回归，提高细粒度评分区分度</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Jain-ActionQualityAssessment-2021,</span><br><span class="line">  title = &#123;Action Quality Assessment Using Siamese Network-Based Deep Metric Learning&#125;,</span><br><span class="line">  author = &#123;Jain, Hiteshi and Harit, Gaurav and Sharma, Avinash&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  journal = &#123;IEEE Transactions on Circuits and Systems for Video Technology&#125;,</span><br><span class="line">  volume = &#123;31&#125;,</span><br><span class="line">  number = &#123;6&#125;,</span><br><span class="line">  pages = &#123;2260--2273&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法77：PHI（渐进层级指令）"><a href="#方法77：PHI（渐进层级指令）" class="headerlink" title="方法77：PHI（渐进层级指令）"></a>方法77：PHI（渐进层级指令）</h3><p> 方法名称： PHI <a href="zotero://select/library/items/RMWBPVUG">📚</a><br> 论文标题： PHI: Bridging Domain Shift in Long-Term Action Quality Assessment via Progressive Hierarchical Instruction<br> 核心技术： 渐进式层级指令对齐 + 域迁移 + 长时序建模<br> 数据集： RG, Fis-V, LOGO<br> 主要贡献： 通过层级指令缩小AQA特征与识别特征的域差，提升跨项目泛化</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhou-PHIBridgingDomain-2025,</span><br><span class="line">  title = &#123;PHI: Bridging Domain Shift in Long-Term Action Quality Assessment via Progressive Hierarchical Instruction&#125;,</span><br><span class="line">  author = &#123;Zhou, Kanglei and Shum, Hubert P. H. and Li, Frederick W. B. and Zhang, Xingxing and Liang, Xiaohui&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;IEEE Transactions on Image Processing&#125;,</span><br><span class="line">  volume = &#123;34&#125;,</span><br><span class="line">  pages = &#123;3718--3732&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Zhou, K., et al. &quot;PHI: Bridging Domain Shift in Long-Term Action Quality Assessment via Progressive Hierarchical Instruction,&quot; in IEEE Transactions on Image Processing, vol. 34, pp. 3718–3732, 2025.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法78：HC-FGAQA（以人为中心的细粒度AQA）"><a href="#方法78：HC-FGAQA（以人为中心的细粒度AQA）" class="headerlink" title="方法78：HC-FGAQA（以人为中心的细粒度AQA）"></a>方法78：HC-FGAQA（以人为中心的细粒度AQA）</h3><p> 方法名称： HC-FGAQA  <a href="zotero://select/library/items/7CHSIHFY">📚</a><br> 论文标题： Human-Centric Fine-Grained Action Quality Assessment<br> 核心技术： 以人为中心的细粒度建模 + 局部关键动作解析<br> 数据集： FineDiving-HM, AQA-7-HM,  MTL-AQA-HM<br> 主要贡献： 聚焦人体主体与关键细节动作的精细建模，提升细微差异的评分敏感度</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Xu-HumanCentricFineGrainedAction-2025,</span><br><span class="line">  title = &#123;Human-Centric Fine-Grained Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Xu, Jinglin and Yin, Sibo and Peng, Yuxin&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;IEEE Transactions on Pattern Analysis and Machine Intelligence&#125;,</span><br><span class="line">  volume = &#123;47&#125;,</span><br><span class="line">  number = &#123;8&#125;,</span><br><span class="line">  pages = &#123;6242--6255&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">J. Xu, S. Yin, Y. Peng. &quot;Human-Centric Fine-Grained Action Quality Assessment,&quot; in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 47, no. 8, pp. 6242–6255, 2025.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法80：B2S（从节拍到评分）"><a href="#方法80：B2S（从节拍到评分）" class="headerlink" title="方法80：B2S（从节拍到评分）"></a>方法80：B2S（从节拍到评分）</h3><p> 方法名称： B2S <a href="zotero://select/library/items/GR5UNVAM">📚</a><br> 论文标题： From Beats to Scores: A Multi-Modal Framework for Comprehensive Figure Skating Assessment<br> 核心技术： 多模态融合（视觉+节拍/音频）+ 全流程评分， RGB， Audio<br> 数据集： Fis-V, FS1000, FineFS<br> 主要贡献： 引入节拍/音乐信息辅助评分，提升对编排与节奏的感知</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Wang-BeatsScoresMultiModal-2025,</span><br><span class="line">  title = &#123;From Beats to Scores: A Multi-Modal Framework for Comprehensive Figure Skating Assessment&#125;,</span><br><span class="line">  booktitle = &#123;CVPRW&#125;,</span><br><span class="line">  author = &#123;Wang, Fengshun and Wang, Qiurui and Chen, Dan&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  pages = &#123;5895--5904&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法81：FineCausal（可解释因果细粒度AQA）"><a href="#方法81：FineCausal（可解释因果细粒度AQA）" class="headerlink" title="方法81：FineCausal（可解释因果细粒度AQA）"></a>方法81：FineCausal（可解释因果细粒度AQA）</h3><p> 方法名称： FineCausal <a href="zotero://select/library/items/5QTGWWGE">📚</a><br> 论文标题： FineCausal: A Causal-Based Framework for Interpretable Fine-Grained Action Quality Assessment<br> 核心技术： 因果建模 + 细粒度可解释评分，对比学习<br> 数据集： FineDiving-HM<br> 主要贡献： 融合因果推断以区分“相关”与“致因”因素，增强评分解释性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Han-FineCausalCausalBasedFramework-2025,</span><br><span class="line">  author=&#123;Han, Ruisheng and Zhou, Kanglei and Atapour-Abarghouei, Amir and Liang, Xiaohui and Shum, Hubert P.H.&#125;,</span><br><span class="line">  booktitle=&#123;CVPRW&#125;, </span><br><span class="line">  title=&#123;FineCausal: A Causal-Based Framework for Interpretable Fine-Grained Action Quality Assessment&#125;, </span><br><span class="line">  year=&#123;2025&#125;,</span><br><span class="line">  pages=&#123;6008-6017&#125;,</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">Han, R., et al, &quot;FineCausal: A Causal-Based Framework for Interpretable Fine-Grained Action Quality Assessment,&quot; in 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2025, pp. 6008-6017.</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法82：TS-MambaPyramid（两流Mamba金字塔）"><a href="#方法82：TS-MambaPyramid（两流Mamba金字塔）" class="headerlink" title="方法82：TS-MambaPyramid（两流Mamba金字塔）"></a>方法82：TS-MambaPyramid（两流Mamba金字塔）</h3><p> 方法名称： TS-MambaPyramid <a href="zotero://select/library/items/89VY7GV8">📚</a><br> 论文标题： Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment<br> 核心技术： 两流结构（RGB+音频）+ 状态空间模型（Mamba）+ 金字塔长程建模<br> 数据集： Fis-V, FS1000, FineFS<br> 主要贡献： 结合Mamba的长程依赖与两流互补以建模长视频</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Wang-LearningLongRangeAction-2025,</span><br><span class="line">  title = &#123;Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment&#125;,</span><br><span class="line">  booktitle = &#123;ACM MM&#125;,</span><br><span class="line">  author = &#123;Wang, Fengshun and Wang, Qiurui and Zhao, Peilin&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  pages = &#123;867--875&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法83：DanceFix（群舞整齐度评估）"><a href="#方法83：DanceFix（群舞整齐度评估）" class="headerlink" title="方法83：DanceFix（群舞整齐度评估）"></a>方法83：DanceFix（群舞整齐度评估）</h3><p> 方法名称： DanceFix <a href="zotero://select/library/items/7PUXR6YI">📚</a><br> 论文标题： DanceFix: An Exploration in Group Dance Neatness Assessment Through Fixing Abnormal Challenges of Human Pose<br> 核心技术： 群体骨架鲁棒化 + 异常姿态修复 + 整齐度度量，Flow, Skeleton, RGB.<br> 主要贡献： 面向群体舞蹈提出整齐度评估并处理骨架异常，根据上下文校正骨架数据。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Xu-DanceFixExplorationGroup-2025,</span><br><span class="line">  title = &#123;DanceFix: An Exploration in Group Dance Neatness Assessment Through Fixing Abnormal Challenges of Human Pose&#125;,</span><br><span class="line">  author = &#123;Xu, Huangbiao and Ke, Xiao and Wu, Huanqi and Xu, Rui and Li, Yuezhou and Xu, Peirong and Guo, Wenzhong&#125;,</span><br><span class="line">  booktitle=&#123;Proceedings of the AAAI Conference on Artificial Intelligence&#125;,</span><br><span class="line">  year=&#123;2025&#125;, </span><br><span class="line">  pages=&#123;8869-8877&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法84：MLAVL（语言引导视听评估）"><a href="#方法84：MLAVL（语言引导视听评估）" class="headerlink" title="方法84：MLAVL（语言引导视听评估）"></a>方法84：MLAVL（语言引导视听评估）</h3><p> 方法名称：MLAVL <a href="zotero://select/library/items/B3HIRA9E">📚</a><br> 论文标题： Language-Guided Audio-Visual Learning for Long-Term Sports Assessment<br> 核心技术： 文本引导 + 视听多模态对齐 + 长时序融合， RGB，Text，Radio<br> 数据集： FS1000, RG, Fis-V, LOGO<br> 主要贡献： 利用语言先验指导视听融合，面向长视频的规则与节奏理解</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Xu-LanguageGuidedAudioVisualLearning-2025,</span><br><span class="line">  author=&#123;Xu, Huangbiao and Ke, Xiao and Wu, Huanqi and Xu, Rui and Li, Yuezhou and Guo, Wenzhong&#125;,</span><br><span class="line">  booktitle=&#123;CVPR&#125;, </span><br><span class="line">  title=&#123;Language-Guided Audio-Visual Learning for Long-Term Sports Assessment&#125;, </span><br><span class="line">  year=&#123;2025&#125;,</span><br><span class="line">  pages=&#123;23967-23977&#125;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法85：QGVL（质量引导视觉语言）"><a href="#方法85：QGVL（质量引导视觉语言）" class="headerlink" title="方法85：QGVL（质量引导视觉语言）"></a>方法85：QGVL（质量引导视觉语言）</h3><p> 方法名称： QGVL <a href="zotero://select/library/items/G9CQGK9F">📚</a><br> 论文标题： Quality-Guided Vision-Language Learning for Long-Term Action Quality Assessment<br> 核心技术： 质量信号引导的视觉-语言对齐 + 长时序建模，RGB，Text<br> 数据集： RG, Fis-V, FS1000,  FineFS<br> 主要贡献： 将质量/评分信号纳入V-L对齐过程，提升语义-评分一致性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Xu-QualityGuidedVisionLanguageLearning-2025,</span><br><span class="line">  title = &#123;Quality-Guided Vision-Language Learning for Long-Term Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Xu, Huangbiao and Wu, Huanqi and Ke, Xiao and Li, Yuezhou and Xu, Rui and Guo, Wenzhong&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;IEEE Transactions on Multimedia&#125;,</span><br><span class="line">  volume = &#123;27&#125;,</span><br><span class="line">  pages = &#123;7326--7339&#125;,</span><br><span class="line">  doi = &#123;10.1109/TMM.2025.3599078&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法86：SBS（尺度化背景置换）"><a href="#方法86：SBS（尺度化背景置换）" class="headerlink" title="方法86：SBS（尺度化背景置换）"></a>方法86：SBS（尺度化背景置换）</h3><p> 方法名称： SBS <a href="zotero://select/library/items/LSYLL8MF">📚</a><br> 论文标题： Scaled Background Swap: Video Augmentation for Action Quality Assessment with Background Debiasing<br> 核心技术： 背景去偏增强（背景置换/缩放）+ 数据增广<br> 数据集： AQA-7, MTL-AQA<br> 主要贡献： 通过背景置换减轻模型对背景的依赖，提升泛化</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhang-ScaledBackgroundSwap-2025,</span><br><span class="line">  title = &#123;Scaled Background Swap: Video Augmentation for Action Quality Assessment with Background Debiasing&#125;,</span><br><span class="line">  author = &#123;Zhang, Xin and Feng, Hongzhi and Hossain, M. Shamim and Chen, Yinzhuo and Wang, Hongbo and Yin, Yuyu&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;ACM Transactions on Multimedia Computing, Communications, and Applications&#125;,</span><br><span class="line">  volume = &#123;21&#125;,</span><br><span class="line">  number = &#123;8&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法87：ASGTN（自适应时空图Transformer）"><a href="#方法87：ASGTN（自适应时空图Transformer）" class="headerlink" title="方法87：ASGTN（自适应时空图Transformer）"></a>方法87：ASGTN（自适应时空图Transformer）</h3><p> 方法名称： ASGTN <a href="zotero://select/library/items/6CSSMWL3">📚</a><br> 论文标题： Adaptive Spatiotemporal Graph Transformer Network for Action Quality Assessment<br> 核心技术： 时空图结构 + Transformer + 自适应拓扑学习<br> 数据集： RG, Fis-V<br> 主要贡献： 学习式图拓扑与Transformer结合，提升帧间的拓扑联系</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Liu-AdaptiveSpatiotemporalGraph-2025,</span><br><span class="line">  author=&#123;Liu, Jiang and Wang, Huasheng and Zhou, Wei and Stawarz, Katarzyna and Corcoran, Padraig and Chen, Ying and Liu, Hantao&#125;,</span><br><span class="line">  journal=&#123;IEEE Transactions on Circuits and Systems for Video Technology&#125;, </span><br><span class="line">  title=&#123;Adaptive Spatiotemporal Graph Transformer Network for Action Quality Assessment&#125;, </span><br><span class="line">  year=&#123;2025&#125;,</span><br><span class="line">  volume=&#123;35&#125;,</span><br><span class="line">  number=&#123;7&#125;,</span><br><span class="line">  pages=&#123;6628-6639&#125;</span><br><span class="line">   &#125;</span><br><span class="line">  </span><br><span class="line">Liu, J., et al. &quot;Adaptive Spatiotemporal Graph Transformer Network for Action Quality Assessment,&quot; in IEEE Transactions on Circuits and Systems for Video Technology, vol. 35, no. 7, pp. 6628-6639, 2025.</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法88：MB-AQA（多分支综合建模）"><a href="#方法88：MB-AQA（多分支综合建模）" class="headerlink" title="方法88：MB-AQA（多分支综合建模）"></a>方法88：MB-AQA（多分支综合建模）</h3><p> 方法名称： MB-AQA <a href="zotero://select/library/items/W3EIE4C6">📚</a><br> 论文标题： Comprehensive Action Quality Assessment Through Multi-Branch Modeling<br> 核心技术：RGB,光流<br> 数据集： FineDiving, MTLAQA, AQA-7,<br> 主要贡献： 通过多分支并行与融合覆盖多种信息粒度<br> 应用背景： 多因素共同影响评分的项目</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Xu-ComprehensiveActionQuality-2025,</span><br><span class="line">  title = &#123;Comprehensive Action Quality Assessment Through Multi-Branch Modeling&#125;,</span><br><span class="line">  author = &#123;Xu, Siyuan and Chen, Peilin and Liu, Yue and Wang, Meng and Wang, Shiqi and Yan, Hong and Kwong, Sam&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;IEEE Transactions on Multimedia&#125;,</span><br><span class="line">  pages = &#123;1--14&#125;,</span><br><span class="line">  publisher = &#123;&#123;Institute of Electrical and Electronics Engineers (IEEE)&#125;&#125;,</span><br><span class="line">  doi = &#123;10.1109/tmm.2025.3607713&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法89：AQA综述（方法与基准）"><a href="#方法89：AQA综述（方法与基准）" class="headerlink" title="方法89：AQA综述（方法与基准）"></a>方法89：AQA综述（方法与基准）</h3><p> 方法名称： <a href="zotero://select/library/items/3HDN8YD5">📚</a><br> 论文标题： A Comprehensive Survey of Action Quality Assessment: Method and Benchmark<br> 核心技术： 综述与分类 + 基准汇总<br> 主要贡献： 全面回顾AQA发展、方法谱系与评测基准<br> 应用背景： 研究者入门与方法对比参考<br> 优缺点： ✅ 全景视角；❌ 随领域发展需持续更新<br> 演变与进步： 为后续指令/长时序/多模态方向奠基</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Zhou-ComprehensiveSurveyAction-2024,</span><br><span class="line">  title = &#123;A Comprehensive Survey of Action Quality Assessment: Method and Benchmark&#125;,</span><br><span class="line">  shorttitle = &#123;A Comprehensive Survey of Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Zhou, Kanglei and Cai, Ruizhi and Wang, Liyuan and Shum, Hubert P. H. and Liang, Xiaohui&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  month = dec,</span><br><span class="line">  number = &#123;arXiv:2412.11149&#125;,</span><br><span class="line">  eprint = &#123;2412.11149&#125;,</span><br><span class="line">  primaryclass = &#123;cs&#125;,</span><br><span class="line">  publisher = &#123;arXiv&#125;,</span><br><span class="line">  archiveprefix = &#123;arXiv&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法90：AQA系统性综述（ESWA）"><a href="#方法90：AQA系统性综述（ESWA）" class="headerlink" title="方法90：AQA系统性综述（ESWA）"></a>方法90：AQA系统性综述（ESWA）</h3><p> 方法名称： <a href="zotero://select/library/items/4R93J3IX">📚</a><br> 论文标题： Vision-Based Human Action Quality Assessment: A Systematic Review<br> 核心技术： 系统性综述 + 方法/数据/挑战总结<br> 主要贡献： 面向视觉AQA的系统性总结与趋势洞察<br> 应用背景： 学术与产业调研<br> 优缺点： ✅ 系统全面；❌ 覆盖范围随时间可能滞后<br> 演变与进步： 为多模态/长时序/指令方法提供路线图</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Liu-VisionbasedHumanAction-2025,</span><br><span class="line">  title = &#123;Vision-Based Human Action Quality Assessment: A Systematic Review&#125;,</span><br><span class="line">  shorttitle = &#123;Vision-Based Human Action Quality Assessment&#125;,</span><br><span class="line">  author = &#123;Liu, Jiang and Wang, Huasheng and Stawarz, Katarzyna and Li, Shiyin and Fu, Yao and Liu, Hantao&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  month = mar,</span><br><span class="line">  journal = &#123;Expert Systems with Applications&#125;,</span><br><span class="line">  volume = &#123;263&#125;,</span><br><span class="line">  pages = &#123;125642&#125;,</span><br><span class="line">  doi = &#123;10.1016/j.eswa.2024.125642&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法91：AQA十年回顾（系统综述）"><a href="#方法91：AQA十年回顾（系统综述）" class="headerlink" title="方法91：AQA十年回顾（系统综述）"></a>方法91：AQA十年回顾（系统综述）</h3><p> 方法名称： <a href="zotero://select/library/items/PV44J326">📚</a><br> 论文标题： A Decade of Action Quality Assessment: Largest Systematic Survey of Trends, Challenges, and Future Directions<br> 核心技术： 十年发展脉络梳理 + 挑战与前景<br> 主要贡献： 汇总趋势与挑战，提出未来研究方向<br> 应用背景： 学术前沿与路线规划<br> 优缺点： ✅ 全景趋势梳理；❌ 需结合最新进展更新<br> 演变与进步： 为因果/指令/多模态/长时序等方向提供指引</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Yin-DecadeActionQuality-,</span><br><span class="line">  title = &#123;A Decade of Action Quality Assessment: Largest Systematic Survey of Trends, Challenges, and Future Directions&#125;,</span><br><span class="line">  author = &#123;Yin, Hao and Parmar, Paritosh and Xu, Daoliang and Zhang, Yang and Zheng, Tianyou and Fu, Weiwei&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="方法92：ResFNN（残差前馈网络AQA）"><a href="#方法92：ResFNN（残差前馈网络AQA）" class="headerlink" title="方法92：ResFNN（残差前馈网络AQA）"></a>方法92：ResFNN（残差前馈网络AQA）</h3><p> 方法名称： ResFNN <a href="zotero://select/library/items/PAQWRJ9J">📚</a><br> 论文标题： ResFNN: Residual Structure-Based Feedforward Neural Network for Action Quality Assessment in Sports Consumer Electronics<br> 核心技术： 残差前馈网络 + 轻量化建模<br> 数据集： AQA-7, MTL-AQA, JIGSAWS<br> 主要贡献： 以轻量残差前馈结构实现低成本AQA推理</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;ResFNNResidualStructureBased2024,</span><br><span class="line">  title = &#123;ResFNN: Residual Structure-Based Feedforward Neural Network for Action Quality Assessment in Sports Consumer Electronics&#125;,</span><br><span class="line">  shorttitle = &#123;ResFNN&#125;,</span><br><span class="line">  author = &#123;Gao, Honghao and Yu, Si and Iqbal, Muddesar and Guizani, Mohsen&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;IEEE Transactions on Consumer Electronics&#125;,</span><br><span class="line">  pages = &#123;1--1&#125;,</span><br><span class="line">  doi = &#123;10.1109/TCE.2024.3482560&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="超图"><a href="#超图" class="headerlink" title="超图"></a>超图</h1><h2 id="Hypergraph-Neural-Networks"><a href="#Hypergraph-Neural-Networks" class="headerlink" title="Hypergraph Neural Networks"></a>Hypergraph Neural Networks</h2><p><a href="zotero://select/library/items/DEWPHNST">📚</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Feng-HypergraphNeuralNetworks-2019,</span><br><span class="line">  title = &#123;Hypergraph Neural Networks&#125;,</span><br><span class="line">  author = &#123;Feng, Yifan and You, Haoxuan and Zhang, Zizhao and Ji, Rongrong and Gao, Yue&#125;,</span><br><span class="line">  year = 2019,</span><br><span class="line">  journal = &#123;AAAI&#125;,</span><br><span class="line">  volume = &#123;33&#125;,</span><br><span class="line">  number = &#123;01&#125;,</span><br><span class="line">  pages = &#123;3558--3565&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Feng, Y., et al. &quot;Hypergraph Neural Networks,&quot; in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, pp. 3558–3565, 2019.</span><br></pre></td></tr></table></figure>
<h2 id="HGNN"><a href="#HGNN" class="headerlink" title="HGNN+"></a>HGNN+</h2><p><a href="zotero://select/library/items/BY92UQJA">📚</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Gao-HGNNGeneralHypergraph-2023,</span><br><span class="line">  title = &#123;HGNN+: General Hypergraph Neural Networks&#125;,</span><br><span class="line">  author = &#123;Gao, Yue and Feng, Yifan and Ji, Shuyi and Ji, Rongrong&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;IEEE Transactions on Pattern Analysis and Machine Intelligence&#125;,</span><br><span class="line">  volume = &#123;45&#125;,</span><br><span class="line">  number = &#123;3&#125;,</span><br><span class="line">  pages = &#123;3181--3199&#125;,</span><br><span class="line">  doi = &#123;10.1109/TPAMI.2022.3182052&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Gao, Y., et al. &quot;HGNN+: General Hypergraph Neural Networks,&quot; in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 3, pp. 3181–3199, 2023.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="HypergraphBasedMultiViewAction"><a href="#HypergraphBasedMultiViewAction" class="headerlink" title="HypergraphBasedMultiViewAction"></a>HypergraphBasedMultiViewAction</h3><p><a href="zotero://select/library/items/QIHMYRB2">📚</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Gao-HypergraphBasedMultiViewAction-2024,</span><br><span class="line">  title = &#123;Hypergraph-Based Multi-View Action Recognition Using Event Cameras&#125;,</span><br><span class="line">  author = &#123;Gao, Yue and Lu, Jiaxuan and Li, Siqi and Li, Yipeng and Du, Shaoyi&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;IEEE Transactions on Pattern Analysis and Machine Intelligence&#125;,</span><br><span class="line">  volume = &#123;46&#125;,</span><br><span class="line">  number = &#123;10&#125;,</span><br><span class="line">  pages = &#123;6610--6622&#125;,</span><br><span class="line">  doi = &#123;10.1109/TPAMI.2024.3382117&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Gao, Y., et al. &quot;Hypergraph-Based Multi-View Action Recognition Using Event Cameras,&quot; in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 10, pp. 6610–6622, 2024.</span><br></pre></td></tr></table></figure>
<h2 id="Adaptive-Hyper-Graph"><a href="#Adaptive-Hyper-Graph" class="headerlink" title="Adaptive Hyper-Graph"></a>Adaptive Hyper-Graph</h2><p><a href="zotero://select/library/items/EWHCHES5">📚</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Zhou-AdaptiveHyperGraphConvolution-2025,</span><br><span class="line">      title=&#123;Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action Recognition with Virtual Connections&#125;, </span><br><span class="line">      author=&#123;Youwei Zhou and Tianyang Xu and Cong Wu and Xiaojun Wu and Josef Kittler&#125;,</span><br><span class="line">      year=&#123;2025&#125;,</span><br><span class="line">      eprint=&#123;2411.14796&#125;,</span><br><span class="line">      archivePrefix=&#123;arXiv&#125;,</span><br><span class="line">      primaryClass=&#123;cs.CV&#125;,</span><br><span class="line">      url=&#123;https://arxiv.org/abs/2411.14796&#125;, </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Hyper-YOLO"><a href="#Hyper-YOLO" class="headerlink" title="Hyper-YOLO"></a>Hyper-YOLO</h3><p><a href="zotero://select/library/items/XTXDSP69">📚</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Feng-HyperYOLOWhenVisual-2025,</span><br><span class="line">  title = &#123;Hyper-YOLO: When Visual Object Detection Meets Hypergraph Computation&#125;,</span><br><span class="line">  author = &#123;Feng, Yifan and Huang, Jiangang and Du, Shaoyi and Ying, Shihui and Yong, Jun-Hai and Li, Yipeng and Ding, Guiguang and Ji, Rongrong and Gao, Yue&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;IEEE Transactions on Pattern Analysis and Machine Intelligence&#125;,</span><br><span class="line">  volume = &#123;47&#125;,</span><br><span class="line">  number = &#123;4&#125;,</span><br><span class="line">  pages = &#123;2388--2401&#125;,</span><br><span class="line">  doi = &#123;10.1109/tpami.2024.3524377&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Feng, Y., et al. &quot;Hyper-YOLO: When Visual Object Detection Meets Hypergraph Computation,&quot; in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 47, no. 4, pp. 2388–2401, 2025.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="GeneratingHypergraphBasedHighOrder"><a href="#GeneratingHypergraphBasedHighOrder" class="headerlink" title="GeneratingHypergraphBasedHighOrder"></a>GeneratingHypergraphBasedHighOrder</h2><p><a href="zotero://select/library/items/GNQKQ35C">📚</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Di-GeneratingHypergraphBasedHighOrder-2022,</span><br><span class="line">  title = &#123;Generating Hypergraph-Based High-Order Representations of Whole-Slide Histopathological Images for Survival Prediction&#125;,</span><br><span class="line">  author = &#123;Di, Donglin and Zou, Changqing and Feng, Yifan and Zhou, Haiyan and Ji, Rongrong and Dai, Qionghai and Gao, Yue&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  journal = &#123;IEEE Transactions on Pattern Analysis and Machine Intelligence&#125;,</span><br><span class="line">  pages = &#123;1--16&#125;,</span><br><span class="line">  doi = &#123;10.1109/tpami.2022.3209652&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Selective-HCN"><a href="#Selective-HCN" class="headerlink" title="Selective-HCN"></a>Selective-HCN</h2><p><a href="zotero://select/library/items/8SU5YM3S">📚</a></p>
<figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Zhu-SelectiveHypergraphConvolutional-2022,</span><br><span class="line">  title = &#123;Selective Hypergraph Convolutional Networks for Skeleton-Based Action Recognition&#125;,</span><br><span class="line">  booktitle = &#123;ICMR&#125;,</span><br><span class="line">  author = &#123;Zhu, Yiran and Huang, Guangji and Xu, Xing and Ji, Yanli and Shen, Fumin&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  pages = &#123;518--526&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="动作识别"><a href="#动作识别" class="headerlink" title="动作识别"></a>动作识别</h1><h2 id="2s-AGCN"><a href="#2s-AGCN" class="headerlink" title="2s-AGCN"></a>2s-AGCN</h2><p><a href="zotero://select/library/items/TUDXAQTZ">📚</a></p>
<figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Shi-TwoStreamAdaptiveGraph-2019,</span><br><span class="line">  title = &#123;Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  author = &#123;Shi, Lei and Zhang, Yifan and Cheng, Jian and Lu, Hanqing&#125;,</span><br><span class="line">  year = 2019,</span><br><span class="line">  pages = &#123;12026--12035&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="InfoGCN"><a href="#InfoGCN" class="headerlink" title="InfoGCN"></a>InfoGCN</h2><p><a href="zotero://select/library/items/TUDXAQTZ">📚</a></p>
<figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Chi-InfoGCNRepresentationLearning-2022,</span><br><span class="line">  title = &#123;InfoGCN: Representation Learning for Human Skeleton-Based Action Recognition&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  author = &#123;Chi, Hyung-Gun and Ha, Myoung Hoon and Chi, Seunggeun and Lee, Sang Wan and Huang, Qixing and Ramani, Karthik&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  pages = &#123;20154--20164&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="DS-STGCN"><a href="#DS-STGCN" class="headerlink" title="DS-STGCN"></a>DS-STGCN</h2><p><a href="zotero://select/library/items/AMIMF39C">📚</a></p>
<figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@article&#123;Xie-DynamicSemanticBasedSpatialTemporal-2024,</span><br><span class="line">  title = &#123;Dynamic Semantic-Based Spatial-Temporal Graph Convolution Network for Skeleton-Based Human Action Recognition&#125;,</span><br><span class="line">  author = &#123;Xie, Jianyang and Meng, Yanda and Zhao, Yitian and Nguyen, Anh and Yang, Xiaoyun and Zheng, Yalin&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  journal = &#123;IEEE Transactions on Image Processing&#125;,</span><br><span class="line">  volume = &#123;33&#125;,</span><br><span class="line">  pages = &#123;6691--6704&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h1><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@ARTICLE&#123;Hochreiter-LSTM-1997,</span><br><span class="line">  author=&#123;Hochreiter, Sepp and Schmidhuber, Jürgen&#125;,</span><br><span class="line">  journal=&#123;Neural Computation&#125;, </span><br><span class="line">  title=&#123;Long Short-Term Memory&#125;, </span><br><span class="line">  year=&#123;1997&#125;,</span><br><span class="line">  volume=&#123;9&#125;,</span><br><span class="line">  number=&#123;8&#125;,</span><br><span class="line">  pages=&#123;1735-1780&#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Vaswani-Transformer-2017,</span><br><span class="line"> author = &#123;Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, <span class="keyword">\L</span> ukasz and Polosukhin, Illia&#125;,</span><br><span class="line"> booktitle = &#123;NeurIPS&#125;,</span><br><span class="line"> pages = &#123;&#125;,</span><br><span class="line"> title = &#123;Attention is All you Need&#125;,</span><br><span class="line"> year = &#123;2017&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Mamba"><a href="#Mamba" class="headerlink" title="Mamba"></a>Mamba</h2><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Albert-Mamba-2024,</span><br><span class="line">title=&#123;Mamba: Linear-Time Sequence Modeling with Selective State Spaces&#125;,</span><br><span class="line">author=&#123;Albert Gu and Tri Dao&#125;,</span><br><span class="line">booktitle=&#123;COLM&#125;,</span><br><span class="line">year=&#123;2024&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Openpose"><a href="#Openpose" class="headerlink" title="Openpose"></a>Openpose</h2><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@article&#123;Cao-Openpose-2021,</span><br><span class="line">author = &#123;Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser&#125;,</span><br><span class="line">title = &#123;OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields&#125;,</span><br><span class="line">year = &#123;2021&#125;,</span><br><span class="line">volume = &#123;43&#125;,</span><br><span class="line">number = &#123;1&#125;,</span><br><span class="line">journal = &#123;IEEE Transactions on Pattern Analysis and Machine Intelligence&#125;,</span><br><span class="line">pages = &#123;172–186&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="inception"><a href="#inception" class="headerlink" title="inception"></a>inception</h2><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Szegedy-GoingDeeperConvolutions-2015,</span><br><span class="line">  title = &#123;Going Deeper with Convolutions&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  author = &#123;Szegedy, Christian and &#123;Wei Liu&#125; and &#123;Yangqing Jia&#125; and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew&#125;,</span><br><span class="line">  year = 2015,</span><br><span class="line">  pages = &#123;1--9&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上列举了AQA领域从1995年至2024年共72个主要方法的发展进程。这些方法在以下几个维度上演进：</p>
<h3 id="1-特征提取演化"><a href="#1-特征提取演化" class="headerlink" title="1. 特征提取演化"></a>1. <strong>特征提取演化</strong></h3><ul>
<li>手工特征(HOG/MBH) → 浅层机器学习(SVM) → 深度CNN(VGG/C3D) → 3D卷积(C3D/I3D) → Transformer架构 → 多模态融合(RGB+骨架+音频+文本)</li>
</ul>
<h3 id="2-模型范式演化"><a href="#2-模型范式演化" class="headerlink" title="2. 模型范式演化"></a>2. <strong>模型范式演化</strong></h3><ul>
<li>确定性回归 → 分类方法 → 排序学习 → 对比学习 → 多任务学习 → 自监督学习 → 半监督学习 → 连续学习</li>
</ul>
<h3 id="3-模态范围扩展"><a href="#3-模态范围扩展" class="headerlink" title="3. 模态范围扩展"></a>3. <strong>模态范围扩展</strong></h3><ul>
<li>单一RGB视频 → RGB+光流 → +骨架数据 → +音频 → +自然语言 → +规则/指令 → +AI生成视频评估</li>
</ul>
<h3 id="4-理论深度提升"><a href="#4-理论深度提升" class="headerlink" title="4. 理论深度提升"></a>4. <strong>理论深度提升</strong></h3><ul>
<li>点估计 → 分布建模(USDL/DAE) → 不确定性量化(UD-AQA) → 可解释性(NS-AQA/IRIS) → 神经-符号混合(NS-AQA/RICA2)</li>
</ul>
<h3 id="5-应用场景扩展"><a href="#5-应用场景扩展" class="headerlink" title="5. 应用场景扩展"></a>5. <strong>应用场景扩展</strong></h3><ul>
<li>竞技体育(跳水、体操、滑冰) → 日常技能(EPIC-Skill) → 医疗康复(JIGSAWS、KIMORE) → 群体动作(LOGO) → AI生成视频(GAIA)</li>
</ul>
<h3 id="6-细粒度程度提升"><a href="#6-细粒度程度提升" class="headerlink" title="6. 细粒度程度提升"></a>6. <strong>细粒度程度提升</strong></h3><ul>
<li>整体评分 → 阶段评分 → 子动作评分 → 关键点评分 → 多维度评分(子分项) → 自然语言反馈</li>
</ul>
<h3 id="7-跨域能力进步"><a href="#7-跨域能力进步" class="headerlink" title="7. 跨域能力进步"></a>7. <strong>跨域能力进步</strong></h3><ul>
<li>单任务单数据集 → 多任务学习 → 领域自适应 → 零样本泛化 → 连续学习 → 参数高效微调</li>
</ul>
<p>这些进展反映了AQA从早期的探索性工作逐步演进为成熟的深度学习领域，结合了计算机视觉、自然语言处理、图神经网络等多个前沿技术方向。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>yao
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://zzhenyao.github.io/2025/11/18/13-26-24/" title="AQA（动作质量评估）方法发展时间线">https://zzhenyao.github.io/2025/11/18/13-26-24/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AQA-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># AQA, 深度学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/08/06/15-40-59/" rel="prev" title="骨架动作识别的backbone 和姿态估计方法">
                  <i class="fa fa-chevron-left"></i> 骨架动作识别的backbone 和姿态估计方法
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/11/24/11-06-55/" rel="next" title="扩散生成与重建方法发展时间线">
                  扩散生成与重建方法发展时间线 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.0/mermaid.min.js","integrity":"sha256-3JloMMI/ZQx6ryuhhZTsQJQmGAkXeni6PkshX7UUO2s="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"PKunicor","repo":"PKunicor.github.io","client_id":"2efe0e153686e5d9e67d","client_secret":"c84e031b6ac86ce366a6b61640a4b1a8e01d05e0","admin_user":"PKunicor","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"37d334f931dd44c1d07fbbddc1449cff"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>

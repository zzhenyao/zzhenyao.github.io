<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zzhenyao.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"æœç´¢...","empty":"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœï¼š${query}","hits_time":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰","hits":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœ"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="ç®€å•æ•´ç†ç›¸å…³äººä½“è¿åŠ¨é¢„æµ‹å’Œæ‰©æ•£è¿åŠ¨ç”Ÿæˆçš„è®ºæ–‡ã€‚">
<meta property="og:type" content="article">
<meta property="og:title" content="æ‰©æ•£ç”Ÿæˆä¸é‡å»ºæ–¹æ³•å‘å±•æ—¶é—´çº¿">
<meta property="og:url" content="https://zzhenyao.github.io/2025/11/24/11-06-55/index.html">
<meta property="og:site_name" content="ä¸”å¬é£åŸ">
<meta property="og:description" content="ç®€å•æ•´ç†ç›¸å…³äººä½“è¿åŠ¨é¢„æµ‹å’Œæ‰©æ•£è¿åŠ¨ç”Ÿæˆçš„è®ºæ–‡ã€‚">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-11-24T03:06:55.000Z">
<meta property="article:modified_time" content="2025-12-20T08:35:15.768Z">
<meta property="article:author" content="yao">
<meta property="article:tag" content="æ·±åº¦å­¦ä¹ ">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zzhenyao.github.io/2025/11/24/11-06-55/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://zzhenyao.github.io/2025/11/24/11-06-55/","path":"2025/11/24/11-06-55/","title":"æ‰©æ•£ç”Ÿæˆä¸é‡å»ºæ–¹æ³•å‘å±•æ—¶é—´çº¿"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>æ‰©æ•£ç”Ÿæˆä¸é‡å»ºæ–¹æ³•å‘å±•æ—¶é—´çº¿ | ä¸”å¬é£åŸ</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ " role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">ä¸”å¬é£åŸ</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">è½»èˆŸè¿‡ä¸‡é‡,é’å±±ä¾æ—§åœ¨</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="æœç´¢" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>æœç´¢
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="æœç´¢..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Nonisotropic-Gaussian-Diffusion-%E9%9D%9E%E5%90%84%E5%90%91%E5%90%8C%E6%80%A7%E9%AB%98%E6%96%AF%E6%89%A9%E6%95%A3"><span class="nav-number">1.</span> <span class="nav-text">Nonisotropic Gaussian Diffusion (éå„å‘åŒæ€§é«˜æ–¯æ‰©æ•£)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Physics-Informed-Learning-with-Sparse-IMUs-%E5%9F%BA%E4%BA%8E%E7%A8%80%E7%96%8FIMU%E7%9A%84%E7%89%A9%E7%90%86%E6%84%9F%E7%9F%A5%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.</span> <span class="nav-text">Physics-Informed Learning with Sparse IMUs (åŸºäºç¨€ç–IMUçš„ç‰©ç†æ„ŸçŸ¥å­¦ä¹ )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HUMOS%EF%BC%88%E5%9F%BA%E4%BA%8E%E4%BD%93%E5%9E%8B%E6%9D%A1%E4%BB%B6%E7%9A%84%E8%BF%90%E5%8A%A8%E5%BB%BA%E6%A8%A1%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">HUMOSï¼ˆåŸºäºä½“å‹æ¡ä»¶çš„è¿åŠ¨å»ºæ¨¡ï¼‰</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ViA-View-invariant-Skeleton-Action-Representation-Learning-via-Motion-Retargeting"><span class="nav-number">4.</span> <span class="nav-text">ViA: View-invariant Skeleton Action Representation Learning via Motion Retargeting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Plug-and-Play-Physical-Motion-Restoration-Approach-for-In-the-Wild-High-Difficulty-Motions"><span class="nav-number">5.</span> <span class="nav-text">A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Robust-2D-Skeleton-Action-Recognition-via-Decoupling-and-Distilling-3D-Latent-Features"><span class="nav-number">6.</span> <span class="nav-text">Robust 2D Skeleton Action Recognition via Decoupling and Distilling 3D Latent Features</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3DPCNet-Pose-Canonicalization-for-Robust-Viewpoint-Invariant-3D-Kinematic-Analysis-from-Monocular-RGB-cameras"><span class="nav-number">7.</span> <span class="nav-text">3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lifting-Motion-to-the-3D-World-via-2D-Diffusion"><span class="nav-number">8.</span> <span class="nav-text">Lifting Motion to the 3D World via 2D Diffusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Holistic-Motion2D-Scalable-Whole-body-Human-Motion-Generation-in-2D-Space"><span class="nav-number">9.</span> <span class="nav-text">Holistic-Motion2D: Scalable Whole-body Human Motion Generation in 2D Space</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Skeleton-in-Context-Unified-Skeleton-Sequence-Modeling-with-In-Context-Learning"><span class="nav-number">10.</span> <span class="nav-text">Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-NeRF-Articulated-Neural-Radiance-Fields-for-Learning-Human-Shape-Appearance-and-Pose"><span class="nav-number">11.</span> <span class="nav-text">A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#E-RayZer-Self-supervised-3D-Reconstruction-as-Spatial-Visual-Pre-training"><span class="nav-number">12.</span> <span class="nav-text">E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Denoising-Hamiltonian-Network-for-Physical-Reasoning"><span class="nav-number">13.</span> <span class="nav-text">Denoising Hamiltonian Network for Physical Reasoning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GFPose-Learning-3D-Human-Pose-Prior-With-Gradient-Fields"><span class="nav-number">14.</span> <span class="nav-text">GFPose: Learning 3D Human Pose Prior With Gradient Fields</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ElePose-Unsupervised-3D-Human-Pose-Estimation-by-Predicting-Camera-Elevation-and-Learning-Normalizing-Flows-on-2D-Poses"><span class="nav-number">15.</span> <span class="nav-text">ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unsupervised-Adversarial-Learning-of-3D-Human-Pose-from-2D-Joint-Locations"><span class="nav-number">16.</span> <span class="nav-text">Unsupervised Adversarial Learning of 3D Human Pose from 2D Joint Locations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MAS-Multi-view-Ancestral-Sampling-for-3D-motion-generation-using-2D-diffusion"><span class="nav-number">17.</span> <span class="nav-text">MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MoDi-Unconditional-Motion-Synthesis-from-Diverse-Data"><span class="nav-number">18.</span> <span class="nav-text">MoDi: Unconditional Motion Synthesis from Diverse Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Temporal-Constrained-Feasible-Subspace-Learning-for-Human-Pose-Forecasting"><span class="nav-number">19.</span> <span class="nav-text">Temporal Constrained Feasible Subspace Learning for Human Pose Forecasting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PUMPS-Skeleton-Agnostic-Point-based-Universal-Motion-Pre-Training-for-Synthesis-in-Human-Motion-Tasks"><span class="nav-number">20.</span> <span class="nav-text">PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Dual-Masked-Auto-Encoder-for-Robust-Motion-Capture-with-Spatial-Temporal-Skeletal-Token-Completion"><span class="nav-number">21.</span> <span class="nav-text">A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Motion-Keyframe-Interpolation-for-Any-Human-Skeleton-via-Temporally-Consistent-Point-Cloud-Sampling-and-Reconstruction"><span class="nav-number">22.</span> <span class="nav-text">Motion Keyframe Interpolation for Any Human Skeleton via Temporally Consistent Point Cloud Sampling and Reconstruction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#S-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Skeletal-Action-Recognition"><span class="nav-number">23.</span> <span class="nav-text">S-JEPA: A Joint Embedding Predictive Architecture for Skeletal Action Recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MacDiff-Unified-Skeleton-Modeling-with-Masked-Conditional-Diffusion"><span class="nav-number">24.</span> <span class="nav-text">MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DSPose-Dual-Space-Driven-Keypoint-Topology-Modeling-for-Human-Pose-Estimation"><span class="nav-number">25.</span> <span class="nav-text">DSPose: Dual-Space-Driven Keypoint Topology Modeling for Human Pose Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaptive-2D-skeleton-deformation-based-on-view-agnostic-network-for-action-recognition"><span class="nav-number">26.</span> <span class="nav-text">Adaptive 2D skeleton deformation based on view agnostic network for action recognition</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="yao"
      src="/images/logo.png">
  <p class="site-author-name" itemprop="name">yao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zzhenyao" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;zzhenyao" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zzhenyao.github.io/2025/11/24/11-06-55/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.png">
      <meta itemprop="name" content="yao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ä¸”å¬é£åŸ">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="æ‰©æ•£ç”Ÿæˆä¸é‡å»ºæ–¹æ³•å‘å±•æ—¶é—´çº¿ | ä¸”å¬é£åŸ">
      <meta itemprop="description" content="ç®€å•æ•´ç†ç›¸å…³äººä½“è¿åŠ¨é¢„æµ‹å’Œæ‰©æ•£è¿åŠ¨ç”Ÿæˆçš„è®ºæ–‡ã€‚">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          æ‰©æ•£ç”Ÿæˆä¸é‡å»ºæ–¹æ³•å‘å±•æ—¶é—´çº¿
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">å‘è¡¨äº</span>

      <time title="åˆ›å»ºæ—¶é—´ï¼š2025-11-24 11:06:55" itemprop="dateCreated datePublished" datetime="2025-11-24T11:06:55+08:00">2025-11-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">æ›´æ–°äº</span>
      <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-12-20 16:35:15" itemprop="dateModified" datetime="2025-12-20T16:35:15+08:00">2025-12-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">åˆ†ç±»äº</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">æ·±åº¦å­¦ä¹ </span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="é˜…è¯»æ¬¡æ•°" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">é˜…è¯»æ¬¡æ•°ï¼š</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>



        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
            <div class="post-description">ç®€å•æ•´ç†ç›¸å…³äººä½“è¿åŠ¨é¢„æµ‹å’Œæ‰©æ•£è¿åŠ¨ç”Ÿæˆçš„è®ºæ–‡ã€‚</div>
	<hr>
        <hr>
<h3 id="Nonisotropic-Gaussian-Diffusion-éå„å‘åŒæ€§é«˜æ–¯æ‰©æ•£"><a href="#Nonisotropic-Gaussian-Diffusion-éå„å‘åŒæ€§é«˜æ–¯æ‰©æ•£" class="headerlink" title="Nonisotropic Gaussian Diffusion (éå„å‘åŒæ€§é«˜æ–¯æ‰©æ•£)"></a>Nonisotropic Gaussian Diffusion (éå„å‘åŒæ€§é«˜æ–¯æ‰©æ•£)</h3><p><strong>æ–¹æ³•åç§°ï¼š</strong> Nonisotropic Gaussian Diffusion <a href="zotero://select/library/items/CXP4N8VV">ğŸ“š</a><br><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction<br><strong>æ ¸å¿ƒæŠ€æœ¯ï¼š</strong> éå„å‘åŒæ€§é«˜æ–¯æ‰©æ•£è¿‡ç¨‹ï¼ˆNonisotropic Gaussian Diffusionï¼‰ã€è¿åŠ¨è½¨è¿¹çš„åæ–¹å·®å»ºæ¨¡ã€æ—¶é—´ä¸€è‡´æ€§çº¦æŸã€åŸºäºç‰©ç†å…ˆéªŒçš„å™ªå£°è°ƒåº¦</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Curreli-NonisotropicGaussianDiffusion-2025a,</span><br><span class="line">  title = &#123;Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction&#125;,</span><br><span class="line">  author = &#123;Curreli, Cecilia and Muhle, Dominik and Saroha, Abhishek and Ye, Zhenzhang and Marin, Riccardo and Cremers, Daniel&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  number = &#123;arXiv:2501.06035&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Physics-Informed-Learning-with-Sparse-IMUs-åŸºäºç¨€ç–IMUçš„ç‰©ç†æ„ŸçŸ¥å­¦ä¹ "><a href="#Physics-Informed-Learning-with-Sparse-IMUs-åŸºäºç¨€ç–IMUçš„ç‰©ç†æ„ŸçŸ¥å­¦ä¹ " class="headerlink" title="Physics-Informed Learning with Sparse IMUs (åŸºäºç¨€ç–IMUçš„ç‰©ç†æ„ŸçŸ¥å­¦ä¹ )"></a>Physics-Informed Learning with Sparse IMUs (åŸºäºç¨€ç–IMUçš„ç‰©ç†æ„ŸçŸ¥å­¦ä¹ )</h3><p><strong>æ–¹æ³•åç§°ï¼š</strong> Physics-Informed Learning with Sparse IMUs <a href="zotero://select/library/items/VD7REEC4">ğŸ“š</a><br><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> Physics-Informed Learning for Human Whole-Body Kinematics Prediction via Sparse IMUs<br><strong>æ ¸å¿ƒæŠ€æœ¯ï¼š</strong> ç¨€ç–æƒ¯æ€§æµ‹é‡å•å…ƒï¼ˆSparse IMUsï¼‰æ•°æ® + ç‰©ç†å…ˆéªŒå»ºæ¨¡ï¼ˆç‰›é¡¿-æ¬§æ‹‰åŠ¨åŠ›å­¦ï¼‰ + å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ + æ— ç›‘ç£ç‰©ç†æŸå¤±å‡½æ•°ï¼ˆPhysics-Informed Lossï¼‰<br><strong>ä¸»è¦è´¡çŒ®ï¼š</strong>é¦–æ¬¡å°† <strong>ç‰©ç†ä¸€è‡´æ€§çº¦æŸ</strong>ï¼ˆå¦‚åŠ¨é‡å®ˆæ’ã€è§’åŠ¨é‡å®ˆæ’ã€åŠ›çŸ©å¹³è¡¡ï¼‰åµŒå…¥ç¨€ç–IMUé©±åŠ¨çš„å…¨èº«è¿åŠ¨é¢„æµ‹æ¡†æ¶ï¼›è®¾è®¡ <strong>å¯å¾®åˆ†çš„ç‰©ç†æŸå¤±å‡½æ•°</strong>ï¼Œåœ¨è®­ç»ƒä¸­æ˜¾å¼æƒ©ç½šè¿åç‰©ç†è§„å¾‹çš„è¿åŠ¨è½¨è¿¹ï¼ˆå¦‚å¼‚å¸¸åŠ é€Ÿåº¦ã€èƒ½é‡çªå˜ï¼‰ï¼›</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Guo-PhysicsInformedLearningHuman-2025,</span><br><span class="line">  title = &#123;Physics-Informed Learning for Human Whole-Body Kinematics Prediction via Sparse IMUs&#125;,</span><br><span class="line">  author = &#123;Guo, Cheng and L&#x27;Erario, Giuseppe and Romualdi, Giulio and Leonori, Mattia and Lorenzini, Marta and Ajoudani, Arash and Pucci, Daniele&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  number = &#123;arXiv:2509.25704&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="HUMOSï¼ˆåŸºäºä½“å‹æ¡ä»¶çš„è¿åŠ¨å»ºæ¨¡ï¼‰"><a href="#HUMOSï¼ˆåŸºäºä½“å‹æ¡ä»¶çš„è¿åŠ¨å»ºæ¨¡ï¼‰" class="headerlink" title="HUMOSï¼ˆåŸºäºä½“å‹æ¡ä»¶çš„è¿åŠ¨å»ºæ¨¡ï¼‰"></a>HUMOSï¼ˆåŸºäºä½“å‹æ¡ä»¶çš„è¿åŠ¨å»ºæ¨¡ï¼‰</h3><p><strong>æ–¹æ³•åç§°ï¼š</strong> HUMOS <a href="zotero://select/library/items/CBE39FQU">ğŸ“š</a><br><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> HUMOS: Human Motion Model Conditioned on Body Shape<br><strong>æ ¸å¿ƒæŠ€æœ¯ï¼š</strong> ä½“å‹æ¡ä»¶ç”Ÿæˆï¼ˆBody Shape Conditioningï¼‰ + 3DåŠ¨ä½œæ‰©æ•£æ¨¡å‹ï¼ˆ3D Motion Diffusionï¼‰ + ä½“å‹-åŠ¨ä½œè”åˆéšç©ºé—´å»ºæ¨¡ï¼ˆShape-Aware Latent Spaceï¼‰ + å¯å¾®åˆ†äººä½“å‡ ä½•å…ˆéªŒï¼ˆHuman Body Priorï¼‰</p>
<p><strong>ä¸»è¦è´¡çŒ®ï¼šæå‡º </strong>æ˜¾å¼å»ºæ¨¡ä½“å‹ï¼ˆBody Shapeï¼‰ä¸åŠ¨ä½œï¼ˆMotionï¼‰è€¦åˆå…³ç³»çš„ç”Ÿæˆå¼äººä½“è¿åŠ¨æ¨¡å‹ HUMOSï¼›æ ¹æ®ä¸åŒä½“å‹ï¼Œå’ŒåŠ¨ä½œç”ŸæˆçœŸå®äººä½“è¿åŠ¨ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Tripathi-HUMOSHumanMotion-2025,</span><br><span class="line">  title = &#123;HUMOS: Human Motion Model Conditioned on Body Shape&#125;,</span><br><span class="line">  author = &#123;Tripathi, Shashank and Taheri, Omid and Lassner, Christoph and Black, Michael J. and Holden, Daniel and Stoll, Carsten&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  number = &#123;arXiv:2409.03944&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="ViA-View-invariant-Skeleton-Action-Representation-Learning-via-Motion-Retargeting"><a href="#ViA-View-invariant-Skeleton-Action-Representation-Learning-via-Motion-Retargeting" class="headerlink" title="ViA: View-invariant Skeleton Action Representation Learning via Motion Retargeting"></a>ViA: View-invariant Skeleton Action Representation Learning via Motion Retargeting</h3><p><strong>æ–¹æ³•åç§°ï¼š</strong> ViA <a href="zotero://select/library/items/3JDB3A6G">ğŸ“š</a><br><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> ViA: View-invariant Skeleton Action Representation Learning via Motion Retargeting<br><strong>æ ¸å¿ƒæŠ€æœ¯ï¼š</strong> åŠ¨ä½œé‡å®šå‘ï¼ˆMotion Retargetingï¼‰ + è§†è§’ä¸å˜è¡¨ç¤ºå­¦ä¹  + éª¨éª¼è¿åŠ¨å»ºæ¨¡ + å¯¹é½æŸå¤±<br><strong>æ•°æ®é›†ï¼š</strong> NTU RGB+D, Kinetics-Skeleton, PKU-MMD<br><strong>ä¸»è¦è´¡çŒ®ï¼š</strong>é€šè¿‡åŠ¨ä½œé‡å®šå‘å®ç°è§†è§’ä¸å˜çš„éª¨éª¼åŠ¨ä½œè¡¨ç¤ºï¼›</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Yang-ViewinvariantSkeletonAction-2022,</span><br><span class="line">  title = &#123;ViA: View-Invariant Skeleton Action Representation Learning via Motion Retargeting&#125;,</span><br><span class="line">  author = &#123;Yang, Di and Wang, Yaohui and Dantcheva, Antitza and Garattoni, Lorenzo and Francesca, Gianpiero and Bremond, Francois&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  number = &#123;arXiv:2209.00065&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="A-Plug-and-Play-Physical-Motion-Restoration-Approach-for-In-the-Wild-High-Difficulty-Motions"><a href="#A-Plug-and-Play-Physical-Motion-Restoration-Approach-for-In-the-Wild-High-Difficulty-Motions" class="headerlink" title="A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions"></a>A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions</h3><p><strong>æ–¹æ³•åç§°ï¼š</strong> Plug-and-Play Physical Motion Restoration <a href="zotero://select/library/items/6V5R6QU7">ğŸ“š</a><br><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions<br><strong>æ ¸å¿ƒæŠ€æœ¯ï¼š</strong> ç‰©ç†å…ˆéªŒåµŒå…¥ï¼ˆPhysics-informed Priorï¼‰ + å¯æ’æ‹”å¼ä¿®å¤æ¨¡å—ï¼ˆPlug-and-Playï¼‰ + æ‰©æ•£æ¨¡å‹ + è¿åŠ¨è¿ç»­æ€§ä¸æƒ¯æ€§å»ºæ¨¡<br><strong>æ•°æ®é›†ï¼š</strong> In-the-wild è§†é¢‘ï¼ˆå¦‚æŠ–éŸ³ã€YouTubeï¼‰ã€CMU Mocapã€AMASS<br><strong>ä¸»è¦è´¡çŒ®ï¼š</strong> åˆ©ç”¨é«˜é€Ÿè¿åŠ¨çš„RGBè§†é¢‘ï¼Œç”Ÿæˆ3Däººä½“</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Zhang-PlugandPlayPhysicalMotion-2024,</span><br><span class="line">  title = &#123;A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions&#125;,</span><br><span class="line">  author = &#123;Zhang, Youliang and Li, Ronghui and Zhang, Yachao and Pan, Liang and Wang, Jingbo and Liu, Yebin and Li, Xiu&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  number = &#123;arXiv:2412.17377&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Robust-2D-Skeleton-Action-Recognition-via-Decoupling-and-Distilling-3D-Latent-Features"><a href="#Robust-2D-Skeleton-Action-Recognition-via-Decoupling-and-Distilling-3D-Latent-Features" class="headerlink" title="Robust 2D Skeleton Action Recognition via Decoupling and Distilling 3D Latent Features"></a>Robust 2D Skeleton Action Recognition via Decoupling and Distilling 3D Latent Features</h3><p>æ–¹æ³•åç§°ï¼š2D3-SkelActï¼ˆå³æ’å³ç”¨2Déª¨éª¼åŠ¨ä½œè¯†åˆ«ï¼‰<a href="zotero://select/library/items/PVRKWNJ6">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šå§¿æ€-è§†è§’è§£è€¦+3Dæ½œåœ¨è’¸é¦+æ—¶ç©ºTransformerè·¨æ³¨æ„åŠ›+2D-to-3Dç›‘ç£<br>æ•°æ®é›†ï¼šNTU-RGB+D 60/120ã€PKU-MMDã€UAV-Humanã€NW-UCLA<br>ä¸»è¦è´¡çŒ®ï¼š2Dè¾“å…¥è¾¾3Dç²¾åº¦ï¼ˆPKU-IIå¤§æå‡ï¼‰ï¼›ä»»æ„éª¨å¹²å³æ’å³ç”¨ï¼›æ¡¥æ¥2D/3Dï¼Œæ— 3Dç¡¬ä»¶éœ€æ±‚ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhang-Robust2DSkeleton-2025,</span><br><span class="line">  title = &#123;Robust 2D Skeleton Action Recognition via Decoupling and Distilling 3D Latent Features&#125;,</span><br><span class="line">  author = &#123;Zhang, Xiangyue and Jia, Yifan and Zhang, Jiaxu and Yang, Yijie and Tu, Zhigang&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  journal = &#123;IEEE Transactions on Circuits and Systems for Video Technology&#125;,</span><br><span class="line">  volume = &#123;35&#125;,</span><br><span class="line">  number = &#123;10&#125;,</span><br><span class="line">  pages = &#123;10410--10422&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="3DPCNet-Pose-Canonicalization-for-Robust-Viewpoint-Invariant-3D-Kinematic-Analysis-from-Monocular-RGB-cameras"><a href="#3DPCNet-Pose-Canonicalization-for-Robust-Viewpoint-Invariant-3D-Kinematic-Analysis-from-Monocular-RGB-cameras" class="headerlink" title="3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras"></a>3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras</h3><p>æ–¹æ³•åç§°ï¼š3DPCNetï¼ˆå§¿æ€è§„èŒƒç½‘ç»œï¼‰<a href="zotero://select/library/items/QDK2LVXP">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šæ··åˆGCN-Transformerç¼–ç å™¨+é—¨æ§è·¨æ³¨æ„åŠ›èåˆ+6Dè¿ç»­æ—‹è½¬é¢„æµ‹ï¼ˆGram-Schmidtæ­£äº¤åŒ–ï¼‰+å¯é€‰æ®‹å·®æ ¡æ­£+å¤åˆæŸå¤±ï¼ˆæ—‹è½¬/é‡å»º/å¾ªç¯ä¸€è‡´ï¼‰<br>æ•°æ®é›†ï¼šMM-Fiï¼ˆè§„èŒƒè¯„ä¼°ï¼‰ã€TotalCaptureï¼ˆIMU kinematicséªŒè¯ï¼‰<br>ä¸»è¦è´¡çŒ®ï¼šå•ç›®3Déª¨éª¼è½¬ä½“å¿ƒè§„èŒƒå¸§ï¼ˆæ—‹è½¬è¯¯å·®3.4Â°ã€MPJPEé™27%ï¼‰ï¼›æ— å›¾åƒ/æ ‡å®šæ³›åŒ–ï¼›è…•åŠ é€Ÿåº¦åŒ¹é…IMUï¼Œæå‡å¥åº·/ä½“è‚²è¿åŠ¨åˆ†æã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Ekanayake-3DPCNetPoseCanonicalization-2025,</span><br><span class="line">  title = &#123;3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB Cameras&#125;,</span><br><span class="line">  author = &#123;Ekanayake, Tharindu and Casado, Constantino &#123;\&#x27;A&#125;lvarez and L&#123;\&#x27;o&#125;pez, Miguel Bordallo&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  number = &#123;arXiv:2509.23455&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Lifting-Motion-to-the-3D-World-via-2D-Diffusion"><a href="#Lifting-Motion-to-the-3D-World-via-2D-Diffusion" class="headerlink" title="Lifting Motion to the 3D World via 2D Diffusion"></a>Lifting Motion to the 3D World via 2D Diffusion</h3><p>æ–¹æ³•åç§°ï¼šMVLiftï¼ˆ2Dæ‰©æ•£æå‡3Dè¿åŠ¨ï¼‰<a href="zotero://select/library/items/HSD8KL3N">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šå¤šé˜¶æ®µ2Dæ‰©æ•£å¤šè§†è§’ä¸€è‡´ï¼ˆçº¿æ¡ä»¶æ‰©æ•£+SDSå¤šè§†è§’ä¼˜åŒ–+åˆæˆæ•°æ®+è·¨è§†è§’Transformeræ‰©æ•£ï¼‰+3Dé‡æŠ•å½±+SMPLæ‹Ÿåˆ<br>æ•°æ®é›†ï¼šAIST/Steezy/NicoleMoveï¼ˆäººç±»ï¼‰ã€CatPlayï¼ˆåŠ¨ç‰©ï¼‰ã€OMOMOï¼ˆäº¤äº’ï¼‰<br>ä¸»è¦è´¡çŒ®ï¼šé›¶3Dç›‘ç£ä»å•2Dåºåˆ—ç”Ÿæˆå…¨å±€3Dè¿åŠ¨ï¼ˆæ ¹è½¨è¿¹+å…³èŠ‚ï¼‰ï¼›è·¨åŸŸæ³›åŒ–ï¼ˆäºº/åŠ¨ç‰©/äº¤äº’ï¼‰ï¼›ä¼˜äºéœ€3DåŸºçº¿ï¼ˆMPJPE/Troot/FIDæå‡ï¼‰ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Li-LiftingMotion3D-2025,</span><br><span class="line">  title = &#123;Lifting Motion to the 3D World via 2D Diffusion&#125;,</span><br><span class="line">  author = &#123;Li, Jiaman and Liu, C. Karen and Wu, Jiajun&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  number = &#123;arXiv:2411.18808&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Holistic-Motion2D-Scalable-Whole-body-Human-Motion-Generation-in-2D-Space"><a href="#Holistic-Motion2D-Scalable-Whole-body-Human-Motion-Generation-in-2D-Space" class="headerlink" title="Holistic-Motion2D: Scalable Whole-body Human Motion Generation in 2D Space"></a>Holistic-Motion2D: Scalable Whole-body Human Motion Generation in 2D Space</h3><p>æ–¹æ³•åç§°ï¼šHolistic-Motion2D + Tenderï¼ˆ2Då…¨èº«è¿åŠ¨ç”Ÿæˆï¼‰<a href="zotero://select/library/items/2LFZILCC">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šéƒ¨ä½ç½®æ„ŸçŸ¥VAEï¼ˆPA-VAEï¼‰+ç½®ä¿¡æ„ŸçŸ¥ç”Ÿæˆï¼ˆCAGï¼‰+Transformeræ‰©æ•£ï¼ˆlatentç©ºé—´ï¼‰+MoLIPæ–‡æœ¬-è¿åŠ¨å¯¹é½è¯„ä¼°<br>æ•°æ®é›†ï¼šHolistic-Motion2Dï¼ˆè‡ªå»º1Må‰ªè¾‘ï¼‰ã€UCF101/Kinetics-400/700ã€Sthv2/DFEW/CAERç­‰<br>ä¸»è¦è´¡çŒ®ï¼šé¦–åˆ›ç™¾ä¸‡çº§2Då…¨èº«è¿åŠ¨æ•°æ®é›†ï¼ˆ10x Motion-Xï¼‰ï¼›TenderåŸºçº¿SOTAï¼ˆFID/MM-Disté™ï¼‰ï¼›ä¸‹æ¸¸è§†é¢‘ç”Ÿæˆ/3Dæå‡ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Wang-HolisticMotion2DScalableWholebody-2024,</span><br><span class="line">  title = &#123;Holistic-Motion2D: Scalable Whole-Body Human Motion Generation in 2D Space&#125;,</span><br><span class="line">  author = &#123;Wang, Yuan and Wang, Zhao and Gong, Junhao and Huang, Di and He, Tong and Ouyang, Wanli and Jiao, Jile and Feng, Xuetao and Dou, Qi and Tang, Shixiang and Xu, Dan&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  number = &#123;arXiv:2406.11253&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Skeleton-in-Context-Unified-Skeleton-Sequence-Modeling-with-In-Context-Learning"><a href="#Skeleton-in-Context-Unified-Skeleton-Sequence-Modeling-with-In-Context-Learning" class="headerlink" title="Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning"></a>Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning</h3><p>æ–¹æ³•åç§°ï¼šSkeleton-in-Contextï¼ˆä¸Šä¸‹æ–‡éª¨éª¼ç»Ÿä¸€å»ºæ¨¡ï¼‰<a href="zotero://select/library/items/5LE4KVDB">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šä»»åŠ¡å¼•å¯¼æç¤ºï¼ˆTGPï¼‰+ä»»åŠ¡ç»Ÿä¸€æç¤ºï¼ˆTUPé™æ€/åŠ¨æ€ä¼ªå§¿æ€ï¼‰+Transformerï¼ˆæ—¶ç©ºäº¤æ›¿æ³¨æ„åŠ›ï¼‰+ç«¯åˆ°ç«¯å¤šä»»åŠ¡ï¼ˆé¢„æµ‹/ä¼°è®¡/è¡¥å…¨/æœªæ¥å§¿æ€ï¼‰<br>æ•°æ®é›†ï¼šH3.6Mï¼ˆå§¿æ€ä¼°è®¡/æœªæ¥ä¼°è®¡ï¼‰ã€AMASSï¼ˆè¿åŠ¨é¢„æµ‹ï¼‰ã€3DPWï¼ˆå…³èŠ‚è¡¥å…¨ï¼‰<br>ä¸»è¦è´¡çŒ®ï¼šé¦–åˆ›éª¨éª¼åºåˆ—ä¸Šä¸‹æ–‡å­¦ä¹ ï¼›å•è®­ç»ƒå¤šä»»åŠ¡SOTAï¼ˆè¶…å•ä»»åŠ¡æ¨¡å‹ï¼‰ï¼›æ³›åŒ–æ–°ä»»åŠ¡/æ•°æ®é›†ï¼ˆå¦‚è¿åŠ¨é—´æ’å€¼ï¼‰ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Wang-SkeletoninContextUnifiedSkeleton-2024,</span><br><span class="line">  title = &#123;Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning&#125;,</span><br><span class="line">  author = &#123;Wang, Xinshun and Fang, Zhongbin and Li, Xia and Li, Xiangtai and Liu, Mengyuan&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  number = &#123;arXiv:2312.03703&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="A-NeRF-Articulated-Neural-Radiance-Fields-for-Learning-Human-Shape-Appearance-and-Pose"><a href="#A-NeRF-Articulated-Neural-Radiance-Fields-for-Learning-Human-Shape-Appearance-and-Pose" class="headerlink" title="A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose"></a>A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose</h3><p>æ–¹æ³•åç§°ï¼šA-NeRFï¼ˆå…³èŠ‚ç¥ç»è¾å°„åœºï¼‰<a href="zotero://select/library/items/36394RMQ">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šNeRFæ‰©å±•ï¼ˆéª¨éª¼ç›¸å¯¹ç¼–ç ï¼šç›¸å¯¹è·ç¦»/æ–¹å‘/å°„çº¿+æˆªæ–­ï¼‰+ä½“ç§¯æ¸²æŸ“+å§¿æ€ç»†åŒ–ï¼ˆå…‰åº¦+å…‰æ»‘å…ˆéªŒï¼‰+æ¯å¸§å¤–è§‚ç <br>æ•°æ®é›†ï¼šHuman3.6Mã€MPI-INF-3DHPã€MonoPerfCapã€SURREAL/Mixamoï¼ˆåˆæˆï¼‰<br>ä¸»è¦è´¡çŒ®ï¼šå•è§†é¢‘æ— æ¨¡æ¿å­¦ä¹ ç¥ç»äººä½“æ¨¡å‹ï¼ˆå½¢çŠ¶/å¤–è§‚ï¼‰ï¼›å§¿æ€ç»†åŒ–SOTAï¼ˆPA-MPJPEæå‡ï¼‰ï¼›æ–°è§†è§’åˆæˆ/è§’è‰²åŠ¨ç”»ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Su-ANeRFArticulatedNeural-2021,</span><br><span class="line">  title = &#123;A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose&#125;,</span><br><span class="line">  author = &#123;Su, Shih-Yang and Yu, Frank and Zollhoefer, Michael and Rhodin, Helge&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  number = &#123;arXiv:2102.06199&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="E-RayZer-Self-supervised-3D-Reconstruction-as-Spatial-Visual-Pre-training"><a href="#E-RayZer-Self-supervised-3D-Reconstruction-as-Spatial-Visual-Pre-training" class="headerlink" title="E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training"></a>E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training</h3><p>æ–¹æ³•åç§°ï¼šE-RayZerï¼ˆæ˜¾å¼3DGSè‡ªç›‘ç£ç©ºé—´é¢„è®­ç»ƒï¼‰<a href="zotero://select/library/items/2EXICEFW">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šæ˜¾å¼3Dé«˜æ–¯è¡¨ç¤ºï¼ˆé¢„æµ‹ç›¸æœº+3D Gaussianså¹¶å¯å¾®æ¸²æŸ“ï¼‰+ å¤šè§†å›¾Transformerï¼ˆVGGTå¼å±€éƒ¨/å…¨å±€äº¤æ›¿æ³¨æ„ï¼‰+ åŸºäºè§†è§‰é‡å çš„åºåˆ—curriculumï¼ˆå‡ ä½•/è¯­ä¹‰ä¸¤ç§overlapåº¦é‡ï¼‰+ çº¯å…‰åº¦è‡ªç›‘ç£è®­ç»ƒ<br>æ•°æ®é›†ï¼šRealEstate10Kã€DL3DV-10Kã€CO3Dv2ã€MVImgNetã€ARKitScenesã€WildRGB-Dã€ACIDï¼ˆé¢„è®­ç»ƒï¼‰ï¼›ScanNetã€BlendedMVSã€StaticThings3D ç­‰ä¸‹æ¸¸è¯„æµ‹<br>ä¸»è¦è´¡çŒ®ï¼šé¦–ä¸ªçœŸæ­£â€œé›¶3Dæ ‡æ³¨â€çš„è‡ªç›‘ç£3DGSå‰é¦ˆé‡å»ºæ¨¡å‹ï¼›åœ¨ç›¸æœºä¼°è®¡ä¸Šæ˜¾è‘—è¶…è¶Š RayZer å¹¶æ¥è¿‘/éƒ¨åˆ†ä¼˜äºç›‘ç£ VGGTï¼›ä½œä¸ºè§†è§‰é¢„è®­ç»ƒåœ¨å¤šè§†æ·±åº¦/ä½å§¿/å…‰æµä»»åŠ¡ä¸Šè¶…è¿‡ DINOv3ã€CroCo v2ã€VideoMAE V2ã€Perception Encoder ç­‰ï¼›åŒæ—¶ä½œä¸º VGGT åˆå§‹åŒ–å¯è¿›ä¸€æ­¥æå‡å…¶ç›‘ç£æ€§èƒ½ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Zhao-ERayZerSelfsupervised3D-2025,</span><br><span class="line">  title = &#123;E-RayZer: Self-Supervised 3D Reconstruction as Spatial Visual Pre-Training&#125;,</span><br><span class="line">  author = &#123;Zhao, Qitao and Tan, Hao and Wang, Qianqian and Bi, Sai and Zhang, Kai and Sunkavalli, Kalyan and Tulsiani, Shubham and Jiang, Hanwen&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  number = &#123;arXiv:2512.10950&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Denoising-Hamiltonian-Network-for-Physical-Reasoning"><a href="#Denoising-Hamiltonian-Network-for-Physical-Reasoning" class="headerlink" title="Denoising Hamiltonian Network for Physical Reasoning"></a>Denoising Hamiltonian Network for Physical Reasoning</h3><p>æ–¹æ³•åç§°ï¼šSE3-Transformerï¼ˆE3ç­‰å˜å›¾ç¥ç»ç½‘ç»œï¼‰<a href="zotero://select/library/items/BAYJCEAB">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šSE3 TFNï¼ˆçƒè°å¼ é‡åœºç½‘ç»œï¼‰+ å¤šå¤´è‡ªæ³¨æ„åŠ› + ç­‰å˜è‡ªäº¤äº’ï¼ˆçº¿æ€§/é€šé“æ··åˆ/æ³¨æ„åŠ›å¼ï¼‰+ å›¾ç¥ç»ç½‘ç»œä¸­çš„ç›¸å¯¹ä½ç½®åµŒå…¥ + ä¸å¯çº¦çƒè°åŸºè¡¨ç¤º SO3 / O3 ç­‰å˜æ€§<br>æ•°æ®é›†ï¼šQM9ï¼ˆåˆ†å­å±æ€§é¢„æµ‹ï¼‰ã€PCN/ShapeNetï¼ˆç‚¹äº‘è¡¥å…¨ï¼‰ã€ModelNet40ï¼ˆç‚¹äº‘åˆ†ç±»ï¼‰ã€COLLISIONï¼ˆåˆšä½“ç‰©ç†é¢„æµ‹ï¼‰<br>ä¸»è¦è´¡çŒ®ï¼šé¦–åˆ› SE(3) ç­‰å˜ Transformerï¼›ä¿æŒæ—‹è½¬å¹³ç§»ç­‰å˜çš„å¤šå¤´è‡ªæ³¨æ„æœºåˆ¶ï¼›è¶…è¶Š GNN/RNN åœ¨åˆ†å­/ç‚¹äº‘/ç‰©ç†ç³»ç»Ÿä»»åŠ¡ä¸Šï¼›å¼€æºä»£ç ä¸é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Deng-DenoisingHamiltonianNetwork-2025,</span><br><span class="line">  title = &#123;Denoising Hamiltonian Network for Physical Reasoning&#125;,</span><br><span class="line">  author = &#123;Deng, Congyue and Feng, Brandon Y. and Garraffo, Cecilia and Garbarz, Alan and Walters, Robin and Freeman, William T. and Guibas, Leonidas and He, Kaiming&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  number = &#123;arXiv:2503.07596&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="GFPose-Learning-3D-Human-Pose-Prior-With-Gradient-Fields"><a href="#GFPose-Learning-3D-Human-Pose-Prior-With-Gradient-Fields" class="headerlink" title="GFPose: Learning 3D Human Pose Prior With Gradient Fields"></a>GFPose: Learning 3D Human Pose Prior With Gradient Fields</h3><p>æ–¹æ³•åç§°ï¼šGFPoseï¼ˆæ¢¯åº¦åœºäººä½“å§¿æ€å…ˆéªŒï¼‰<a href="zotero://select/library/items/XLWWUFWB">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šæ¢¯åº¦åœºå…ˆéªŒï¼ˆGradient Field Priorï¼šå…³èŠ‚æ¢¯åº¦åœº+æ–¹å‘ä¸€è‡´æ€§æŸå¤±ï¼‰+ æ— é”šç‚¹è‡ªå›å½’MLPï¼ˆé¢„æµ‹å…³èŠ‚åˆ°éª¨æ¶æ¡ä»¶çš„æ¡ä»¶æ¦‚ç‡ï¼‰+ å…¨å±€å§¿æ€åˆ†å¸ƒå»ºæ¨¡ï¼ˆæ— æ¡ä»¶/æ¡ä»¶å§¿æ€å…ˆéªŒï¼‰+ ç«¯åˆ°ç«¯å§¿æ€æ‹Ÿåˆä¼˜<br>æ•°æ®é›†ï¼š3DPW/AMASS/Human3.6Mï¼ˆå§¿æ€ä¼˜åŒ–ï¼‰ã€Human3.6M/MPI-INF-3DPHï¼ˆå•è§†3Då§¿æ€ä¼°è®¡ï¼‰ã€3DPW/SPIN-Human3.6Mï¼ˆäººä½“é‡å»º)<br>ä¸»è¦è´¡çŒ®ï¼šé¦–ä¸ªæ— æ¨¡æ¿/æ— æ‰«æçš„3Däººä½“å§¿æ€å…ˆéªŒï¼›GFPoseå…ˆéªŒä¸‹SMPLify-X PA-MPJPEæå‡7-22mmï¼›åº”ç”¨äºå•è§†/å¤šè§†ä»»åŠ¡SOTAï¼›å¼€æºä»£ç ä¸é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Ci-GFPoseLearning3D-,</span><br><span class="line">  title = &#123;GFPose: Learning 3D Human Pose Prior With Gradient Fields&#125;,</span><br><span class="line">  author = &#123;Ci, Hai and Wu, Mingdong and Zhu, Wentao and Ma, Xiaoxuan and Dong, Hao and Zhong, Fangwei and Wang, Yizhou&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="ElePose-Unsupervised-3D-Human-Pose-Estimation-by-Predicting-Camera-Elevation-and-Learning-Normalizing-Flows-on-2D-Poses"><a href="#ElePose-Unsupervised-3D-Human-Pose-Estimation-by-Predicting-Camera-Elevation-and-Learning-Normalizing-Flows-on-2D-Poses" class="headerlink" title="ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses"></a>ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses</h3><p>æ–¹æ³•åç§°ï¼šGF-Netï¼ˆGradient Flow Networkï¼‰<a href="zotero://select/library/items/KE7P999T">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šæ¢¯åº¦åœºç½‘ç»œï¼ˆGradient Field Networkï¼šå…³èŠ‚æ¢¯åº¦åœºç¼–ç +å¯¹ç§°æ€§çº¦æŸï¼‰+ å¤šè§†å‡ ä½•ä¸€è‡´è‡ªç›‘ç£ï¼ˆå…³é”®ç‚¹é‡æŠ•å½±ä¸€è‡´+å…‰åº¦æŸå¤±ï¼‰+ éšå¼æ·±åº¦å›¾å›å½’+ æ— å§¿æ€å…ˆéªŒçš„3Däººä½“å§¿æ€ä¼°è®¡<br>æ•°æ®é›†ï¼šHuman3.6Mï¼ˆå¤šè§†å®¤å†…ï¼‰ã€MPI-INF-3DHPï¼ˆé‡å¤–ï¼‰ã€Campusï¼ˆå®¤å¤–å¤§åœºæ™¯ï¼‰ï¼›è‡ªç›‘ç£é¢„è®­ç»ƒäºæœªæ ‡æ³¨è§†é¢‘<br>ä¸»è¦è´¡çŒ®ï¼šé¦–ä¸ªé›¶æ ‡æ³¨å¤šè§†3Däººä½“å§¿æ€ä¼°è®¡ï¼›æ— å§¿æ€å…ˆéªŒä¸‹ MPJPE æ¯” VIBE/HoloPose æå‡25-40mmï¼›æ³›åŒ–åˆ°é‡å¤–/å®¤å¤–åœºæ™¯ï¼›å¼€æºä»£ç ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Wandt-ElePoseUnsupervised3D-2021,</span><br><span class="line">  title = &#123;ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses&#125;,</span><br><span class="line">  author = &#123;Wandt, Bastian and Little, James J. and Rhodin, Helge&#125;,</span><br><span class="line">  year = 2021,</span><br><span class="line">  number = &#123;arXiv:2112.07088&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Unsupervised-Adversarial-Learning-of-3D-Human-Pose-from-2D-Joint-Locations"><a href="#Unsupervised-Adversarial-Learning-of-3D-Human-Pose-from-2D-Joint-Locations" class="headerlink" title="Unsupervised Adversarial Learning of 3D Human Pose from 2D Joint Locations"></a>Unsupervised Adversarial Learning of 3D Human Pose from 2D Joint Locations</h3><p>æ–¹æ³•åç§°ï¼šU-Net + Discriminatorï¼ˆæ— ç›‘ç£å¯¹æŠ—3Då§¿æ€ï¼‰<a href="zotero://select/library/items/QLU8LMIW">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šåˆ¤åˆ«å™¨å¯¹æŠ—è®­ç»ƒï¼ˆåˆ¤åˆ«çœŸå®/ç”Ÿæˆ3Då§¿æ€åˆ†å¸ƒï¼‰+ U-Netç¼–ç å™¨ï¼ˆ2Dåˆ°3Då‡ç»´ï¼‰+ å‡ ä½•çº¦æŸæŸå¤±ï¼ˆéª¨éª¼é•¿åº¦+å¯¹ç§°æ€§ï¼‰+ æ— 3Dæ ‡æ³¨çš„è‡ªç›‘ç£å­¦ä¹ <br>æ•°æ®é›†ï¼šHuman3.6Mï¼ˆå•/å¤šè§†åŸºå‡†ï¼‰ã€MPI-INF-3DHPã€HumanEvaï¼›æ— ç›‘ç£é¢„è®­ç»ƒäºMPI-INF-3DHPåæ³›åŒ–<br>ä¸»è¦è´¡çŒ®ï¼šé¦–ä¸ªé›¶3Dæ ‡æ³¨çš„2Dâ†’3Då§¿æ€æŠ¬å‡ï¼›Human3.6M P1 MPJPE 84.1mmï¼ˆä»…2Dè¾“å…¥ï¼‰ï¼›ä¼˜äºç›‘ç£baselineï¼›ä»£ç å¼€æºã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Kudo-UnsupervisedAdversarialLearning-2018,</span><br><span class="line">  title = &#123;Unsupervised Adversarial Learning of 3D Human Pose from 2D Joint Locations&#125;,</span><br><span class="line">  author = &#123;Kudo, Yasunori and Ogaki, Keisuke and Matsui, Yusuke and Odagiri, Yuri&#125;,</span><br><span class="line">  year = 2018,</span><br><span class="line">  number = &#123;arXiv:1803.08244&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="MAS-Multi-view-Ancestral-Sampling-for-3D-motion-generation-using-2D-diffusion"><a href="#MAS-Multi-view-Ancestral-Sampling-for-3D-motion-generation-using-2D-diffusion" class="headerlink" title="MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion"></a>MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion</h3><p>æ–¹æ³•åç§°ï¼šMAS<a href="zotero://select/library/items/437AG2MX">ğŸ“š</a>ï¼›<br>æ ¸å¿ƒæŠ€æœ¯ï¼šå…ˆåœ¨ä»è§†é¢‘æå–çš„2Déª¨æ¶ä¸Šè®­ç»ƒæ— æ¡ä»¶2DåŠ¨ä½œæ‰©æ•£æ¨¡å‹ï¼Œå†åœ¨é‡‡æ ·é˜¶æ®µå¯¹å¤šä¸ªè§†è§’çš„2Dè½¨è¿¹è¿›è¡ŒåŒæ­¥å»å™ªã€æ¯æ­¥ä¸‰è§’åŒ–ä¸ºå•ä¸€3DåŠ¨ä½œå¹¶é‡æŠ•å½±ä»¥ä¿æŒå¤šè§†ä¸€è‡´ï¼ŒåŒæ—¶ä½¿ç”¨3Då™ªå£°æŠ•å½±åˆ°å„è§†è§’ä¿è¯å™ªå£°ä¸é¢„æµ‹çš„ä¸€è‡´æ€§ï¼›<br>æ•°æ®é›†ï¼šæ¥æºäºç½‘ç»œçš„NBAèŒä¸šç¯®çƒã€é©¬æœ¯éšœç¢èµ›ã€å¸¦çƒçš„è‰ºæœ¯ä½“æ“è§†é¢‘ï¼Œä»¥åŠæŠ•å½±è‡ª Human3.6M çš„åˆæˆ2DåŠ¨ä½œï¼›<br>ä¸»è¦è´¡çŒ®ï¼šåªä¾èµ–2Dè§†é¢‘æ•°æ®å³å¯ç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„3DåŠ¨ä½œï¼Œåœ¨æ— 3Dæ ‡æ³¨çš„æƒ…å†µä¸‹è¶…è¶Š ElePoseã€MotionBERT ç­‰ææ‹‰æ–¹æ³•ï¼Œå¹¶ä¼˜äºåŸºäº DreamFusion çš„SDSæ”¹å†™æ–¹æ¡ˆã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Kapon-MASMultiviewAncestral-2024,</span><br><span class="line">  title = &#123;MAS: Multi-View Ancestral Sampling for 3D Motion Generation Using 2D Diffusion&#125;,</span><br><span class="line">  author = &#123;Kapon, Roy and Tevet, Guy and &#123;Cohen-Or&#125;, Daniel and Bermano, Amit H.&#125;,</span><br><span class="line">  year = 2024,</span><br><span class="line">  number = &#123;arXiv:2310.14729&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="MoDi-Unconditional-Motion-Synthesis-from-Diverse-Data"><a href="#MoDi-Unconditional-Motion-Synthesis-from-Diverse-Data" class="headerlink" title="MoDi: Unconditional Motion Synthesis from Diverse Data"></a>MoDi: Unconditional Motion Synthesis from Diverse Data</h3><p>æ–¹æ³•åç§°ï¼šMoDi<a href="zotero://select/library/items/MRIVRBVD">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šStyleGAN å¼ç”Ÿæˆå™¨ + æ˜ å°„ç½‘ç»œå­¦ä¹ ç»“æ„åŒ–æ½œç©ºé—´ W/W+ + éª¨æ¶æ„ŸçŸ¥ 3D å·ç§¯ä¸â€œconvolutional scalerâ€å±‚ï¼ˆæŒ‰éª¨æ¶æ‹“æ‰‘åšæ—¶ç©ºä¸Šä¸‹é‡‡æ ·ï¼‰+ å…³èŠ‚çº§ä¸“ç”¨å·ç§¯æ ¸ + è¶³æ¥è§¦çº¦æŸä¸ä¸€è‡´æ€§æŸå¤±ä½œä¸ºè¿åŠ¨å…ˆéªŒæ­£åˆ™<br>æ•°æ®é›†ï¼šMixamoï¼ˆæå¤šæ ·ã€æ— æ ‡æ³¨åŠ¨ä½œï¼Œç”¨äºè®­ç»ƒå’Œä¸»è¯„ä¼°ï¼‰ã€HumanAct12ï¼ˆ12ç±»åŠ¨ä½œï¼Œç”¨äºä¸ ACTORã€MDM å®šé‡å¯¹æ¯”ï¼‰<br>ä¸»è¦è´¡çŒ®ï¼šåœ¨â€œå¤šæ ·ã€æ— ç»“æ„ã€æ— æ ‡ç­¾â€åŠ¨ä½œé›†ä¸Šé¦–æ¬¡å®ç°é«˜è´¨é‡æ— æ¡ä»¶åŠ¨ä½œç”Ÿæˆï¼›å­¦åˆ°å¯èšç±»çš„è¯­ä¹‰æ½œç©ºé—´ï¼Œå¯åšè¯­ä¹‰ç¼–è¾‘ã€æ’å€¼ä¸ç¾¤ä½“ä»¿çœŸï¼›æå‡ºç¼–ç å™¨å°†çœŸå®åŠ¨ä½œåµŒå…¥è¯¥æ½œç©ºé—´ï¼Œç”¨ç»Ÿä¸€ç”Ÿæˆå…ˆéªŒè§£å†³è¡¥å…¨ã€èåˆã€é™å™ªã€ç©ºé—´ç¼–è¾‘ç­‰ä¸€ç³»åˆ—æ¬ å®šé—®é¢˜ï¼Œå¹¶åœ¨ FID/KID/Precision/Recall ç­‰æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äº ACTOR ä¸ MDMã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Raab-MoDiUnconditionalMotion-2022,</span><br><span class="line">  title = &#123;MoDi: Unconditional Motion Synthesis from Diverse Data&#125;,</span><br><span class="line">  author = &#123;Raab, Sigal and Leibovitch, Inbal and Li, Peizhuo and Aberman, Kfir and &#123;Sorkine-Hornung&#125;, Olga and &#123;Cohen-Or&#125;, Daniel&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  number = &#123;arXiv:2206.08010&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Temporal-Constrained-Feasible-Subspace-Learning-for-Human-Pose-Forecasting"><a href="#Temporal-Constrained-Feasible-Subspace-Learning-for-Human-Pose-Forecasting" class="headerlink" title="Temporal Constrained Feasible Subspace Learning for Human Pose Forecasting"></a>Temporal Constrained Feasible Subspace Learning for Human Pose Forecasting</h3><p>æ–¹æ³•åç§°ï¼šTCSL<a href="zotero://select/library/items/8ZYN7SCF">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šåœ¨é€Ÿåº¦å˜åŒ–å­ç©ºé—´ä¸­æ˜¾å¼å»ºæ¨¡æ—¶é—´çº¦æŸï¼ˆå…³èŠ‚é€Ÿåº¦äºŒé˜¶å·®åˆ†ä¸Šç•Œï¼‰+ çº¿æ€§å˜æ¢ H æŠŠå§¿æ€åºåˆ—æ˜ å°„åˆ°â€œé€Ÿåº¦å˜åŒ–â€å­ç©ºé—´ + ReLU/ELU æŠ•å½±ç®—å­ä¿è¯æ‰€æœ‰é¢„æµ‹éƒ½è½åœ¨å¯è¡ŒåŸŸå†… + é€†å˜æ¢ Hâ»Â¹ å›åˆ°å…³èŠ‚åæ ‡ç©ºé—´ + ä»¥ STS-GCN ä½œä¸ºç¼–ç éª¨å¹²ç«¯åˆ°ç«¯è®­ç»ƒ<br>æ•°æ®é›†ï¼šHuman3.6Mï¼ˆä¸»åŸºå‡†ï¼ŒçŸ­/ä¸­/é•¿æ—¶é¢„æµ‹ï¼‰ã€AMASSï¼ˆå¤§è§„æ¨¡ MoCapï¼Œè·¨å­é›†æ³›åŒ–ï¼‰ã€3DPWï¼ˆAMASS è®­ç»ƒã€3DPW æµ‹è¯•åš in-the-wild æ³›åŒ–ï¼‰<br>ä¸»è¦è´¡çŒ®ï¼šé¦–æ¬¡åœ¨äººä½“å§¿æ€é¢„æµ‹ä¸­æ˜¾å¼å­¦ä¹ â€œæ—¶é—´å¯è¡Œå­ç©ºé—´â€ï¼Œä¿è¯æ¨ç†é˜¶æ®µä¸¥æ ¼æ»¡è¶³é€Ÿåº¦çº¦æŸä¸”æ— éœ€è¿­ä»£æŠ•å½±ï¼›åœ¨ Human3.6Mã€AMASSã€3DPW ä¸Šåœ¨ MPJPE ä¸Šä¼˜äº STS-GCN ç­‰ SOTA åŒæ—¶æŠŠ infeasible rate é™åˆ° 0ï¼Œå¹¶ç»™å‡ºç³»ç»Ÿæ¶ˆèæ¯”è¾ƒä¸åŒæŠ•å½±å‡½æ•°å’Œæ­£åˆ™é¡¹çš„å½±å“ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Wang-TemporalConstrainedFeasible-2023,</span><br><span class="line">  title = &#123;Temporal Constrained Feasible Subspace Learning for Human Pose Forecasting&#125;,</span><br><span class="line">  booktitle = &#123;Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence&#125;,</span><br><span class="line">  author = &#123;Wang, Gaoang and Song, Mingli&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  pages = &#123;1451--1459&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="PUMPS-Skeleton-Agnostic-Point-based-Universal-Motion-Pre-Training-for-Synthesis-in-Human-Motion-Tasks"><a href="#PUMPS-Skeleton-Agnostic-Point-based-Universal-Motion-Pre-Training-for-Synthesis-in-Human-Motion-Tasks" class="headerlink" title="PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks"></a>PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks</h3><p>æ–¹æ³•åç§°ï¼šPUMPS<a href="zotero://select/library/items/I2IHXCJW">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šå°†éª¨æ¶åŠ¨ä½œç»Ÿä¸€è¡¨ç¤ºä¸º Temporal Point Cloudsï¼ˆæ—¶åºç‚¹äº‘ï¼ŒTPCï¼‰ï¼Œå¯¹æ¯å¸§ç‚¹äº‘ç”¨ PointTransformer ç¼–ç æˆæ½œå‘é‡ï¼Œå†ç”¨åŸºäºé«˜æ–¯å™ªå£°æ ‡è¯†ç¬¦çš„è§£ç å™¨é‡å»ºæ—¶åºç‚¹äº‘ä»¥ä¿æŒç‚¹è½¨è¿¹ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡åŒˆç‰™åˆ©åŒ¹é…çš„çº¿æ€§åˆ†é…ä¼˜åŒ–é‡å»ºæŸå¤±ï¼›åœ¨è¯¥æ½œç©ºé—´ä¸Šç”¨ Transformer åšé®ç›–å¼è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå®Œæˆå…³é”®å¸§æ’å€¼ã€è¿‡æ¸¡ç”Ÿæˆå’ŒçŸ­æœŸé¢„æµ‹ç­‰ä»»åŠ¡ã€‚<br>æ•°æ®ä¸ä»»åŠ¡ï¼šåœ¨ AMASSã€LaFAN1ã€Human3.6Mã€100STYLE ç­‰å¤šæ•°æ®é›†æ··åˆä¸Šåš TPC é¢„è®­ç»ƒï¼Œå®ç°éª¨æ¶æ— å…³çš„ masked motion completionï¼Œåœ¨å…³é”®å¸§æ’å€¼å’Œè¿‡æ¸¡ä»»åŠ¡ä¸Šä»¥ L2P/L2Q/NPSS æŒ‡æ ‡è¾¾åˆ°æˆ–è¶…è¿‡ä¸“é—¨æ–¹æ³•ï¼›å†å¾®è°ƒåˆ° 2Dâ†’3D åŠ¨ä½œä¼°è®¡å’ŒåŠ¨ä½œå»å™ªä»»åŠ¡ï¼Œåœ¨ MPJPE/MPJVE å’Œå»å™ªè¯¯å·®ä¸Šä¼˜äºå¤šæ•°ç°æœ‰æ–¹æ³•ï¼Œè¯æ˜è¯¥é€šç”¨æ½œè¡¨å¾å¯¹ä¸‹æ¸¸éª¨æ¶ç‰¹å®šä»»åŠ¡åŒæ ·æœ‰æ•ˆã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Mo-PUMPSSkeletonAgnosticPointbased-2025,</span><br><span class="line">  title = &#123;PUMPS: Skeleton-Agnostic Point-Based Universal Motion Pre-Training for Synthesis in Human Motion Tasks&#125;,</span><br><span class="line">  author = &#123;Mo, Clinton Ansun and Hu, Kun and Long, Chengjiang and Yuan, Dong and Siu, Wan-Chi and Wang, Zhiyong&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  number = &#123;arXiv:2507.20170&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="A-Dual-Masked-Auto-Encoder-for-Robust-Motion-Capture-with-Spatial-Temporal-Skeletal-Token-Completion"><a href="#A-Dual-Masked-Auto-Encoder-for-Robust-Motion-Capture-with-Spatial-Temporal-Skeletal-Token-Completion" class="headerlink" title="A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion"></a>A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion</h3><p>æ–¹æ³•åç§°ï¼šD-MAE<a href="zotero://select/library/items/6Q89Z7YZ">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šå¤šè§†è§’å¤šäººäººä½“æ•æ‰ä¸­ï¼Œå…ˆç”¨è‡ªé€‚åº” triangulation æ¨¡å—åšèº«ä»½æ„ŸçŸ¥çš„ 3D é‡å»ºå¹¶æ˜¾å¼æ ‡è®°ç¼ºå¤±å…³èŠ‚ï¼Œå†ç”¨åŒæ©ç  Transformer è‡ªç¼–ç å™¨ï¼ŒæŠŠâ€œå•å…³èŠ‚å•æ—¶åˆ»â€ä½œä¸º tokenï¼Œç»“åˆå…³èŠ‚æ‹“æ‰‘ç¼–ç ï¼ˆskeletal-structural encodingï¼‰å’Œæ—¶é—´ä½ç½®ç¼–ç ï¼ˆtemporal encodingï¼‰ï¼Œå¯¹ç¼ºå¤± 3D å…³èŠ‚è½¨è¿¹è¿›è¡Œè‡ªå›å½’è¡¥å…¨ï¼›æ”¯æŒä»»æ„éª¨æ¶å®šä¹‰ï¼Œå¹¶é€šè¿‡â€œPose Fusionâ€è”åˆä¸¤ç§éª¨æ¶æ ¼å¼åšçŸ¥è¯†è¿ç§»ã€‚<br>æ•°æ®ä¸æ•ˆæœï¼šåœ¨ Shelfã€CMU Panoptic å’Œæ–°é‡‡é›†çš„ BU-Mocapï¼ˆå¤šäººäººæœºäº¤äº’ã€ä¸¥é‡é®æŒ¡ï¼‰ä¸Šï¼ŒD-MAE åœ¨ PCP/PCK/MPJPE ä¸Šä¼˜äºç°æœ‰å¤šè§†è§’ä¸‰è§’åŒ–æ–¹æ³•å’Œç«¯åˆ°ç«¯ä½“ç´ æ–¹æ³•ï¼Œå°¤å…¶åœ¨å…³èŠ‚ç¼ºå¤±ä¸¥é‡ã€ç›¸æœºè§†è§’ä¸è¶³æ—¶ä¾ç„¶èƒ½æ¢å¤å®Œæ•´ã€æ—¶åºä¸€è‡´çš„ 3D éª¨æ¶ï¼ŒéªŒè¯äº†å…¶åœ¨é²æ£’ mo-cap åœºæ™¯ä¸­çš„è¡¥å…¨èƒ½åŠ›ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc&#123;Jiang-DualMaskedAutoEncoderRobust-2022,</span><br><span class="line">  title = &#123;A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion&#125;,</span><br><span class="line">  author = &#123;Jiang, Junkun and Chen, Jie and Guo, Yike&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  number = &#123;arXiv:2207.07381&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Motion-Keyframe-Interpolation-for-Any-Human-Skeleton-via-Temporally-Consistent-Point-Cloud-Sampling-and-Reconstruction"><a href="#Motion-Keyframe-Interpolation-for-Any-Human-Skeleton-via-Temporally-Consistent-Point-Cloud-Sampling-and-Reconstruction" class="headerlink" title="Motion Keyframe Interpolation for Any Human Skeleton via Temporally Consistent Point Cloud Sampling and Reconstruction"></a>Motion Keyframe Interpolation for Any Human Skeleton via Temporally Consistent Point Cloud Sampling and Reconstruction</h3><p>æ–¹æ³•åç§°ï¼šPC-MRL<a href="zotero://select/library/items/9KA4BI2K">ğŸ“š</a><br>æ ¸å¿ƒæ€æƒ³ï¼šæŠŠä»»æ„éª¨æ¶çš„åŠ¨ä½œåºåˆ—å…ˆç”¨â€œæ—¶åºä¸€è‡´çš„ç‚¹äº‘é‡‡æ ·â€è¿›è¡Œéª¨æ¶å»ç»“æ„åŒ–ï¼ˆæ¯æ ¹éª¨éª¼æ²¿çº¿æ®µåŠå‘¨å›´ä½“ç§¯é‡‡æ ·ç‚¹ï¼Œå¹¶ç»´æŠ¤æ—¶é—´ä¸€è‡´çš„ç‚¹è½¨è¿¹ä¸èº«ä½“éƒ¨ä½åˆ†ç»„ï¼‰ï¼Œå†ç”¨ Point Transformer + æ—¶åº Transformer å­¦ä¸€ä¸ªä»æºéª¨æ¶ç‚¹äº‘åˆ°ç›®æ ‡éª¨æ¶åŠ¨ä½œçš„æ— ç›‘ç£æ˜ å°„ï¼Œé€šè¿‡æ—¶åº KNN æŸå¤±åœ¨ç‚¹äº‘ç©ºé—´çº¦æŸå‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶è¾…ä»¥æœ«ç«¯æ•ˆåº”å™¨æŸå¤±ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@incollection&#123;Mo-MotionKeyframeInterpolation-2025,</span><br><span class="line">  title = &#123;Motion Keyframe Interpolation for Any Human Skeleton via Temporally Consistent Point Cloud Sampling and Reconstruction&#125;,</span><br><span class="line">  author = &#123;Mo, Clinton and Hu, Kun and Long, Chengjiang and Yuan, Dong and Wang, Zhiyong&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  volume = &#123;15140&#125;,</span><br><span class="line">  pages = &#123;159--175&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="S-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Skeletal-Action-Recognition"><a href="#S-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Skeletal-Action-Recognition" class="headerlink" title="S-JEPA: A Joint Embedding Predictive Architecture for Skeletal Action Recognition"></a>S-JEPA: A Joint Embedding Predictive Architecture for Skeletal Action Recognition</h3><p>æ–¹æ³•åç§°ï¼šS-JEPA<a href="zotero://select/library/items/3NC6QYE7">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šä¸é¢„æµ‹è¢« Mask å…³èŠ‚çš„åŸå§‹ 3D åæ ‡ï¼Œè€Œæ˜¯é¢„æµ‹å®ƒä»¬åœ¨ç¼–ç å™¨ Latent ç©ºé—´ä¸­çš„é«˜å±‚ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶å¼•å…¥ Centering æ“ä½œç¨³å®šè®­ç»ƒåˆ†å¸ƒã€‚<br>æ•°æ®é›†ï¼šNTU60ï¼ŒNTU120ï¼ŒPKU-MMDã€‚<br>æ•ˆæœï¼šç›¸æ¯”äºé‡å»ºåŸå§‹åæ ‡çš„æ–¹æ³•ï¼ˆå¦‚ MAEï¼‰ï¼ŒS-JEPA èƒ½å­¦åˆ°æ›´é²æ£’çš„é«˜å±‚è¯­ä¹‰ç‰¹å¾ï¼Œåœ¨ä¸Šè¿°æ•°æ®é›†çš„åŠ¨ä½œè¯†åˆ«ä»»åŠ¡ä¸Šå‡å–å¾— SOTA æ€§èƒ½ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@incollection&#123;Abdelfattah-SJEPAJointEmbedding-2025,</span><br><span class="line">  title = &#123;S-JEPA: A Joint Embedding Predictive Architecture for Skeletal Action Recognition&#125;,</span><br><span class="line">  booktitle = &#123;Computer Vision -- ECCV 2024&#125;,</span><br><span class="line">  author = &#123;Abdelfattah, Mohamed and Alahi, Alexandre&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  volume = &#123;15090&#125;,</span><br><span class="line">  pages = &#123;367--384&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="MacDiff-Unified-Skeleton-Modeling-with-Masked-Conditional-Diffusion"><a href="#MacDiff-Unified-Skeleton-Modeling-with-Masked-Conditional-Diffusion" class="headerlink" title="MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion"></a>MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion</h3><p>æ–¹æ³•åç§°ï¼šMacDiff<a href="zotero://select/library/items/L8T89M87">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šéšæœºé«˜æ¯”ä¾‹ Mask ç¼–ç å™¨ + æ¡ä»¶æ‰©æ•£è§£ç å™¨ï¼Œç»Ÿä¸€è¡¨å¾å­¦ä¹ ã€ç”Ÿæˆä¸æ•°æ®å¢å¼ºã€‚<br>æ•°æ®é›†ï¼šNTU RGB+D 60ï¼ŒNTU RGB+D 120ï¼ŒPKU-MMDã€‚<br>æ•ˆæœï¼šåœ¨è‡ªç›‘ç£ã€è¿ç§»å­¦ä¹ å’Œå°æ ·æœ¬åŠ¨ä½œè¯†åˆ«ä¸Šè¾¾æˆæˆ–æ¥è¿‘ SOTAï¼Œå¹¶æ”¯æŒåŠ¨ä½œé‡å»ºä¸æ— æ¡ä»¶ç”Ÿæˆã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@incollection&#123;Wu-MacDiffUnifiedSkeleton-2025,</span><br><span class="line">  title = &#123;MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion&#125;,</span><br><span class="line">  booktitle = &#123;Computer Vision -- ECCV 2024&#125;,</span><br><span class="line">  author = &#123;Wu, Lehong and Lin, Lilang and Zhang, Jiahang and Ma, Yiyang and Liu, Jiaying&#125;,</span><br><span class="line">  year = 2025,</span><br><span class="line">  volume = &#123;15084&#125;,</span><br><span class="line">  pages = &#123;110--128&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="DSPose-Dual-Space-Driven-Keypoint-Topology-Modeling-for-Human-Pose-Estimation"><a href="#DSPose-Dual-Space-Driven-Keypoint-Topology-Modeling-for-Human-Pose-Estimation" class="headerlink" title="DSPose: Dual-Space-Driven Keypoint Topology Modeling for Human Pose Estimation"></a>DSPose: Dual-Space-Driven Keypoint Topology Modeling for Human Pose Estimation</h3><p>æ–¹æ³•åç§°ï¼šDSPose<a href="zotero://select/library/items/XX8YM757">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šä½¿ç”¨ Transformer æå–å…³é”®ç‚¹ç‰¹å¾ï¼Œå¹¶å¼•å…¥ç‰©ç†ç©ºé—´ï¼ˆäººä½“ç»“æ„å…ˆéªŒï¼‰ä¸ç‰¹å¾ç©ºé—´ï¼ˆå›¾åƒåƒç´ å…³ç³»ï¼‰çš„åŒé‡æ‹“æ‰‘ç›¸å…³æ€§ï¼Œåˆ©ç”¨å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰èåˆä¸¤ç±»ä¿¡æ¯ä»¥ç¼“è§£é®æŒ¡å’Œå¹²æ‰°é—®é¢˜ã€‚<br>æ•°æ®é›†ï¼šçœŸå®åœºæ™¯äººä½“å§¿æ€ä¼°è®¡æ•°æ®é›†ï¼ˆå¦‚ COCO ç­‰å¸¸ç”¨æ•°æ®é›†ï¼Œæ–‡ä¸­ç»Ÿç§°ä¸º real datasetsï¼‰ã€‚<br>æ•ˆæœï¼šé€šè¿‡å¼•å…¥ç‰©ç†ç©ºé—´çº¦æŸï¼Œæœ‰æ•ˆç¼“è§£äº†ç‰©ä½“é®æŒ¡ã€é‡å½±å’Œé‚»è¿‘å§¿æ€å¹²æ‰°é—®é¢˜ï¼Œæå‡äº†äººä½“å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Zhao-DSPoseDualSpaceDrivenKeypoint-2023,</span><br><span class="line">  title = &#123;DSPose: Dual-Space-Driven Keypoint Topology Modeling for Human Pose Estimation&#125;,</span><br><span class="line">  author = &#123;Zhao, Anran and Li, Jingli and Zeng, Hongtao and Cheng, Hongren and Dong, Liangshan&#125;,</span><br><span class="line">  year = 2023,</span><br><span class="line">  journal = &#123;Sensors&#125;,</span><br><span class="line">  volume = &#123;23&#125;,</span><br><span class="line">  number = &#123;17&#125;,</span><br><span class="line">  pages = &#123;7626&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Adaptive-2D-skeleton-deformation-based-on-view-agnostic-network-for-action-recognition"><a href="#Adaptive-2D-skeleton-deformation-based-on-view-agnostic-network-for-action-recognition" class="headerlink" title="Adaptive 2D skeleton deformation based on view agnostic network for action recognition"></a>Adaptive 2D skeleton deformation based on view agnostic network for action recognition</h3><p>æ–¹æ³•åç§°ï¼šVAN<a href="zotero://select/library/items/CE6ST64F">ğŸ“š</a><br>æ ¸å¿ƒæŠ€æœ¯ï¼šåˆ©ç”¨è‡ªé€‚åº”çš„éª¨æ¶å˜å½¢æŠ€æœ¯æ¥æ¶ˆé™¤è§†è§’å·®å¼‚ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå­æ¨¡å—ï¼šèº«ä½“çº§ä»¿å°„å˜æ¢ï¼ˆbody-level affine transformationï¼‰è°ƒæ•´æ•´ä½“æœå‘ã€‚å…³èŠ‚çº§åç§»è¡¥å¿ç²¾ç»†å¾®è°ƒå…³èŠ‚ä½ç½®ã€‚<br>æ•°æ®é›†ï¼šNTU RGB+D 120ï¼ŒKinetics-Skeletonã€‚<br>æ•ˆæœï¼šæ˜¾è‘—æå‡äº† 2D éª¨æ¶åŠ¨ä½œè¯†åˆ«åœ¨è§†è§’å˜åŒ–ä¸‹çš„é²æ£’æ€§ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Dong-Adaptive2DSkeleton-2022,</span><br><span class="line">  title = &#123;Adaptive 2D Skeleton Deformation Based on View Agnostic Network for Action Recognition&#125;,</span><br><span class="line">  author = &#123;Dong, Qianyun and Zhong, Xin&#125;,</span><br><span class="line">  year = 2022,</span><br><span class="line">  journal = &#123;Computers and Electrical Engineering&#125;,</span><br><span class="line">  volume = &#123;102&#125;,</span><br><span class="line">  pages = &#123;108112&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>æœ¬æ–‡ä½œè€…ï¼š </strong>yao
  </li>
  <li class="post-copyright-link">
      <strong>æœ¬æ–‡é“¾æ¥ï¼š</strong>
      <a href="https://zzhenyao.github.io/2025/11/24/11-06-55/" title="æ‰©æ•£ç”Ÿæˆä¸é‡å»ºæ–¹æ³•å‘å±•æ—¶é—´çº¿">https://zzhenyao.github.io/2025/11/24/11-06-55/</a>
  </li>
  <li class="post-copyright-license">
    <strong>ç‰ˆæƒå£°æ˜ï¼š </strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ï¼
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># æ·±åº¦å­¦ä¹ </a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/11/18/13-26-24/" rel="prev" title="AQAï¼ˆåŠ¨ä½œè´¨é‡è¯„ä¼°ï¼‰æ–¹æ³•å‘å±•æ—¶é—´çº¿">
                  <i class="fa fa-chevron-left"></i> AQAï¼ˆåŠ¨ä½œè´¨é‡è¯„ä¼°ï¼‰æ–¹æ³•å‘å±•æ—¶é—´çº¿
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yao</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="æ€»è®¿å®¢é‡">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="æ€»è®¿é—®é‡">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> å¼ºåŠ›é©±åŠ¨
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="è¿”å›é¡¶éƒ¨">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.4.0/mermaid.min.js","integrity":"sha256-3JloMMI/ZQx6ryuhhZTsQJQmGAkXeni6PkshX7UUO2s="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"PKunicor","repo":"PKunicor.github.io","client_id":"2efe0e153686e5d9e67d","client_secret":"c84e031b6ac86ce366a6b61640a4b1a8e01d05e0","admin_user":"PKunicor","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"0acbbe66d699b9d9d010a7ee356a1c3d"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
